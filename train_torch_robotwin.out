Sat Jan 24 23:37:03 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   21C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   31C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   24C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   24C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   22C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   25C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
23:37:43.944 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (2382957:train_pytorch.py:340)
wandb: Currently logged in as: 1364904434 (1364904434-hong-kong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /project/peilab/wzj/RoboTwin/policy/openpi_test/wandb/run-20260124_233744-i8nvjklg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robotwin_aloha_lerobot
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi
wandb: üöÄ View run at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi/runs/i8nvjklg
23:37:46.261 [I] Using batch size per GPU: 128 (total batch size across 1 GPUs: 128)              (2382957:train_pytorch.py:354)
23:37:46.341 [I] Norm stats not found in /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo, skipping. (2382957:config.py:199)
23:37:46.341 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats=None, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x15532baa81d0>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (2382957:data_loader.py:243)
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 632, in <module>
    main()
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 628, in main
    train_loop(config)
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 359, in train_loop
    loader, data_config = build_datasets(config)
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 127, in build_datasets
    data_loader = _data.create_data_loader(config, framework="pytorch", shuffle=True)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/src/openpi/training/data_loader.py", line 256, in create_data_loader
    return create_torch_data_loader(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/src/openpi/training/data_loader.py", line 303, in create_torch_data_loader
    dataset = transform_dataset(dataset, data_config, skip_norm_stats=skip_norm_stats)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/src/openpi/training/data_loader.py", line 177, in transform_dataset
    raise ValueError(
ValueError: Normalization stats not found. Make sure to run `scripts/compute_norm_stats.py --config-name=<your-config>`.
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mrobotwin_aloha_lerobot[0m at: [34mhttps://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi/runs/i8nvjklg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260124_233744-i8nvjklg/logs[0m
Wed Jan 28 10:11:41 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   19C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   20C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   21C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   21C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   21C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   20C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   21C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   23C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
10:14:03.454 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (76335:train_pytorch.py:340)
wandb: Currently logged in as: 1364904434 (1364904434-hong-kong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /project/peilab/wzj/RoboTwin/policy/openpi_test/wandb/run-20260128_101407-46fckrwy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robotwin_aloha_lerobot
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi
wandb: üöÄ View run at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi/runs/46fckrwy
10:14:10.802 [I] Using batch size per GPU: 128 (total batch size across 1 GPUs: 128)              (76335:train_pytorch.py:354)
10:14:10.987 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (76335:config.py:196)
10:14:10.988 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x15532bd5a290>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (76335:data_loader.py:243)
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/lerobot/common/datasets/lerobot_dataset.py", line 498, in __init__
    assert all((self.root / fpath).is_file() for fpath in self.get_episodes_file_paths())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/datasets/robotwin_aloha_lerobot_repo/refs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 632, in <module>
    main()
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 628, in main
    train_loop(config)
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 359, in train_loop
    loader, data_config = build_datasets(config)
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 127, in build_datasets
    data_loader = _data.create_data_loader(config, framework="pytorch", shuffle=True)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/src/openpi/training/data_loader.py", line 256, in create_data_loader
    return create_torch_data_loader(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/src/openpi/training/data_loader.py", line 302, in create_torch_data_loader
    dataset = create_torch_dataset(data_config, action_horizon, model_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/src/openpi/training/data_loader.py", line 141, in create_torch_dataset
    dataset = lerobot_dataset.LeRobotDataset(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/lerobot/common/datasets/lerobot_dataset.py", line 501, in __init__
    self.revision = get_safe_version(self.repo_id, self.revision)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/lerobot/common/datasets/utils.py", line 327, in get_safe_version
    hub_versions = get_repo_versions(repo_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/lerobot/common/datasets/utils.py", line 309, in get_repo_versions
    repo_refs = api.list_repo_refs(repo_id, repo_type="dataset")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 3200, in list_repo_refs
    hf_raise_for_status(response)
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-697970f6-724f6c1f677ba49a4b0f3119;66c0b309-6b69-43a5-9836-231b6ab99558)

Repository Not Found for url: https://huggingface.co/api/datasets/robotwin_aloha_lerobot_repo/refs.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mrobotwin_aloha_lerobot[0m at: [34mhttps://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi/runs/46fckrwy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260128_101407-46fckrwy/logs[0m
Fri Jan 30 19:48:55 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H800                    On  | 00000000:1B:00.0 Off |                    0 |
| N/A   18C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H800                    On  | 00000000:43:00.0 Off |                    0 |
| N/A   19C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H800                    On  | 00000000:52:00.0 Off |                    0 |
| N/A   21C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H800                    On  | 00000000:61:00.0 Off |                    0 |
| N/A   19C    P0              67W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H800                    On  | 00000000:9D:00.0 Off |                    0 |
| N/A   19C    P0              66W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H800                    On  | 00000000:C3:00.0 Off |                    0 |
| N/A   19C    P0              65W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H800                    On  | 00000000:D1:00.0 Off |                    0 |
| N/A   19C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H800                    On  | 00000000:DF:00.0 Off |                    0 |
| N/A   21C    P0              67W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
19:49:47.208 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (486094:train_pytorch.py:340)
wandb: Currently logged in as: 1364904434 (1364904434-hong-kong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /project/peilab/wzj/RoboTwin/policy/openpi_test/wandb/run-20260130_194948-qzda7uiu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robotwin_aloha_lerobot
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi
wandb: üöÄ View run at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi/runs/qzda7uiu
19:49:50.371 [I] Using batch size per GPU: 128 (total batch size across 1 GPUs: 128)              (486094:train_pytorch.py:354)
19:49:50.468 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (486094:config.py:196)
19:49:50.469 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x15532e6ff450>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (486094:data_loader.py:243)
Downloading data:   0%|          | 0/2500 [00:00<?, ?files/s]Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [00:00<00:00, 1011260.49files/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 173 examples [00:00, 631.42 examples/s]Generating train split: 740 examples [00:00, 2106.08 examples/s]Generating train split: 1320 examples [00:00, 3092.30 examples/s]Generating train split: 2026 examples [00:00, 3894.02 examples/s]Generating train split: 2744 examples [00:00, 4501.07 examples/s]Generating train split: 3315 examples [00:01, 2066.22 examples/s]Generating train split: 3886 examples [00:01, 2568.09 examples/s]Generating train split: 4593 examples [00:01, 3192.43 examples/s]Generating train split: 5313 examples [00:01, 3766.48 examples/s]Generating train split: 6044 examples [00:01, 4235.66 examples/s]Generating train split: 6758 examples [00:02, 2162.85 examples/s]Generating train split: 7413 examples [00:02, 2636.62 examples/s]Generating train split: 8206 examples [00:02, 3303.76 examples/s]Generating train split: 8877 examples [00:02, 3813.58 examples/s]Generating train split: 9563 examples [00:02, 4360.43 examples/s]Generating train split: 10246 examples [00:03, 4819.48 examples/s]Generating train split: 10942 examples [00:03, 5163.71 examples/s]Generating train split: 11615 examples [00:03, 5494.06 examples/s]Generating train split: 12297 examples [00:03, 5668.39 examples/s]Generating train split: 13326 examples [00:04, 1430.93 examples/s]Generating train split: 13795 examples [00:05, 1660.78 examples/s]Generating train split: 14258 examples [00:05, 1922.76 examples/s]Generating train split: 15166 examples [00:05, 2514.77 examples/s]Generating train split: 16069 examples [00:05, 3093.12 examples/s]Generating train split: 16973 examples [00:05, 3522.26 examples/s]Generating train split: 17883 examples [00:05, 3999.75 examples/s]Generating train split: 18814 examples [00:06, 4463.02 examples/s]Generating train split: 19758 examples [00:07, 1429.13 examples/s]Generating train split: 20674 examples [00:07, 1875.63 examples/s]Generating train split: 21620 examples [00:07, 2391.55 examples/s]Generating train split: 22536 examples [00:08, 2944.91 examples/s]Generating train split: 23457 examples [00:08, 3527.22 examples/s]Generating train split: 24368 examples [00:08, 4053.71 examples/s]Generating train split: 25290 examples [00:08, 4563.99 examples/s]Generating train split: 26204 examples [00:10, 1518.58 examples/s]Generating train split: 27128 examples [00:10, 1960.39 examples/s]Generating train split: 28060 examples [00:10, 2469.50 examples/s]Generating train split: 28966 examples [00:10, 3005.57 examples/s]Generating train split: 29892 examples [00:10, 3566.53 examples/s]Generating train split: 30802 examples [00:10, 4138.34 examples/s]Generating train split: 31758 examples [00:10, 4621.00 examples/s]Generating train split: 32690 examples [00:12, 1538.13 examples/s]Generating train split: 33599 examples [00:12, 1966.21 examples/s]Generating train split: 34513 examples [00:12, 2450.40 examples/s]Generating train split: 35433 examples [00:12, 2970.09 examples/s]Generating train split: 36375 examples [00:13, 3494.67 examples/s]Generating train split: 37298 examples [00:13, 4012.51 examples/s]Generating train split: 38246 examples [00:13, 4442.44 examples/s]Generating train split: 39176 examples [00:14, 1553.07 examples/s]Generating train split: 40101 examples [00:15, 1997.08 examples/s]Generating train split: 41021 examples [00:15, 2518.02 examples/s]Generating train split: 41966 examples [00:15, 3084.41 examples/s]Generating train split: 42876 examples [00:15, 3696.17 examples/s]Generating train split: 43789 examples [00:15, 4304.03 examples/s]Generating train split: 44731 examples [00:15, 4827.49 examples/s]Generating train split: 45672 examples [00:17, 1594.93 examples/s]Generating train split: 46600 examples [00:17, 2034.98 examples/s]Generating train split: 47545 examples [00:17, 2568.47 examples/s]Generating train split: 48471 examples [00:17, 3141.64 examples/s]Generating train split: 49402 examples [00:17, 3713.42 examples/s]Generating train split: 50350 examples [00:18, 4174.84 examples/s]Generating train split: 51242 examples [00:18, 4684.71 examples/s]Generating train split: 52177 examples [00:19, 1563.61 examples/s]Generating train split: 53082 examples [00:19, 2003.85 examples/s]Generating train split: 54026 examples [00:20, 2530.20 examples/s]Generating train split: 54948 examples [00:20, 3044.93 examples/s]Generating train split: 55883 examples [00:20, 3638.12 examples/s]Generating train split: 56787 examples [00:20, 4120.02 examples/s]Generating train split: 57682 examples [00:20, 4615.65 examples/s]Generating train split: 58607 examples [00:22, 1591.34 examples/s]Generating train split: 59346 examples [00:22, 1945.58 examples/s]Generating train split: 60029 examples [00:22, 2348.54 examples/s]Generating train split: 60806 examples [00:22, 2890.95 examples/s]Generating train split: 61563 examples [00:22, 3420.44 examples/s]Generating train split: 62307 examples [00:22, 3994.59 examples/s]Generating train split: 63003 examples [00:22, 4525.70 examples/s]Generating train split: 63794 examples [00:22, 5155.13 examples/s]Generating train split: 64810 examples [00:24, 1534.39 examples/s]Generating train split: 65727 examples [00:24, 2068.77 examples/s]Generating train split: 66501 examples [00:24, 2585.80 examples/s]Generating train split: 67350 examples [00:24, 3216.67 examples/s]Generating train split: 68335 examples [00:24, 3802.36 examples/s]Generating train split: 68993 examples [00:24, 4187.81 examples/s]Generating train split: 69653 examples [00:25, 4582.48 examples/s]Generating train split: 70478 examples [00:25, 4476.51 examples/s]Generating train split: 71305 examples [00:26, 1374.33 examples/s]Generating train split: 71983 examples [00:26, 1732.48 examples/s]Generating train split: 72844 examples [00:27, 2289.13 examples/s]Generating train split: 73536 examples [00:27, 2791.30 examples/s]Generating train split: 74390 examples [00:27, 3374.53 examples/s]Generating train split: 75253 examples [00:27, 3933.91 examples/s]Generating train split: 76098 examples [00:27, 4426.55 examples/s]Generating train split: 76777 examples [00:27, 4813.96 examples/s]Generating train split: 77793 examples [00:29, 1508.65 examples/s]Generating train split: 78652 examples [00:29, 1967.35 examples/s]Generating train split: 79310 examples [00:29, 2371.01 examples/s]Generating train split: 79966 examples [00:29, 2794.41 examples/s]Generating train split: 80707 examples [00:29, 3296.69 examples/s]Generating train split: 81376 examples [00:29, 3693.77 examples/s]Generating train split: 82128 examples [00:29, 4154.88 examples/s]Generating train split: 82738 examples [00:31, 1277.10 examples/s]Generating train split: 83383 examples [00:31, 1633.85 examples/s]Generating train split: 83946 examples [00:31, 1992.65 examples/s]Generating train split: 84898 examples [00:31, 2713.65 examples/s]Generating train split: 85724 examples [00:31, 3289.24 examples/s]Generating train split: 86831 examples [00:31, 4040.95 examples/s]Generating train split: 87964 examples [00:32, 4669.08 examples/s]Generating train split: 89078 examples [00:33, 1684.89 examples/s]Generating train split: 90222 examples [00:33, 2226.28 examples/s]Generating train split: 91350 examples [00:33, 2799.81 examples/s]Generating train split: 92475 examples [00:34, 3395.99 examples/s]Generating train split: 93600 examples [00:34, 3977.41 examples/s]Generating train split: 94727 examples [00:34, 4528.52 examples/s]Generating train split: 95559 examples [00:35, 1699.72 examples/s]Generating train split: 96685 examples [00:36, 2223.29 examples/s]Generating train split: 97828 examples [00:36, 2805.67 examples/s]Generating train split: 98785 examples [00:36, 3344.68 examples/s]Generating train split: 99669 examples [00:36, 3849.55 examples/s]Generating train split: 100552 examples [00:36, 4230.59 examples/s]Generating train split: 101439 examples [00:36, 4617.18 examples/s]Generating train split: 102100 examples [00:38, 1457.70 examples/s]Generating train split: 102988 examples [00:38, 1904.57 examples/s]Generating train split: 103882 examples [00:38, 2435.55 examples/s]Generating train split: 104773 examples [00:38, 3006.59 examples/s]Generating train split: 105660 examples [00:38, 3614.66 examples/s]Generating train split: 106547 examples [00:39, 4198.60 examples/s]Generating train split: 107437 examples [00:39, 4697.22 examples/s]Generating train split: 108108 examples [00:40, 1471.11 examples/s]Generating train split: 108990 examples [00:40, 1949.92 examples/s]Generating train split: 109882 examples [00:40, 2467.83 examples/s]Generating train split: 110560 examples [00:41, 2877.73 examples/s]Generating train split: 111236 examples [00:41, 3329.00 examples/s]Generating train split: 111904 examples [00:41, 3767.47 examples/s]Generating train split: 112578 examples [00:41, 4212.48 examples/s]Generating train split: 113253 examples [00:41, 4515.78 examples/s]Generating train split: 113926 examples [00:42, 1298.59 examples/s]Generating train split: 114609 examples [00:43, 1665.24 examples/s]Generating train split: 115272 examples [00:43, 2103.99 examples/s]Generating train split: 115932 examples [00:43, 2576.89 examples/s]Generating train split: 116601 examples [00:43, 3087.30 examples/s]Generating train split: 117261 examples [00:43, 3544.99 examples/s]Generating train split: 117943 examples [00:43, 4014.02 examples/s]Generating train split: 118613 examples [00:43, 4434.28 examples/s]Generating train split: 119286 examples [00:45, 1235.48 examples/s]Generating train split: 119979 examples [00:45, 1629.25 examples/s]Generating train split: 120657 examples [00:45, 2057.88 examples/s]Generating train split: 121328 examples [00:45, 2509.25 examples/s]Generating train split: 121993 examples [00:45, 2987.22 examples/s]Generating train split: 122700 examples [00:45, 3559.85 examples/s]Generating train split: 123382 examples [00:45, 3939.22 examples/s]Generating train split: 124050 examples [00:46, 4374.93 examples/s]Generating train split: 124742 examples [00:46, 4804.67 examples/s]Generating train split: 125430 examples [00:47, 1302.05 examples/s]Generating train split: 126103 examples [00:47, 1699.48 examples/s]Generating train split: 126656 examples [00:47, 2021.99 examples/s]Generating train split: 127322 examples [00:47, 2468.67 examples/s]Generating train split: 127995 examples [00:48, 2915.45 examples/s]Generating train split: 128661 examples [00:48, 3296.13 examples/s]Generating train split: 129330 examples [00:48, 3663.16 examples/s]Generating train split: 129992 examples [00:48, 3981.50 examples/s]Generating train split: 130541 examples [00:49, 1149.45 examples/s]Generating train split: 131211 examples [00:50, 1514.29 examples/s]Generating train split: 131814 examples [00:50, 1894.42 examples/s]Generating train split: 132415 examples [00:50, 2342.57 examples/s]Generating train split: 133022 examples [00:50, 2814.85 examples/s]Generating train split: 133640 examples [00:50, 3244.16 examples/s]Generating train split: 134243 examples [00:50, 3669.59 examples/s]Generating train split: 134830 examples [00:50, 4049.24 examples/s]Generating train split: 135438 examples [00:50, 4336.33 examples/s]Generating train split: 136227 examples [00:52, 1169.26 examples/s]Generating train split: 136831 examples [00:52, 1509.04 examples/s]Generating train split: 137424 examples [00:52, 1905.92 examples/s]Generating train split: 138030 examples [00:52, 2368.98 examples/s]Generating train split: 138619 examples [00:52, 2839.83 examples/s]Generating train split: 139225 examples [00:52, 3324.24 examples/s]Generating train split: 139948 examples [00:53, 3891.63 examples/s]Generating train split: 140607 examples [00:53, 4346.20 examples/s]Generating train split: 141340 examples [00:53, 4774.86 examples/s]Generating train split: 142330 examples [00:54, 1462.13 examples/s]Generating train split: 143067 examples [00:54, 1870.96 examples/s]Generating train split: 143899 examples [00:54, 2383.57 examples/s]Generating train split: 144789 examples [00:55, 2958.56 examples/s]Generating train split: 145707 examples [00:55, 3350.39 examples/s]Generating train split: 146428 examples [00:55, 3782.17 examples/s]Generating train split: 147159 examples [00:55, 4203.51 examples/s]Generating train split: 147855 examples [00:55, 4490.27 examples/s]Generating train split: 148461 examples [00:57, 1322.15 examples/s]Generating train split: 149174 examples [00:57, 1724.11 examples/s]Generating train split: 149890 examples [00:57, 2183.03 examples/s]Generating train split: 150705 examples [00:57, 2816.26 examples/s]Generating train split: 151409 examples [00:57, 3350.67 examples/s]Generating train split: 152224 examples [00:57, 4030.18 examples/s]Generating train split: 153080 examples [00:57, 4583.56 examples/s]Generating train split: 154025 examples [00:57, 5113.18 examples/s]Generating train split: 154960 examples [00:59, 1594.50 examples/s]Generating train split: 155884 examples [00:59, 2090.31 examples/s]Generating train split: 156781 examples [00:59, 2623.31 examples/s]Generating train split: 157655 examples [00:59, 3200.13 examples/s]Generating train split: 158417 examples [00:59, 3668.48 examples/s]Generating train split: 159230 examples [01:00, 4252.97 examples/s]Generating train split: 160203 examples [01:00, 4839.13 examples/s]Generating train split: 161063 examples [01:00, 5309.16 examples/s]Generating train split: 161982 examples [01:01, 1561.27 examples/s]Generating train split: 162890 examples [01:01, 2037.74 examples/s]Generating train split: 163639 examples [01:02, 2495.87 examples/s]Generating train split: 164534 examples [01:02, 3063.09 examples/s]Generating train split: 165398 examples [01:02, 3696.10 examples/s]Generating train split: 166210 examples [01:02, 4311.77 examples/s]Generating train split: 167061 examples [01:02, 4858.31 examples/s]Generating train split: 168125 examples [01:02, 5430.54 examples/s]Generating train split: 168945 examples [01:04, 1639.99 examples/s]Generating train split: 169724 examples [01:04, 2049.91 examples/s]Generating train split: 170615 examples [01:04, 2629.42 examples/s]Generating train split: 171336 examples [01:04, 3119.86 examples/s]Generating train split: 172216 examples [01:04, 3625.43 examples/s]Generating train split: 173054 examples [01:04, 4038.08 examples/s]Generating train split: 174292 examples [01:05, 4696.90 examples/s]Generating train split: 175161 examples [01:05, 4928.41 examples/s]Generating train split: 176482 examples [01:06, 1741.40 examples/s]Generating train split: 177333 examples [01:06, 2112.02 examples/s]Generating train split: 178249 examples [01:07, 2589.37 examples/s]Generating train split: 179128 examples [01:07, 3071.62 examples/s]Generating train split: 180029 examples [01:07, 3579.03 examples/s]Generating train split: 180874 examples [01:07, 3988.32 examples/s]Generating train split: 182199 examples [01:09, 1625.28 examples/s]Generating train split: 182627 examples [01:09, 1678.24 examples/s]Generating train split: 183490 examples [01:09, 2140.26 examples/s]Generating train split: 184410 examples [01:09, 2691.56 examples/s]Generating train split: 185481 examples [01:09, 3317.42 examples/s]Generating train split: 186368 examples [01:09, 3819.00 examples/s]Generating train split: 187234 examples [01:10, 4265.46 examples/s]Generating train split: 188078 examples [01:11, 1622.00 examples/s]Generating train split: 188974 examples [01:11, 2081.04 examples/s]Generating train split: 189847 examples [01:11, 2595.22 examples/s]Generating train split: 191141 examples [01:11, 3361.25 examples/s]Generating train split: 192028 examples [01:12, 3766.91 examples/s]Generating train split: 192758 examples [01:12, 4204.83 examples/s]Generating train split: 193645 examples [01:12, 4624.09 examples/s]Generating train split: 194522 examples [01:13, 1490.29 examples/s]Generating train split: 195183 examples [01:13, 1801.98 examples/s]Generating train split: 195685 examples [01:14, 2048.23 examples/s]Generating train split: 196183 examples [01:14, 2332.84 examples/s]Generating train split: 196669 examples [01:14, 2625.47 examples/s]Generating train split: 197143 examples [01:14, 2833.61 examples/s]Generating train split: 197628 examples [01:14, 3130.75 examples/s]Generating train split: 198109 examples [01:14, 3374.61 examples/s]Generating train split: 198576 examples [01:16, 885.29 examples/s] Generating train split: 199072 examples [01:16, 1156.50 examples/s]Generating train split: 199552 examples [01:16, 1468.00 examples/s]Generating train split: 200024 examples [01:16, 1806.92 examples/s]Generating train split: 200523 examples [01:16, 2203.78 examples/s]Generating train split: 201008 examples [01:16, 2615.17 examples/s]Generating train split: 201512 examples [01:16, 2982.60 examples/s]Generating train split: 202010 examples [01:16, 3323.60 examples/s]Generating train split: 202486 examples [01:17, 3621.78 examples/s]Generating train split: 202956 examples [01:17, 3839.85 examples/s]Generating train split: 203563 examples [01:18, 973.72 examples/s] Generating train split: 204071 examples [01:18, 1264.09 examples/s]Generating train split: 204576 examples [01:18, 1601.88 examples/s]Generating train split: 205054 examples [01:19, 1955.05 examples/s]Generating train split: 205535 examples [01:19, 2323.89 examples/s]Generating train split: 206042 examples [01:19, 2715.78 examples/s]Generating train split: 206523 examples [01:19, 3070.80 examples/s]Generating train split: 207013 examples [01:19, 3374.42 examples/s]Generating train split: 207570 examples [01:19, 3850.03 examples/s]Generating train split: 208313 examples [01:21, 1047.52 examples/s]Generating train split: 209186 examples [01:21, 1546.04 examples/s]Generating train split: 209795 examples [01:21, 1924.59 examples/s]Generating train split: 210531 examples [01:21, 2464.59 examples/s]Generating train split: 211116 examples [01:21, 2918.57 examples/s]Generating train split: 211721 examples [01:21, 3398.20 examples/s]Generating train split: 212471 examples [01:21, 3930.34 examples/s]Generating train split: 213063 examples [01:21, 4249.05 examples/s]Generating train split: 213970 examples [01:23, 1226.14 examples/s]Generating train split: 214588 examples [01:23, 1555.53 examples/s]Generating train split: 215333 examples [01:23, 2027.31 examples/s]Generating train split: 215917 examples [01:23, 2431.57 examples/s]Generating train split: 216798 examples [01:24, 3098.75 examples/s]Generating train split: 217526 examples [01:24, 3607.46 examples/s]Generating train split: 218255 examples [01:24, 4086.90 examples/s]Generating train split: 218868 examples [01:24, 4405.88 examples/s]Generating train split: 219469 examples [01:24, 4696.62 examples/s]Generating train split: 220186 examples [01:26, 1181.74 examples/s]Generating train split: 220764 examples [01:26, 1499.19 examples/s]Generating train split: 221498 examples [01:26, 1975.23 examples/s]Generating train split: 222163 examples [01:26, 2394.97 examples/s]Generating train split: 222866 examples [01:26, 2833.66 examples/s]Generating train split: 223486 examples [01:26, 3129.11 examples/s]Generating train split: 224192 examples [01:26, 3525.88 examples/s]Generating train split: 224802 examples [01:27, 3563.65 examples/s]Generating train split: 225415 examples [01:28, 1031.65 examples/s]Generating train split: 226038 examples [01:28, 1328.01 examples/s]Generating train split: 226661 examples [01:29, 1680.34 examples/s]Generating train split: 227295 examples [01:29, 2094.89 examples/s]Generating train split: 227751 examples [01:29, 2311.82 examples/s]Generating train split: 228373 examples [01:29, 2663.62 examples/s]Generating train split: 228997 examples [01:29, 3047.76 examples/s]Generating train split: 229458 examples [01:29, 3170.04 examples/s]Generating train split: 229914 examples [01:31, 910.40 examples/s] Generating train split: 230620 examples [01:31, 1296.84 examples/s]Generating train split: 231161 examples [01:31, 1610.95 examples/s]Generating train split: 231702 examples [01:31, 1974.50 examples/s]Generating train split: 232160 examples [01:31, 2303.28 examples/s]Generating train split: 232710 examples [01:31, 2692.71 examples/s]Generating train split: 233352 examples [01:31, 3186.69 examples/s]Generating train split: 233893 examples [01:32, 3471.63 examples/s]Generating train split: 234397 examples [01:33, 969.80 examples/s] Generating train split: 235074 examples [01:33, 1343.74 examples/s]Generating train split: 235740 examples [01:33, 1755.84 examples/s]Generating train split: 236411 examples [01:33, 2212.52 examples/s]Generating train split: 237058 examples [01:34, 2651.94 examples/s]Generating train split: 237704 examples [01:34, 3087.88 examples/s]Generating train split: 238356 examples [01:34, 3464.39 examples/s]Generating train split: 239025 examples [01:34, 3821.21 examples/s]Generating train split: 239689 examples [01:35, 1228.66 examples/s]Generating train split: 240372 examples [01:36, 1607.18 examples/s]Generating train split: 241018 examples [01:36, 2018.81 examples/s]Generating train split: 241671 examples [01:36, 2470.86 examples/s]Generating train split: 242412 examples [01:36, 2929.84 examples/s]Generating train split: 242895 examples [01:36, 3045.22 examples/s]Generating train split: 243378 examples [01:36, 3265.78 examples/s]Generating train split: 243857 examples [01:36, 3307.25 examples/s]Generating train split: 244338 examples [01:38, 992.84 examples/s] Generating train split: 244822 examples [01:38, 1268.05 examples/s]Generating train split: 245304 examples [01:38, 1548.76 examples/s]Generating train split: 246030 examples [01:38, 2003.64 examples/s]Generating train split: 246511 examples [01:38, 2354.33 examples/s]Generating train split: 246992 examples [01:38, 2670.27 examples/s]Generating train split: 247473 examples [01:39, 2908.66 examples/s]Generating train split: 247953 examples [01:39, 3180.27 examples/s]Generating train split: 248437 examples [01:40, 928.78 examples/s] Generating train split: 248918 examples [01:40, 1201.83 examples/s]Generating train split: 249399 examples [01:40, 1519.14 examples/s]Generating train split: 249880 examples [01:40, 1896.43 examples/s]Generating train split: 250365 examples [01:41, 2229.26 examples/s]Generating train split: 250847 examples [01:41, 2580.63 examples/s]Generating train split: 251325 examples [01:41, 2934.16 examples/s]Generating train split: 251806 examples [01:41, 3281.08 examples/s]Generating train split: 252285 examples [01:42, 875.54 examples/s] Generating train split: 252769 examples [01:42, 1151.15 examples/s]Generating train split: 253250 examples [01:43, 1465.31 examples/s]Generating train split: 253734 examples [01:43, 1848.39 examples/s]Generating train split: 254216 examples [01:43, 2210.50 examples/s]Generating train split: 254971 examples [01:43, 2783.32 examples/s]Generating train split: 255711 examples [01:43, 3299.38 examples/s]Generating train split: 256218 examples [01:43, 3546.44 examples/s]Generating train split: 256710 examples [01:45, 1041.44 examples/s]Generating train split: 257475 examples [01:45, 1445.16 examples/s]Generating train split: 257985 examples [01:45, 1776.23 examples/s]Generating train split: 258499 examples [01:45, 2158.86 examples/s]Generating train split: 259010 examples [01:45, 2544.75 examples/s]Generating train split: 259523 examples [01:45, 2868.91 examples/s]Generating train split: 260266 examples [01:45, 3362.46 examples/s]Generating train split: 261264 examples [01:46, 3937.62 examples/s]Generating train split: 261774 examples [01:47, 1238.49 examples/s]Generating train split: 262262 examples [01:47, 1505.32 examples/s]Generating train split: 262777 examples [01:47, 1851.71 examples/s]Generating train split: 263516 examples [01:47, 2388.20 examples/s]Generating train split: 264270 examples [01:47, 2865.48 examples/s]Generating train split: 265301 examples [01:48, 3515.94 examples/s]Generating train split: 266341 examples [01:48, 4000.28 examples/s]Generating train split: 266834 examples [01:49, 1286.70 examples/s]Generating train split: 267408 examples [01:49, 1552.78 examples/s]Generating train split: 267981 examples [01:50, 1867.65 examples/s]Generating train split: 268555 examples [01:50, 2189.68 examples/s]Generating train split: 269130 examples [01:50, 2563.93 examples/s]Generating train split: 269701 examples [01:50, 2845.84 examples/s]Generating train split: 270272 examples [01:50, 3167.80 examples/s]Generating train split: 270842 examples [01:50, 3455.86 examples/s]Generating train split: 271411 examples [01:52, 997.51 examples/s] Generating train split: 271988 examples [01:52, 1278.31 examples/s]Generating train split: 272565 examples [01:52, 1599.86 examples/s]Generating train split: 273143 examples [01:52, 1966.55 examples/s]Generating train split: 273721 examples [01:52, 2364.28 examples/s]Generating train split: 274289 examples [01:52, 2664.06 examples/s]Generating train split: 274868 examples [01:53, 3007.08 examples/s]Generating train split: 275443 examples [01:54, 957.29 examples/s] Generating train split: 276014 examples [01:54, 1250.66 examples/s]Generating train split: 276592 examples [01:54, 1598.97 examples/s]Generating train split: 277174 examples [01:55, 1979.58 examples/s]Generating train split: 277749 examples [01:55, 2259.29 examples/s]Generating train split: 278326 examples [01:55, 2638.59 examples/s]Generating train split: 278905 examples [01:55, 2975.13 examples/s]Generating train split: 279478 examples [01:56, 1057.70 examples/s]Generating train split: 280056 examples [01:56, 1368.73 examples/s]Generating train split: 280634 examples [01:57, 1735.94 examples/s]Generating train split: 281209 examples [01:57, 2129.05 examples/s]Generating train split: 281980 examples [01:57, 2779.59 examples/s]Generating train split: 282762 examples [01:57, 3434.82 examples/s]Generating train split: 283569 examples [01:57, 4057.99 examples/s]Generating train split: 284203 examples [01:57, 4444.04 examples/s]Generating train split: 284982 examples [01:59, 1353.62 examples/s]Generating train split: 285774 examples [01:59, 1811.13 examples/s]Generating train split: 286602 examples [01:59, 2365.17 examples/s]Generating train split: 287250 examples [01:59, 2835.27 examples/s]Generating train split: 287904 examples [01:59, 3358.60 examples/s]Generating train split: 288837 examples [01:59, 4091.68 examples/s]Generating train split: 289815 examples [01:59, 4379.48 examples/s]Generating train split: 290699 examples [02:01, 1462.85 examples/s]Generating train split: 291366 examples [02:01, 1757.17 examples/s]Generating train split: 292040 examples [02:01, 2097.47 examples/s]Generating train split: 292952 examples [02:01, 2591.84 examples/s]Generating train split: 293487 examples [02:02, 2864.07 examples/s]Generating train split: 294155 examples [02:02, 3156.31 examples/s]Generating train split: 294615 examples [02:02, 3347.08 examples/s]Generating train split: 295064 examples [02:03, 1029.22 examples/s]Generating train split: 295503 examples [02:03, 1254.97 examples/s]Generating train split: 295940 examples [02:03, 1527.93 examples/s]Generating train split: 296426 examples [02:04, 1902.48 examples/s]Generating train split: 297158 examples [02:04, 2437.91 examples/s]Generating train split: 297867 examples [02:04, 2865.66 examples/s]Generating train split: 298313 examples [02:04, 3083.02 examples/s]Generating train split: 298770 examples [02:04, 3350.70 examples/s]Generating train split: 299216 examples [02:06, 939.41 examples/s] Generating train split: 299886 examples [02:06, 1318.15 examples/s]Generating train split: 300430 examples [02:06, 1665.76 examples/s]Generating train split: 301175 examples [02:06, 2231.21 examples/s]Generating train split: 301868 examples [02:06, 2856.79 examples/s]Generating train split: 302558 examples [02:06, 3476.29 examples/s]Generating train split: 303245 examples [02:06, 3990.10 examples/s]Generating train split: 303930 examples [02:06, 4561.73 examples/s]Generating train split: 304790 examples [02:06, 5140.98 examples/s]Generating train split: 305667 examples [02:08, 1440.94 examples/s]Generating train split: 306354 examples [02:08, 1826.43 examples/s]Generating train split: 307385 examples [02:08, 2522.53 examples/s]Generating train split: 308243 examples [02:08, 3101.67 examples/s]Generating train split: 309091 examples [02:08, 3713.65 examples/s]Generating train split: 310120 examples [02:09, 4178.79 examples/s]Generating train split: 311005 examples [02:09, 4595.03 examples/s]Generating train split: 311724 examples [02:10, 1419.13 examples/s]Generating train split: 312601 examples [02:10, 1856.22 examples/s]Generating train split: 313216 examples [02:11, 2205.59 examples/s]Generating train split: 314084 examples [02:11, 2763.75 examples/s]Generating train split: 314838 examples [02:11, 3276.78 examples/s]Generating train split: 315451 examples [02:11, 3652.34 examples/s]Generating train split: 316187 examples [02:11, 4111.84 examples/s]Generating train split: 317094 examples [02:11, 4731.16 examples/s]Generating train split: 317839 examples [02:13, 1479.10 examples/s]Generating train split: 318707 examples [02:13, 1986.96 examples/s]Generating train split: 319621 examples [02:13, 2600.85 examples/s]Generating train split: 320501 examples [02:13, 3213.02 examples/s]Generating train split: 321425 examples [02:13, 3926.07 examples/s]Generating train split: 322319 examples [02:13, 4552.36 examples/s]Generating train split: 323236 examples [02:13, 5093.20 examples/s]Generating train split: 324159 examples [02:13, 5554.46 examples/s]Generating train split: 324890 examples [02:15, 1609.78 examples/s]Generating train split: 325400 examples [02:15, 1868.37 examples/s]Generating train split: 326152 examples [02:15, 2304.64 examples/s]Generating train split: 327122 examples [02:15, 2898.94 examples/s]Generating train split: 328101 examples [02:15, 3452.60 examples/s]Generating train split: 328842 examples [02:16, 3833.08 examples/s]Generating train split: 329379 examples [02:16, 4023.24 examples/s]Generating train split: 330123 examples [02:17, 1371.53 examples/s]Generating train split: 331118 examples [02:17, 1895.30 examples/s]Generating train split: 332078 examples [02:17, 2416.07 examples/s]Generating train split: 333066 examples [02:18, 2985.17 examples/s]Generating train split: 334054 examples [02:18, 3504.82 examples/s]Generating train split: 334784 examples [02:18, 3818.76 examples/s]Generating train split: 335487 examples [02:19, 1422.93 examples/s]Generating train split: 336203 examples [02:19, 1757.05 examples/s]Generating train split: 336909 examples [02:20, 1945.93 examples/s]Generating train split: 337642 examples [02:20, 2418.17 examples/s]Generating train split: 338535 examples [02:20, 3045.07 examples/s]Generating train split: 339393 examples [02:20, 3601.23 examples/s]Generating train split: 340262 examples [02:20, 4133.36 examples/s]Generating train split: 340827 examples [02:21, 1918.32 examples/s]Generating train split: 341690 examples [02:21, 2489.91 examples/s]Generating train split: 342277 examples [02:21, 2879.81 examples/s]Generating train split: 343135 examples [02:21, 3487.58 examples/s]Generating train split: 344002 examples [02:22, 4112.73 examples/s]Generating train split: 344832 examples [02:22, 4547.27 examples/s]Generating train split: 345666 examples [02:22, 4989.06 examples/s]Generating train split: 346495 examples [02:22, 5342.94 examples/s]Generating train split: 347461 examples [02:24, 1463.71 examples/s]Generating train split: 348169 examples [02:24, 1820.16 examples/s]Generating train split: 348998 examples [02:24, 2334.67 examples/s]Generating train split: 349829 examples [02:24, 2877.44 examples/s]Generating train split: 350675 examples [02:24, 3472.98 examples/s]Generating train split: 351438 examples [02:24, 3932.62 examples/s]Generating train split: 352196 examples [02:24, 4367.17 examples/s]Generating train split: 352956 examples [02:25, 4744.89 examples/s]Generating train split: 353840 examples [02:26, 1480.78 examples/s]Generating train split: 354599 examples [02:26, 1879.54 examples/s]Generating train split: 355376 examples [02:26, 2348.40 examples/s]Generating train split: 356145 examples [02:26, 2878.63 examples/s]Generating train split: 356902 examples [02:26, 3425.96 examples/s]Generating train split: 357571 examples [02:27, 3891.52 examples/s]Generating train split: 358298 examples [02:27, 4323.31 examples/s]Generating train split: 359014 examples [02:27, 4659.79 examples/s]Generating train split: 359720 examples [02:28, 1356.60 examples/s]Generating train split: 360479 examples [02:28, 1774.04 examples/s]Generating train split: 361188 examples [02:28, 2222.39 examples/s]Generating train split: 361919 examples [02:29, 2716.49 examples/s]Generating train split: 362645 examples [02:29, 3235.43 examples/s]Generating train split: 363338 examples [02:29, 3733.27 examples/s]Generating train split: 364046 examples [02:29, 4103.83 examples/s]Generating train split: 364737 examples [02:29, 4539.35 examples/s]Generating train split: 365479 examples [02:29, 4949.85 examples/s]Generating train split: 366370 examples [02:31, 1471.36 examples/s]Generating train split: 367322 examples [02:31, 2043.25 examples/s]Generating train split: 368143 examples [02:31, 2583.31 examples/s]Generating train split: 368988 examples [02:31, 3208.01 examples/s]Generating train split: 369836 examples [02:31, 3895.09 examples/s]Generating train split: 370675 examples [02:31, 4544.97 examples/s]Generating train split: 371488 examples [02:31, 5122.30 examples/s]Generating train split: 372485 examples [02:32, 5075.57 examples/s]Generating train split: 373151 examples [02:33, 1348.67 examples/s]Generating train split: 373933 examples [02:33, 1666.95 examples/s]Generating train split: 374453 examples [02:33, 1918.84 examples/s]Generating train split: 374977 examples [02:34, 2225.29 examples/s]Generating train split: 375628 examples [02:34, 2616.17 examples/s]Generating train split: 376312 examples [02:34, 3000.22 examples/s]Generating train split: 376970 examples [02:34, 3308.34 examples/s]Generating train split: 377493 examples [02:35, 1098.19 examples/s]Generating train split: 378149 examples [02:36, 1414.52 examples/s]Generating train split: 378698 examples [02:36, 1755.74 examples/s]Generating train split: 379353 examples [02:36, 2187.81 examples/s]Generating train split: 380002 examples [02:36, 2574.79 examples/s]Generating train split: 380525 examples [02:36, 2820.47 examples/s]Generating train split: 381045 examples [02:36, 3114.29 examples/s]Generating train split: 381698 examples [02:36, 3457.67 examples/s]Generating train split: 382351 examples [02:38, 1053.97 examples/s]Generating train split: 382870 examples [02:38, 1322.72 examples/s]Generating train split: 383527 examples [02:38, 1694.85 examples/s]Generating train split: 384351 examples [02:38, 2209.38 examples/s]Generating train split: 385013 examples [02:39, 2554.97 examples/s]Generating train split: 385704 examples [02:39, 2910.99 examples/s]Generating train split: 386363 examples [02:39, 3249.73 examples/s]Generating train split: 386875 examples [02:40, 1053.06 examples/s]Generating train split: 387529 examples [02:41, 1373.80 examples/s]Generating train split: 388199 examples [02:41, 1759.11 examples/s]Generating train split: 388863 examples [02:41, 2158.97 examples/s]Generating train split: 389508 examples [02:41, 2547.25 examples/s]Generating train split: 390173 examples [02:41, 2906.38 examples/s]Generating train split: 390822 examples [02:41, 3232.31 examples/s]Generating train split: 391344 examples [02:43, 1075.71 examples/s]Generating train split: 391858 examples [02:43, 1348.76 examples/s]Generating train split: 392512 examples [02:43, 1752.66 examples/s]Generating train split: 393167 examples [02:43, 2155.25 examples/s]Generating train split: 393833 examples [02:43, 2569.20 examples/s]Generating train split: 394493 examples [02:43, 2988.54 examples/s]Generating train split: 395564 examples [02:44, 3488.78 examples/s]Generating train split: 396228 examples [02:45, 1295.73 examples/s]Generating train split: 396738 examples [02:45, 1539.14 examples/s]Generating train split: 397390 examples [02:45, 1905.10 examples/s]Generating train split: 398051 examples [02:45, 2302.95 examples/s]Generating train split: 398717 examples [02:46, 2700.95 examples/s]Generating train split: 399374 examples [02:46, 3054.14 examples/s]Generating train split: 400030 examples [02:46, 3403.59 examples/s]Generating train split: 400546 examples [02:47, 1110.82 examples/s]Generating train split: 401208 examples [02:47, 1460.77 examples/s]Generating train split: 401871 examples [02:48, 1852.96 examples/s]Generating train split: 402532 examples [02:48, 2257.19 examples/s]Generating train split: 403198 examples [02:48, 2668.05 examples/s]Generating train split: 403746 examples [02:48, 2989.27 examples/s]Generating train split: 404292 examples [02:48, 3346.02 examples/s]Generating train split: 404817 examples [02:49, 1160.94 examples/s]Generating train split: 405355 examples [02:49, 1492.77 examples/s]Generating train split: 405894 examples [02:50, 1845.04 examples/s]Generating train split: 406438 examples [02:50, 2252.10 examples/s]Generating train split: 406966 examples [02:50, 2684.48 examples/s]Generating train split: 407504 examples [02:50, 3079.19 examples/s]Generating train split: 408036 examples [02:50, 3377.11 examples/s]Generating train split: 408578 examples [02:50, 3709.30 examples/s]Generating train split: 409115 examples [02:51, 1063.61 examples/s]Generating train split: 409656 examples [02:52, 1382.07 examples/s]Generating train split: 410183 examples [02:52, 1740.06 examples/s]Generating train split: 410721 examples [02:52, 2149.95 examples/s]Generating train split: 411259 examples [02:52, 1434.16 examples/s]Generating train split: 411797 examples [02:53, 1739.55 examples/s]Generating train split: 412334 examples [02:53, 2128.80 examples/s]Generating train split: 412885 examples [02:53, 2558.09 examples/s]Generating train split: 413421 examples [02:53, 1698.01 examples/s]Generating train split: 413969 examples [02:54, 2093.01 examples/s]Generating train split: 414511 examples [02:54, 2518.63 examples/s]Generating train split: 415050 examples [02:54, 2905.64 examples/s]Generating train split: 415587 examples [02:54, 3338.33 examples/s]Generating train split: 416124 examples [02:54, 3666.86 examples/s]Generating train split: 416658 examples [02:54, 3911.09 examples/s]Generating train split: 417569 examples [02:55, 1309.54 examples/s]Generating train split: 418371 examples [02:56, 1814.07 examples/s]Generating train split: 419301 examples [02:56, 2473.26 examples/s]Generating train split: 420225 examples [02:56, 3126.38 examples/s]Generating train split: 421171 examples [02:56, 3816.41 examples/s]Generating train split: 422081 examples [02:56, 4390.31 examples/s]Generating train split: 423012 examples [02:56, 4936.70 examples/s]Generating train split: 423924 examples [02:56, 5323.28 examples/s]Generating train split: 424717 examples [02:58, 1640.16 examples/s]Generating train split: 425393 examples [02:58, 1990.84 examples/s]Generating train split: 426063 examples [02:58, 2384.91 examples/s]Generating train split: 426731 examples [02:58, 2795.49 examples/s]Generating train split: 427410 examples [02:58, 3199.12 examples/s]Generating train split: 428071 examples [02:58, 3612.65 examples/s]Generating train split: 428742 examples [02:59, 4004.09 examples/s]Generating train split: 429436 examples [03:00, 1267.59 examples/s]Generating train split: 430117 examples [03:00, 1627.60 examples/s]Generating train split: 430784 examples [03:00, 2037.46 examples/s]Generating train split: 431471 examples [03:00, 2502.78 examples/s]Generating train split: 432161 examples [03:00, 2948.29 examples/s]Generating train split: 432845 examples [03:01, 3432.20 examples/s]Generating train split: 433839 examples [03:01, 4188.54 examples/s]Generating train split: 434844 examples [03:01, 4924.07 examples/s]Generating train split: 435612 examples [03:02, 1478.06 examples/s]Generating train split: 436577 examples [03:02, 2005.70 examples/s]Generating train split: 437552 examples [03:03, 2601.47 examples/s]Generating train split: 438547 examples [03:03, 3261.46 examples/s]Generating train split: 439544 examples [03:03, 3844.95 examples/s]Generating train split: 440534 examples [03:03, 4447.61 examples/s]Generating train split: 441539 examples [03:03, 5037.93 examples/s]Generating train split: 442541 examples [03:05, 1692.96 examples/s]Generating train split: 443537 examples [03:05, 2190.56 examples/s]Generating train split: 444544 examples [03:05, 2764.78 examples/s]Generating train split: 445554 examples [03:05, 3391.86 examples/s]Generating train split: 446400 examples [03:05, 3907.45 examples/s]Generating train split: 447519 examples [03:05, 4637.15 examples/s]Generating train split: 448647 examples [03:06, 5153.71 examples/s]Generating train split: 449468 examples [03:07, 1727.61 examples/s]Generating train split: 450555 examples [03:07, 2287.79 examples/s]Generating train split: 451387 examples [03:07, 2762.06 examples/s]Generating train split: 452501 examples [03:07, 3437.97 examples/s]Generating train split: 453623 examples [03:08, 4165.03 examples/s]Generating train split: 454733 examples [03:08, 4766.15 examples/s]Generating train split: 455836 examples [03:09, 1810.30 examples/s]Generating train split: 456968 examples [03:09, 2349.03 examples/s]Generating train split: 458080 examples [03:10, 2919.81 examples/s]Generating train split: 459192 examples [03:10, 3587.21 examples/s]Generating train split: 460104 examples [03:10, 4070.12 examples/s]Generating train split: 461026 examples [03:10, 4524.41 examples/s]Generating train split: 461979 examples [03:10, 4980.95 examples/s]Generating train split: 462916 examples [03:12, 1354.89 examples/s]Generating train split: 463827 examples [03:12, 1733.63 examples/s]Generating train split: 464784 examples [03:12, 2225.04 examples/s]Generating train split: 465727 examples [03:13, 2745.18 examples/s]Generating train split: 466660 examples [03:13, 3304.93 examples/s]Generating train split: 467612 examples [03:13, 3829.54 examples/s]Generating train split: 468566 examples [03:14, 1543.05 examples/s]Generating train split: 469540 examples [03:14, 1998.50 examples/s]Generating train split: 470491 examples [03:15, 2490.20 examples/s]Generating train split: 471454 examples [03:15, 3056.57 examples/s]Generating train split: 472400 examples [03:15, 3599.77 examples/s]Generating train split: 473359 examples [03:15, 4147.17 examples/s]Generating train split: 474297 examples [03:15, 4649.14 examples/s]Generating train split: 475242 examples [03:17, 1640.95 examples/s]Generating train split: 476183 examples [03:17, 2098.19 examples/s]Generating train split: 477136 examples [03:17, 2627.03 examples/s]Generating train split: 478087 examples [03:17, 3204.66 examples/s]Generating train split: 479015 examples [03:17, 3741.94 examples/s]Generating train split: 479964 examples [03:17, 4316.93 examples/s]Generating train split: 480934 examples [03:19, 1602.96 examples/s]Generating train split: 481865 examples [03:19, 2057.23 examples/s]Generating train split: 482811 examples [03:19, 2565.55 examples/s]Generating train split: 484033 examples [03:19, 3304.19 examples/s]Generating train split: 485312 examples [03:20, 4038.50 examples/s]Generating train split: 486590 examples [03:20, 4681.51 examples/s]Generating train split: 487852 examples [03:21, 1896.31 examples/s]Generating train split: 489069 examples [03:21, 2430.46 examples/s]Generating train split: 489701 examples [03:22, 2732.95 examples/s]Generating train split: 490338 examples [03:22, 3088.54 examples/s]Generating train split: 491599 examples [03:22, 3880.42 examples/s]Generating train split: 492852 examples [03:22, 4537.74 examples/s]Generating train split: 494100 examples [03:23, 1825.75 examples/s]Generating train split: 495056 examples [03:24, 2251.04 examples/s]Generating train split: 496296 examples [03:24, 2893.28 examples/s]Generating train split: 497523 examples [03:24, 3518.65 examples/s]Generating train split: 498896 examples [03:24, 4152.24 examples/s]Generating train split: 499807 examples [03:26, 1862.39 examples/s]Generating train split: 500727 examples [03:26, 2276.13 examples/s]Generating train split: 501654 examples [03:26, 2696.85 examples/s]Generating train split: 502559 examples [03:26, 3117.57 examples/s]Generating train split: 503505 examples [03:26, 3579.54 examples/s]Generating train split: 504448 examples [03:26, 4033.92 examples/s]Generating train split: 505431 examples [03:28, 1601.28 examples/s]Generating train split: 506374 examples [03:28, 2024.36 examples/s]Generating train split: 507323 examples [03:28, 2492.13 examples/s]Generating train split: 508303 examples [03:28, 2959.04 examples/s]Generating train split: 509292 examples [03:29, 3365.54 examples/s]Generating train split: 510242 examples [03:29, 3761.84 examples/s]Generating train split: 511181 examples [03:30, 1492.23 examples/s]Generating train split: 512121 examples [03:30, 1907.14 examples/s]Generating train split: 513093 examples [03:31, 2354.39 examples/s]Generating train split: 514015 examples [03:31, 2831.72 examples/s]Generating train split: 514982 examples [03:31, 3280.53 examples/s]Generating train split: 515925 examples [03:31, 3693.63 examples/s]Generating train split: 516826 examples [03:33, 1474.06 examples/s]Generating train split: 517736 examples [03:33, 1892.65 examples/s]Generating train split: 518710 examples [03:33, 2384.32 examples/s]Generating train split: 519665 examples [03:33, 2889.41 examples/s]Generating train split: 520586 examples [03:33, 3339.04 examples/s]Generating train split: 521567 examples [03:34, 3785.49 examples/s]Generating train split: 522322 examples [03:35, 1414.50 examples/s]Generating train split: 523233 examples [03:35, 1835.25 examples/s]Generating train split: 524164 examples [03:35, 2346.35 examples/s]Generating train split: 524805 examples [03:35, 2750.20 examples/s]Generating train split: 526015 examples [03:36, 3514.67 examples/s]Generating train split: 526681 examples [03:36, 3901.97 examples/s]Generating train split: 527636 examples [03:36, 4388.42 examples/s]Generating train split: 528247 examples [03:37, 1412.72 examples/s]Generating train split: 528871 examples [03:37, 1749.36 examples/s]Generating train split: 530165 examples [03:38, 2547.64 examples/s]Generating train split: 531113 examples [03:38, 3130.48 examples/s]Generating train split: 532354 examples [03:38, 3862.84 examples/s]Generating train split: 533288 examples [03:38, 4396.40 examples/s]Generating train split: 533922 examples [03:38, 4675.46 examples/s]Generating train split: 534834 examples [03:40, 1580.79 examples/s]Generating train split: 535798 examples [03:40, 2074.93 examples/s]Generating train split: 536719 examples [03:40, 2597.76 examples/s]Generating train split: 537334 examples [03:40, 2966.16 examples/s]Generating train split: 538209 examples [03:40, 3539.16 examples/s]Generating train split: 539058 examples [03:40, 4038.26 examples/s]Generating train split: 539927 examples [03:41, 4556.98 examples/s]Generating train split: 540659 examples [03:42, 1467.00 examples/s]Generating train split: 541518 examples [03:42, 1924.43 examples/s]Generating train split: 542408 examples [03:42, 2469.20 examples/s]Generating train split: 543147 examples [03:42, 2960.20 examples/s]Generating train split: 544051 examples [03:42, 3599.54 examples/s]Generating train split: 544924 examples [03:43, 4207.93 examples/s]Generating train split: 545899 examples [03:43, 5062.42 examples/s]Generating train split: 546868 examples [03:43, 5878.16 examples/s]Generating train split: 547635 examples [03:44, 1637.88 examples/s]Generating train split: 548587 examples [03:44, 2224.34 examples/s]Generating train split: 549573 examples [03:44, 2932.48 examples/s]Generating train split: 549787 examples [03:45, 2439.35 examples/s]
19:54:10.581 [I] local_batch_size: 128                                                            (486094:data_loader.py:324)
INFO:2026-01-30 19:54:12,918:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
19:54:12.918 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (486094:xla_bridge.py:925)
INFO:2026-01-30 19:54:12,923:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
19:54:12.923 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (486094:xla_bridge.py:925)
19:54:12.984 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (486094:config.py:196)
19:54:12.985 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x1550fa991590>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (486094:data_loader.py:243)
19:54:21.689 [I] local_batch_size: 128                                                            (486094:data_loader.py:324)
19:54:51.572 [I] Cleared sample batch and data loader from memory                                 (486094:train_pytorch.py:390)
19:55:32.896 [I] Enabled gradient checkpointing for PI0Pytorch model                              (486094:pi0_pytorch.py:133)
19:55:32.897 [I] Enabled gradient checkpointing for memory optimization                           (486094:train_pytorch.py:414)
19:55:32.897 [I] Step 0 (after_model_creation): GPU memory - allocated: 7.02GB, reserved: 7.09GB, free: 0.07GB, peak_allocated: 7.02GB, peak_reserved: 7.09GB (486094:train_pytorch.py:304)
19:55:32.897 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (486094:train_pytorch.py:443)
19:56:28.290 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (486094:train_pytorch.py:449)
19:56:28.293 [I] Running on: dgx-40 | world_size=1                                                (486094:train_pytorch.py:486)
19:56:28.294 [I] Training config: batch_size=128, effective_batch_size=128, num_train_steps=200000 (486094:train_pytorch.py:489)
19:56:28.294 [I] Memory optimizations: gradient_checkpointing=True                                (486094:train_pytorch.py:492)
19:56:28.294 [I] LR schedule: warmup=10000, peak_lr=5.00e-05, decay_steps=1000000, end_lr=5.00e-05 (486094:train_pytorch.py:493)
19:56:28.294 [I] Optimizer: AdamW, weight_decay=1e-10, clip_norm=1.0                              (486094:train_pytorch.py:496)
19:56:28.294 [I] EMA is not supported for PyTorch training                                        (486094:train_pytorch.py:499)
19:56:28.294 [I] Training precision: bfloat16                                                     (486094:train_pytorch.py:500)
Training:   0%|          | 0/200000 [00:00<?, ?it/s]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
19:57:06.246 [I] Step 0 (after_backward): GPU memory - allocated: 13.63GB, reserved: 52.55GB, free: 38.92GB, peak_allocated: 48.37GB, peak_reserved: 52.55GB (486094:train_pytorch.py:304)
19:57:07.909 [I] step=0 loss=0.2438 lr=5.00e-09 grad_norm=5.62 time=39.6s                         (486094:train_pytorch.py:582)
Training:   0%|          | 1/200000 [00:39<2200:50:12, 39.62s/it]Training:   0%|          | 1/200000 [00:39<2200:50:12, 39.62s/it, loss=0.2438, lr=5.00e-09, step=1]19:57:14.878 [I] Step 1 (after_backward): GPU memory - allocated: 26.18GB, reserved: 69.82GB, free: 43.65GB, peak_allocated: 60.92GB, peak_reserved: 69.82GB (486094:train_pytorch.py:304)
Training:   0%|          | 2/200000 [00:46<1139:35:16, 20.51s/it, loss=0.2438, lr=5.00e-09, step=1]Training:   0%|          | 2/200000 [00:46<1139:35:16, 20.51s/it, loss=0.2086, lr=1.00e-08, step=2]19:57:22.122 [I] Step 2 (after_backward): GPU memory - allocated: 26.19GB, reserved: 69.83GB, free: 43.64GB, peak_allocated: 60.92GB, peak_reserved: 69.83GB (486094:train_pytorch.py:304)
Training:   0%|          | 3/200000 [00:54<802:59:58, 14.45s/it, loss=0.2086, lr=1.00e-08, step=2] Training:   0%|          | 3/200000 [00:54<802:59:58, 14.45s/it, loss=0.2335, lr=1.50e-08, step=3]19:57:29.381 [I] Step 3 (after_backward): GPU memory - allocated: 26.19GB, reserved: 69.83GB, free: 43.64GB, peak_allocated: 60.92GB, peak_reserved: 69.83GB (486094:train_pytorch.py:304)
Training:   0%|          | 4/200000 [01:01<645:09:33, 11.61s/it, loss=0.2335, lr=1.50e-08, step=3]Training:   0%|          | 4/200000 [01:01<645:09:33, 11.61s/it, loss=0.2327, lr=2.00e-08, step=4]19:57:36.637 [I] Step 4 (after_backward): GPU memory - allocated: 26.19GB, reserved: 69.83GB, free: 43.64GB, peak_allocated: 60.92GB, peak_reserved: 69.83GB (486094:train_pytorch.py:304)
Training:   0%|          | 5/200000 [01:08<557:54:09, 10.04s/it, loss=0.2327, lr=2.00e-08, step=4]Training:   0%|          | 5/200000 [01:08<557:54:09, 10.04s/it, loss=0.2904, lr=2.50e-08, step=5]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 6/200000 [01:15<504:10:38,  9.08s/it, loss=0.2904, lr=2.50e-08, step=5]Training:   0%|          | 6/200000 [01:15<504:10:38,  9.08s/it, loss=0.2218, lr=3.00e-08, step=6]Training:   0%|          | 7/200000 [01:22<469:28:11,  8.45s/it, loss=0.2218, lr=3.00e-08, step=6]Training:   0%|          | 7/200000 [01:22<469:28:11,  8.45s/it, loss=0.2720, lr=3.50e-08, step=7]Training:   0%|          | 8/200000 [01:30<446:47:57,  8.04s/it, loss=0.2720, lr=3.50e-08, step=7]Training:   0%|          | 8/200000 [01:30<446:47:57,  8.04s/it, loss=0.2334, lr=4.00e-08, step=8]Training:   0%|          | 9/200000 [01:37<432:09:55,  7.78s/it, loss=0.2334, lr=4.00e-08, step=8]Training:   0%|          | 9/200000 [01:37<432:09:55,  7.78s/it, loss=0.2193, lr=4.50e-08, step=9]Training:   0%|          | 10/200000 [01:44<422:16:45,  7.60s/it, loss=0.2193, lr=4.50e-08, step=9]Training:   0%|          | 10/200000 [01:44<422:16:45,  7.60s/it, loss=0.3248, lr=5.00e-08, step=10]Training:   0%|          | 11/200000 [01:51<415:04:43,  7.47s/it, loss=0.3248, lr=5.00e-08, step=10]Training:   0%|          | 11/200000 [01:51<415:04:43,  7.47s/it, loss=0.2369, lr=5.50e-08, step=11]Training:   0%|          | 12/200000 [01:58<410:01:09,  7.38s/it, loss=0.2369, lr=5.50e-08, step=11]Training:   0%|          | 12/200000 [01:58<410:01:09,  7.38s/it, loss=0.3273, lr=6.00e-08, step=12]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 13/200000 [02:05<406:37:39,  7.32s/it, loss=0.3273, lr=6.00e-08, step=12]Training:   0%|          | 13/200000 [02:05<406:37:39,  7.32s/it, loss=0.2823, lr=6.50e-08, step=13]Training:   0%|          | 14/200000 [02:13<404:27:27,  7.28s/it, loss=0.2823, lr=6.50e-08, step=13]Training:   0%|          | 14/200000 [02:13<404:27:27,  7.28s/it, loss=0.2751, lr=7.00e-08, step=14]Training:   0%|          | 15/200000 [02:20<402:55:31,  7.25s/it, loss=0.2751, lr=7.00e-08, step=14]Training:   0%|          | 15/200000 [02:20<402:55:31,  7.25s/it, loss=0.2452, lr=7.50e-08, step=15]Training:   0%|          | 16/200000 [02:27<401:34:34,  7.23s/it, loss=0.2452, lr=7.50e-08, step=15]Training:   0%|          | 16/200000 [02:27<401:34:34,  7.23s/it, loss=0.2547, lr=8.00e-08, step=16]Training:   0%|          | 17/200000 [02:34<404:16:58,  7.28s/it, loss=0.2547, lr=8.00e-08, step=16]Training:   0%|          | 17/200000 [02:34<404:16:58,  7.28s/it, loss=0.2532, lr=8.50e-08, step=17]Training:   0%|          | 18/200000 [02:42<403:04:53,  7.26s/it, loss=0.2532, lr=8.50e-08, step=17]Training:   0%|          | 18/200000 [02:42<403:04:53,  7.26s/it, loss=0.3223, lr=9.00e-08, step=18]Training:   0%|          | 19/200000 [02:49<401:31:15,  7.23s/it, loss=0.3223, lr=9.00e-08, step=18]Training:   0%|          | 19/200000 [02:49<401:31:15,  7.23s/it, loss=0.2838, lr=9.50e-08, step=19]Training:   0%|          | 20/200000 [02:56<400:37:51,  7.21s/it, loss=0.2838, lr=9.50e-08, step=19]Training:   0%|          | 20/200000 [02:56<400:37:51,  7.21s/it, loss=0.3335, lr=1.00e-07, step=20]Training:   0%|          | 21/200000 [03:03<400:19:03,  7.21s/it, loss=0.3335, lr=1.00e-07, step=20]Training:   0%|          | 21/200000 [03:03<400:19:03,  7.21s/it, loss=0.2803, lr=1.05e-07, step=21]Training:   0%|          | 22/200000 [03:10<400:01:19,  7.20s/it, loss=0.2803, lr=1.05e-07, step=21]Training:   0%|          | 22/200000 [03:10<400:01:19,  7.20s/it, loss=0.2346, lr=1.10e-07, step=22]Training:   0%|          | 23/200000 [03:18<399:54:39,  7.20s/it, loss=0.2346, lr=1.10e-07, step=22]Training:   0%|          | 23/200000 [03:18<399:54:39,  7.20s/it, loss=0.2131, lr=1.15e-07, step=23]Training:   0%|          | 24/200000 [03:25<399:57:15,  7.20s/it, loss=0.2131, lr=1.15e-07, step=23]Training:   0%|          | 24/200000 [03:25<399:57:15,  7.20s/it, loss=0.2791, lr=1.20e-07, step=24]Training:   0%|          | 25/200000 [03:32<399:46:44,  7.20s/it, loss=0.2791, lr=1.20e-07, step=24]Training:   0%|          | 25/200000 [03:32<399:46:44,  7.20s/it, loss=0.2284, lr=1.25e-07, step=25]Training:   0%|          | 26/200000 [03:39<399:42:36,  7.20s/it, loss=0.2284, lr=1.25e-07, step=25]Training:   0%|          | 26/200000 [03:39<399:42:36,  7.20s/it, loss=0.3383, lr=1.30e-07, step=26]Training:   0%|          | 27/200000 [03:46<400:09:30,  7.20s/it, loss=0.3383, lr=1.30e-07, step=26]Training:   0%|          | 27/200000 [03:46<400:09:30,  7.20s/it, loss=0.3069, lr=1.35e-07, step=27]Training:   0%|          | 28/200000 [03:54<399:28:06,  7.19s/it, loss=0.3069, lr=1.35e-07, step=27]Training:   0%|          | 28/200000 [03:54<399:28:06,  7.19s/it, loss=0.2564, lr=1.40e-07, step=28]Training:   0%|          | 29/200000 [04:01<399:31:03,  7.19s/it, loss=0.2564, lr=1.40e-07, step=28]Training:   0%|          | 29/200000 [04:01<399:31:03,  7.19s/it, loss=0.2785, lr=1.45e-07, step=29]Training:   0%|          | 30/200000 [04:08<399:46:21,  7.20s/it, loss=0.2785, lr=1.45e-07, step=29]Training:   0%|          | 30/200000 [04:08<399:46:21,  7.20s/it, loss=0.2385, lr=1.50e-07, step=30]Training:   0%|          | 31/200000 [04:15<399:16:52,  7.19s/it, loss=0.2385, lr=1.50e-07, step=30]Training:   0%|          | 31/200000 [04:15<399:16:52,  7.19s/it, loss=0.2262, lr=1.55e-07, step=31]Training:   0%|          | 32/200000 [04:22<399:36:58,  7.19s/it, loss=0.2262, lr=1.55e-07, step=31]Training:   0%|          | 32/200000 [04:22<399:36:58,  7.19s/it, loss=0.1929, lr=1.60e-07, step=32]Training:   0%|          | 33/200000 [04:29<399:14:06,  7.19s/it, loss=0.1929, lr=1.60e-07, step=32]Training:   0%|          | 33/200000 [04:29<399:14:06,  7.19s/it, loss=0.2334, lr=1.65e-07, step=33]Training:   0%|          | 34/200000 [04:37<399:04:43,  7.18s/it, loss=0.2334, lr=1.65e-07, step=33]Training:   0%|          | 34/200000 [04:37<399:04:43,  7.18s/it, loss=0.2491, lr=1.70e-07, step=34]Training:   0%|          | 35/200000 [04:44<399:00:12,  7.18s/it, loss=0.2491, lr=1.70e-07, step=34]Training:   0%|          | 35/200000 [04:44<399:00:12,  7.18s/it, loss=0.2068, lr=1.75e-07, step=35]Training:   0%|          | 36/200000 [04:51<399:15:38,  7.19s/it, loss=0.2068, lr=1.75e-07, step=35]Training:   0%|          | 36/200000 [04:51<399:15:38,  7.19s/it, loss=0.2683, lr=1.80e-07, step=36]Training:   0%|          | 37/200000 [04:58<398:46:03,  7.18s/it, loss=0.2683, lr=1.80e-07, step=36]Training:   0%|          | 37/200000 [04:58<398:46:03,  7.18s/it, loss=0.2548, lr=1.85e-07, step=37]Training:   0%|          | 38/200000 [05:05<398:58:27,  7.18s/it, loss=0.2548, lr=1.85e-07, step=37]Training:   0%|          | 38/200000 [05:05<398:58:27,  7.18s/it, loss=0.2052, lr=1.90e-07, step=38]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 39/200000 [05:13<398:57:43,  7.18s/it, loss=0.2052, lr=1.90e-07, step=38]Training:   0%|          | 39/200000 [05:13<398:57:43,  7.18s/it, loss=0.2628, lr=1.95e-07, step=39]Training:   0%|          | 40/200000 [05:20<398:50:21,  7.18s/it, loss=0.2628, lr=1.95e-07, step=39]Training:   0%|          | 40/200000 [05:20<398:50:21,  7.18s/it, loss=0.3181, lr=2.00e-07, step=40]Training:   0%|          | 41/200000 [05:27<398:44:58,  7.18s/it, loss=0.3181, lr=2.00e-07, step=40]Training:   0%|          | 41/200000 [05:27<398:44:58,  7.18s/it, loss=0.2835, lr=2.05e-07, step=41]Training:   0%|          | 42/200000 [05:34<399:19:55,  7.19s/it, loss=0.2835, lr=2.05e-07, step=41]Training:   0%|          | 42/200000 [05:34<399:19:55,  7.19s/it, loss=0.2236, lr=2.10e-07, step=42]Training:   0%|          | 43/200000 [05:41<398:48:54,  7.18s/it, loss=0.2236, lr=2.10e-07, step=42]Training:   0%|          | 43/200000 [05:41<398:48:54,  7.18s/it, loss=0.2032, lr=2.15e-07, step=43]Training:   0%|          | 44/200000 [05:48<399:10:43,  7.19s/it, loss=0.2032, lr=2.15e-07, step=43]Training:   0%|          | 44/200000 [05:48<399:10:43,  7.19s/it, loss=0.2413, lr=2.20e-07, step=44]Training:   0%|          | 45/200000 [05:56<398:49:26,  7.18s/it, loss=0.2413, lr=2.20e-07, step=44]Training:   0%|          | 45/200000 [05:56<398:49:26,  7.18s/it, loss=0.2388, lr=2.25e-07, step=45]Training:   0%|          | 46/200000 [06:03<399:09:44,  7.19s/it, loss=0.2388, lr=2.25e-07, step=45]Training:   0%|          | 46/200000 [06:03<399:09:44,  7.19s/it, loss=0.2985, lr=2.30e-07, step=46]Training:   0%|          | 47/200000 [06:10<398:46:51,  7.18s/it, loss=0.2985, lr=2.30e-07, step=46]Training:   0%|          | 47/200000 [06:10<398:46:51,  7.18s/it, loss=0.2130, lr=2.35e-07, step=47]Training:   0%|          | 48/200000 [06:17<401:15:20,  7.22s/it, loss=0.2130, lr=2.35e-07, step=47]Training:   0%|          | 48/200000 [06:17<401:15:20,  7.22s/it, loss=0.2734, lr=2.40e-07, step=48]Training:   0%|          | 49/200000 [06:25<400:12:51,  7.21s/it, loss=0.2734, lr=2.40e-07, step=48]Training:   0%|          | 49/200000 [06:25<400:12:51,  7.21s/it, loss=0.1249, lr=2.45e-07, step=49]Training:   0%|          | 50/200000 [06:32<400:10:08,  7.20s/it, loss=0.1249, lr=2.45e-07, step=49]Training:   0%|          | 50/200000 [06:32<400:10:08,  7.20s/it, loss=0.2610, lr=2.50e-07, step=50]Training:   0%|          | 51/200000 [06:39<399:34:43,  7.19s/it, loss=0.2610, lr=2.50e-07, step=50]Training:   0%|          | 51/200000 [06:39<399:34:43,  7.19s/it, loss=0.2262, lr=2.55e-07, step=51]Training:   0%|          | 52/200000 [06:46<399:17:22,  7.19s/it, loss=0.2262, lr=2.55e-07, step=51]Training:   0%|          | 52/200000 [06:46<399:17:22,  7.19s/it, loss=0.2950, lr=2.60e-07, step=52]Training:   0%|          | 53/200000 [06:53<398:50:14,  7.18s/it, loss=0.2950, lr=2.60e-07, step=52]Training:   0%|          | 53/200000 [06:53<398:50:14,  7.18s/it, loss=0.1939, lr=2.65e-07, step=53]Training:   0%|          | 54/200000 [07:00<399:33:32,  7.19s/it, loss=0.1939, lr=2.65e-07, step=53]Training:   0%|          | 54/200000 [07:00<399:33:32,  7.19s/it, loss=0.2379, lr=2.70e-07, step=54]Training:   0%|          | 55/200000 [07:08<398:55:49,  7.18s/it, loss=0.2379, lr=2.70e-07, step=54]Training:   0%|          | 55/200000 [07:08<398:55:49,  7.18s/it, loss=0.2967, lr=2.75e-07, step=55]Training:   0%|          | 56/200000 [07:15<398:38:51,  7.18s/it, loss=0.2967, lr=2.75e-07, step=55]Training:   0%|          | 56/200000 [07:15<398:38:51,  7.18s/it, loss=0.2214, lr=2.80e-07, step=56]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 57/200000 [07:22<398:16:07,  7.17s/it, loss=0.2214, lr=2.80e-07, step=56]Training:   0%|          | 57/200000 [07:22<398:16:07,  7.17s/it, loss=0.2633, lr=2.85e-07, step=57]Training:   0%|          | 58/200000 [07:29<398:19:59,  7.17s/it, loss=0.2633, lr=2.85e-07, step=57]Training:   0%|          | 58/200000 [07:29<398:19:59,  7.17s/it, loss=0.1994, lr=2.90e-07, step=58]Training:   0%|          | 59/200000 [07:36<398:26:14,  7.17s/it, loss=0.1994, lr=2.90e-07, step=58]Training:   0%|          | 59/200000 [07:36<398:26:14,  7.17s/it, loss=0.3134, lr=2.95e-07, step=59]Training:   0%|          | 60/200000 [07:43<398:33:57,  7.18s/it, loss=0.3134, lr=2.95e-07, step=59]Training:   0%|          | 60/200000 [07:43<398:33:57,  7.18s/it, loss=0.3464, lr=3.00e-07, step=60]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 61/200000 [07:51<398:44:08,  7.18s/it, loss=0.3464, lr=3.00e-07, step=60]Training:   0%|          | 61/200000 [07:51<398:44:08,  7.18s/it, loss=0.2212, lr=3.05e-07, step=61]Training:   0%|          | 62/200000 [07:58<398:30:46,  7.18s/it, loss=0.2212, lr=3.05e-07, step=61]Training:   0%|          | 62/200000 [07:58<398:30:46,  7.18s/it, loss=0.2666, lr=3.10e-07, step=62]Training:   0%|          | 63/200000 [08:05<398:47:52,  7.18s/it, loss=0.2666, lr=3.10e-07, step=62]Training:   0%|          | 63/200000 [08:05<398:47:52,  7.18s/it, loss=0.1680, lr=3.15e-07, step=63]Training:   0%|          | 64/200000 [08:12<398:39:26,  7.18s/it, loss=0.1680, lr=3.15e-07, step=63]Training:   0%|          | 64/200000 [08:12<398:39:26,  7.18s/it, loss=0.2896, lr=3.20e-07, step=64]Training:   0%|          | 65/200000 [08:19<398:36:14,  7.18s/it, loss=0.2896, lr=3.20e-07, step=64]Training:   0%|          | 65/200000 [08:19<398:36:14,  7.18s/it, loss=0.2569, lr=3.25e-07, step=65]Training:   0%|          | 66/200000 [08:27<398:43:58,  7.18s/it, loss=0.2569, lr=3.25e-07, step=65]Training:   0%|          | 66/200000 [08:27<398:43:58,  7.18s/it, loss=0.2532, lr=3.30e-07, step=66]Training:   0%|          | 67/200000 [08:34<398:53:09,  7.18s/it, loss=0.2532, lr=3.30e-07, step=66]Training:   0%|          | 67/200000 [08:34<398:53:09,  7.18s/it, loss=0.2485, lr=3.35e-07, step=67]Training:   0%|          | 68/200000 [08:41<398:50:30,  7.18s/it, loss=0.2485, lr=3.35e-07, step=67]Training:   0%|          | 68/200000 [08:41<398:50:30,  7.18s/it, loss=0.2602, lr=3.40e-07, step=68]Training:   0%|          | 69/200000 [08:48<400:30:41,  7.21s/it, loss=0.2602, lr=3.40e-07, step=68]Training:   0%|          | 69/200000 [08:48<400:30:41,  7.21s/it, loss=0.2460, lr=3.45e-07, step=69]Training:   0%|          | 70/200000 [08:55<400:00:29,  7.20s/it, loss=0.2460, lr=3.45e-07, step=69]Training:   0%|          | 70/200000 [08:55<400:00:29,  7.20s/it, loss=0.2147, lr=3.50e-07, step=70]Training:   0%|          | 71/200000 [09:03<399:42:52,  7.20s/it, loss=0.2147, lr=3.50e-07, step=70]Training:   0%|          | 71/200000 [09:03<399:42:52,  7.20s/it, loss=0.2574, lr=3.55e-07, step=71]Training:   0%|          | 72/200000 [09:10<399:39:57,  7.20s/it, loss=0.2574, lr=3.55e-07, step=71]Training:   0%|          | 72/200000 [09:10<399:39:57,  7.20s/it, loss=0.2382, lr=3.60e-07, step=72]Training:   0%|          | 73/200000 [09:17<399:03:32,  7.19s/it, loss=0.2382, lr=3.60e-07, step=72]Training:   0%|          | 73/200000 [09:17<399:03:32,  7.19s/it, loss=0.1831, lr=3.65e-07, step=73]Training:   0%|          | 74/200000 [09:24<398:28:14,  7.18s/it, loss=0.1831, lr=3.65e-07, step=73]Training:   0%|          | 74/200000 [09:24<398:28:14,  7.18s/it, loss=0.2682, lr=3.70e-07, step=74]Training:   0%|          | 75/200000 [09:31<398:24:01,  7.17s/it, loss=0.2682, lr=3.70e-07, step=74]Training:   0%|          | 75/200000 [09:31<398:24:01,  7.17s/it, loss=0.3678, lr=3.75e-07, step=75]Training:   0%|          | 76/200000 [09:38<398:31:41,  7.18s/it, loss=0.3678, lr=3.75e-07, step=75]Training:   0%|          | 76/200000 [09:38<398:31:41,  7.18s/it, loss=0.2357, lr=3.80e-07, step=76]Training:   0%|          | 77/200000 [09:46<398:52:40,  7.18s/it, loss=0.2357, lr=3.80e-07, step=76]Training:   0%|          | 77/200000 [09:46<398:52:40,  7.18s/it, loss=0.2365, lr=3.85e-07, step=77]Training:   0%|          | 78/200000 [09:53<398:50:06,  7.18s/it, loss=0.2365, lr=3.85e-07, step=77]Training:   0%|          | 78/200000 [09:53<398:50:06,  7.18s/it, loss=0.2080, lr=3.90e-07, step=78]Training:   0%|          | 79/200000 [10:00<399:16:55,  7.19s/it, loss=0.2080, lr=3.90e-07, step=78]Training:   0%|          | 79/200000 [10:00<399:16:55,  7.19s/it, loss=0.2569, lr=3.95e-07, step=79]Training:   0%|          | 80/200000 [10:07<399:19:04,  7.19s/it, loss=0.2569, lr=3.95e-07, step=79]Training:   0%|          | 80/200000 [10:07<399:19:04,  7.19s/it, loss=0.2672, lr=4.00e-07, step=80]Training:   0%|          | 81/200000 [10:14<398:58:29,  7.18s/it, loss=0.2672, lr=4.00e-07, step=80]Training:   0%|          | 81/200000 [10:14<398:58:29,  7.18s/it, loss=0.2606, lr=4.05e-07, step=81]Training:   0%|          | 82/200000 [10:22<399:10:57,  7.19s/it, loss=0.2606, lr=4.05e-07, step=81]Training:   0%|          | 82/200000 [10:22<399:10:57,  7.19s/it, loss=0.3312, lr=4.10e-07, step=82]Training:   0%|          | 83/200000 [10:29<398:41:48,  7.18s/it, loss=0.3312, lr=4.10e-07, step=82]Training:   0%|          | 83/200000 [10:29<398:41:48,  7.18s/it, loss=0.1924, lr=4.15e-07, step=83]Training:   0%|          | 84/200000 [10:36<398:53:47,  7.18s/it, loss=0.1924, lr=4.15e-07, step=83]Training:   0%|          | 84/200000 [10:36<398:53:47,  7.18s/it, loss=0.1714, lr=4.20e-07, step=84]Training:   0%|          | 85/200000 [10:43<398:24:53,  7.17s/it, loss=0.1714, lr=4.20e-07, step=84]Training:   0%|          | 85/200000 [10:43<398:24:53,  7.17s/it, loss=0.2592, lr=4.25e-07, step=85]Training:   0%|          | 86/200000 [10:50<398:49:40,  7.18s/it, loss=0.2592, lr=4.25e-07, step=85]Training:   0%|          | 86/200000 [10:50<398:49:40,  7.18s/it, loss=0.2965, lr=4.30e-07, step=86]Training:   0%|          | 87/200000 [10:57<398:46:21,  7.18s/it, loss=0.2965, lr=4.30e-07, step=86]Training:   0%|          | 87/200000 [10:57<398:46:21,  7.18s/it, loss=0.2994, lr=4.35e-07, step=87]Training:   0%|          | 88/200000 [11:05<398:46:07,  7.18s/it, loss=0.2994, lr=4.35e-07, step=87]Training:   0%|          | 88/200000 [11:05<398:46:07,  7.18s/it, loss=0.2183, lr=4.40e-07, step=88]Training:   0%|          | 89/200000 [11:12<399:13:48,  7.19s/it, loss=0.2183, lr=4.40e-07, step=88]Training:   0%|          | 89/200000 [11:12<399:13:48,  7.19s/it, loss=0.2052, lr=4.45e-07, step=89]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 90/200000 [11:19<399:38:29,  7.20s/it, loss=0.2052, lr=4.45e-07, step=89]Training:   0%|          | 90/200000 [11:19<399:38:29,  7.20s/it, loss=0.3180, lr=4.50e-07, step=90]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 91/200000 [11:26<399:54:54,  7.20s/it, loss=0.3180, lr=4.50e-07, step=90]Training:   0%|          | 91/200000 [11:26<399:54:54,  7.20s/it, loss=0.2383, lr=4.55e-07, step=91]Training:   0%|          | 92/200000 [11:33<399:17:12,  7.19s/it, loss=0.2383, lr=4.55e-07, step=91]Training:   0%|          | 92/200000 [11:33<399:17:12,  7.19s/it, loss=0.3269, lr=4.60e-07, step=92]Training:   0%|          | 93/200000 [11:41<398:43:04,  7.18s/it, loss=0.3269, lr=4.60e-07, step=92]Training:   0%|          | 93/200000 [11:41<398:43:04,  7.18s/it, loss=0.1937, lr=4.65e-07, step=93]Training:   0%|          | 94/200000 [11:48<397:29:49,  7.16s/it, loss=0.1937, lr=4.65e-07, step=93]Training:   0%|          | 94/200000 [11:48<397:29:49,  7.16s/it, loss=0.2653, lr=4.70e-07, step=94]Training:   0%|          | 95/200000 [11:55<398:02:50,  7.17s/it, loss=0.2653, lr=4.70e-07, step=94]Training:   0%|          | 95/200000 [11:55<398:02:50,  7.17s/it, loss=0.2013, lr=4.75e-07, step=95]Training:   0%|          | 96/200000 [12:02<397:48:09,  7.16s/it, loss=0.2013, lr=4.75e-07, step=95]Training:   0%|          | 96/200000 [12:02<397:48:09,  7.16s/it, loss=0.2442, lr=4.80e-07, step=96]Training:   0%|          | 97/200000 [12:09<397:47:36,  7.16s/it, loss=0.2442, lr=4.80e-07, step=96]Training:   0%|          | 97/200000 [12:09<397:47:36,  7.16s/it, loss=0.2474, lr=4.85e-07, step=97]Training:   0%|          | 98/200000 [12:16<397:59:32,  7.17s/it, loss=0.2474, lr=4.85e-07, step=97]Training:   0%|          | 98/200000 [12:16<397:59:32,  7.17s/it, loss=0.2471, lr=4.90e-07, step=98]Training:   0%|          | 99/200000 [12:24<398:06:55,  7.17s/it, loss=0.2471, lr=4.90e-07, step=98]Training:   0%|          | 99/200000 [12:24<398:06:55,  7.17s/it, loss=0.2404, lr=4.95e-07, step=99]Training:   0%|          | 100/200000 [12:31<398:14:38,  7.17s/it, loss=0.2404, lr=4.95e-07, step=99]Training:   0%|          | 100/200000 [12:31<398:14:38,  7.17s/it, loss=0.2674, lr=5.00e-07, step=100]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
20:09:06.718 [I] step=100 loss=0.2531 lr=2.57e-07 grad_norm=4.09 time=718.8s                      (486094:train_pytorch.py:582)
Training:   0%|          | 101/200000 [12:38<398:52:29,  7.18s/it, loss=0.2674, lr=5.00e-07, step=100]Training:   0%|          | 101/200000 [12:38<398:52:29,  7.18s/it, loss=0.2632, lr=5.05e-07, step=101]Training:   0%|          | 102/200000 [12:45<398:37:14,  7.18s/it, loss=0.2632, lr=5.05e-07, step=101]Training:   0%|          | 102/200000 [12:45<398:37:14,  7.18s/it, loss=0.2708, lr=5.10e-07, step=102]Training:   0%|          | 103/200000 [12:52<398:41:59,  7.18s/it, loss=0.2708, lr=5.10e-07, step=102]Training:   0%|          | 103/200000 [12:52<398:41:59,  7.18s/it, loss=0.2249, lr=5.15e-07, step=103]Training:   0%|          | 104/200000 [12:59<398:31:06,  7.18s/it, loss=0.2249, lr=5.15e-07, step=103]Training:   0%|          | 104/200000 [12:59<398:31:06,  7.18s/it, loss=0.2171, lr=5.20e-07, step=104]Training:   0%|          | 105/200000 [13:07<398:27:34,  7.18s/it, loss=0.2171, lr=5.20e-07, step=104]Training:   0%|          | 105/200000 [13:07<398:27:34,  7.18s/it, loss=0.2858, lr=5.25e-07, step=105]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 106/200000 [13:14<398:38:59,  7.18s/it, loss=0.2858, lr=5.25e-07, step=105]Training:   0%|          | 106/200000 [13:14<398:38:59,  7.18s/it, loss=0.1976, lr=5.30e-07, step=106]Training:   0%|          | 107/200000 [13:21<399:25:36,  7.19s/it, loss=0.1976, lr=5.30e-07, step=106]Training:   0%|          | 107/200000 [13:21<399:25:36,  7.19s/it, loss=0.2495, lr=5.35e-07, step=107]Training:   0%|          | 108/200000 [13:28<399:35:15,  7.20s/it, loss=0.2495, lr=5.35e-07, step=107]Training:   0%|          | 108/200000 [13:28<399:35:15,  7.20s/it, loss=0.2607, lr=5.40e-07, step=108]Training:   0%|          | 109/200000 [13:35<399:17:36,  7.19s/it, loss=0.2607, lr=5.40e-07, step=108]Training:   0%|          | 109/200000 [13:35<399:17:36,  7.19s/it, loss=0.2088, lr=5.45e-07, step=109]Training:   0%|          | 110/200000 [13:43<399:32:43,  7.20s/it, loss=0.2088, lr=5.45e-07, step=109]Training:   0%|          | 110/200000 [13:43<399:32:43,  7.20s/it, loss=0.2425, lr=5.50e-07, step=110]Training:   0%|          | 111/200000 [13:50<401:45:33,  7.24s/it, loss=0.2425, lr=5.50e-07, step=110]Training:   0%|          | 111/200000 [13:50<401:45:33,  7.24s/it, loss=0.1999, lr=5.55e-07, step=111]Training:   0%|          | 112/200000 [13:57<400:51:52,  7.22s/it, loss=0.1999, lr=5.55e-07, step=111]Training:   0%|          | 112/200000 [13:57<400:51:52,  7.22s/it, loss=0.2446, lr=5.60e-07, step=112]Training:   0%|          | 113/200000 [14:04<399:52:48,  7.20s/it, loss=0.2446, lr=5.60e-07, step=112]Training:   0%|          | 113/200000 [14:04<399:52:48,  7.20s/it, loss=0.2755, lr=5.65e-07, step=113]Training:   0%|          | 114/200000 [14:11<399:57:31,  7.20s/it, loss=0.2755, lr=5.65e-07, step=113]Training:   0%|          | 114/200000 [14:11<399:57:31,  7.20s/it, loss=0.1984, lr=5.70e-07, step=114]Training:   0%|          | 115/200000 [14:19<399:39:19,  7.20s/it, loss=0.1984, lr=5.70e-07, step=114]Training:   0%|          | 115/200000 [14:19<399:39:19,  7.20s/it, loss=0.2131, lr=5.75e-07, step=115]Training:   0%|          | 116/200000 [14:26<399:33:43,  7.20s/it, loss=0.2131, lr=5.75e-07, step=115]Training:   0%|          | 116/200000 [14:26<399:33:43,  7.20s/it, loss=0.2507, lr=5.80e-07, step=116]Training:   0%|          | 117/200000 [14:33<399:34:44,  7.20s/it, loss=0.2507, lr=5.80e-07, step=116]Training:   0%|          | 117/200000 [14:33<399:34:44,  7.20s/it, loss=0.1971, lr=5.85e-07, step=117]Training:   0%|          | 118/200000 [14:40<399:12:02,  7.19s/it, loss=0.1971, lr=5.85e-07, step=117]Training:   0%|          | 118/200000 [14:40<399:12:02,  7.19s/it, loss=0.3400, lr=5.90e-07, step=118]Training:   0%|          | 119/200000 [14:47<399:34:21,  7.20s/it, loss=0.3400, lr=5.90e-07, step=118]Training:   0%|          | 119/200000 [14:47<399:34:21,  7.20s/it, loss=0.2065, lr=5.95e-07, step=119]Training:   0%|          | 120/200000 [14:55<399:22:56,  7.19s/it, loss=0.2065, lr=5.95e-07, step=119]Training:   0%|          | 120/200000 [14:55<399:22:56,  7.19s/it, loss=0.2054, lr=6.00e-07, step=120]Training:   0%|          | 121/200000 [15:02<399:03:46,  7.19s/it, loss=0.2054, lr=6.00e-07, step=120]Training:   0%|          | 121/200000 [15:02<399:03:46,  7.19s/it, loss=0.2589, lr=6.05e-07, step=121]Training:   0%|          | 122/200000 [15:09<399:26:16,  7.19s/it, loss=0.2589, lr=6.05e-07, step=121]Training:   0%|          | 122/200000 [15:09<399:26:16,  7.19s/it, loss=0.2465, lr=6.10e-07, step=122]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 123/200000 [15:16<399:37:28,  7.20s/it, loss=0.2465, lr=6.10e-07, step=122]Training:   0%|          | 123/200000 [15:16<399:37:28,  7.20s/it, loss=0.2825, lr=6.15e-07, step=123]Training:   0%|          | 124/200000 [15:23<399:59:25,  7.20s/it, loss=0.2825, lr=6.15e-07, step=123]Training:   0%|          | 124/200000 [15:23<399:59:25,  7.20s/it, loss=0.2715, lr=6.20e-07, step=124]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 125/200000 [15:31<399:17:52,  7.19s/it, loss=0.2715, lr=6.20e-07, step=124]Training:   0%|          | 125/200000 [15:31<399:17:52,  7.19s/it, loss=0.1986, lr=6.25e-07, step=125]Training:   0%|          | 126/200000 [15:38<399:36:46,  7.20s/it, loss=0.1986, lr=6.25e-07, step=125]Training:   0%|          | 126/200000 [15:38<399:36:46,  7.20s/it, loss=0.2880, lr=6.30e-07, step=126]Training:   0%|          | 127/200000 [15:45<399:17:41,  7.19s/it, loss=0.2880, lr=6.30e-07, step=126]Training:   0%|          | 127/200000 [15:45<399:17:41,  7.19s/it, loss=0.2294, lr=6.35e-07, step=127]Training:   0%|          | 128/200000 [15:52<398:57:04,  7.19s/it, loss=0.2294, lr=6.35e-07, step=127]Training:   0%|          | 128/200000 [15:52<398:57:04,  7.19s/it, loss=0.3074, lr=6.40e-07, step=128]Training:   0%|          | 129/200000 [15:59<398:33:33,  7.18s/it, loss=0.3074, lr=6.40e-07, step=128]Training:   0%|          | 129/200000 [15:59<398:33:33,  7.18s/it, loss=0.2215, lr=6.45e-07, step=129]Training:   0%|          | 130/200000 [16:07<398:37:07,  7.18s/it, loss=0.2215, lr=6.45e-07, step=129]Training:   0%|          | 130/200000 [16:07<398:37:07,  7.18s/it, loss=0.3156, lr=6.50e-07, step=130]Training:   0%|          | 131/200000 [16:14<398:48:05,  7.18s/it, loss=0.3156, lr=6.50e-07, step=130]Training:   0%|          | 131/200000 [16:14<398:48:05,  7.18s/it, loss=0.2273, lr=6.55e-07, step=131]Training:   0%|          | 132/200000 [16:21<400:49:35,  7.22s/it, loss=0.2273, lr=6.55e-07, step=131]Training:   0%|          | 132/200000 [16:21<400:49:35,  7.22s/it, loss=0.2701, lr=6.60e-07, step=132]Training:   0%|          | 133/200000 [16:28<400:03:35,  7.21s/it, loss=0.2701, lr=6.60e-07, step=132]Training:   0%|          | 133/200000 [16:28<400:03:35,  7.21s/it, loss=0.2683, lr=6.65e-07, step=133]Training:   0%|          | 134/200000 [16:35<399:46:40,  7.20s/it, loss=0.2683, lr=6.65e-07, step=133]Training:   0%|          | 134/200000 [16:35<399:46:40,  7.20s/it, loss=0.2046, lr=6.70e-07, step=134]Training:   0%|          | 135/200000 [16:43<399:20:53,  7.19s/it, loss=0.2046, lr=6.70e-07, step=134]Training:   0%|          | 135/200000 [16:43<399:20:53,  7.19s/it, loss=0.2464, lr=6.75e-07, step=135]Training:   0%|          | 136/200000 [16:50<398:58:45,  7.19s/it, loss=0.2464, lr=6.75e-07, step=135]Training:   0%|          | 136/200000 [16:50<398:58:45,  7.19s/it, loss=0.2254, lr=6.80e-07, step=136]Training:   0%|          | 137/200000 [16:57<399:06:10,  7.19s/it, loss=0.2254, lr=6.80e-07, step=136]Training:   0%|          | 137/200000 [16:57<399:06:10,  7.19s/it, loss=0.2178, lr=6.85e-07, step=137]Training:   0%|          | 138/200000 [17:04<398:54:52,  7.19s/it, loss=0.2178, lr=6.85e-07, step=137]Training:   0%|          | 138/200000 [17:04<398:54:52,  7.19s/it, loss=0.2222, lr=6.90e-07, step=138]Training:   0%|          | 139/200000 [17:11<398:56:32,  7.19s/it, loss=0.2222, lr=6.90e-07, step=138]Training:   0%|          | 139/200000 [17:11<398:56:32,  7.19s/it, loss=0.1999, lr=6.95e-07, step=139]Training:   0%|          | 140/200000 [17:18<399:01:28,  7.19s/it, loss=0.1999, lr=6.95e-07, step=139]Training:   0%|          | 140/200000 [17:18<399:01:28,  7.19s/it, loss=0.2553, lr=7.00e-07, step=140]Training:   0%|          | 141/200000 [17:26<399:05:43,  7.19s/it, loss=0.2553, lr=7.00e-07, step=140]Training:   0%|          | 141/200000 [17:26<399:05:43,  7.19s/it, loss=0.2481, lr=7.05e-07, step=141]Training:   0%|          | 142/200000 [17:33<398:54:22,  7.19s/it, loss=0.2481, lr=7.05e-07, step=141]Training:   0%|          | 142/200000 [17:33<398:54:22,  7.19s/it, loss=0.2449, lr=7.10e-07, step=142]Training:   0%|          | 143/200000 [17:40<398:57:11,  7.19s/it, loss=0.2449, lr=7.10e-07, step=142]Training:   0%|          | 143/200000 [17:40<398:57:11,  7.19s/it, loss=0.1709, lr=7.15e-07, step=143]Training:   0%|          | 144/200000 [17:47<398:43:28,  7.18s/it, loss=0.1709, lr=7.15e-07, step=143]Training:   0%|          | 144/200000 [17:47<398:43:28,  7.18s/it, loss=0.1621, lr=7.20e-07, step=144]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 145/200000 [17:54<398:52:16,  7.18s/it, loss=0.1621, lr=7.20e-07, step=144]Training:   0%|          | 145/200000 [17:54<398:52:16,  7.18s/it, loss=0.1804, lr=7.25e-07, step=145]Training:   0%|          | 146/200000 [18:02<398:23:09,  7.18s/it, loss=0.1804, lr=7.25e-07, step=145]Training:   0%|          | 146/200000 [18:02<398:23:09,  7.18s/it, loss=0.2181, lr=7.30e-07, step=146]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 147/200000 [18:09<398:17:41,  7.17s/it, loss=0.2181, lr=7.30e-07, step=146]Training:   0%|          | 147/200000 [18:09<398:17:41,  7.17s/it, loss=0.2530, lr=7.35e-07, step=147]Training:   0%|          | 148/200000 [18:16<398:50:56,  7.18s/it, loss=0.2530, lr=7.35e-07, step=147]Training:   0%|          | 148/200000 [18:16<398:50:56,  7.18s/it, loss=0.2362, lr=7.40e-07, step=148]Training:   0%|          | 149/200000 [18:23<398:44:09,  7.18s/it, loss=0.2362, lr=7.40e-07, step=148]Training:   0%|          | 149/200000 [18:23<398:44:09,  7.18s/it, loss=0.1585, lr=7.45e-07, step=149]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 150/200000 [18:30<398:43:08,  7.18s/it, loss=0.1585, lr=7.45e-07, step=149]Training:   0%|          | 150/200000 [18:30<398:43:08,  7.18s/it, loss=0.2250, lr=7.50e-07, step=150]Training:   0%|          | 151/200000 [18:37<398:48:58,  7.18s/it, loss=0.2250, lr=7.50e-07, step=150]Training:   0%|          | 151/200000 [18:37<398:48:58,  7.18s/it, loss=0.2754, lr=7.55e-07, step=151]Training:   0%|          | 152/200000 [18:45<398:39:37,  7.18s/it, loss=0.2754, lr=7.55e-07, step=151]Training:   0%|          | 152/200000 [18:45<398:39:37,  7.18s/it, loss=0.2236, lr=7.60e-07, step=152]Training:   0%|          | 153/200000 [18:52<399:25:39,  7.20s/it, loss=0.2236, lr=7.60e-07, step=152]Training:   0%|          | 153/200000 [18:52<399:25:39,  7.20s/it, loss=0.2241, lr=7.65e-07, step=153]Training:   0%|          | 154/200000 [18:59<399:00:57,  7.19s/it, loss=0.2241, lr=7.65e-07, step=153]Training:   0%|          | 154/200000 [18:59<399:00:57,  7.19s/it, loss=0.1895, lr=7.70e-07, step=154]Training:   0%|          | 155/200000 [19:06<399:08:34,  7.19s/it, loss=0.1895, lr=7.70e-07, step=154]Training:   0%|          | 155/200000 [19:06<399:08:34,  7.19s/it, loss=0.2082, lr=7.75e-07, step=155]Training:   0%|          | 156/200000 [19:13<399:19:40,  7.19s/it, loss=0.2082, lr=7.75e-07, step=155]Training:   0%|          | 156/200000 [19:13<399:19:40,  7.19s/it, loss=0.1590, lr=7.80e-07, step=156]Training:   0%|          | 157/200000 [19:21<399:18:56,  7.19s/it, loss=0.1590, lr=7.80e-07, step=156]Training:   0%|          | 157/200000 [19:21<399:18:56,  7.19s/it, loss=0.2333, lr=7.85e-07, step=157]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 158/200000 [19:28<398:42:19,  7.18s/it, loss=0.2333, lr=7.85e-07, step=157]Training:   0%|          | 158/200000 [19:28<398:42:19,  7.18s/it, loss=0.2146, lr=7.90e-07, step=158]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 159/200000 [19:35<398:12:41,  7.17s/it, loss=0.2146, lr=7.90e-07, step=158]Training:   0%|          | 159/200000 [19:35<398:12:41,  7.17s/it, loss=0.2937, lr=7.95e-07, step=159]Training:   0%|          | 160/200000 [19:42<398:12:39,  7.17s/it, loss=0.2937, lr=7.95e-07, step=159]Training:   0%|          | 160/200000 [19:42<398:12:39,  7.17s/it, loss=0.2080, lr=8.00e-07, step=160]Training:   0%|          | 161/200000 [19:49<398:17:15,  7.17s/it, loss=0.2080, lr=8.00e-07, step=160]Training:   0%|          | 161/200000 [19:49<398:17:15,  7.17s/it, loss=0.2431, lr=8.05e-07, step=161]Training:   0%|          | 162/200000 [19:56<398:08:08,  7.17s/it, loss=0.2431, lr=8.05e-07, step=161]Training:   0%|          | 162/200000 [19:56<398:08:08,  7.17s/it, loss=0.2205, lr=8.10e-07, step=162]Training:   0%|          | 163/200000 [20:04<397:25:27,  7.16s/it, loss=0.2205, lr=8.10e-07, step=162]Training:   0%|          | 163/200000 [20:04<397:25:27,  7.16s/it, loss=0.2129, lr=8.15e-07, step=163]Training:   0%|          | 164/200000 [20:11<397:49:15,  7.17s/it, loss=0.2129, lr=8.15e-07, step=163]Training:   0%|          | 164/200000 [20:11<397:49:15,  7.17s/it, loss=0.2441, lr=8.20e-07, step=164]Training:   0%|          | 165/200000 [20:18<398:20:58,  7.18s/it, loss=0.2441, lr=8.20e-07, step=164]Training:   0%|          | 165/200000 [20:18<398:20:58,  7.18s/it, loss=0.2370, lr=8.25e-07, step=165]Training:   0%|          | 166/200000 [20:25<398:34:53,  7.18s/it, loss=0.2370, lr=8.25e-07, step=165]Training:   0%|          | 166/200000 [20:25<398:34:53,  7.18s/it, loss=0.2834, lr=8.30e-07, step=166]Training:   0%|          | 167/200000 [20:32<398:59:01,  7.19s/it, loss=0.2834, lr=8.30e-07, step=166]Training:   0%|          | 167/200000 [20:32<398:59:01,  7.19s/it, loss=0.1751, lr=8.35e-07, step=167]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 168/200000 [20:40<398:45:41,  7.18s/it, loss=0.1751, lr=8.35e-07, step=167]Training:   0%|          | 168/200000 [20:40<398:45:41,  7.18s/it, loss=0.1909, lr=8.40e-07, step=168]Training:   0%|          | 169/200000 [20:47<398:28:21,  7.18s/it, loss=0.1909, lr=8.40e-07, step=168]Training:   0%|          | 169/200000 [20:47<398:28:21,  7.18s/it, loss=0.1944, lr=8.45e-07, step=169]Training:   0%|          | 170/200000 [20:54<399:00:06,  7.19s/it, loss=0.1944, lr=8.45e-07, step=169]Training:   0%|          | 170/200000 [20:54<399:00:06,  7.19s/it, loss=0.2470, lr=8.50e-07, step=170]Training:   0%|          | 171/200000 [21:01<399:16:42,  7.19s/it, loss=0.2470, lr=8.50e-07, step=170]Training:   0%|          | 171/200000 [21:01<399:16:42,  7.19s/it, loss=0.3590, lr=8.55e-07, step=171]Training:   0%|          | 172/200000 [21:08<399:23:58,  7.20s/it, loss=0.3590, lr=8.55e-07, step=171]Training:   0%|          | 172/200000 [21:08<399:23:58,  7.20s/it, loss=0.1925, lr=8.60e-07, step=172]Training:   0%|          | 173/200000 [21:16<399:24:34,  7.20s/it, loss=0.1925, lr=8.60e-07, step=172]Training:   0%|          | 173/200000 [21:16<399:24:34,  7.20s/it, loss=0.1932, lr=8.65e-07, step=173]Training:   0%|          | 174/200000 [21:23<401:32:32,  7.23s/it, loss=0.1932, lr=8.65e-07, step=173]Training:   0%|          | 174/200000 [21:23<401:32:32,  7.23s/it, loss=0.3019, lr=8.70e-07, step=174]Training:   0%|          | 175/200000 [21:30<400:41:31,  7.22s/it, loss=0.3019, lr=8.70e-07, step=174]Training:   0%|          | 175/200000 [21:30<400:41:31,  7.22s/it, loss=0.2115, lr=8.75e-07, step=175]Training:   0%|          | 176/200000 [21:37<400:38:12,  7.22s/it, loss=0.2115, lr=8.75e-07, step=175]Training:   0%|          | 176/200000 [21:37<400:38:12,  7.22s/it, loss=0.3037, lr=8.80e-07, step=176]Training:   0%|          | 177/200000 [21:44<400:09:25,  7.21s/it, loss=0.3037, lr=8.80e-07, step=176]Training:   0%|          | 177/200000 [21:44<400:09:25,  7.21s/it, loss=0.1804, lr=8.85e-07, step=177]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 178/200000 [21:52<400:08:53,  7.21s/it, loss=0.1804, lr=8.85e-07, step=177]Training:   0%|          | 178/200000 [21:52<400:08:53,  7.21s/it, loss=0.2238, lr=8.90e-07, step=178]Training:   0%|          | 179/200000 [21:59<399:45:09,  7.20s/it, loss=0.2238, lr=8.90e-07, step=178]Training:   0%|          | 179/200000 [21:59<399:45:09,  7.20s/it, loss=0.1913, lr=8.95e-07, step=179]Training:   0%|          | 180/200000 [22:06<399:12:28,  7.19s/it, loss=0.1913, lr=8.95e-07, step=179]Training:   0%|          | 180/200000 [22:06<399:12:28,  7.19s/it, loss=0.1891, lr=9.00e-07, step=180]Training:   0%|          | 181/200000 [22:13<398:50:28,  7.19s/it, loss=0.1891, lr=9.00e-07, step=180]Training:   0%|          | 181/200000 [22:13<398:50:28,  7.19s/it, loss=0.2227, lr=9.05e-07, step=181]Training:   0%|          | 182/200000 [22:20<398:27:16,  7.18s/it, loss=0.2227, lr=9.05e-07, step=181]Training:   0%|          | 182/200000 [22:20<398:27:16,  7.18s/it, loss=0.2226, lr=9.10e-07, step=182]Training:   0%|          | 183/200000 [22:28<398:46:39,  7.18s/it, loss=0.2226, lr=9.10e-07, step=182]Training:   0%|          | 183/200000 [22:28<398:46:39,  7.18s/it, loss=0.2085, lr=9.15e-07, step=183]Training:   0%|          | 184/200000 [22:35<398:49:21,  7.19s/it, loss=0.2085, lr=9.15e-07, step=183]Training:   0%|          | 184/200000 [22:35<398:49:21,  7.19s/it, loss=0.2022, lr=9.20e-07, step=184]Training:   0%|          | 185/200000 [22:42<399:14:06,  7.19s/it, loss=0.2022, lr=9.20e-07, step=184]Training:   0%|          | 185/200000 [22:42<399:14:06,  7.19s/it, loss=0.1780, lr=9.25e-07, step=185]Training:   0%|          | 186/200000 [22:49<399:24:26,  7.20s/it, loss=0.1780, lr=9.25e-07, step=185]Training:   0%|          | 186/200000 [22:49<399:24:26,  7.20s/it, loss=0.1753, lr=9.30e-07, step=186]Training:   0%|          | 187/200000 [22:56<399:24:53,  7.20s/it, loss=0.1753, lr=9.30e-07, step=186]Training:   0%|          | 187/200000 [22:56<399:24:53,  7.20s/it, loss=0.1820, lr=9.35e-07, step=187]Training:   0%|          | 188/200000 [23:04<398:53:06,  7.19s/it, loss=0.1820, lr=9.35e-07, step=187]Training:   0%|          | 188/200000 [23:04<398:53:06,  7.19s/it, loss=0.2306, lr=9.40e-07, step=188]Training:   0%|          | 189/200000 [23:11<399:04:19,  7.19s/it, loss=0.2306, lr=9.40e-07, step=188]Training:   0%|          | 189/200000 [23:11<399:04:19,  7.19s/it, loss=0.1686, lr=9.45e-07, step=189]Training:   0%|          | 190/200000 [23:18<398:29:26,  7.18s/it, loss=0.1686, lr=9.45e-07, step=189]Training:   0%|          | 190/200000 [23:18<398:29:26,  7.18s/it, loss=0.1740, lr=9.50e-07, step=190]Training:   0%|          | 191/200000 [23:25<398:22:41,  7.18s/it, loss=0.1740, lr=9.50e-07, step=190]Training:   0%|          | 191/200000 [23:25<398:22:41,  7.18s/it, loss=0.1879, lr=9.55e-07, step=191]Training:   0%|          | 192/200000 [23:32<398:52:44,  7.19s/it, loss=0.1879, lr=9.55e-07, step=191]Training:   0%|          | 192/200000 [23:32<398:52:44,  7.19s/it, loss=0.1813, lr=9.60e-07, step=192]Training:   0%|          | 193/200000 [23:39<399:09:00,  7.19s/it, loss=0.1813, lr=9.60e-07, step=192]Training:   0%|          | 193/200000 [23:39<399:09:00,  7.19s/it, loss=0.1891, lr=9.65e-07, step=193]Training:   0%|          | 194/200000 [23:47<399:01:20,  7.19s/it, loss=0.1891, lr=9.65e-07, step=193]Training:   0%|          | 194/200000 [23:47<399:01:20,  7.19s/it, loss=0.2500, lr=9.70e-07, step=194]Training:   0%|          | 195/200000 [23:54<400:47:44,  7.22s/it, loss=0.2500, lr=9.70e-07, step=194]Training:   0%|          | 195/200000 [23:54<400:47:44,  7.22s/it, loss=0.2619, lr=9.75e-07, step=195]Training:   0%|          | 196/200000 [24:01<400:32:24,  7.22s/it, loss=0.2619, lr=9.75e-07, step=195]Training:   0%|          | 196/200000 [24:01<400:32:24,  7.22s/it, loss=0.2159, lr=9.80e-07, step=196]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 197/200000 [24:08<400:05:11,  7.21s/it, loss=0.2159, lr=9.80e-07, step=196]Training:   0%|          | 197/200000 [24:08<400:05:11,  7.21s/it, loss=0.1637, lr=9.85e-07, step=197]Training:   0%|          | 198/200000 [24:16<399:59:00,  7.21s/it, loss=0.1637, lr=9.85e-07, step=197]Training:   0%|          | 198/200000 [24:16<399:59:00,  7.21s/it, loss=0.2099, lr=9.90e-07, step=198]Training:   0%|          | 199/200000 [24:23<399:54:57,  7.21s/it, loss=0.2099, lr=9.90e-07, step=198]Training:   0%|          | 199/200000 [24:23<399:54:57,  7.21s/it, loss=0.1887, lr=9.95e-07, step=199]Training:   0%|          | 200/200000 [24:30<399:06:21,  7.19s/it, loss=0.1887, lr=9.95e-07, step=199]Training:   0%|          | 200/200000 [24:30<399:06:21,  7.19s/it, loss=0.2583, lr=1.00e-06, step=200]20:21:05.837 [I] step=200 loss=0.2266 lr=7.57e-07 grad_norm=3.53 time=719.1s                      (486094:train_pytorch.py:582)
Training:   0%|          | 201/200000 [24:37<398:40:21,  7.18s/it, loss=0.2583, lr=1.00e-06, step=200]Training:   0%|          | 201/200000 [24:37<398:40:21,  7.18s/it, loss=0.2222, lr=1.00e-06, step=201]Training:   0%|          | 202/200000 [24:44<397:40:09,  7.17s/it, loss=0.2222, lr=1.00e-06, step=201]Training:   0%|          | 202/200000 [24:44<397:40:09,  7.17s/it, loss=0.1782, lr=1.01e-06, step=202]Training:   0%|          | 203/200000 [24:51<398:05:43,  7.17s/it, loss=0.1782, lr=1.01e-06, step=202]Training:   0%|          | 203/200000 [24:51<398:05:43,  7.17s/it, loss=0.1688, lr=1.01e-06, step=203]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 204/200000 [24:59<398:43:06,  7.18s/it, loss=0.1688, lr=1.01e-06, step=203]Training:   0%|          | 204/200000 [24:59<398:43:06,  7.18s/it, loss=0.2452, lr=1.02e-06, step=204]Training:   0%|          | 205/200000 [25:06<398:27:14,  7.18s/it, loss=0.2452, lr=1.02e-06, step=204]Training:   0%|          | 205/200000 [25:06<398:27:14,  7.18s/it, loss=0.2251, lr=1.02e-06, step=205]Training:   0%|          | 206/200000 [25:13<398:50:06,  7.19s/it, loss=0.2251, lr=1.02e-06, step=205]Training:   0%|          | 206/200000 [25:13<398:50:06,  7.19s/it, loss=0.2241, lr=1.03e-06, step=206]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 207/200000 [25:20<398:42:09,  7.18s/it, loss=0.2241, lr=1.03e-06, step=206]Training:   0%|          | 207/200000 [25:20<398:42:09,  7.18s/it, loss=0.1984, lr=1.03e-06, step=207]Training:   0%|          | 208/200000 [25:27<399:03:59,  7.19s/it, loss=0.1984, lr=1.03e-06, step=207]Training:   0%|          | 208/200000 [25:27<399:03:59,  7.19s/it, loss=0.1896, lr=1.04e-06, step=208]Training:   0%|          | 209/200000 [25:34<398:46:17,  7.19s/it, loss=0.1896, lr=1.04e-06, step=208]Training:   0%|          | 209/200000 [25:34<398:46:17,  7.19s/it, loss=0.2576, lr=1.04e-06, step=209]Training:   0%|          | 210/200000 [25:42<398:57:38,  7.19s/it, loss=0.2576, lr=1.04e-06, step=209]Training:   0%|          | 210/200000 [25:42<398:57:38,  7.19s/it, loss=0.2031, lr=1.05e-06, step=210]Training:   0%|          | 211/200000 [25:49<398:52:29,  7.19s/it, loss=0.2031, lr=1.05e-06, step=210]Training:   0%|          | 211/200000 [25:49<398:52:29,  7.19s/it, loss=0.2601, lr=1.05e-06, step=211]Training:   0%|          | 212/200000 [25:56<398:23:01,  7.18s/it, loss=0.2601, lr=1.05e-06, step=211]Training:   0%|          | 212/200000 [25:56<398:23:01,  7.18s/it, loss=0.2127, lr=1.06e-06, step=212]Training:   0%|          | 213/200000 [26:03<398:20:05,  7.18s/it, loss=0.2127, lr=1.06e-06, step=212]Training:   0%|          | 213/200000 [26:03<398:20:05,  7.18s/it, loss=0.2019, lr=1.06e-06, step=213]Training:   0%|          | 214/200000 [26:10<398:51:17,  7.19s/it, loss=0.2019, lr=1.06e-06, step=213]Training:   0%|          | 214/200000 [26:10<398:51:17,  7.19s/it, loss=0.1884, lr=1.07e-06, step=214]Training:   0%|          | 215/200000 [26:18<398:31:22,  7.18s/it, loss=0.1884, lr=1.07e-06, step=214]Training:   0%|          | 215/200000 [26:18<398:31:22,  7.18s/it, loss=0.1977, lr=1.07e-06, step=215]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 216/200000 [26:25<399:44:08,  7.20s/it, loss=0.1977, lr=1.07e-06, step=215]Training:   0%|          | 216/200000 [26:25<399:44:08,  7.20s/it, loss=0.1637, lr=1.08e-06, step=216]Training:   0%|          | 217/200000 [26:32<399:28:31,  7.20s/it, loss=0.1637, lr=1.08e-06, step=216]Training:   0%|          | 217/200000 [26:32<399:28:31,  7.20s/it, loss=0.2259, lr=1.08e-06, step=217]Training:   0%|          | 218/200000 [26:39<399:03:37,  7.19s/it, loss=0.2259, lr=1.08e-06, step=217]Training:   0%|          | 218/200000 [26:39<399:03:37,  7.19s/it, loss=0.1635, lr=1.09e-06, step=218]Training:   0%|          | 219/200000 [26:46<398:50:48,  7.19s/it, loss=0.1635, lr=1.09e-06, step=218]Training:   0%|          | 219/200000 [26:46<398:50:48,  7.19s/it, loss=0.2188, lr=1.09e-06, step=219]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 220/200000 [26:54<399:03:37,  7.19s/it, loss=0.2188, lr=1.09e-06, step=219]Training:   0%|          | 220/200000 [26:54<399:03:37,  7.19s/it, loss=0.1792, lr=1.10e-06, step=220]Training:   0%|          | 221/200000 [27:01<398:54:12,  7.19s/it, loss=0.1792, lr=1.10e-06, step=220]Training:   0%|          | 221/200000 [27:01<398:54:12,  7.19s/it, loss=0.1588, lr=1.10e-06, step=221]Training:   0%|          | 222/200000 [27:08<398:44:22,  7.19s/it, loss=0.1588, lr=1.10e-06, step=221]Training:   0%|          | 222/200000 [27:08<398:44:22,  7.19s/it, loss=0.2017, lr=1.11e-06, step=222]Training:   0%|          | 223/200000 [27:15<398:53:25,  7.19s/it, loss=0.2017, lr=1.11e-06, step=222]Training:   0%|          | 223/200000 [27:15<398:53:25,  7.19s/it, loss=0.1855, lr=1.11e-06, step=223]Training:   0%|          | 224/200000 [27:22<399:13:13,  7.19s/it, loss=0.1855, lr=1.11e-06, step=223]Training:   0%|          | 224/200000 [27:22<399:13:13,  7.19s/it, loss=0.1463, lr=1.12e-06, step=224]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 225/200000 [27:30<398:53:26,  7.19s/it, loss=0.1463, lr=1.12e-06, step=224]Training:   0%|          | 225/200000 [27:30<398:53:26,  7.19s/it, loss=0.1575, lr=1.12e-06, step=225]Training:   0%|          | 226/200000 [27:37<397:25:42,  7.16s/it, loss=0.1575, lr=1.12e-06, step=225]Training:   0%|          | 226/200000 [27:37<397:25:42,  7.16s/it, loss=0.1564, lr=1.13e-06, step=226]Training:   0%|          | 227/200000 [27:44<397:56:00,  7.17s/it, loss=0.1564, lr=1.13e-06, step=226]Training:   0%|          | 227/200000 [27:44<397:56:00,  7.17s/it, loss=0.3158, lr=1.13e-06, step=227]Training:   0%|          | 228/200000 [27:51<397:43:29,  7.17s/it, loss=0.3158, lr=1.13e-06, step=227]Training:   0%|          | 228/200000 [27:51<397:43:29,  7.17s/it, loss=0.1603, lr=1.14e-06, step=228]Training:   0%|          | 229/200000 [27:58<398:19:09,  7.18s/it, loss=0.1603, lr=1.14e-06, step=228]Training:   0%|          | 229/200000 [27:58<398:19:09,  7.18s/it, loss=0.1145, lr=1.14e-06, step=229]Training:   0%|          | 230/200000 [28:05<398:28:44,  7.18s/it, loss=0.1145, lr=1.14e-06, step=229]Training:   0%|          | 230/200000 [28:05<398:28:44,  7.18s/it, loss=0.1430, lr=1.15e-06, step=230]Training:   0%|          | 231/200000 [28:13<398:10:20,  7.18s/it, loss=0.1430, lr=1.15e-06, step=230]Training:   0%|          | 231/200000 [28:13<398:10:20,  7.18s/it, loss=0.1526, lr=1.15e-06, step=231]Training:   0%|          | 232/200000 [28:20<398:30:23,  7.18s/it, loss=0.1526, lr=1.15e-06, step=231]Training:   0%|          | 232/200000 [28:20<398:30:23,  7.18s/it, loss=0.2105, lr=1.16e-06, step=232]Training:   0%|          | 233/200000 [28:27<398:50:25,  7.19s/it, loss=0.2105, lr=1.16e-06, step=232]Training:   0%|          | 233/200000 [28:27<398:50:25,  7.19s/it, loss=0.1594, lr=1.16e-06, step=233]Training:   0%|          | 234/200000 [28:34<398:49:34,  7.19s/it, loss=0.1594, lr=1.16e-06, step=233]Training:   0%|          | 234/200000 [28:34<398:49:34,  7.19s/it, loss=0.1378, lr=1.17e-06, step=234]Training:   0%|          | 235/200000 [28:41<398:34:46,  7.18s/it, loss=0.1378, lr=1.17e-06, step=234]Training:   0%|          | 235/200000 [28:41<398:34:46,  7.18s/it, loss=0.1976, lr=1.17e-06, step=235]Training:   0%|          | 236/200000 [28:48<398:25:41,  7.18s/it, loss=0.1976, lr=1.17e-06, step=235]Training:   0%|          | 236/200000 [28:48<398:25:41,  7.18s/it, loss=0.1526, lr=1.18e-06, step=236]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 237/200000 [28:56<401:08:46,  7.23s/it, loss=0.1526, lr=1.18e-06, step=236]Training:   0%|          | 237/200000 [28:56<401:08:46,  7.23s/it, loss=0.2187, lr=1.18e-06, step=237]Training:   0%|          | 238/200000 [29:03<400:03:00,  7.21s/it, loss=0.2187, lr=1.18e-06, step=237]Training:   0%|          | 238/200000 [29:03<400:03:00,  7.21s/it, loss=0.1793, lr=1.19e-06, step=238]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 239/200000 [29:10<399:13:59,  7.19s/it, loss=0.1793, lr=1.19e-06, step=238]Training:   0%|          | 239/200000 [29:10<399:13:59,  7.19s/it, loss=0.2239, lr=1.19e-06, step=239]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 240/200000 [29:17<399:24:52,  7.20s/it, loss=0.2239, lr=1.19e-06, step=239]Training:   0%|          | 240/200000 [29:17<399:24:52,  7.20s/it, loss=0.1617, lr=1.20e-06, step=240]Training:   0%|          | 241/200000 [29:24<398:48:45,  7.19s/it, loss=0.1617, lr=1.20e-06, step=240]Training:   0%|          | 241/200000 [29:24<398:48:45,  7.19s/it, loss=0.1957, lr=1.20e-06, step=241]Training:   0%|          | 242/200000 [29:32<398:59:02,  7.19s/it, loss=0.1957, lr=1.20e-06, step=241]Training:   0%|          | 242/200000 [29:32<398:59:02,  7.19s/it, loss=0.1452, lr=1.21e-06, step=242]Training:   0%|          | 243/200000 [29:39<398:34:02,  7.18s/it, loss=0.1452, lr=1.21e-06, step=242]Training:   0%|          | 243/200000 [29:39<398:34:02,  7.18s/it, loss=0.1604, lr=1.21e-06, step=243]Training:   0%|          | 244/200000 [29:46<398:58:31,  7.19s/it, loss=0.1604, lr=1.21e-06, step=243]Training:   0%|          | 244/200000 [29:46<398:58:31,  7.19s/it, loss=0.1897, lr=1.22e-06, step=244]Training:   0%|          | 245/200000 [29:53<398:30:06,  7.18s/it, loss=0.1897, lr=1.22e-06, step=244]Training:   0%|          | 245/200000 [29:53<398:30:06,  7.18s/it, loss=0.1876, lr=1.22e-06, step=245]Training:   0%|          | 246/200000 [30:00<398:13:09,  7.18s/it, loss=0.1876, lr=1.22e-06, step=245]Training:   0%|          | 246/200000 [30:00<398:13:09,  7.18s/it, loss=0.1198, lr=1.23e-06, step=246]Training:   0%|          | 247/200000 [30:08<398:09:16,  7.18s/it, loss=0.1198, lr=1.23e-06, step=246]Training:   0%|          | 247/200000 [30:08<398:09:16,  7.18s/it, loss=0.1965, lr=1.23e-06, step=247]Training:   0%|          | 248/200000 [30:15<397:53:58,  7.17s/it, loss=0.1965, lr=1.23e-06, step=247]Training:   0%|          | 248/200000 [30:15<397:53:58,  7.17s/it, loss=0.2239, lr=1.24e-06, step=248]Training:   0%|          | 249/200000 [30:22<397:54:13,  7.17s/it, loss=0.2239, lr=1.24e-06, step=248]Training:   0%|          | 249/200000 [30:22<397:54:13,  7.17s/it, loss=0.1902, lr=1.24e-06, step=249]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 250/200000 [30:29<398:21:09,  7.18s/it, loss=0.1902, lr=1.24e-06, step=249]Training:   0%|          | 250/200000 [30:29<398:21:09,  7.18s/it, loss=0.1241, lr=1.25e-06, step=250]Training:   0%|          | 251/200000 [30:36<399:07:15,  7.19s/it, loss=0.1241, lr=1.25e-06, step=250]Training:   0%|          | 251/200000 [30:36<399:07:15,  7.19s/it, loss=0.2649, lr=1.25e-06, step=251]Training:   0%|          | 252/200000 [30:43<398:36:40,  7.18s/it, loss=0.2649, lr=1.25e-06, step=251]Training:   0%|          | 252/200000 [30:43<398:36:40,  7.18s/it, loss=0.2024, lr=1.26e-06, step=252]Training:   0%|          | 253/200000 [30:51<398:18:22,  7.18s/it, loss=0.2024, lr=1.26e-06, step=252]Training:   0%|          | 253/200000 [30:51<398:18:22,  7.18s/it, loss=0.1854, lr=1.26e-06, step=253]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 254/200000 [30:58<398:31:10,  7.18s/it, loss=0.1854, lr=1.26e-06, step=253]Training:   0%|          | 254/200000 [30:58<398:31:10,  7.18s/it, loss=0.2074, lr=1.27e-06, step=254]Training:   0%|          | 255/200000 [31:05<398:13:36,  7.18s/it, loss=0.2074, lr=1.27e-06, step=254]Training:   0%|          | 255/200000 [31:05<398:13:36,  7.18s/it, loss=0.1828, lr=1.27e-06, step=255]Training:   0%|          | 256/200000 [31:12<398:21:44,  7.18s/it, loss=0.1828, lr=1.27e-06, step=255]Training:   0%|          | 256/200000 [31:12<398:21:44,  7.18s/it, loss=0.1957, lr=1.28e-06, step=256]Training:   0%|          | 257/200000 [31:19<398:27:05,  7.18s/it, loss=0.1957, lr=1.28e-06, step=256]Training:   0%|          | 257/200000 [31:19<398:27:05,  7.18s/it, loss=0.2057, lr=1.28e-06, step=257]Training:   0%|          | 258/200000 [31:27<400:06:48,  7.21s/it, loss=0.2057, lr=1.28e-06, step=257]Training:   0%|          | 258/200000 [31:27<400:06:48,  7.21s/it, loss=0.2185, lr=1.29e-06, step=258]Training:   0%|          | 259/200000 [31:34<399:21:30,  7.20s/it, loss=0.2185, lr=1.29e-06, step=258]Training:   0%|          | 259/200000 [31:34<399:21:30,  7.20s/it, loss=0.1818, lr=1.29e-06, step=259]Training:   0%|          | 260/200000 [31:41<401:25:34,  7.24s/it, loss=0.1818, lr=1.29e-06, step=259]Training:   0%|          | 260/200000 [31:41<401:25:34,  7.24s/it, loss=0.2392, lr=1.30e-06, step=260]Training:   0%|          | 261/200000 [31:48<400:52:37,  7.23s/it, loss=0.2392, lr=1.30e-06, step=260]Training:   0%|          | 261/200000 [31:48<400:52:37,  7.23s/it, loss=0.1731, lr=1.30e-06, step=261]Training:   0%|          | 262/200000 [31:56<400:42:55,  7.22s/it, loss=0.1731, lr=1.30e-06, step=261]Training:   0%|          | 262/200000 [31:56<400:42:55,  7.22s/it, loss=0.1748, lr=1.31e-06, step=262]Training:   0%|          | 263/200000 [32:03<399:46:39,  7.21s/it, loss=0.1748, lr=1.31e-06, step=262]Training:   0%|          | 263/200000 [32:03<399:46:39,  7.21s/it, loss=0.2509, lr=1.31e-06, step=263]Training:   0%|          | 264/200000 [32:10<399:49:36,  7.21s/it, loss=0.2509, lr=1.31e-06, step=263]Training:   0%|          | 264/200000 [32:10<399:49:36,  7.21s/it, loss=0.1764, lr=1.32e-06, step=264]Training:   0%|          | 265/200000 [32:17<399:31:27,  7.20s/it, loss=0.1764, lr=1.32e-06, step=264]Training:   0%|          | 265/200000 [32:17<399:31:27,  7.20s/it, loss=0.1777, lr=1.32e-06, step=265]Training:   0%|          | 266/200000 [32:24<399:31:01,  7.20s/it, loss=0.1777, lr=1.32e-06, step=265]Training:   0%|          | 266/200000 [32:24<399:31:01,  7.20s/it, loss=0.1700, lr=1.33e-06, step=266]Training:   0%|          | 267/200000 [32:31<398:58:31,  7.19s/it, loss=0.1700, lr=1.33e-06, step=266]Training:   0%|          | 267/200000 [32:31<398:58:31,  7.19s/it, loss=0.2132, lr=1.33e-06, step=267]Training:   0%|          | 268/200000 [32:39<398:30:45,  7.18s/it, loss=0.2132, lr=1.33e-06, step=267]Training:   0%|          | 268/200000 [32:39<398:30:45,  7.18s/it, loss=0.1369, lr=1.34e-06, step=268]Training:   0%|          | 269/200000 [32:46<398:41:41,  7.19s/it, loss=0.1369, lr=1.34e-06, step=268]Training:   0%|          | 269/200000 [32:46<398:41:41,  7.19s/it, loss=0.1577, lr=1.34e-06, step=269]Training:   0%|          | 270/200000 [32:53<398:45:58,  7.19s/it, loss=0.1577, lr=1.34e-06, step=269]Training:   0%|          | 270/200000 [32:53<398:45:58,  7.19s/it, loss=0.2085, lr=1.35e-06, step=270]Training:   0%|          | 271/200000 [33:00<398:43:09,  7.19s/it, loss=0.2085, lr=1.35e-06, step=270]Training:   0%|          | 271/200000 [33:00<398:43:09,  7.19s/it, loss=0.1884, lr=1.35e-06, step=271]Training:   0%|          | 272/200000 [33:07<398:57:40,  7.19s/it, loss=0.1884, lr=1.35e-06, step=271]Training:   0%|          | 272/200000 [33:07<398:57:40,  7.19s/it, loss=0.1775, lr=1.36e-06, step=272]Training:   0%|          | 273/200000 [33:15<399:01:10,  7.19s/it, loss=0.1775, lr=1.36e-06, step=272]Training:   0%|          | 273/200000 [33:15<399:01:10,  7.19s/it, loss=0.1347, lr=1.36e-06, step=273]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 274/200000 [33:22<398:38:47,  7.19s/it, loss=0.1347, lr=1.36e-06, step=273]Training:   0%|          | 274/200000 [33:22<398:38:47,  7.19s/it, loss=0.1634, lr=1.37e-06, step=274]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 275/200000 [33:29<398:50:31,  7.19s/it, loss=0.1634, lr=1.37e-06, step=274]Training:   0%|          | 275/200000 [33:29<398:50:31,  7.19s/it, loss=0.1667, lr=1.37e-06, step=275]Training:   0%|          | 276/200000 [33:36<398:33:24,  7.18s/it, loss=0.1667, lr=1.37e-06, step=275]Training:   0%|          | 276/200000 [33:36<398:33:24,  7.18s/it, loss=0.1365, lr=1.38e-06, step=276]Training:   0%|          | 277/200000 [33:43<398:43:03,  7.19s/it, loss=0.1365, lr=1.38e-06, step=276]Training:   0%|          | 277/200000 [33:43<398:43:03,  7.19s/it, loss=0.1865, lr=1.38e-06, step=277]Training:   0%|          | 278/200000 [33:51<398:47:50,  7.19s/it, loss=0.1865, lr=1.38e-06, step=277]Training:   0%|          | 278/200000 [33:51<398:47:50,  7.19s/it, loss=0.1624, lr=1.39e-06, step=278]Training:   0%|          | 279/200000 [33:58<398:54:33,  7.19s/it, loss=0.1624, lr=1.39e-06, step=278]Training:   0%|          | 279/200000 [33:58<398:54:33,  7.19s/it, loss=0.1990, lr=1.39e-06, step=279]Training:   0%|          | 280/200000 [34:05<399:01:36,  7.19s/it, loss=0.1990, lr=1.39e-06, step=279]Training:   0%|          | 280/200000 [34:05<399:01:36,  7.19s/it, loss=0.2389, lr=1.40e-06, step=280]Training:   0%|          | 281/200000 [34:12<398:50:42,  7.19s/it, loss=0.2389, lr=1.40e-06, step=280]Training:   0%|          | 281/200000 [34:12<398:50:42,  7.19s/it, loss=0.1704, lr=1.40e-06, step=281]Training:   0%|          | 282/200000 [34:19<398:45:02,  7.19s/it, loss=0.1704, lr=1.40e-06, step=281]Training:   0%|          | 282/200000 [34:19<398:45:02,  7.19s/it, loss=0.1388, lr=1.41e-06, step=282]Training:   0%|          | 283/200000 [34:26<398:54:25,  7.19s/it, loss=0.1388, lr=1.41e-06, step=282]Training:   0%|          | 283/200000 [34:26<398:54:25,  7.19s/it, loss=0.2124, lr=1.41e-06, step=283]Training:   0%|          | 284/200000 [34:34<398:46:35,  7.19s/it, loss=0.2124, lr=1.41e-06, step=283]Training:   0%|          | 284/200000 [34:34<398:46:35,  7.19s/it, loss=0.2023, lr=1.42e-06, step=284]Training:   0%|          | 285/200000 [34:41<398:31:29,  7.18s/it, loss=0.2023, lr=1.42e-06, step=284]Training:   0%|          | 285/200000 [34:41<398:31:29,  7.18s/it, loss=0.1394, lr=1.42e-06, step=285]Training:   0%|          | 286/200000 [34:48<398:53:22,  7.19s/it, loss=0.1394, lr=1.42e-06, step=285]Training:   0%|          | 286/200000 [34:48<398:53:22,  7.19s/it, loss=0.1512, lr=1.43e-06, step=286]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 287/200000 [34:55<398:47:53,  7.19s/it, loss=0.1512, lr=1.43e-06, step=286]Training:   0%|          | 287/200000 [34:55<398:47:53,  7.19s/it, loss=0.1945, lr=1.43e-06, step=287]Training:   0%|          | 288/200000 [35:02<398:47:28,  7.19s/it, loss=0.1945, lr=1.43e-06, step=287]Training:   0%|          | 288/200000 [35:02<398:47:28,  7.19s/it, loss=0.1673, lr=1.44e-06, step=288]Training:   0%|          | 289/200000 [35:10<398:43:52,  7.19s/it, loss=0.1673, lr=1.44e-06, step=288]Training:   0%|          | 289/200000 [35:10<398:43:52,  7.19s/it, loss=0.2093, lr=1.44e-06, step=289]Training:   0%|          | 290/200000 [35:17<398:26:46,  7.18s/it, loss=0.2093, lr=1.44e-06, step=289]Training:   0%|          | 290/200000 [35:17<398:26:46,  7.18s/it, loss=0.1542, lr=1.45e-06, step=290]Training:   0%|          | 291/200000 [35:24<398:19:30,  7.18s/it, loss=0.1542, lr=1.45e-06, step=290]Training:   0%|          | 291/200000 [35:24<398:19:30,  7.18s/it, loss=0.1482, lr=1.45e-06, step=291]Training:   0%|          | 292/200000 [35:31<398:16:11,  7.18s/it, loss=0.1482, lr=1.45e-06, step=291]Training:   0%|          | 292/200000 [35:31<398:16:11,  7.18s/it, loss=0.1665, lr=1.46e-06, step=292]Training:   0%|          | 293/200000 [35:38<397:51:39,  7.17s/it, loss=0.1665, lr=1.46e-06, step=292]Training:   0%|          | 293/200000 [35:38<397:51:39,  7.17s/it, loss=0.1705, lr=1.46e-06, step=293]Training:   0%|          | 294/200000 [35:45<398:07:34,  7.18s/it, loss=0.1705, lr=1.46e-06, step=293]Training:   0%|          | 294/200000 [35:45<398:07:34,  7.18s/it, loss=0.1632, lr=1.47e-06, step=294]Training:   0%|          | 295/200000 [35:53<398:30:37,  7.18s/it, loss=0.1632, lr=1.47e-06, step=294]Training:   0%|          | 295/200000 [35:53<398:30:37,  7.18s/it, loss=0.1751, lr=1.47e-06, step=295]Training:   0%|          | 296/200000 [36:00<398:31:40,  7.18s/it, loss=0.1751, lr=1.47e-06, step=295]Training:   0%|          | 296/200000 [36:00<398:31:40,  7.18s/it, loss=0.1878, lr=1.48e-06, step=296]Training:   0%|          | 297/200000 [36:07<398:56:40,  7.19s/it, loss=0.1878, lr=1.48e-06, step=296]Training:   0%|          | 297/200000 [36:07<398:56:40,  7.19s/it, loss=0.1385, lr=1.48e-06, step=297]Training:   0%|          | 298/200000 [36:14<399:02:08,  7.19s/it, loss=0.1385, lr=1.48e-06, step=297]Training:   0%|          | 298/200000 [36:14<399:02:08,  7.19s/it, loss=0.1344, lr=1.49e-06, step=298]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 299/200000 [36:21<398:56:01,  7.19s/it, loss=0.1344, lr=1.49e-06, step=298]Training:   0%|          | 299/200000 [36:21<398:56:01,  7.19s/it, loss=0.2000, lr=1.49e-06, step=299]Training:   0%|          | 300/200000 [36:29<401:12:35,  7.23s/it, loss=0.2000, lr=1.49e-06, step=299]Training:   0%|          | 300/200000 [36:29<401:12:35,  7.23s/it, loss=0.1478, lr=1.50e-06, step=300]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
20:33:04.756 [I] step=300 loss=0.1858 lr=1.26e-06 grad_norm=2.03 time=718.9s                      (486094:train_pytorch.py:582)
Training:   0%|          | 301/200000 [36:36<400:10:05,  7.21s/it, loss=0.1478, lr=1.50e-06, step=300]Training:   0%|          | 301/200000 [36:36<400:10:05,  7.21s/it, loss=0.3608, lr=1.50e-06, step=301]Training:   0%|          | 302/200000 [36:43<399:29:05,  7.20s/it, loss=0.3608, lr=1.50e-06, step=301]Training:   0%|          | 302/200000 [36:43<399:29:05,  7.20s/it, loss=0.1392, lr=1.51e-06, step=302]Training:   0%|          | 303/200000 [36:50<399:16:19,  7.20s/it, loss=0.1392, lr=1.51e-06, step=302]Training:   0%|          | 303/200000 [36:50<399:16:19,  7.20s/it, loss=0.1790, lr=1.51e-06, step=303]Training:   0%|          | 304/200000 [36:58<399:14:58,  7.20s/it, loss=0.1790, lr=1.51e-06, step=303]Training:   0%|          | 304/200000 [36:58<399:14:58,  7.20s/it, loss=0.1358, lr=1.52e-06, step=304]Training:   0%|          | 305/200000 [37:05<399:13:32,  7.20s/it, loss=0.1358, lr=1.52e-06, step=304]Training:   0%|          | 305/200000 [37:05<399:13:32,  7.20s/it, loss=0.1255, lr=1.52e-06, step=305]Training:   0%|          | 306/200000 [37:12<399:27:03,  7.20s/it, loss=0.1255, lr=1.52e-06, step=305]Training:   0%|          | 306/200000 [37:12<399:27:03,  7.20s/it, loss=0.1563, lr=1.53e-06, step=306]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 307/200000 [37:19<398:56:53,  7.19s/it, loss=0.1563, lr=1.53e-06, step=306]Training:   0%|          | 307/200000 [37:19<398:56:53,  7.19s/it, loss=0.1862, lr=1.53e-06, step=307]Training:   0%|          | 308/200000 [37:26<398:47:41,  7.19s/it, loss=0.1862, lr=1.53e-06, step=307]Training:   0%|          | 308/200000 [37:26<398:47:41,  7.19s/it, loss=0.1526, lr=1.54e-06, step=308]Training:   0%|          | 309/200000 [37:33<398:28:49,  7.18s/it, loss=0.1526, lr=1.54e-06, step=308]Training:   0%|          | 309/200000 [37:33<398:28:49,  7.18s/it, loss=0.1656, lr=1.54e-06, step=309]Training:   0%|          | 310/200000 [37:41<398:18:32,  7.18s/it, loss=0.1656, lr=1.54e-06, step=309]Training:   0%|          | 310/200000 [37:41<398:18:32,  7.18s/it, loss=0.1946, lr=1.55e-06, step=310]Training:   0%|          | 311/200000 [37:48<398:26:27,  7.18s/it, loss=0.1946, lr=1.55e-06, step=310]Training:   0%|          | 311/200000 [37:48<398:26:27,  7.18s/it, loss=0.1522, lr=1.55e-06, step=311]Training:   0%|          | 312/200000 [37:55<398:17:17,  7.18s/it, loss=0.1522, lr=1.55e-06, step=311]Training:   0%|          | 312/200000 [37:55<398:17:17,  7.18s/it, loss=0.1518, lr=1.56e-06, step=312]Training:   0%|          | 313/200000 [38:02<398:30:03,  7.18s/it, loss=0.1518, lr=1.56e-06, step=312]Training:   0%|          | 313/200000 [38:02<398:30:03,  7.18s/it, loss=0.1681, lr=1.56e-06, step=313]Training:   0%|          | 314/200000 [38:09<398:10:23,  7.18s/it, loss=0.1681, lr=1.56e-06, step=313]Training:   0%|          | 314/200000 [38:09<398:10:23,  7.18s/it, loss=0.1897, lr=1.57e-06, step=314]Training:   0%|          | 315/200000 [38:17<398:12:18,  7.18s/it, loss=0.1897, lr=1.57e-06, step=314]Training:   0%|          | 315/200000 [38:17<398:12:18,  7.18s/it, loss=0.1517, lr=1.57e-06, step=315]Training:   0%|          | 316/200000 [38:24<398:35:02,  7.19s/it, loss=0.1517, lr=1.57e-06, step=315]Training:   0%|          | 316/200000 [38:24<398:35:02,  7.19s/it, loss=0.1634, lr=1.58e-06, step=316]Training:   0%|          | 317/200000 [38:31<398:56:26,  7.19s/it, loss=0.1634, lr=1.58e-06, step=316]Training:   0%|          | 317/200000 [38:31<398:56:26,  7.19s/it, loss=0.1736, lr=1.58e-06, step=317]Training:   0%|          | 318/200000 [38:38<398:38:07,  7.19s/it, loss=0.1736, lr=1.58e-06, step=317]Training:   0%|          | 318/200000 [38:38<398:38:07,  7.19s/it, loss=0.1847, lr=1.59e-06, step=318]Training:   0%|          | 319/200000 [38:45<398:07:09,  7.18s/it, loss=0.1847, lr=1.59e-06, step=318]Training:   0%|          | 319/200000 [38:45<398:07:09,  7.18s/it, loss=0.2958, lr=1.59e-06, step=319]Training:   0%|          | 320/200000 [38:52<398:31:26,  7.18s/it, loss=0.2958, lr=1.59e-06, step=319]Training:   0%|          | 320/200000 [38:52<398:31:26,  7.18s/it, loss=0.1825, lr=1.60e-06, step=320]Training:   0%|          | 321/200000 [39:00<400:16:32,  7.22s/it, loss=0.1825, lr=1.60e-06, step=320]Training:   0%|          | 321/200000 [39:00<400:16:32,  7.22s/it, loss=0.1053, lr=1.60e-06, step=321]Training:   0%|          | 322/200000 [39:07<399:48:23,  7.21s/it, loss=0.1053, lr=1.60e-06, step=321]Training:   0%|          | 322/200000 [39:07<399:48:23,  7.21s/it, loss=0.1550, lr=1.61e-06, step=322]Training:   0%|          | 323/200000 [39:14<399:26:42,  7.20s/it, loss=0.1550, lr=1.61e-06, step=322]Training:   0%|          | 323/200000 [39:14<399:26:42,  7.20s/it, loss=0.1337, lr=1.61e-06, step=323]Training:   0%|          | 324/200000 [39:21<399:28:36,  7.20s/it, loss=0.1337, lr=1.61e-06, step=323]Training:   0%|          | 324/200000 [39:21<399:28:36,  7.20s/it, loss=0.1459, lr=1.62e-06, step=324]Training:   0%|          | 325/200000 [39:29<399:12:21,  7.20s/it, loss=0.1459, lr=1.62e-06, step=324]Training:   0%|          | 325/200000 [39:29<399:12:21,  7.20s/it, loss=0.1990, lr=1.62e-06, step=325]Training:   0%|          | 326/200000 [39:36<399:11:48,  7.20s/it, loss=0.1990, lr=1.62e-06, step=325]Training:   0%|          | 326/200000 [39:36<399:11:48,  7.20s/it, loss=0.3283, lr=1.63e-06, step=326]Training:   0%|          | 327/200000 [39:43<399:18:32,  7.20s/it, loss=0.3283, lr=1.63e-06, step=326]Training:   0%|          | 327/200000 [39:43<399:18:32,  7.20s/it, loss=0.2516, lr=1.63e-06, step=327]Training:   0%|          | 328/200000 [39:50<399:24:54,  7.20s/it, loss=0.2516, lr=1.63e-06, step=327]Training:   0%|          | 328/200000 [39:50<399:24:54,  7.20s/it, loss=0.1334, lr=1.64e-06, step=328]Training:   0%|          | 329/200000 [39:57<398:45:47,  7.19s/it, loss=0.1334, lr=1.64e-06, step=328]Training:   0%|          | 329/200000 [39:57<398:45:47,  7.19s/it, loss=0.1319, lr=1.64e-06, step=329]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 330/200000 [40:04<398:24:10,  7.18s/it, loss=0.1319, lr=1.64e-06, step=329]Training:   0%|          | 330/200000 [40:04<398:24:10,  7.18s/it, loss=0.2051, lr=1.65e-06, step=330]Training:   0%|          | 331/200000 [40:12<398:25:42,  7.18s/it, loss=0.2051, lr=1.65e-06, step=330]Training:   0%|          | 331/200000 [40:12<398:25:42,  7.18s/it, loss=0.1730, lr=1.65e-06, step=331]Training:   0%|          | 332/200000 [40:19<398:19:21,  7.18s/it, loss=0.1730, lr=1.65e-06, step=331]Training:   0%|          | 332/200000 [40:19<398:19:21,  7.18s/it, loss=0.1457, lr=1.66e-06, step=332]Training:   0%|          | 333/200000 [40:26<398:44:14,  7.19s/it, loss=0.1457, lr=1.66e-06, step=332]Training:   0%|          | 333/200000 [40:26<398:44:14,  7.19s/it, loss=0.1228, lr=1.66e-06, step=333]Training:   0%|          | 334/200000 [40:33<398:42:44,  7.19s/it, loss=0.1228, lr=1.66e-06, step=333]Training:   0%|          | 334/200000 [40:33<398:42:44,  7.19s/it, loss=0.1802, lr=1.67e-06, step=334]Training:   0%|          | 335/200000 [40:40<398:57:25,  7.19s/it, loss=0.1802, lr=1.67e-06, step=334]Training:   0%|          | 335/200000 [40:40<398:57:25,  7.19s/it, loss=0.2016, lr=1.67e-06, step=335]Training:   0%|          | 336/200000 [40:48<399:12:03,  7.20s/it, loss=0.2016, lr=1.67e-06, step=335]Training:   0%|          | 336/200000 [40:48<399:12:03,  7.20s/it, loss=0.1862, lr=1.68e-06, step=336]Training:   0%|          | 337/200000 [40:55<398:45:03,  7.19s/it, loss=0.1862, lr=1.68e-06, step=336]Training:   0%|          | 337/200000 [40:55<398:45:03,  7.19s/it, loss=0.2403, lr=1.68e-06, step=337]Training:   0%|          | 338/200000 [41:02<398:30:04,  7.19s/it, loss=0.2403, lr=1.68e-06, step=337]Training:   0%|          | 338/200000 [41:02<398:30:04,  7.19s/it, loss=0.1981, lr=1.69e-06, step=338]Training:   0%|          | 339/200000 [41:09<398:08:08,  7.18s/it, loss=0.1981, lr=1.69e-06, step=338]Training:   0%|          | 339/200000 [41:09<398:08:08,  7.18s/it, loss=0.1721, lr=1.69e-06, step=339]Training:   0%|          | 340/200000 [41:16<398:30:08,  7.19s/it, loss=0.1721, lr=1.69e-06, step=339]Training:   0%|          | 340/200000 [41:16<398:30:08,  7.19s/it, loss=0.1221, lr=1.70e-06, step=340]Training:   0%|          | 341/200000 [41:24<398:30:51,  7.19s/it, loss=0.1221, lr=1.70e-06, step=340]Training:   0%|          | 341/200000 [41:24<398:30:51,  7.19s/it, loss=0.1572, lr=1.70e-06, step=341]Training:   0%|          | 342/200000 [41:31<398:58:13,  7.19s/it, loss=0.1572, lr=1.70e-06, step=341]Training:   0%|          | 342/200000 [41:31<398:58:13,  7.19s/it, loss=0.1427, lr=1.71e-06, step=342]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 343/200000 [41:38<398:50:55,  7.19s/it, loss=0.1427, lr=1.71e-06, step=342]Training:   0%|          | 343/200000 [41:38<398:50:55,  7.19s/it, loss=0.1004, lr=1.71e-06, step=343]Training:   0%|          | 344/200000 [41:45<398:33:56,  7.19s/it, loss=0.1004, lr=1.71e-06, step=343]Training:   0%|          | 344/200000 [41:45<398:33:56,  7.19s/it, loss=0.1968, lr=1.72e-06, step=344]Training:   0%|          | 345/200000 [41:52<398:34:34,  7.19s/it, loss=0.1968, lr=1.72e-06, step=344]Training:   0%|          | 345/200000 [41:52<398:34:34,  7.19s/it, loss=0.1770, lr=1.72e-06, step=345]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 346/200000 [41:59<398:46:38,  7.19s/it, loss=0.1770, lr=1.72e-06, step=345]Training:   0%|          | 346/200000 [41:59<398:46:38,  7.19s/it, loss=0.1145, lr=1.73e-06, step=346]Training:   0%|          | 347/200000 [42:07<398:22:07,  7.18s/it, loss=0.1145, lr=1.73e-06, step=346]Training:   0%|          | 347/200000 [42:07<398:22:07,  7.18s/it, loss=0.1442, lr=1.73e-06, step=347]Training:   0%|          | 348/200000 [42:14<398:37:07,  7.19s/it, loss=0.1442, lr=1.73e-06, step=347]Training:   0%|          | 348/200000 [42:14<398:37:07,  7.19s/it, loss=0.1843, lr=1.74e-06, step=348]Training:   0%|          | 349/200000 [42:21<398:42:22,  7.19s/it, loss=0.1843, lr=1.74e-06, step=348]Training:   0%|          | 349/200000 [42:21<398:42:22,  7.19s/it, loss=0.1273, lr=1.74e-06, step=349]Training:   0%|          | 350/200000 [42:28<398:46:06,  7.19s/it, loss=0.1273, lr=1.74e-06, step=349]Training:   0%|          | 350/200000 [42:28<398:46:06,  7.19s/it, loss=0.2192, lr=1.75e-06, step=350]Training:   0%|          | 351/200000 [42:35<399:00:43,  7.19s/it, loss=0.2192, lr=1.75e-06, step=350]Training:   0%|          | 351/200000 [42:35<399:00:43,  7.19s/it, loss=0.1116, lr=1.75e-06, step=351]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 352/200000 [42:43<398:59:24,  7.19s/it, loss=0.1116, lr=1.75e-06, step=351]Training:   0%|          | 352/200000 [42:43<398:59:24,  7.19s/it, loss=0.2215, lr=1.76e-06, step=352]Training:   0%|          | 353/200000 [42:50<399:04:49,  7.20s/it, loss=0.2215, lr=1.76e-06, step=352]Training:   0%|          | 353/200000 [42:50<399:04:49,  7.20s/it, loss=0.1221, lr=1.76e-06, step=353]Training:   0%|          | 354/200000 [42:57<399:12:39,  7.20s/it, loss=0.1221, lr=1.76e-06, step=353]Training:   0%|          | 354/200000 [42:57<399:12:39,  7.20s/it, loss=0.1468, lr=1.77e-06, step=354]Training:   0%|          | 355/200000 [43:04<398:59:51,  7.19s/it, loss=0.1468, lr=1.77e-06, step=354]Training:   0%|          | 355/200000 [43:04<398:59:51,  7.19s/it, loss=0.2953, lr=1.77e-06, step=355]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 356/200000 [43:11<399:01:08,  7.20s/it, loss=0.2953, lr=1.77e-06, step=355]Training:   0%|          | 356/200000 [43:11<399:01:08,  7.20s/it, loss=0.1415, lr=1.78e-06, step=356]Training:   0%|          | 357/200000 [43:19<398:33:11,  7.19s/it, loss=0.1415, lr=1.78e-06, step=356]Training:   0%|          | 357/200000 [43:19<398:33:11,  7.19s/it, loss=0.1472, lr=1.78e-06, step=357]Training:   0%|          | 358/200000 [43:26<398:39:36,  7.19s/it, loss=0.1472, lr=1.78e-06, step=357]Training:   0%|          | 358/200000 [43:26<398:39:36,  7.19s/it, loss=0.1782, lr=1.79e-06, step=358]Training:   0%|          | 359/200000 [43:33<398:24:34,  7.18s/it, loss=0.1782, lr=1.79e-06, step=358]Training:   0%|          | 359/200000 [43:33<398:24:34,  7.18s/it, loss=0.1873, lr=1.79e-06, step=359]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 360/200000 [43:40<398:26:47,  7.18s/it, loss=0.1873, lr=1.79e-06, step=359]Training:   0%|          | 360/200000 [43:40<398:26:47,  7.18s/it, loss=0.1444, lr=1.80e-06, step=360]Training:   0%|          | 361/200000 [43:47<398:10:07,  7.18s/it, loss=0.1444, lr=1.80e-06, step=360]Training:   0%|          | 361/200000 [43:47<398:10:07,  7.18s/it, loss=0.1646, lr=1.80e-06, step=361]Training:   0%|          | 362/200000 [43:55<398:25:00,  7.18s/it, loss=0.1646, lr=1.80e-06, step=361]Training:   0%|          | 362/200000 [43:55<398:25:00,  7.18s/it, loss=0.1552, lr=1.81e-06, step=362]Training:   0%|          | 363/200000 [44:02<400:38:14,  7.22s/it, loss=0.1552, lr=1.81e-06, step=362]Training:   0%|          | 363/200000 [44:02<400:38:14,  7.22s/it, loss=0.1322, lr=1.81e-06, step=363]Training:   0%|          | 364/200000 [44:09<400:06:56,  7.22s/it, loss=0.1322, lr=1.81e-06, step=363]Training:   0%|          | 364/200000 [44:09<400:06:56,  7.22s/it, loss=0.1425, lr=1.82e-06, step=364]Training:   0%|          | 365/200000 [44:16<399:40:54,  7.21s/it, loss=0.1425, lr=1.82e-06, step=364]Training:   0%|          | 365/200000 [44:16<399:40:54,  7.21s/it, loss=0.1314, lr=1.82e-06, step=365]Training:   0%|          | 366/200000 [44:23<399:01:26,  7.20s/it, loss=0.1314, lr=1.82e-06, step=365]Training:   0%|          | 366/200000 [44:23<399:01:26,  7.20s/it, loss=0.1112, lr=1.83e-06, step=366]Training:   0%|          | 367/200000 [44:31<399:01:38,  7.20s/it, loss=0.1112, lr=1.83e-06, step=366]Training:   0%|          | 367/200000 [44:31<399:01:38,  7.20s/it, loss=0.1470, lr=1.83e-06, step=367]Training:   0%|          | 368/200000 [44:38<398:38:21,  7.19s/it, loss=0.1470, lr=1.83e-06, step=367]Training:   0%|          | 368/200000 [44:38<398:38:21,  7.19s/it, loss=0.1308, lr=1.84e-06, step=368]Training:   0%|          | 369/200000 [44:45<398:44:18,  7.19s/it, loss=0.1308, lr=1.84e-06, step=368]Training:   0%|          | 369/200000 [44:45<398:44:18,  7.19s/it, loss=0.1270, lr=1.84e-06, step=369]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 370/200000 [44:52<398:18:00,  7.18s/it, loss=0.1270, lr=1.84e-06, step=369]Training:   0%|          | 370/200000 [44:52<398:18:00,  7.18s/it, loss=0.1679, lr=1.85e-06, step=370]Training:   0%|          | 371/200000 [44:59<398:18:15,  7.18s/it, loss=0.1679, lr=1.85e-06, step=370]Training:   0%|          | 371/200000 [44:59<398:18:15,  7.18s/it, loss=0.1312, lr=1.85e-06, step=371]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 372/200000 [45:06<397:57:44,  7.18s/it, loss=0.1312, lr=1.85e-06, step=371]Training:   0%|          | 372/200000 [45:06<397:57:44,  7.18s/it, loss=0.1375, lr=1.86e-06, step=372]Training:   0%|          | 373/200000 [45:14<398:29:58,  7.19s/it, loss=0.1375, lr=1.86e-06, step=372]Training:   0%|          | 373/200000 [45:14<398:29:58,  7.19s/it, loss=0.1640, lr=1.86e-06, step=373]Training:   0%|          | 374/200000 [45:21<398:12:26,  7.18s/it, loss=0.1640, lr=1.86e-06, step=373]Training:   0%|          | 374/200000 [45:21<398:12:26,  7.18s/it, loss=0.1283, lr=1.87e-06, step=374]Training:   0%|          | 375/200000 [45:28<397:59:22,  7.18s/it, loss=0.1283, lr=1.87e-06, step=374]Training:   0%|          | 375/200000 [45:28<397:59:22,  7.18s/it, loss=0.1451, lr=1.87e-06, step=375]Training:   0%|          | 376/200000 [45:35<397:55:53,  7.18s/it, loss=0.1451, lr=1.87e-06, step=375]Training:   0%|          | 376/200000 [45:35<397:55:53,  7.18s/it, loss=0.1045, lr=1.88e-06, step=376]Training:   0%|          | 377/200000 [45:42<397:56:12,  7.18s/it, loss=0.1045, lr=1.88e-06, step=376]Training:   0%|          | 377/200000 [45:42<397:56:12,  7.18s/it, loss=0.1762, lr=1.88e-06, step=377]Training:   0%|          | 378/200000 [45:50<397:42:36,  7.17s/it, loss=0.1762, lr=1.88e-06, step=377]Training:   0%|          | 378/200000 [45:50<397:42:36,  7.17s/it, loss=0.1412, lr=1.89e-06, step=378]Training:   0%|          | 379/200000 [45:57<397:36:08,  7.17s/it, loss=0.1412, lr=1.89e-06, step=378]Training:   0%|          | 379/200000 [45:57<397:36:08,  7.17s/it, loss=0.0996, lr=1.89e-06, step=379]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 380/200000 [46:04<397:49:06,  7.17s/it, loss=0.0996, lr=1.89e-06, step=379]Training:   0%|          | 380/200000 [46:04<397:49:06,  7.17s/it, loss=0.1234, lr=1.90e-06, step=380]Training:   0%|          | 381/200000 [46:11<398:17:04,  7.18s/it, loss=0.1234, lr=1.90e-06, step=380]Training:   0%|          | 381/200000 [46:11<398:17:04,  7.18s/it, loss=0.1969, lr=1.90e-06, step=381]Training:   0%|          | 382/200000 [46:18<399:01:35,  7.20s/it, loss=0.1969, lr=1.90e-06, step=381]Training:   0%|          | 382/200000 [46:18<399:01:35,  7.20s/it, loss=0.1599, lr=1.91e-06, step=382]Training:   0%|          | 383/200000 [46:25<398:33:42,  7.19s/it, loss=0.1599, lr=1.91e-06, step=382]Training:   0%|          | 383/200000 [46:25<398:33:42,  7.19s/it, loss=0.1350, lr=1.91e-06, step=383]Training:   0%|          | 384/200000 [46:33<400:11:49,  7.22s/it, loss=0.1350, lr=1.91e-06, step=383]Training:   0%|          | 384/200000 [46:33<400:11:49,  7.22s/it, loss=0.1960, lr=1.92e-06, step=384]Training:   0%|          | 385/200000 [46:40<399:51:25,  7.21s/it, loss=0.1960, lr=1.92e-06, step=384]Training:   0%|          | 385/200000 [46:40<399:51:25,  7.21s/it, loss=0.1733, lr=1.92e-06, step=385]Training:   0%|          | 386/200000 [46:47<399:45:44,  7.21s/it, loss=0.1733, lr=1.92e-06, step=385]Training:   0%|          | 386/200000 [46:47<399:45:44,  7.21s/it, loss=0.1517, lr=1.93e-06, step=386]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 387/200000 [46:54<398:53:09,  7.19s/it, loss=0.1517, lr=1.93e-06, step=386]Training:   0%|          | 387/200000 [46:54<398:53:09,  7.19s/it, loss=0.1817, lr=1.93e-06, step=387]Training:   0%|          | 388/200000 [47:01<398:47:19,  7.19s/it, loss=0.1817, lr=1.93e-06, step=387]Training:   0%|          | 388/200000 [47:01<398:47:19,  7.19s/it, loss=0.1919, lr=1.94e-06, step=388]Training:   0%|          | 389/200000 [47:09<398:48:19,  7.19s/it, loss=0.1919, lr=1.94e-06, step=388]Training:   0%|          | 389/200000 [47:09<398:48:19,  7.19s/it, loss=0.2879, lr=1.94e-06, step=389]Training:   0%|          | 390/200000 [47:16<398:28:05,  7.19s/it, loss=0.2879, lr=1.94e-06, step=389]Training:   0%|          | 390/200000 [47:16<398:28:05,  7.19s/it, loss=0.1386, lr=1.95e-06, step=390]Training:   0%|          | 391/200000 [47:23<398:39:20,  7.19s/it, loss=0.1386, lr=1.95e-06, step=390]Training:   0%|          | 391/200000 [47:23<398:39:20,  7.19s/it, loss=0.1394, lr=1.95e-06, step=391]Training:   0%|          | 392/200000 [47:30<398:56:51,  7.20s/it, loss=0.1394, lr=1.95e-06, step=391]Training:   0%|          | 392/200000 [47:30<398:56:51,  7.20s/it, loss=0.1721, lr=1.96e-06, step=392]Training:   0%|          | 393/200000 [47:37<398:39:30,  7.19s/it, loss=0.1721, lr=1.96e-06, step=392]Training:   0%|          | 393/200000 [47:37<398:39:30,  7.19s/it, loss=0.1272, lr=1.96e-06, step=393]Training:   0%|          | 394/200000 [47:45<398:27:55,  7.19s/it, loss=0.1272, lr=1.96e-06, step=393]Training:   0%|          | 394/200000 [47:45<398:27:55,  7.19s/it, loss=0.1491, lr=1.97e-06, step=394]Training:   0%|          | 395/200000 [47:52<398:00:28,  7.18s/it, loss=0.1491, lr=1.97e-06, step=394]Training:   0%|          | 395/200000 [47:52<398:00:28,  7.18s/it, loss=0.1383, lr=1.97e-06, step=395]Training:   0%|          | 396/200000 [47:59<397:56:18,  7.18s/it, loss=0.1383, lr=1.97e-06, step=395]Training:   0%|          | 396/200000 [47:59<397:56:18,  7.18s/it, loss=0.1467, lr=1.98e-06, step=396]Training:   0%|          | 397/200000 [48:06<398:08:44,  7.18s/it, loss=0.1467, lr=1.98e-06, step=396]Training:   0%|          | 397/200000 [48:06<398:08:44,  7.18s/it, loss=0.2464, lr=1.98e-06, step=397]Training:   0%|          | 398/200000 [48:13<398:33:46,  7.19s/it, loss=0.2464, lr=1.98e-06, step=397]Training:   0%|          | 398/200000 [48:13<398:33:46,  7.19s/it, loss=0.1436, lr=1.99e-06, step=398]Training:   0%|          | 399/200000 [48:21<398:09:30,  7.18s/it, loss=0.1436, lr=1.99e-06, step=398]Training:   0%|          | 399/200000 [48:21<398:09:30,  7.18s/it, loss=0.1513, lr=1.99e-06, step=399]Training:   0%|          | 400/200000 [48:28<398:10:51,  7.18s/it, loss=0.1513, lr=1.99e-06, step=399]Training:   0%|          | 400/200000 [48:28<398:10:51,  7.18s/it, loss=0.1536, lr=2.00e-06, step=400]20:45:03.688 [I] step=400 loss=0.1636 lr=1.76e-06 grad_norm=1.36 time=718.9s                      (486094:train_pytorch.py:582)
Training:   0%|          | 401/200000 [48:35<398:30:44,  7.19s/it, loss=0.1536, lr=2.00e-06, step=400]Training:   0%|          | 401/200000 [48:35<398:30:44,  7.19s/it, loss=0.1792, lr=2.00e-06, step=401]Training:   0%|          | 402/200000 [48:42<398:10:42,  7.18s/it, loss=0.1792, lr=2.00e-06, step=401]Training:   0%|          | 402/200000 [48:42<398:10:42,  7.18s/it, loss=0.1219, lr=2.01e-06, step=402]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 403/200000 [48:49<398:08:23,  7.18s/it, loss=0.1219, lr=2.01e-06, step=402]Training:   0%|          | 403/200000 [48:49<398:08:23,  7.18s/it, loss=0.1327, lr=2.01e-06, step=403]Training:   0%|          | 404/200000 [48:56<398:05:24,  7.18s/it, loss=0.1327, lr=2.01e-06, step=403]Training:   0%|          | 404/200000 [48:56<398:05:24,  7.18s/it, loss=0.1458, lr=2.02e-06, step=404]Training:   0%|          | 405/200000 [49:04<398:48:16,  7.19s/it, loss=0.1458, lr=2.02e-06, step=404]Training:   0%|          | 405/200000 [49:04<398:48:16,  7.19s/it, loss=0.2474, lr=2.02e-06, step=405]Training:   0%|          | 406/200000 [49:11<398:28:51,  7.19s/it, loss=0.2474, lr=2.02e-06, step=405]Training:   0%|          | 406/200000 [49:11<398:28:51,  7.19s/it, loss=0.1565, lr=2.03e-06, step=406]Training:   0%|          | 407/200000 [49:18<398:17:33,  7.18s/it, loss=0.1565, lr=2.03e-06, step=406]Training:   0%|          | 407/200000 [49:18<398:17:33,  7.18s/it, loss=0.1373, lr=2.03e-06, step=407]Training:   0%|          | 408/200000 [49:25<398:34:30,  7.19s/it, loss=0.1373, lr=2.03e-06, step=407]Training:   0%|          | 408/200000 [49:25<398:34:30,  7.19s/it, loss=0.1396, lr=2.04e-06, step=408]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 409/200000 [49:32<398:25:38,  7.19s/it, loss=0.1396, lr=2.04e-06, step=408]Training:   0%|          | 409/200000 [49:32<398:25:38,  7.19s/it, loss=0.1294, lr=2.04e-06, step=409]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 410/200000 [49:40<398:51:23,  7.19s/it, loss=0.1294, lr=2.04e-06, step=409]Training:   0%|          | 410/200000 [49:40<398:51:23,  7.19s/it, loss=0.1589, lr=2.05e-06, step=410]Training:   0%|          | 411/200000 [49:47<398:25:18,  7.19s/it, loss=0.1589, lr=2.05e-06, step=410]Training:   0%|          | 411/200000 [49:47<398:25:18,  7.19s/it, loss=0.1103, lr=2.05e-06, step=411]Training:   0%|          | 412/200000 [49:54<398:22:05,  7.19s/it, loss=0.1103, lr=2.05e-06, step=411]Training:   0%|          | 412/200000 [49:54<398:22:05,  7.19s/it, loss=0.1099, lr=2.06e-06, step=412]Training:   0%|          | 413/200000 [50:01<398:21:21,  7.19s/it, loss=0.1099, lr=2.06e-06, step=412]Training:   0%|          | 413/200000 [50:01<398:21:21,  7.19s/it, loss=0.1591, lr=2.06e-06, step=413]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 414/200000 [50:08<398:00:23,  7.18s/it, loss=0.1591, lr=2.06e-06, step=413]Training:   0%|          | 414/200000 [50:08<398:00:23,  7.18s/it, loss=0.1718, lr=2.07e-06, step=414]Training:   0%|          | 415/200000 [50:15<397:56:39,  7.18s/it, loss=0.1718, lr=2.07e-06, step=414]Training:   0%|          | 415/200000 [50:15<397:56:39,  7.18s/it, loss=0.1576, lr=2.07e-06, step=415]Training:   0%|          | 416/200000 [50:23<398:20:31,  7.19s/it, loss=0.1576, lr=2.07e-06, step=415]Training:   0%|          | 416/200000 [50:23<398:20:31,  7.19s/it, loss=0.1365, lr=2.08e-06, step=416]Training:   0%|          | 417/200000 [50:30<397:51:07,  7.18s/it, loss=0.1365, lr=2.08e-06, step=416]Training:   0%|          | 417/200000 [50:30<397:51:07,  7.18s/it, loss=0.1078, lr=2.08e-06, step=417]Training:   0%|          | 418/200000 [50:37<398:02:02,  7.18s/it, loss=0.1078, lr=2.08e-06, step=417]Training:   0%|          | 418/200000 [50:37<398:02:02,  7.18s/it, loss=0.1697, lr=2.09e-06, step=418]Training:   0%|          | 419/200000 [50:44<397:56:58,  7.18s/it, loss=0.1697, lr=2.09e-06, step=418]Training:   0%|          | 419/200000 [50:44<397:56:58,  7.18s/it, loss=0.1558, lr=2.09e-06, step=419]Training:   0%|          | 420/200000 [50:51<397:45:54,  7.17s/it, loss=0.1558, lr=2.09e-06, step=419]Training:   0%|          | 420/200000 [50:51<397:45:54,  7.17s/it, loss=0.1442, lr=2.10e-06, step=420]Training:   0%|          | 421/200000 [50:59<398:05:19,  7.18s/it, loss=0.1442, lr=2.10e-06, step=420]Training:   0%|          | 421/200000 [50:59<398:05:19,  7.18s/it, loss=0.2825, lr=2.10e-06, step=421]Training:   0%|          | 422/200000 [51:06<398:35:05,  7.19s/it, loss=0.2825, lr=2.10e-06, step=421]Training:   0%|          | 422/200000 [51:06<398:35:05,  7.19s/it, loss=0.1106, lr=2.11e-06, step=422]Training:   0%|          | 423/200000 [51:13<398:34:47,  7.19s/it, loss=0.1106, lr=2.11e-06, step=422]Training:   0%|          | 423/200000 [51:13<398:34:47,  7.19s/it, loss=0.1351, lr=2.11e-06, step=423]Training:   0%|          | 424/200000 [51:20<398:59:32,  7.20s/it, loss=0.1351, lr=2.11e-06, step=423]Training:   0%|          | 424/200000 [51:20<398:59:32,  7.20s/it, loss=0.1040, lr=2.12e-06, step=424]Training:   0%|          | 425/200000 [51:27<398:29:12,  7.19s/it, loss=0.1040, lr=2.12e-06, step=424]Training:   0%|          | 425/200000 [51:27<398:29:12,  7.19s/it, loss=0.1358, lr=2.12e-06, step=425]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 426/200000 [51:35<400:22:03,  7.22s/it, loss=0.1358, lr=2.12e-06, step=425]Training:   0%|          | 426/200000 [51:35<400:22:03,  7.22s/it, loss=0.1689, lr=2.13e-06, step=426]Training:   0%|          | 427/200000 [51:42<399:26:26,  7.21s/it, loss=0.1689, lr=2.13e-06, step=426]Training:   0%|          | 427/200000 [51:42<399:26:26,  7.21s/it, loss=0.1512, lr=2.13e-06, step=427]Training:   0%|          | 428/200000 [51:49<399:32:15,  7.21s/it, loss=0.1512, lr=2.13e-06, step=427]Training:   0%|          | 428/200000 [51:49<399:32:15,  7.21s/it, loss=0.1447, lr=2.14e-06, step=428]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 429/200000 [51:56<398:54:50,  7.20s/it, loss=0.1447, lr=2.14e-06, step=428]Training:   0%|          | 429/200000 [51:56<398:54:50,  7.20s/it, loss=0.1486, lr=2.14e-06, step=429]Training:   0%|          | 430/200000 [52:03<398:45:19,  7.19s/it, loss=0.1486, lr=2.14e-06, step=429]Training:   0%|          | 430/200000 [52:03<398:45:19,  7.19s/it, loss=0.1479, lr=2.15e-06, step=430]Training:   0%|          | 431/200000 [52:11<398:53:55,  7.20s/it, loss=0.1479, lr=2.15e-06, step=430]Training:   0%|          | 431/200000 [52:11<398:53:55,  7.20s/it, loss=0.1078, lr=2.15e-06, step=431]Training:   0%|          | 432/200000 [52:18<398:52:26,  7.20s/it, loss=0.1078, lr=2.15e-06, step=431]Training:   0%|          | 432/200000 [52:18<398:52:26,  7.20s/it, loss=0.1330, lr=2.16e-06, step=432]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 433/200000 [52:25<398:40:31,  7.19s/it, loss=0.1330, lr=2.16e-06, step=432]Training:   0%|          | 433/200000 [52:25<398:40:31,  7.19s/it, loss=0.1168, lr=2.16e-06, step=433]Training:   0%|          | 434/200000 [52:32<398:31:52,  7.19s/it, loss=0.1168, lr=2.16e-06, step=433]Training:   0%|          | 434/200000 [52:32<398:31:52,  7.19s/it, loss=0.1183, lr=2.17e-06, step=434]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 435/200000 [52:39<398:09:48,  7.18s/it, loss=0.1183, lr=2.17e-06, step=434]Training:   0%|          | 435/200000 [52:39<398:09:48,  7.18s/it, loss=0.2838, lr=2.17e-06, step=435]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 436/200000 [52:46<398:03:21,  7.18s/it, loss=0.2838, lr=2.17e-06, step=435]Training:   0%|          | 436/200000 [52:46<398:03:21,  7.18s/it, loss=0.1454, lr=2.18e-06, step=436]Training:   0%|          | 437/200000 [52:54<398:23:00,  7.19s/it, loss=0.1454, lr=2.18e-06, step=436]Training:   0%|          | 437/200000 [52:54<398:23:00,  7.19s/it, loss=0.2103, lr=2.18e-06, step=437]Training:   0%|          | 438/200000 [53:01<398:01:35,  7.18s/it, loss=0.2103, lr=2.18e-06, step=437]Training:   0%|          | 438/200000 [53:01<398:01:35,  7.18s/it, loss=0.1143, lr=2.19e-06, step=438]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 439/200000 [53:08<397:58:13,  7.18s/it, loss=0.1143, lr=2.19e-06, step=438]Training:   0%|          | 439/200000 [53:08<397:58:13,  7.18s/it, loss=0.1510, lr=2.19e-06, step=439]Training:   0%|          | 440/200000 [53:15<397:53:19,  7.18s/it, loss=0.1510, lr=2.19e-06, step=439]Training:   0%|          | 440/200000 [53:15<397:53:19,  7.18s/it, loss=0.1628, lr=2.20e-06, step=440]Training:   0%|          | 441/200000 [53:22<397:30:06,  7.17s/it, loss=0.1628, lr=2.20e-06, step=440]Training:   0%|          | 441/200000 [53:22<397:30:06,  7.17s/it, loss=0.1296, lr=2.20e-06, step=441]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 442/200000 [53:30<397:58:42,  7.18s/it, loss=0.1296, lr=2.20e-06, step=441]Training:   0%|          | 442/200000 [53:30<397:58:42,  7.18s/it, loss=0.1863, lr=2.21e-06, step=442]Training:   0%|          | 443/200000 [53:37<398:10:13,  7.18s/it, loss=0.1863, lr=2.21e-06, step=442]Training:   0%|          | 443/200000 [53:37<398:10:13,  7.18s/it, loss=0.1603, lr=2.21e-06, step=443]Training:   0%|          | 444/200000 [53:44<397:58:40,  7.18s/it, loss=0.1603, lr=2.21e-06, step=443]Training:   0%|          | 444/200000 [53:44<397:58:40,  7.18s/it, loss=0.1274, lr=2.22e-06, step=444]Training:   0%|          | 445/200000 [53:51<398:16:06,  7.18s/it, loss=0.1274, lr=2.22e-06, step=444]Training:   0%|          | 445/200000 [53:51<398:16:06,  7.18s/it, loss=0.1303, lr=2.22e-06, step=445]Training:   0%|          | 446/200000 [53:58<398:01:20,  7.18s/it, loss=0.1303, lr=2.22e-06, step=445]Training:   0%|          | 446/200000 [53:58<398:01:20,  7.18s/it, loss=0.1712, lr=2.23e-06, step=446]Training:   0%|          | 447/200000 [54:06<399:50:28,  7.21s/it, loss=0.1712, lr=2.23e-06, step=446]Training:   0%|          | 447/200000 [54:06<399:50:28,  7.21s/it, loss=0.1270, lr=2.23e-06, step=447]Training:   0%|          | 448/200000 [54:13<399:26:20,  7.21s/it, loss=0.1270, lr=2.23e-06, step=447]Training:   0%|          | 448/200000 [54:13<399:26:20,  7.21s/it, loss=0.1648, lr=2.24e-06, step=448]Training:   0%|          | 449/200000 [54:20<398:56:46,  7.20s/it, loss=0.1648, lr=2.24e-06, step=448]Training:   0%|          | 449/200000 [54:20<398:56:46,  7.20s/it, loss=0.2049, lr=2.24e-06, step=449]Training:   0%|          | 450/200000 [54:27<398:58:55,  7.20s/it, loss=0.2049, lr=2.24e-06, step=449]Training:   0%|          | 450/200000 [54:27<398:58:55,  7.20s/it, loss=0.1710, lr=2.25e-06, step=450]Training:   0%|          | 451/200000 [54:34<399:15:22,  7.20s/it, loss=0.1710, lr=2.25e-06, step=450]Training:   0%|          | 451/200000 [54:34<399:15:22,  7.20s/it, loss=0.1524, lr=2.25e-06, step=451]Training:   0%|          | 452/200000 [54:41<398:32:07,  7.19s/it, loss=0.1524, lr=2.25e-06, step=451]Training:   0%|          | 452/200000 [54:41<398:32:07,  7.19s/it, loss=0.1270, lr=2.26e-06, step=452]Training:   0%|          | 453/200000 [54:49<398:49:22,  7.20s/it, loss=0.1270, lr=2.26e-06, step=452]Training:   0%|          | 453/200000 [54:49<398:49:22,  7.20s/it, loss=0.1608, lr=2.26e-06, step=453]Training:   0%|          | 454/200000 [54:56<399:12:48,  7.20s/it, loss=0.1608, lr=2.26e-06, step=453]Training:   0%|          | 454/200000 [54:56<399:12:48,  7.20s/it, loss=0.1523, lr=2.27e-06, step=454]Training:   0%|          | 455/200000 [55:03<398:44:50,  7.19s/it, loss=0.1523, lr=2.27e-06, step=454]Training:   0%|          | 455/200000 [55:03<398:44:50,  7.19s/it, loss=0.1511, lr=2.27e-06, step=455]Training:   0%|          | 456/200000 [55:10<398:27:44,  7.19s/it, loss=0.1511, lr=2.27e-06, step=455]Training:   0%|          | 456/200000 [55:10<398:27:44,  7.19s/it, loss=0.1380, lr=2.28e-06, step=456]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 457/200000 [55:17<397:44:02,  7.18s/it, loss=0.1380, lr=2.28e-06, step=456]Training:   0%|          | 457/200000 [55:17<397:44:02,  7.18s/it, loss=0.1211, lr=2.28e-06, step=457]Training:   0%|          | 458/200000 [55:25<397:44:14,  7.18s/it, loss=0.1211, lr=2.28e-06, step=457]Training:   0%|          | 458/200000 [55:25<397:44:14,  7.18s/it, loss=0.1488, lr=2.29e-06, step=458]Training:   0%|          | 459/200000 [55:32<397:47:54,  7.18s/it, loss=0.1488, lr=2.29e-06, step=458]Training:   0%|          | 459/200000 [55:32<397:47:54,  7.18s/it, loss=0.1343, lr=2.29e-06, step=459]Training:   0%|          | 460/200000 [55:39<397:40:52,  7.17s/it, loss=0.1343, lr=2.29e-06, step=459]Training:   0%|          | 460/200000 [55:39<397:40:52,  7.17s/it, loss=0.1352, lr=2.30e-06, step=460]Training:   0%|          | 461/200000 [55:46<397:49:01,  7.18s/it, loss=0.1352, lr=2.30e-06, step=460]Training:   0%|          | 461/200000 [55:46<397:49:01,  7.18s/it, loss=0.2200, lr=2.30e-06, step=461]Training:   0%|          | 462/200000 [55:53<398:08:54,  7.18s/it, loss=0.2200, lr=2.30e-06, step=461]Training:   0%|          | 462/200000 [55:53<398:08:54,  7.18s/it, loss=0.1248, lr=2.31e-06, step=462]Training:   0%|          | 463/200000 [56:00<397:54:28,  7.18s/it, loss=0.1248, lr=2.31e-06, step=462]Training:   0%|          | 463/200000 [56:00<397:54:28,  7.18s/it, loss=0.3041, lr=2.31e-06, step=463]Training:   0%|          | 464/200000 [56:08<397:39:33,  7.17s/it, loss=0.3041, lr=2.31e-06, step=463]Training:   0%|          | 464/200000 [56:08<397:39:33,  7.17s/it, loss=0.0956, lr=2.32e-06, step=464]Training:   0%|          | 465/200000 [56:15<398:03:28,  7.18s/it, loss=0.0956, lr=2.32e-06, step=464]Training:   0%|          | 465/200000 [56:15<398:03:28,  7.18s/it, loss=0.1421, lr=2.32e-06, step=465]Training:   0%|          | 466/200000 [56:22<397:53:18,  7.18s/it, loss=0.1421, lr=2.32e-06, step=465]Training:   0%|          | 466/200000 [56:22<397:53:18,  7.18s/it, loss=0.1646, lr=2.33e-06, step=466]Training:   0%|          | 467/200000 [56:29<398:07:53,  7.18s/it, loss=0.1646, lr=2.33e-06, step=466]Training:   0%|          | 467/200000 [56:29<398:07:53,  7.18s/it, loss=0.1789, lr=2.33e-06, step=467]Training:   0%|          | 468/200000 [56:36<398:57:20,  7.20s/it, loss=0.1789, lr=2.33e-06, step=467]Training:   0%|          | 468/200000 [56:36<398:57:20,  7.20s/it, loss=0.1473, lr=2.34e-06, step=468]Training:   0%|          | 469/200000 [56:44<398:45:11,  7.19s/it, loss=0.1473, lr=2.34e-06, step=468]Training:   0%|          | 469/200000 [56:44<398:45:11,  7.19s/it, loss=0.1727, lr=2.34e-06, step=469]Training:   0%|          | 470/200000 [56:51<398:55:00,  7.20s/it, loss=0.1727, lr=2.34e-06, step=469]Training:   0%|          | 470/200000 [56:51<398:55:00,  7.20s/it, loss=0.1509, lr=2.35e-06, step=470]Training:   0%|          | 471/200000 [56:58<398:36:40,  7.19s/it, loss=0.1509, lr=2.35e-06, step=470]Training:   0%|          | 471/200000 [56:58<398:36:40,  7.19s/it, loss=0.1110, lr=2.35e-06, step=471]Training:   0%|          | 472/200000 [57:05<398:18:59,  7.19s/it, loss=0.1110, lr=2.35e-06, step=471]Training:   0%|          | 472/200000 [57:05<398:18:59,  7.19s/it, loss=0.1329, lr=2.36e-06, step=472]Training:   0%|          | 473/200000 [57:12<398:06:07,  7.18s/it, loss=0.1329, lr=2.36e-06, step=472]Training:   0%|          | 473/200000 [57:12<398:06:07,  7.18s/it, loss=0.1841, lr=2.36e-06, step=473]Training:   0%|          | 474/200000 [57:20<398:12:17,  7.18s/it, loss=0.1841, lr=2.36e-06, step=473]Training:   0%|          | 474/200000 [57:20<398:12:17,  7.18s/it, loss=0.1451, lr=2.37e-06, step=474]Training:   0%|          | 475/200000 [57:27<398:11:48,  7.18s/it, loss=0.1451, lr=2.37e-06, step=474]Training:   0%|          | 475/200000 [57:27<398:11:48,  7.18s/it, loss=0.1485, lr=2.37e-06, step=475]Training:   0%|          | 476/200000 [57:34<398:12:14,  7.18s/it, loss=0.1485, lr=2.37e-06, step=475]Training:   0%|          | 476/200000 [57:34<398:12:14,  7.18s/it, loss=0.1005, lr=2.38e-06, step=476]Training:   0%|          | 477/200000 [57:41<398:34:12,  7.19s/it, loss=0.1005, lr=2.38e-06, step=476]Training:   0%|          | 477/200000 [57:41<398:34:12,  7.19s/it, loss=0.2272, lr=2.38e-06, step=477]Training:   0%|          | 478/200000 [57:48<398:08:58,  7.18s/it, loss=0.2272, lr=2.38e-06, step=477]Training:   0%|          | 478/200000 [57:48<398:08:58,  7.18s/it, loss=0.0999, lr=2.39e-06, step=478]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 479/200000 [57:55<398:20:28,  7.19s/it, loss=0.0999, lr=2.39e-06, step=478]Training:   0%|          | 479/200000 [57:55<398:20:28,  7.19s/it, loss=0.1051, lr=2.39e-06, step=479]Training:   0%|          | 480/200000 [58:03<397:55:23,  7.18s/it, loss=0.1051, lr=2.39e-06, step=479]Training:   0%|          | 480/200000 [58:03<397:55:23,  7.18s/it, loss=0.0841, lr=2.40e-06, step=480]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 481/200000 [58:10<398:01:51,  7.18s/it, loss=0.0841, lr=2.40e-06, step=480]Training:   0%|          | 481/200000 [58:10<398:01:51,  7.18s/it, loss=0.1248, lr=2.40e-06, step=481]Training:   0%|          | 482/200000 [58:17<397:49:20,  7.18s/it, loss=0.1248, lr=2.40e-06, step=481]Training:   0%|          | 482/200000 [58:17<397:49:20,  7.18s/it, loss=0.1625, lr=2.41e-06, step=482]Training:   0%|          | 483/200000 [58:24<398:12:15,  7.19s/it, loss=0.1625, lr=2.41e-06, step=482]Training:   0%|          | 483/200000 [58:24<398:12:15,  7.19s/it, loss=0.1877, lr=2.41e-06, step=483]Training:   0%|          | 484/200000 [58:31<398:25:39,  7.19s/it, loss=0.1877, lr=2.41e-06, step=483]Training:   0%|          | 484/200000 [58:31<398:25:39,  7.19s/it, loss=0.1659, lr=2.42e-06, step=484]Training:   0%|          | 485/200000 [58:39<398:04:22,  7.18s/it, loss=0.1659, lr=2.42e-06, step=484]Training:   0%|          | 485/200000 [58:39<398:04:22,  7.18s/it, loss=0.1473, lr=2.42e-06, step=485]Training:   0%|          | 486/200000 [58:46<397:47:25,  7.18s/it, loss=0.1473, lr=2.42e-06, step=485]Training:   0%|          | 486/200000 [58:46<397:47:25,  7.18s/it, loss=0.1322, lr=2.43e-06, step=486]Training:   0%|          | 487/200000 [58:53<398:07:05,  7.18s/it, loss=0.1322, lr=2.43e-06, step=486]Training:   0%|          | 487/200000 [58:53<398:07:05,  7.18s/it, loss=0.1298, lr=2.43e-06, step=487]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 488/200000 [59:00<397:54:47,  7.18s/it, loss=0.1298, lr=2.43e-06, step=487]Training:   0%|          | 488/200000 [59:00<397:54:47,  7.18s/it, loss=0.1221, lr=2.44e-06, step=488]Training:   0%|          | 489/200000 [59:07<400:39:05,  7.23s/it, loss=0.1221, lr=2.44e-06, step=488]Training:   0%|          | 489/200000 [59:07<400:39:05,  7.23s/it, loss=0.1207, lr=2.44e-06, step=489]Training:   0%|          | 490/200000 [59:15<400:04:45,  7.22s/it, loss=0.1207, lr=2.44e-06, step=489]Training:   0%|          | 490/200000 [59:15<400:04:45,  7.22s/it, loss=0.1468, lr=2.45e-06, step=490]Training:   0%|          | 491/200000 [59:22<399:31:52,  7.21s/it, loss=0.1468, lr=2.45e-06, step=490]Training:   0%|          | 491/200000 [59:22<399:31:52,  7.21s/it, loss=0.1491, lr=2.45e-06, step=491]Training:   0%|          | 492/200000 [59:29<398:47:07,  7.20s/it, loss=0.1491, lr=2.45e-06, step=491]Training:   0%|          | 492/200000 [59:29<398:47:07,  7.20s/it, loss=0.2131, lr=2.46e-06, step=492]Training:   0%|          | 493/200000 [59:36<398:32:34,  7.19s/it, loss=0.2131, lr=2.46e-06, step=492]Training:   0%|          | 493/200000 [59:36<398:32:34,  7.19s/it, loss=0.1176, lr=2.46e-06, step=493]Training:   0%|          | 494/200000 [59:43<398:43:39,  7.19s/it, loss=0.1176, lr=2.46e-06, step=493]Training:   0%|          | 494/200000 [59:43<398:43:39,  7.19s/it, loss=0.1887, lr=2.47e-06, step=494]Training:   0%|          | 495/200000 [59:51<398:21:00,  7.19s/it, loss=0.1887, lr=2.47e-06, step=494]Training:   0%|          | 495/200000 [59:51<398:21:00,  7.19s/it, loss=0.1113, lr=2.47e-06, step=495]Training:   0%|          | 496/200000 [59:58<398:17:31,  7.19s/it, loss=0.1113, lr=2.47e-06, step=495]Training:   0%|          | 496/200000 [59:58<398:17:31,  7.19s/it, loss=0.1438, lr=2.48e-06, step=496]Training:   0%|          | 497/200000 [1:00:05<398:22:03,  7.19s/it, loss=0.1438, lr=2.48e-06, step=496]Training:   0%|          | 497/200000 [1:00:05<398:22:03,  7.19s/it, loss=0.1446, lr=2.48e-06, step=497]Training:   0%|          | 498/200000 [1:00:12<398:16:47,  7.19s/it, loss=0.1446, lr=2.48e-06, step=497]Training:   0%|          | 498/200000 [1:00:12<398:16:47,  7.19s/it, loss=0.1329, lr=2.49e-06, step=498]Training:   0%|          | 499/200000 [1:00:19<398:21:02,  7.19s/it, loss=0.1329, lr=2.49e-06, step=498]Training:   0%|          | 499/200000 [1:00:19<398:21:02,  7.19s/it, loss=0.1416, lr=2.49e-06, step=499]Training:   0%|          | 500/200000 [1:00:26<397:27:23,  7.17s/it, loss=0.1416, lr=2.49e-06, step=499]Training:   0%|          | 500/200000 [1:00:26<397:27:23,  7.17s/it, loss=0.1241, lr=2.50e-06, step=500]20:57:02.418 [I] step=500 loss=0.1496 lr=2.26e-06 grad_norm=1.04 time=718.7s                      (486094:train_pytorch.py:582)
Training:   0%|          | 501/200000 [1:00:34<397:27:22,  7.17s/it, loss=0.1241, lr=2.50e-06, step=500]Training:   0%|          | 501/200000 [1:00:34<397:27:22,  7.17s/it, loss=0.1576, lr=2.50e-06, step=501]Training:   0%|          | 502/200000 [1:00:41<397:40:51,  7.18s/it, loss=0.1576, lr=2.50e-06, step=501]Training:   0%|          | 502/200000 [1:00:41<397:40:51,  7.18s/it, loss=0.0970, lr=2.51e-06, step=502]Training:   0%|          | 503/200000 [1:00:48<398:19:29,  7.19s/it, loss=0.0970, lr=2.51e-06, step=502]Training:   0%|          | 503/200000 [1:00:48<398:19:29,  7.19s/it, loss=0.1369, lr=2.51e-06, step=503]Training:   0%|          | 504/200000 [1:00:55<398:01:04,  7.18s/it, loss=0.1369, lr=2.51e-06, step=503]Training:   0%|          | 504/200000 [1:00:55<398:01:04,  7.18s/it, loss=0.1203, lr=2.52e-06, step=504]Training:   0%|          | 505/200000 [1:01:02<397:47:18,  7.18s/it, loss=0.1203, lr=2.52e-06, step=504]Training:   0%|          | 505/200000 [1:01:02<397:47:18,  7.18s/it, loss=0.1427, lr=2.52e-06, step=505]Training:   0%|          | 506/200000 [1:01:10<397:25:12,  7.17s/it, loss=0.1427, lr=2.52e-06, step=505]Training:   0%|          | 506/200000 [1:01:10<397:25:12,  7.17s/it, loss=0.2065, lr=2.53e-06, step=506]Training:   0%|          | 507/200000 [1:01:17<397:18:47,  7.17s/it, loss=0.2065, lr=2.53e-06, step=506]Training:   0%|          | 507/200000 [1:01:17<397:18:47,  7.17s/it, loss=0.1539, lr=2.53e-06, step=507]Training:   0%|          | 508/200000 [1:01:24<397:28:44,  7.17s/it, loss=0.1539, lr=2.53e-06, step=507]Training:   0%|          | 508/200000 [1:01:24<397:28:44,  7.17s/it, loss=0.1701, lr=2.54e-06, step=508]Training:   0%|          | 509/200000 [1:01:31<397:54:01,  7.18s/it, loss=0.1701, lr=2.54e-06, step=508]Training:   0%|          | 509/200000 [1:01:31<397:54:01,  7.18s/it, loss=0.1617, lr=2.54e-06, step=509]Training:   0%|          | 510/200000 [1:01:38<400:11:53,  7.22s/it, loss=0.1617, lr=2.54e-06, step=509]Training:   0%|          | 510/200000 [1:01:38<400:11:53,  7.22s/it, loss=0.1904, lr=2.55e-06, step=510]Training:   0%|          | 511/200000 [1:01:46<399:33:36,  7.21s/it, loss=0.1904, lr=2.55e-06, step=510]Training:   0%|          | 511/200000 [1:01:46<399:33:36,  7.21s/it, loss=0.1247, lr=2.55e-06, step=511]Training:   0%|          | 512/200000 [1:01:53<398:57:23,  7.20s/it, loss=0.1247, lr=2.55e-06, step=511]Training:   0%|          | 512/200000 [1:01:53<398:57:23,  7.20s/it, loss=0.1230, lr=2.56e-06, step=512]Training:   0%|          | 513/200000 [1:02:00<398:40:01,  7.19s/it, loss=0.1230, lr=2.56e-06, step=512]Training:   0%|          | 513/200000 [1:02:00<398:40:01,  7.19s/it, loss=0.1359, lr=2.56e-06, step=513]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 514/200000 [1:02:07<398:18:52,  7.19s/it, loss=0.1359, lr=2.56e-06, step=513]Training:   0%|          | 514/200000 [1:02:07<398:18:52,  7.19s/it, loss=0.1497, lr=2.57e-06, step=514]Training:   0%|          | 515/200000 [1:02:14<399:39:45,  7.21s/it, loss=0.1497, lr=2.57e-06, step=514]Training:   0%|          | 515/200000 [1:02:14<399:39:45,  7.21s/it, loss=0.1607, lr=2.57e-06, step=515]Training:   0%|          | 516/200000 [1:02:22<399:38:27,  7.21s/it, loss=0.1607, lr=2.57e-06, step=515]Training:   0%|          | 516/200000 [1:02:22<399:38:27,  7.21s/it, loss=0.1609, lr=2.58e-06, step=516]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 517/200000 [1:02:29<398:54:12,  7.20s/it, loss=0.1609, lr=2.58e-06, step=516]Training:   0%|          | 517/200000 [1:02:29<398:54:12,  7.20s/it, loss=0.1865, lr=2.58e-06, step=517]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 518/200000 [1:02:36<399:11:47,  7.20s/it, loss=0.1865, lr=2.58e-06, step=517]Training:   0%|          | 518/200000 [1:02:36<399:11:47,  7.20s/it, loss=0.1420, lr=2.59e-06, step=518]Training:   0%|          | 519/200000 [1:02:43<399:11:56,  7.20s/it, loss=0.1420, lr=2.59e-06, step=518]Training:   0%|          | 519/200000 [1:02:43<399:11:56,  7.20s/it, loss=0.0984, lr=2.59e-06, step=519]Training:   0%|          | 520/200000 [1:02:50<399:14:34,  7.21s/it, loss=0.0984, lr=2.59e-06, step=519]Training:   0%|          | 520/200000 [1:02:50<399:14:34,  7.21s/it, loss=0.1031, lr=2.60e-06, step=520]Training:   0%|          | 521/200000 [1:02:58<399:14:38,  7.21s/it, loss=0.1031, lr=2.60e-06, step=520]Training:   0%|          | 521/200000 [1:02:58<399:14:38,  7.21s/it, loss=0.1173, lr=2.60e-06, step=521]Training:   0%|          | 522/200000 [1:03:05<399:11:17,  7.20s/it, loss=0.1173, lr=2.60e-06, step=521]Training:   0%|          | 522/200000 [1:03:05<399:11:17,  7.20s/it, loss=0.0929, lr=2.61e-06, step=522]Training:   0%|          | 523/200000 [1:03:12<398:45:07,  7.20s/it, loss=0.0929, lr=2.61e-06, step=522]Training:   0%|          | 523/200000 [1:03:12<398:45:07,  7.20s/it, loss=0.1161, lr=2.61e-06, step=523]Training:   0%|          | 524/200000 [1:03:19<398:32:16,  7.19s/it, loss=0.1161, lr=2.61e-06, step=523]Training:   0%|          | 524/200000 [1:03:19<398:32:16,  7.19s/it, loss=0.1346, lr=2.62e-06, step=524]Training:   0%|          | 525/200000 [1:03:26<398:26:39,  7.19s/it, loss=0.1346, lr=2.62e-06, step=524]Training:   0%|          | 525/200000 [1:03:26<398:26:39,  7.19s/it, loss=0.1216, lr=2.62e-06, step=525]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 526/200000 [1:03:34<398:52:50,  7.20s/it, loss=0.1216, lr=2.62e-06, step=525]Training:   0%|          | 526/200000 [1:03:34<398:52:50,  7.20s/it, loss=0.1403, lr=2.63e-06, step=526]Training:   0%|          | 527/200000 [1:03:41<398:43:36,  7.20s/it, loss=0.1403, lr=2.63e-06, step=526]Training:   0%|          | 527/200000 [1:03:41<398:43:36,  7.20s/it, loss=0.1248, lr=2.63e-06, step=527]Training:   0%|          | 528/200000 [1:03:48<398:18:43,  7.19s/it, loss=0.1248, lr=2.63e-06, step=527]Training:   0%|          | 528/200000 [1:03:48<398:18:43,  7.19s/it, loss=0.1162, lr=2.64e-06, step=528]Training:   0%|          | 529/200000 [1:03:55<398:10:27,  7.19s/it, loss=0.1162, lr=2.64e-06, step=528]Training:   0%|          | 529/200000 [1:03:55<398:10:27,  7.19s/it, loss=0.1338, lr=2.64e-06, step=529]Training:   0%|          | 530/200000 [1:04:02<398:03:32,  7.18s/it, loss=0.1338, lr=2.64e-06, step=529]Training:   0%|          | 530/200000 [1:04:02<398:03:32,  7.18s/it, loss=0.1489, lr=2.65e-06, step=530]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 531/200000 [1:04:09<398:36:32,  7.19s/it, loss=0.1489, lr=2.65e-06, step=530]Training:   0%|          | 531/200000 [1:04:09<398:36:32,  7.19s/it, loss=0.1164, lr=2.65e-06, step=531]Training:   0%|          | 532/200000 [1:04:17<398:12:43,  7.19s/it, loss=0.1164, lr=2.65e-06, step=531]Training:   0%|          | 532/200000 [1:04:17<398:12:43,  7.19s/it, loss=0.1310, lr=2.66e-06, step=532]Training:   0%|          | 533/200000 [1:04:24<398:11:21,  7.19s/it, loss=0.1310, lr=2.66e-06, step=532]Training:   0%|          | 533/200000 [1:04:24<398:11:21,  7.19s/it, loss=0.1174, lr=2.66e-06, step=533]Training:   0%|          | 534/200000 [1:04:31<397:59:52,  7.18s/it, loss=0.1174, lr=2.66e-06, step=533]Training:   0%|          | 534/200000 [1:04:31<397:59:52,  7.18s/it, loss=0.2023, lr=2.67e-06, step=534]Training:   0%|          | 535/200000 [1:04:38<398:09:48,  7.19s/it, loss=0.2023, lr=2.67e-06, step=534]Training:   0%|          | 535/200000 [1:04:38<398:09:48,  7.19s/it, loss=0.1323, lr=2.67e-06, step=535]Training:   0%|          | 536/200000 [1:04:45<397:46:44,  7.18s/it, loss=0.1323, lr=2.67e-06, step=535]Training:   0%|          | 536/200000 [1:04:45<397:46:44,  7.18s/it, loss=0.1392, lr=2.68e-06, step=536]Training:   0%|          | 537/200000 [1:04:53<397:34:03,  7.18s/it, loss=0.1392, lr=2.68e-06, step=536]Training:   0%|          | 537/200000 [1:04:53<397:34:03,  7.18s/it, loss=0.1783, lr=2.68e-06, step=537]Training:   0%|          | 538/200000 [1:05:00<398:09:47,  7.19s/it, loss=0.1783, lr=2.68e-06, step=537]Training:   0%|          | 538/200000 [1:05:00<398:09:47,  7.19s/it, loss=0.1199, lr=2.69e-06, step=538]Training:   0%|          | 539/200000 [1:05:07<398:01:30,  7.18s/it, loss=0.1199, lr=2.69e-06, step=538]Training:   0%|          | 539/200000 [1:05:07<398:01:30,  7.18s/it, loss=0.1309, lr=2.69e-06, step=539]Training:   0%|          | 540/200000 [1:05:14<397:50:24,  7.18s/it, loss=0.1309, lr=2.69e-06, step=539]Training:   0%|          | 540/200000 [1:05:14<397:50:24,  7.18s/it, loss=0.0733, lr=2.70e-06, step=540]Training:   0%|          | 541/200000 [1:05:21<397:51:06,  7.18s/it, loss=0.0733, lr=2.70e-06, step=540]Training:   0%|          | 541/200000 [1:05:21<397:51:06,  7.18s/it, loss=0.2155, lr=2.70e-06, step=541]Training:   0%|          | 542/200000 [1:05:28<397:38:20,  7.18s/it, loss=0.2155, lr=2.70e-06, step=541]Training:   0%|          | 542/200000 [1:05:28<397:38:20,  7.18s/it, loss=0.1186, lr=2.71e-06, step=542]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 543/200000 [1:05:36<397:35:10,  7.18s/it, loss=0.1186, lr=2.71e-06, step=542]Training:   0%|          | 543/200000 [1:05:36<397:35:10,  7.18s/it, loss=0.1254, lr=2.71e-06, step=543]Training:   0%|          | 544/200000 [1:05:43<397:24:27,  7.17s/it, loss=0.1254, lr=2.71e-06, step=543]Training:   0%|          | 544/200000 [1:05:43<397:24:27,  7.17s/it, loss=0.1474, lr=2.72e-06, step=544]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 545/200000 [1:05:50<397:52:05,  7.18s/it, loss=0.1474, lr=2.72e-06, step=544]Training:   0%|          | 545/200000 [1:05:50<397:52:05,  7.18s/it, loss=0.1596, lr=2.72e-06, step=545]Training:   0%|          | 546/200000 [1:05:57<397:31:47,  7.18s/it, loss=0.1596, lr=2.72e-06, step=545]Training:   0%|          | 546/200000 [1:05:57<397:31:47,  7.18s/it, loss=0.1686, lr=2.73e-06, step=546]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 547/200000 [1:06:04<397:55:53,  7.18s/it, loss=0.1686, lr=2.73e-06, step=546]Training:   0%|          | 547/200000 [1:06:04<397:55:53,  7.18s/it, loss=0.1211, lr=2.73e-06, step=547]Training:   0%|          | 548/200000 [1:06:12<398:08:01,  7.19s/it, loss=0.1211, lr=2.73e-06, step=547]Training:   0%|          | 548/200000 [1:06:12<398:08:01,  7.19s/it, loss=0.1061, lr=2.74e-06, step=548]Training:   0%|          | 549/200000 [1:06:19<398:14:53,  7.19s/it, loss=0.1061, lr=2.74e-06, step=548]Training:   0%|          | 549/200000 [1:06:19<398:14:53,  7.19s/it, loss=0.1759, lr=2.74e-06, step=549]Training:   0%|          | 550/200000 [1:06:26<398:25:35,  7.19s/it, loss=0.1759, lr=2.74e-06, step=549]Training:   0%|          | 550/200000 [1:06:26<398:25:35,  7.19s/it, loss=0.0985, lr=2.75e-06, step=550]Training:   0%|          | 551/200000 [1:06:33<397:53:29,  7.18s/it, loss=0.0985, lr=2.75e-06, step=550]Training:   0%|          | 551/200000 [1:06:33<397:53:29,  7.18s/it, loss=0.1467, lr=2.75e-06, step=551]Training:   0%|          | 552/200000 [1:06:40<399:42:02,  7.21s/it, loss=0.1467, lr=2.75e-06, step=551]Training:   0%|          | 552/200000 [1:06:40<399:42:02,  7.21s/it, loss=0.1709, lr=2.76e-06, step=552]Training:   0%|          | 553/200000 [1:06:48<399:07:58,  7.20s/it, loss=0.1709, lr=2.76e-06, step=552]Training:   0%|          | 553/200000 [1:06:48<399:07:58,  7.20s/it, loss=0.1522, lr=2.76e-06, step=553]Training:   0%|          | 554/200000 [1:06:55<398:42:08,  7.20s/it, loss=0.1522, lr=2.76e-06, step=553]Training:   0%|          | 554/200000 [1:06:55<398:42:08,  7.20s/it, loss=0.0975, lr=2.77e-06, step=554]Training:   0%|          | 555/200000 [1:07:02<398:42:38,  7.20s/it, loss=0.0975, lr=2.77e-06, step=554]Training:   0%|          | 555/200000 [1:07:02<398:42:38,  7.20s/it, loss=0.0996, lr=2.77e-06, step=555]Training:   0%|          | 556/200000 [1:07:09<398:35:56,  7.19s/it, loss=0.0996, lr=2.77e-06, step=555]Training:   0%|          | 556/200000 [1:07:09<398:35:56,  7.19s/it, loss=0.0935, lr=2.78e-06, step=556]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 557/200000 [1:07:16<398:35:44,  7.19s/it, loss=0.0935, lr=2.78e-06, step=556]Training:   0%|          | 557/200000 [1:07:16<398:35:44,  7.19s/it, loss=0.1027, lr=2.78e-06, step=557]Training:   0%|          | 558/200000 [1:07:24<398:37:54,  7.20s/it, loss=0.1027, lr=2.78e-06, step=557]Training:   0%|          | 558/200000 [1:07:24<398:37:54,  7.20s/it, loss=0.1236, lr=2.79e-06, step=558]Training:   0%|          | 559/200000 [1:07:31<398:43:42,  7.20s/it, loss=0.1236, lr=2.79e-06, step=558]Training:   0%|          | 559/200000 [1:07:31<398:43:42,  7.20s/it, loss=0.0963, lr=2.79e-06, step=559]Training:   0%|          | 560/200000 [1:07:38<398:13:41,  7.19s/it, loss=0.0963, lr=2.79e-06, step=559]Training:   0%|          | 560/200000 [1:07:38<398:13:41,  7.19s/it, loss=0.1361, lr=2.80e-06, step=560]Training:   0%|          | 561/200000 [1:07:45<397:52:47,  7.18s/it, loss=0.1361, lr=2.80e-06, step=560]Training:   0%|          | 561/200000 [1:07:45<397:52:47,  7.18s/it, loss=0.1149, lr=2.80e-06, step=561]Training:   0%|          | 562/200000 [1:07:52<397:30:44,  7.18s/it, loss=0.1149, lr=2.80e-06, step=561]Training:   0%|          | 562/200000 [1:07:52<397:30:44,  7.18s/it, loss=0.1120, lr=2.81e-06, step=562]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 563/200000 [1:07:59<397:27:05,  7.17s/it, loss=0.1120, lr=2.81e-06, step=562]Training:   0%|          | 563/200000 [1:07:59<397:27:05,  7.17s/it, loss=0.1194, lr=2.81e-06, step=563]Training:   0%|          | 564/200000 [1:08:07<397:05:24,  7.17s/it, loss=0.1194, lr=2.81e-06, step=563]Training:   0%|          | 564/200000 [1:08:07<397:05:24,  7.17s/it, loss=0.1639, lr=2.82e-06, step=564]Training:   0%|          | 565/200000 [1:08:14<397:15:02,  7.17s/it, loss=0.1639, lr=2.82e-06, step=564]Training:   0%|          | 565/200000 [1:08:14<397:15:02,  7.17s/it, loss=0.1341, lr=2.82e-06, step=565]Training:   0%|          | 566/200000 [1:08:21<397:50:34,  7.18s/it, loss=0.1341, lr=2.82e-06, step=565]Training:   0%|          | 566/200000 [1:08:21<397:50:34,  7.18s/it, loss=0.1402, lr=2.83e-06, step=566]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 567/200000 [1:08:28<398:10:51,  7.19s/it, loss=0.1402, lr=2.83e-06, step=566]Training:   0%|          | 567/200000 [1:08:28<398:10:51,  7.19s/it, loss=0.1030, lr=2.83e-06, step=567]Training:   0%|          | 568/200000 [1:08:35<398:10:59,  7.19s/it, loss=0.1030, lr=2.83e-06, step=567]Training:   0%|          | 568/200000 [1:08:35<398:10:59,  7.19s/it, loss=0.2072, lr=2.84e-06, step=568]Training:   0%|          | 569/200000 [1:08:43<398:11:06,  7.19s/it, loss=0.2072, lr=2.84e-06, step=568]Training:   0%|          | 569/200000 [1:08:43<398:11:06,  7.19s/it, loss=0.1732, lr=2.84e-06, step=569]Training:   0%|          | 570/200000 [1:08:50<397:57:45,  7.18s/it, loss=0.1732, lr=2.84e-06, step=569]Training:   0%|          | 570/200000 [1:08:50<397:57:45,  7.18s/it, loss=0.1101, lr=2.85e-06, step=570]Training:   0%|          | 571/200000 [1:08:57<398:09:59,  7.19s/it, loss=0.1101, lr=2.85e-06, step=570]Training:   0%|          | 571/200000 [1:08:57<398:09:59,  7.19s/it, loss=0.1188, lr=2.85e-06, step=571]Training:   0%|          | 572/200000 [1:09:04<397:53:57,  7.18s/it, loss=0.1188, lr=2.85e-06, step=571]Training:   0%|          | 572/200000 [1:09:04<397:53:57,  7.18s/it, loss=0.1281, lr=2.86e-06, step=572]Training:   0%|          | 573/200000 [1:09:11<400:29:01,  7.23s/it, loss=0.1281, lr=2.86e-06, step=572]Training:   0%|          | 573/200000 [1:09:11<400:29:01,  7.23s/it, loss=0.1272, lr=2.86e-06, step=573]Training:   0%|          | 574/200000 [1:09:19<399:48:00,  7.22s/it, loss=0.1272, lr=2.86e-06, step=573]Training:   0%|          | 574/200000 [1:09:19<399:48:00,  7.22s/it, loss=0.1046, lr=2.87e-06, step=574]Training:   0%|          | 575/200000 [1:09:26<399:25:57,  7.21s/it, loss=0.1046, lr=2.87e-06, step=574]Training:   0%|          | 575/200000 [1:09:26<399:25:57,  7.21s/it, loss=0.1336, lr=2.87e-06, step=575]Training:   0%|          | 576/200000 [1:09:33<399:04:20,  7.20s/it, loss=0.1336, lr=2.87e-06, step=575]Training:   0%|          | 576/200000 [1:09:33<399:04:20,  7.20s/it, loss=0.1216, lr=2.88e-06, step=576]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 577/200000 [1:09:40<398:47:30,  7.20s/it, loss=0.1216, lr=2.88e-06, step=576]Training:   0%|          | 577/200000 [1:09:40<398:47:30,  7.20s/it, loss=0.0840, lr=2.88e-06, step=577]Training:   0%|          | 578/200000 [1:09:47<398:32:58,  7.19s/it, loss=0.0840, lr=2.88e-06, step=577]Training:   0%|          | 578/200000 [1:09:47<398:32:58,  7.19s/it, loss=0.1273, lr=2.89e-06, step=578]Training:   0%|          | 579/200000 [1:09:55<398:27:21,  7.19s/it, loss=0.1273, lr=2.89e-06, step=578]Training:   0%|          | 579/200000 [1:09:55<398:27:21,  7.19s/it, loss=0.1057, lr=2.89e-06, step=579]Training:   0%|          | 580/200000 [1:10:02<398:53:55,  7.20s/it, loss=0.1057, lr=2.89e-06, step=579]Training:   0%|          | 580/200000 [1:10:02<398:53:55,  7.20s/it, loss=0.1488, lr=2.90e-06, step=580]Training:   0%|          | 581/200000 [1:10:09<398:46:47,  7.20s/it, loss=0.1488, lr=2.90e-06, step=580]Training:   0%|          | 581/200000 [1:10:09<398:46:47,  7.20s/it, loss=0.2040, lr=2.90e-06, step=581]Training:   0%|          | 582/200000 [1:10:16<398:39:06,  7.20s/it, loss=0.2040, lr=2.90e-06, step=581]Training:   0%|          | 582/200000 [1:10:16<398:39:06,  7.20s/it, loss=0.1494, lr=2.91e-06, step=582]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 583/200000 [1:10:23<398:07:15,  7.19s/it, loss=0.1494, lr=2.91e-06, step=582]Training:   0%|          | 583/200000 [1:10:23<398:07:15,  7.19s/it, loss=0.1117, lr=2.91e-06, step=583]Training:   0%|          | 584/200000 [1:10:30<398:00:01,  7.18s/it, loss=0.1117, lr=2.91e-06, step=583]Training:   0%|          | 584/200000 [1:10:30<398:00:01,  7.18s/it, loss=0.1363, lr=2.92e-06, step=584]Training:   0%|          | 585/200000 [1:10:38<397:41:21,  7.18s/it, loss=0.1363, lr=2.92e-06, step=584]Training:   0%|          | 585/200000 [1:10:38<397:41:21,  7.18s/it, loss=0.1223, lr=2.92e-06, step=585]Training:   0%|          | 586/200000 [1:10:45<397:43:21,  7.18s/it, loss=0.1223, lr=2.92e-06, step=585]Training:   0%|          | 586/200000 [1:10:45<397:43:21,  7.18s/it, loss=0.1206, lr=2.93e-06, step=586]Training:   0%|          | 587/200000 [1:10:52<397:16:54,  7.17s/it, loss=0.1206, lr=2.93e-06, step=586]Training:   0%|          | 587/200000 [1:10:52<397:16:54,  7.17s/it, loss=0.1488, lr=2.93e-06, step=587]Training:   0%|          | 588/200000 [1:10:59<397:32:07,  7.18s/it, loss=0.1488, lr=2.93e-06, step=587]Training:   0%|          | 588/200000 [1:10:59<397:32:07,  7.18s/it, loss=0.1313, lr=2.94e-06, step=588]Training:   0%|          | 589/200000 [1:11:06<397:43:19,  7.18s/it, loss=0.1313, lr=2.94e-06, step=588]Training:   0%|          | 589/200000 [1:11:06<397:43:19,  7.18s/it, loss=0.2274, lr=2.94e-06, step=589]Training:   0%|          | 590/200000 [1:11:14<398:15:42,  7.19s/it, loss=0.2274, lr=2.94e-06, step=589]Training:   0%|          | 590/200000 [1:11:14<398:15:42,  7.19s/it, loss=0.1023, lr=2.95e-06, step=590]Training:   0%|          | 591/200000 [1:11:21<397:50:19,  7.18s/it, loss=0.1023, lr=2.95e-06, step=590]Training:   0%|          | 591/200000 [1:11:21<397:50:19,  7.18s/it, loss=0.1088, lr=2.95e-06, step=591]Training:   0%|          | 592/200000 [1:11:28<398:10:07,  7.19s/it, loss=0.1088, lr=2.95e-06, step=591]Training:   0%|          | 592/200000 [1:11:28<398:10:07,  7.19s/it, loss=0.1447, lr=2.96e-06, step=592]Training:   0%|          | 593/200000 [1:11:35<397:45:50,  7.18s/it, loss=0.1447, lr=2.96e-06, step=592]Training:   0%|          | 593/200000 [1:11:35<397:45:50,  7.18s/it, loss=0.1213, lr=2.96e-06, step=593]Training:   0%|          | 594/200000 [1:11:42<398:18:55,  7.19s/it, loss=0.1213, lr=2.96e-06, step=593]Training:   0%|          | 594/200000 [1:11:42<398:18:55,  7.19s/it, loss=0.1561, lr=2.97e-06, step=594]Training:   0%|          | 595/200000 [1:11:49<397:52:58,  7.18s/it, loss=0.1561, lr=2.97e-06, step=594]Training:   0%|          | 595/200000 [1:11:49<397:52:58,  7.18s/it, loss=0.0945, lr=2.97e-06, step=595]Training:   0%|          | 596/200000 [1:11:57<398:08:41,  7.19s/it, loss=0.0945, lr=2.97e-06, step=595]Training:   0%|          | 596/200000 [1:11:57<398:08:41,  7.19s/it, loss=0.1168, lr=2.98e-06, step=596]Training:   0%|          | 597/200000 [1:12:04<397:58:16,  7.18s/it, loss=0.1168, lr=2.98e-06, step=596]Training:   0%|          | 597/200000 [1:12:04<397:58:16,  7.18s/it, loss=0.2021, lr=2.98e-06, step=597]Training:   0%|          | 598/200000 [1:12:11<398:14:03,  7.19s/it, loss=0.2021, lr=2.98e-06, step=597]Training:   0%|          | 598/200000 [1:12:11<398:14:03,  7.19s/it, loss=0.1300, lr=2.99e-06, step=598]Training:   0%|          | 599/200000 [1:12:18<397:58:52,  7.19s/it, loss=0.1300, lr=2.99e-06, step=598]Training:   0%|          | 599/200000 [1:12:18<397:58:52,  7.19s/it, loss=0.1027, lr=2.99e-06, step=599]Training:   0%|          | 600/200000 [1:12:25<397:55:19,  7.18s/it, loss=0.1027, lr=2.99e-06, step=599]Training:   0%|          | 600/200000 [1:12:25<397:55:19,  7.18s/it, loss=0.1441, lr=3.00e-06, step=600]21:09:01.362 [I] step=600 loss=0.1351 lr=2.76e-06 grad_norm=0.90 time=718.9s                      (486094:train_pytorch.py:582)
Training:   0%|          | 601/200000 [1:12:33<397:32:41,  7.18s/it, loss=0.1441, lr=3.00e-06, step=600]Training:   0%|          | 601/200000 [1:12:33<397:32:41,  7.18s/it, loss=0.1525, lr=3.00e-06, step=601]Training:   0%|          | 602/200000 [1:12:40<397:49:45,  7.18s/it, loss=0.1525, lr=3.00e-06, step=601]Training:   0%|          | 602/200000 [1:12:40<397:49:45,  7.18s/it, loss=0.1105, lr=3.01e-06, step=602]Training:   0%|          | 603/200000 [1:12:47<397:38:11,  7.18s/it, loss=0.1105, lr=3.01e-06, step=602]Training:   0%|          | 603/200000 [1:12:47<397:38:11,  7.18s/it, loss=0.1016, lr=3.01e-06, step=603]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 604/200000 [1:12:54<397:48:44,  7.18s/it, loss=0.1016, lr=3.01e-06, step=603]Training:   0%|          | 604/200000 [1:12:54<397:48:44,  7.18s/it, loss=0.1345, lr=3.02e-06, step=604]Training:   0%|          | 605/200000 [1:13:01<397:48:44,  7.18s/it, loss=0.1345, lr=3.02e-06, step=604]Training:   0%|          | 605/200000 [1:13:01<397:48:44,  7.18s/it, loss=0.0896, lr=3.02e-06, step=605]Training:   0%|          | 606/200000 [1:13:08<397:55:31,  7.18s/it, loss=0.0896, lr=3.02e-06, step=605]Training:   0%|          | 606/200000 [1:13:08<397:55:31,  7.18s/it, loss=0.1260, lr=3.03e-06, step=606]Training:   0%|          | 607/200000 [1:13:16<397:36:53,  7.18s/it, loss=0.1260, lr=3.03e-06, step=606]Training:   0%|          | 607/200000 [1:13:16<397:36:53,  7.18s/it, loss=0.1028, lr=3.03e-06, step=607]Training:   0%|          | 608/200000 [1:13:23<397:20:12,  7.17s/it, loss=0.1028, lr=3.03e-06, step=607]Training:   0%|          | 608/200000 [1:13:23<397:20:12,  7.17s/it, loss=0.1065, lr=3.04e-06, step=608]Training:   0%|          | 609/200000 [1:13:30<397:40:31,  7.18s/it, loss=0.1065, lr=3.04e-06, step=608]Training:   0%|          | 609/200000 [1:13:30<397:40:31,  7.18s/it, loss=0.0946, lr=3.04e-06, step=609]Training:   0%|          | 610/200000 [1:13:37<397:25:05,  7.18s/it, loss=0.0946, lr=3.04e-06, step=609]Training:   0%|          | 610/200000 [1:13:37<397:25:05,  7.18s/it, loss=0.1668, lr=3.05e-06, step=610]Training:   0%|          | 611/200000 [1:13:44<397:54:22,  7.18s/it, loss=0.1668, lr=3.05e-06, step=610]Training:   0%|          | 611/200000 [1:13:44<397:54:22,  7.18s/it, loss=0.1542, lr=3.05e-06, step=611]Training:   0%|          | 612/200000 [1:13:52<397:43:37,  7.18s/it, loss=0.1542, lr=3.05e-06, step=611]Training:   0%|          | 612/200000 [1:13:52<397:43:37,  7.18s/it, loss=0.1090, lr=3.06e-06, step=612]Training:   0%|          | 613/200000 [1:13:59<397:36:51,  7.18s/it, loss=0.1090, lr=3.06e-06, step=612]Training:   0%|          | 613/200000 [1:13:59<397:36:51,  7.18s/it, loss=0.1059, lr=3.06e-06, step=613]Training:   0%|          | 614/200000 [1:14:06<397:49:53,  7.18s/it, loss=0.1059, lr=3.06e-06, step=613]Training:   0%|          | 614/200000 [1:14:06<397:49:53,  7.18s/it, loss=0.1209, lr=3.07e-06, step=614]Training:   0%|          | 615/200000 [1:14:13<399:51:58,  7.22s/it, loss=0.1209, lr=3.07e-06, step=614]Training:   0%|          | 615/200000 [1:14:13<399:51:58,  7.22s/it, loss=0.1274, lr=3.07e-06, step=615]Training:   0%|          | 616/200000 [1:14:20<399:20:33,  7.21s/it, loss=0.1274, lr=3.07e-06, step=615]Training:   0%|          | 616/200000 [1:14:20<399:20:33,  7.21s/it, loss=0.0941, lr=3.08e-06, step=616]Training:   0%|          | 617/200000 [1:14:28<399:02:07,  7.20s/it, loss=0.0941, lr=3.08e-06, step=616]Training:   0%|          | 617/200000 [1:14:28<399:02:07,  7.20s/it, loss=0.1125, lr=3.08e-06, step=617]Training:   0%|          | 618/200000 [1:14:35<398:58:51,  7.20s/it, loss=0.1125, lr=3.08e-06, step=617]Training:   0%|          | 618/200000 [1:14:35<398:58:51,  7.20s/it, loss=0.2280, lr=3.09e-06, step=618]Training:   0%|          | 619/200000 [1:14:42<398:25:51,  7.19s/it, loss=0.2280, lr=3.09e-06, step=618]Training:   0%|          | 619/200000 [1:14:42<398:25:51,  7.19s/it, loss=0.1912, lr=3.09e-06, step=619]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 620/200000 [1:14:49<398:33:40,  7.20s/it, loss=0.1912, lr=3.09e-06, step=619]Training:   0%|          | 620/200000 [1:14:49<398:33:40,  7.20s/it, loss=0.1421, lr=3.10e-06, step=620]Training:   0%|          | 621/200000 [1:14:56<397:18:08,  7.17s/it, loss=0.1421, lr=3.10e-06, step=620]Training:   0%|          | 621/200000 [1:14:56<397:18:08,  7.17s/it, loss=0.0999, lr=3.10e-06, step=621]Training:   0%|          | 622/200000 [1:15:04<397:46:32,  7.18s/it, loss=0.0999, lr=3.10e-06, step=621]Training:   0%|          | 622/200000 [1:15:04<397:46:32,  7.18s/it, loss=0.1224, lr=3.11e-06, step=622]Training:   0%|          | 623/200000 [1:15:11<397:14:31,  7.17s/it, loss=0.1224, lr=3.11e-06, step=622]Training:   0%|          | 623/200000 [1:15:11<397:14:31,  7.17s/it, loss=0.1289, lr=3.11e-06, step=623]Training:   0%|          | 624/200000 [1:15:18<397:35:06,  7.18s/it, loss=0.1289, lr=3.11e-06, step=623]Training:   0%|          | 624/200000 [1:15:18<397:35:06,  7.18s/it, loss=0.0973, lr=3.12e-06, step=624]Training:   0%|          | 625/200000 [1:15:25<397:38:46,  7.18s/it, loss=0.0973, lr=3.12e-06, step=624]Training:   0%|          | 625/200000 [1:15:25<397:38:46,  7.18s/it, loss=0.0834, lr=3.12e-06, step=625]Training:   0%|          | 626/200000 [1:15:32<397:31:02,  7.18s/it, loss=0.0834, lr=3.12e-06, step=625]Training:   0%|          | 626/200000 [1:15:32<397:31:02,  7.18s/it, loss=0.1100, lr=3.13e-06, step=626]Training:   0%|          | 627/200000 [1:15:39<397:52:58,  7.18s/it, loss=0.1100, lr=3.13e-06, step=626]Training:   0%|          | 627/200000 [1:15:39<397:52:58,  7.18s/it, loss=0.0872, lr=3.13e-06, step=627]Training:   0%|          | 628/200000 [1:15:47<397:33:29,  7.18s/it, loss=0.0872, lr=3.13e-06, step=627]Training:   0%|          | 628/200000 [1:15:47<397:33:29,  7.18s/it, loss=0.0891, lr=3.14e-06, step=628]Training:   0%|          | 629/200000 [1:15:54<397:54:25,  7.18s/it, loss=0.0891, lr=3.14e-06, step=628]Training:   0%|          | 629/200000 [1:15:54<397:54:25,  7.18s/it, loss=0.1741, lr=3.14e-06, step=629]Training:   0%|          | 630/200000 [1:16:01<397:42:48,  7.18s/it, loss=0.1741, lr=3.14e-06, step=629]Training:   0%|          | 630/200000 [1:16:01<397:42:48,  7.18s/it, loss=0.2484, lr=3.15e-06, step=630]Training:   0%|          | 631/200000 [1:16:08<397:56:47,  7.19s/it, loss=0.2484, lr=3.15e-06, step=630]Training:   0%|          | 631/200000 [1:16:08<397:56:47,  7.19s/it, loss=0.1406, lr=3.15e-06, step=631]Training:   0%|          | 632/200000 [1:16:15<397:58:21,  7.19s/it, loss=0.1406, lr=3.15e-06, step=631]Training:   0%|          | 632/200000 [1:16:15<397:58:21,  7.19s/it, loss=0.1443, lr=3.16e-06, step=632]Training:   0%|          | 633/200000 [1:16:23<398:11:31,  7.19s/it, loss=0.1443, lr=3.16e-06, step=632]Training:   0%|          | 633/200000 [1:16:23<398:11:31,  7.19s/it, loss=0.1076, lr=3.16e-06, step=633]Training:   0%|          | 634/200000 [1:16:30<398:17:43,  7.19s/it, loss=0.1076, lr=3.16e-06, step=633]Training:   0%|          | 634/200000 [1:16:30<398:17:43,  7.19s/it, loss=0.1834, lr=3.17e-06, step=634]Training:   0%|          | 635/200000 [1:16:37<398:32:38,  7.20s/it, loss=0.1834, lr=3.17e-06, step=634]Training:   0%|          | 635/200000 [1:16:37<398:32:38,  7.20s/it, loss=0.0913, lr=3.17e-06, step=635]Training:   0%|          | 636/200000 [1:16:44<400:02:25,  7.22s/it, loss=0.0913, lr=3.17e-06, step=635]Training:   0%|          | 636/200000 [1:16:44<400:02:25,  7.22s/it, loss=0.0853, lr=3.18e-06, step=636]Training:   0%|          | 637/200000 [1:16:51<399:38:37,  7.22s/it, loss=0.0853, lr=3.18e-06, step=636]Training:   0%|          | 637/200000 [1:16:51<399:38:37,  7.22s/it, loss=0.1269, lr=3.18e-06, step=637]Training:   0%|          | 638/200000 [1:16:59<398:55:38,  7.20s/it, loss=0.1269, lr=3.18e-06, step=637]Training:   0%|          | 638/200000 [1:16:59<398:55:38,  7.20s/it, loss=0.1566, lr=3.19e-06, step=638]Training:   0%|          | 639/200000 [1:17:06<398:50:53,  7.20s/it, loss=0.1566, lr=3.19e-06, step=638]Training:   0%|          | 639/200000 [1:17:06<398:50:53,  7.20s/it, loss=0.1305, lr=3.19e-06, step=639]Training:   0%|          | 640/200000 [1:17:13<398:16:09,  7.19s/it, loss=0.1305, lr=3.19e-06, step=639]Training:   0%|          | 640/200000 [1:17:13<398:16:09,  7.19s/it, loss=0.1617, lr=3.20e-06, step=640]Training:   0%|          | 641/200000 [1:17:20<398:28:36,  7.20s/it, loss=0.1617, lr=3.20e-06, step=640]Training:   0%|          | 641/200000 [1:17:20<398:28:36,  7.20s/it, loss=0.1071, lr=3.20e-06, step=641]Training:   0%|          | 642/200000 [1:17:27<398:27:19,  7.20s/it, loss=0.1071, lr=3.20e-06, step=641]Training:   0%|          | 642/200000 [1:17:27<398:27:19,  7.20s/it, loss=0.1275, lr=3.21e-06, step=642]Training:   0%|          | 643/200000 [1:17:35<398:22:30,  7.19s/it, loss=0.1275, lr=3.21e-06, step=642]Training:   0%|          | 643/200000 [1:17:35<398:22:30,  7.19s/it, loss=0.1436, lr=3.21e-06, step=643]Training:   0%|          | 644/200000 [1:17:42<398:12:10,  7.19s/it, loss=0.1436, lr=3.21e-06, step=643]Training:   0%|          | 644/200000 [1:17:42<398:12:10,  7.19s/it, loss=0.0944, lr=3.22e-06, step=644]Training:   0%|          | 645/200000 [1:17:49<398:12:54,  7.19s/it, loss=0.0944, lr=3.22e-06, step=644]Training:   0%|          | 645/200000 [1:17:49<398:12:54,  7.19s/it, loss=0.0786, lr=3.22e-06, step=645]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 646/200000 [1:17:56<398:28:22,  7.20s/it, loss=0.0786, lr=3.22e-06, step=645]Training:   0%|          | 646/200000 [1:17:56<398:28:22,  7.20s/it, loss=0.1213, lr=3.23e-06, step=646]Training:   0%|          | 647/200000 [1:18:03<398:29:56,  7.20s/it, loss=0.1213, lr=3.23e-06, step=646]Training:   0%|          | 647/200000 [1:18:03<398:29:56,  7.20s/it, loss=0.1139, lr=3.23e-06, step=647]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 648/200000 [1:18:11<398:34:56,  7.20s/it, loss=0.1139, lr=3.23e-06, step=647]Training:   0%|          | 648/200000 [1:18:11<398:34:56,  7.20s/it, loss=0.1012, lr=3.24e-06, step=648]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 649/200000 [1:18:18<397:54:34,  7.19s/it, loss=0.1012, lr=3.24e-06, step=648]Training:   0%|          | 649/200000 [1:18:18<397:54:34,  7.19s/it, loss=0.1182, lr=3.24e-06, step=649]Training:   0%|          | 650/200000 [1:18:25<397:47:38,  7.18s/it, loss=0.1182, lr=3.24e-06, step=649]Training:   0%|          | 650/200000 [1:18:25<397:47:38,  7.18s/it, loss=0.1050, lr=3.25e-06, step=650]Training:   0%|          | 651/200000 [1:18:32<398:03:41,  7.19s/it, loss=0.1050, lr=3.25e-06, step=650]Training:   0%|          | 651/200000 [1:18:32<398:03:41,  7.19s/it, loss=0.1163, lr=3.25e-06, step=651]Training:   0%|          | 652/200000 [1:18:39<397:49:03,  7.18s/it, loss=0.1163, lr=3.25e-06, step=651]Training:   0%|          | 652/200000 [1:18:39<397:49:03,  7.18s/it, loss=0.1436, lr=3.26e-06, step=652]Training:   0%|          | 653/200000 [1:18:46<397:50:32,  7.18s/it, loss=0.1436, lr=3.26e-06, step=652]Training:   0%|          | 653/200000 [1:18:46<397:50:32,  7.18s/it, loss=0.1238, lr=3.26e-06, step=653]Training:   0%|          | 654/200000 [1:18:54<398:10:49,  7.19s/it, loss=0.1238, lr=3.26e-06, step=653]Training:   0%|          | 654/200000 [1:18:54<398:10:49,  7.19s/it, loss=0.1294, lr=3.27e-06, step=654]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 655/200000 [1:19:01<397:44:18,  7.18s/it, loss=0.1294, lr=3.27e-06, step=654]Training:   0%|          | 655/200000 [1:19:01<397:44:18,  7.18s/it, loss=0.1054, lr=3.27e-06, step=655]Training:   0%|          | 656/200000 [1:19:08<398:15:32,  7.19s/it, loss=0.1054, lr=3.27e-06, step=655]Training:   0%|          | 656/200000 [1:19:08<398:15:32,  7.19s/it, loss=0.1398, lr=3.28e-06, step=656]Training:   0%|          | 657/200000 [1:19:15<398:30:31,  7.20s/it, loss=0.1398, lr=3.28e-06, step=656]Training:   0%|          | 657/200000 [1:19:15<398:30:31,  7.20s/it, loss=0.1265, lr=3.28e-06, step=657]Training:   0%|          | 658/200000 [1:19:22<398:46:45,  7.20s/it, loss=0.1265, lr=3.28e-06, step=657]Training:   0%|          | 658/200000 [1:19:22<398:46:45,  7.20s/it, loss=0.0952, lr=3.29e-06, step=658]Training:   0%|          | 659/200000 [1:19:30<398:23:48,  7.19s/it, loss=0.0952, lr=3.29e-06, step=658]Training:   0%|          | 659/200000 [1:19:30<398:23:48,  7.19s/it, loss=0.1424, lr=3.29e-06, step=659]Training:   0%|          | 660/200000 [1:19:37<398:17:37,  7.19s/it, loss=0.1424, lr=3.29e-06, step=659]Training:   0%|          | 660/200000 [1:19:37<398:17:37,  7.19s/it, loss=0.1613, lr=3.30e-06, step=660]Training:   0%|          | 661/200000 [1:19:44<398:10:21,  7.19s/it, loss=0.1613, lr=3.30e-06, step=660]Training:   0%|          | 661/200000 [1:19:44<398:10:21,  7.19s/it, loss=0.1334, lr=3.30e-06, step=661]Training:   0%|          | 662/200000 [1:19:51<397:37:35,  7.18s/it, loss=0.1334, lr=3.30e-06, step=661]Training:   0%|          | 662/200000 [1:19:51<397:37:35,  7.18s/it, loss=0.0978, lr=3.31e-06, step=662]Training:   0%|          | 663/200000 [1:19:58<397:50:23,  7.18s/it, loss=0.0978, lr=3.31e-06, step=662]Training:   0%|          | 663/200000 [1:19:58<397:50:23,  7.18s/it, loss=0.1162, lr=3.31e-06, step=663]Training:   0%|          | 664/200000 [1:20:06<397:43:13,  7.18s/it, loss=0.1162, lr=3.31e-06, step=663]Training:   0%|          | 664/200000 [1:20:06<397:43:13,  7.18s/it, loss=0.1148, lr=3.32e-06, step=664]Training:   0%|          | 665/200000 [1:20:13<397:42:37,  7.18s/it, loss=0.1148, lr=3.32e-06, step=664]Training:   0%|          | 665/200000 [1:20:13<397:42:37,  7.18s/it, loss=0.1447, lr=3.32e-06, step=665]Training:   0%|          | 666/200000 [1:20:20<397:54:51,  7.19s/it, loss=0.1447, lr=3.32e-06, step=665]Training:   0%|          | 666/200000 [1:20:20<397:54:51,  7.19s/it, loss=0.1494, lr=3.33e-06, step=666]Training:   0%|          | 667/200000 [1:20:27<397:32:04,  7.18s/it, loss=0.1494, lr=3.33e-06, step=666]Training:   0%|          | 667/200000 [1:20:27<397:32:04,  7.18s/it, loss=0.1198, lr=3.33e-06, step=667]Training:   0%|          | 668/200000 [1:20:34<397:25:14,  7.18s/it, loss=0.1198, lr=3.33e-06, step=667]Training:   0%|          | 668/200000 [1:20:34<397:25:14,  7.18s/it, loss=0.1087, lr=3.34e-06, step=668]Training:   0%|          | 669/200000 [1:20:41<397:41:34,  7.18s/it, loss=0.1087, lr=3.34e-06, step=668]Training:   0%|          | 669/200000 [1:20:41<397:41:34,  7.18s/it, loss=0.1422, lr=3.34e-06, step=669]Training:   0%|          | 670/200000 [1:20:49<397:25:34,  7.18s/it, loss=0.1422, lr=3.34e-06, step=669]Training:   0%|          | 670/200000 [1:20:49<397:25:34,  7.18s/it, loss=0.1295, lr=3.35e-06, step=670]Training:   0%|          | 671/200000 [1:20:56<397:12:02,  7.17s/it, loss=0.1295, lr=3.35e-06, step=670]Training:   0%|          | 671/200000 [1:20:56<397:12:02,  7.17s/it, loss=0.1345, lr=3.35e-06, step=671]Training:   0%|          | 672/200000 [1:21:03<397:24:43,  7.18s/it, loss=0.1345, lr=3.35e-06, step=671]Training:   0%|          | 672/200000 [1:21:03<397:24:43,  7.18s/it, loss=0.0934, lr=3.36e-06, step=672]Training:   0%|          | 673/200000 [1:21:10<397:29:06,  7.18s/it, loss=0.0934, lr=3.36e-06, step=672]Training:   0%|          | 673/200000 [1:21:10<397:29:06,  7.18s/it, loss=0.0960, lr=3.36e-06, step=673]Training:   0%|          | 674/200000 [1:21:17<397:48:51,  7.18s/it, loss=0.0960, lr=3.36e-06, step=673]Training:   0%|          | 674/200000 [1:21:17<397:48:51,  7.18s/it, loss=0.0999, lr=3.37e-06, step=674]Training:   0%|          | 675/200000 [1:21:25<398:08:23,  7.19s/it, loss=0.0999, lr=3.37e-06, step=674]Training:   0%|          | 675/200000 [1:21:25<398:08:23,  7.19s/it, loss=0.1242, lr=3.37e-06, step=675]Training:   0%|          | 676/200000 [1:21:32<397:39:37,  7.18s/it, loss=0.1242, lr=3.37e-06, step=675]Training:   0%|          | 676/200000 [1:21:32<397:39:37,  7.18s/it, loss=0.1090, lr=3.38e-06, step=676]Training:   0%|          | 677/200000 [1:21:39<397:55:40,  7.19s/it, loss=0.1090, lr=3.38e-06, step=676]Training:   0%|          | 677/200000 [1:21:39<397:55:40,  7.19s/it, loss=0.2562, lr=3.38e-06, step=677]Training:   0%|          | 678/200000 [1:21:46<399:46:18,  7.22s/it, loss=0.2562, lr=3.38e-06, step=677]Training:   0%|          | 678/200000 [1:21:46<399:46:18,  7.22s/it, loss=0.1128, lr=3.39e-06, step=678]Training:   0%|          | 679/200000 [1:21:53<399:24:01,  7.21s/it, loss=0.1128, lr=3.39e-06, step=678]Training:   0%|          | 679/200000 [1:21:53<399:24:01,  7.21s/it, loss=0.0998, lr=3.39e-06, step=679]Training:   0%|          | 680/200000 [1:22:01<398:42:23,  7.20s/it, loss=0.0998, lr=3.39e-06, step=679]Training:   0%|          | 680/200000 [1:22:01<398:42:23,  7.20s/it, loss=0.0871, lr=3.40e-06, step=680]Training:   0%|          | 681/200000 [1:22:08<398:49:20,  7.20s/it, loss=0.0871, lr=3.40e-06, step=680]Training:   0%|          | 681/200000 [1:22:08<398:49:20,  7.20s/it, loss=0.1136, lr=3.40e-06, step=681]Training:   0%|          | 682/200000 [1:22:15<398:48:23,  7.20s/it, loss=0.1136, lr=3.40e-06, step=681]Training:   0%|          | 682/200000 [1:22:15<398:48:23,  7.20s/it, loss=0.1310, lr=3.41e-06, step=682]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 683/200000 [1:22:22<399:02:04,  7.21s/it, loss=0.1310, lr=3.41e-06, step=682]Training:   0%|          | 683/200000 [1:22:22<399:02:04,  7.21s/it, loss=0.1137, lr=3.41e-06, step=683]Training:   0%|          | 684/200000 [1:22:29<398:28:03,  7.20s/it, loss=0.1137, lr=3.41e-06, step=683]Training:   0%|          | 684/200000 [1:22:29<398:28:03,  7.20s/it, loss=0.1006, lr=3.42e-06, step=684]Training:   0%|          | 685/200000 [1:22:37<398:19:30,  7.19s/it, loss=0.1006, lr=3.42e-06, step=684]Training:   0%|          | 685/200000 [1:22:37<398:19:30,  7.19s/it, loss=0.1157, lr=3.42e-06, step=685]Training:   0%|          | 686/200000 [1:22:44<398:14:29,  7.19s/it, loss=0.1157, lr=3.42e-06, step=685]Training:   0%|          | 686/200000 [1:22:44<398:14:29,  7.19s/it, loss=0.1014, lr=3.43e-06, step=686]Training:   0%|          | 687/200000 [1:22:51<398:14:11,  7.19s/it, loss=0.1014, lr=3.43e-06, step=686]Training:   0%|          | 687/200000 [1:22:51<398:14:11,  7.19s/it, loss=0.1711, lr=3.43e-06, step=687]Training:   0%|          | 688/200000 [1:22:58<398:29:57,  7.20s/it, loss=0.1711, lr=3.43e-06, step=687]Training:   0%|          | 688/200000 [1:22:58<398:29:57,  7.20s/it, loss=0.0970, lr=3.44e-06, step=688]Training:   0%|          | 689/200000 [1:23:05<397:57:34,  7.19s/it, loss=0.0970, lr=3.44e-06, step=688]Training:   0%|          | 689/200000 [1:23:05<397:57:34,  7.19s/it, loss=0.1049, lr=3.44e-06, step=689]Training:   0%|          | 690/200000 [1:23:12<398:07:43,  7.19s/it, loss=0.1049, lr=3.44e-06, step=689]Training:   0%|          | 690/200000 [1:23:12<398:07:43,  7.19s/it, loss=0.1101, lr=3.45e-06, step=690]Training:   0%|          | 691/200000 [1:23:20<397:51:17,  7.19s/it, loss=0.1101, lr=3.45e-06, step=690]Training:   0%|          | 691/200000 [1:23:20<397:51:17,  7.19s/it, loss=0.1204, lr=3.45e-06, step=691]Training:   0%|          | 692/200000 [1:23:27<397:45:26,  7.18s/it, loss=0.1204, lr=3.45e-06, step=691]Training:   0%|          | 692/200000 [1:23:27<397:45:26,  7.18s/it, loss=0.1476, lr=3.46e-06, step=692]Training:   0%|          | 693/200000 [1:23:34<397:14:11,  7.18s/it, loss=0.1476, lr=3.46e-06, step=692]Training:   0%|          | 693/200000 [1:23:34<397:14:11,  7.18s/it, loss=0.1201, lr=3.46e-06, step=693]Training:   0%|          | 694/200000 [1:23:41<397:02:32,  7.17s/it, loss=0.1201, lr=3.46e-06, step=693]Training:   0%|          | 694/200000 [1:23:41<397:02:32,  7.17s/it, loss=0.1260, lr=3.47e-06, step=694]Training:   0%|          | 695/200000 [1:23:48<397:07:58,  7.17s/it, loss=0.1260, lr=3.47e-06, step=694]Training:   0%|          | 695/200000 [1:23:48<397:07:58,  7.17s/it, loss=0.1166, lr=3.47e-06, step=695]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 696/200000 [1:23:56<397:16:45,  7.18s/it, loss=0.1166, lr=3.47e-06, step=695]Training:   0%|          | 696/200000 [1:23:56<397:16:45,  7.18s/it, loss=0.0873, lr=3.48e-06, step=696]Training:   0%|          | 697/200000 [1:24:03<397:07:22,  7.17s/it, loss=0.0873, lr=3.48e-06, step=696]Training:   0%|          | 697/200000 [1:24:03<397:07:22,  7.17s/it, loss=0.1228, lr=3.48e-06, step=697]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 698/200000 [1:24:10<397:16:21,  7.18s/it, loss=0.1228, lr=3.48e-06, step=697]Training:   0%|          | 698/200000 [1:24:10<397:16:21,  7.18s/it, loss=0.1361, lr=3.49e-06, step=698]Training:   0%|          | 699/200000 [1:24:17<399:05:23,  7.21s/it, loss=0.1361, lr=3.49e-06, step=698]Training:   0%|          | 699/200000 [1:24:17<399:05:23,  7.21s/it, loss=0.1022, lr=3.49e-06, step=699]Training:   0%|          | 700/200000 [1:24:24<398:27:11,  7.20s/it, loss=0.1022, lr=3.49e-06, step=699]Training:   0%|          | 700/200000 [1:24:24<398:27:11,  7.20s/it, loss=0.1488, lr=3.50e-06, step=700]21:21:00.324 [I] step=700 loss=0.1234 lr=3.26e-06 grad_norm=0.83 time=719.0s                      (486094:train_pytorch.py:582)
Training:   0%|          | 701/200000 [1:24:32<398:23:45,  7.20s/it, loss=0.1488, lr=3.50e-06, step=700]Training:   0%|          | 701/200000 [1:24:32<398:23:45,  7.20s/it, loss=0.0979, lr=3.50e-06, step=701]Training:   0%|          | 702/200000 [1:24:39<398:43:35,  7.20s/it, loss=0.0979, lr=3.50e-06, step=701]Training:   0%|          | 702/200000 [1:24:39<398:43:35,  7.20s/it, loss=0.1019, lr=3.51e-06, step=702]Training:   0%|          | 703/200000 [1:24:46<398:23:52,  7.20s/it, loss=0.1019, lr=3.51e-06, step=702]Training:   0%|          | 703/200000 [1:24:46<398:23:52,  7.20s/it, loss=0.1442, lr=3.51e-06, step=703]Training:   0%|          | 704/200000 [1:24:53<398:20:43,  7.20s/it, loss=0.1442, lr=3.51e-06, step=703]Training:   0%|          | 704/200000 [1:24:53<398:20:43,  7.20s/it, loss=0.1565, lr=3.52e-06, step=704]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 705/200000 [1:25:00<396:43:46,  7.17s/it, loss=0.1565, lr=3.52e-06, step=704]Training:   0%|          | 705/200000 [1:25:00<396:43:46,  7.17s/it, loss=0.1115, lr=3.52e-06, step=705]Training:   0%|          | 706/200000 [1:25:07<397:22:03,  7.18s/it, loss=0.1115, lr=3.52e-06, step=705]Training:   0%|          | 706/200000 [1:25:07<397:22:03,  7.18s/it, loss=0.1160, lr=3.53e-06, step=706]Training:   0%|          | 707/200000 [1:25:15<396:55:08,  7.17s/it, loss=0.1160, lr=3.53e-06, step=706]Training:   0%|          | 707/200000 [1:25:15<396:55:08,  7.17s/it, loss=0.1018, lr=3.53e-06, step=707]Training:   0%|          | 708/200000 [1:25:22<396:48:43,  7.17s/it, loss=0.1018, lr=3.53e-06, step=707]Training:   0%|          | 708/200000 [1:25:22<396:48:43,  7.17s/it, loss=0.1266, lr=3.54e-06, step=708]Training:   0%|          | 709/200000 [1:25:29<397:18:44,  7.18s/it, loss=0.1266, lr=3.54e-06, step=708]Training:   0%|          | 709/200000 [1:25:29<397:18:44,  7.18s/it, loss=0.2016, lr=3.54e-06, step=709]Training:   0%|          | 710/200000 [1:25:36<397:31:31,  7.18s/it, loss=0.2016, lr=3.54e-06, step=709]Training:   0%|          | 710/200000 [1:25:36<397:31:31,  7.18s/it, loss=0.1324, lr=3.55e-06, step=710]Training:   0%|          | 711/200000 [1:25:43<397:23:34,  7.18s/it, loss=0.1324, lr=3.55e-06, step=710]Training:   0%|          | 711/200000 [1:25:43<397:23:34,  7.18s/it, loss=0.0976, lr=3.55e-06, step=711]Training:   0%|          | 712/200000 [1:25:50<397:05:35,  7.17s/it, loss=0.0976, lr=3.55e-06, step=711]Training:   0%|          | 712/200000 [1:25:50<397:05:35,  7.17s/it, loss=0.1727, lr=3.56e-06, step=712]Training:   0%|          | 713/200000 [1:25:58<397:08:41,  7.17s/it, loss=0.1727, lr=3.56e-06, step=712]Training:   0%|          | 713/200000 [1:25:58<397:08:41,  7.17s/it, loss=0.1171, lr=3.56e-06, step=713]Training:   0%|          | 714/200000 [1:26:05<397:42:59,  7.18s/it, loss=0.1171, lr=3.56e-06, step=713]Training:   0%|          | 714/200000 [1:26:05<397:42:59,  7.18s/it, loss=0.0964, lr=3.57e-06, step=714]Training:   0%|          | 715/200000 [1:26:12<398:00:57,  7.19s/it, loss=0.0964, lr=3.57e-06, step=714]Training:   0%|          | 715/200000 [1:26:12<398:00:57,  7.19s/it, loss=0.0883, lr=3.57e-06, step=715]Training:   0%|          | 716/200000 [1:26:19<397:48:03,  7.19s/it, loss=0.0883, lr=3.57e-06, step=715]Training:   0%|          | 716/200000 [1:26:19<397:48:03,  7.19s/it, loss=0.1065, lr=3.58e-06, step=716]Training:   0%|          | 717/200000 [1:26:26<398:05:51,  7.19s/it, loss=0.1065, lr=3.58e-06, step=716]Training:   0%|          | 717/200000 [1:26:26<398:05:51,  7.19s/it, loss=0.1093, lr=3.58e-06, step=717]Training:   0%|          | 718/200000 [1:26:34<398:11:36,  7.19s/it, loss=0.1093, lr=3.58e-06, step=717]Training:   0%|          | 718/200000 [1:26:34<398:11:36,  7.19s/it, loss=0.1229, lr=3.59e-06, step=718]Training:   0%|          | 719/200000 [1:26:41<398:24:46,  7.20s/it, loss=0.1229, lr=3.59e-06, step=718]Training:   0%|          | 719/200000 [1:26:41<398:24:46,  7.20s/it, loss=0.0890, lr=3.59e-06, step=719]Training:   0%|          | 720/200000 [1:26:48<399:11:32,  7.21s/it, loss=0.0890, lr=3.59e-06, step=719]Training:   0%|          | 720/200000 [1:26:48<399:11:32,  7.21s/it, loss=0.0978, lr=3.60e-06, step=720]Training:   0%|          | 721/200000 [1:26:55<398:25:25,  7.20s/it, loss=0.0978, lr=3.60e-06, step=720]Training:   0%|          | 721/200000 [1:26:55<398:25:25,  7.20s/it, loss=0.0958, lr=3.60e-06, step=721]Training:   0%|          | 722/200000 [1:27:02<397:57:07,  7.19s/it, loss=0.0958, lr=3.60e-06, step=721]Training:   0%|          | 722/200000 [1:27:02<397:57:07,  7.19s/it, loss=0.1099, lr=3.61e-06, step=722]Training:   0%|          | 723/200000 [1:27:10<397:18:49,  7.18s/it, loss=0.1099, lr=3.61e-06, step=722]Training:   0%|          | 723/200000 [1:27:10<397:18:49,  7.18s/it, loss=0.0918, lr=3.61e-06, step=723]Training:   0%|          | 724/200000 [1:27:17<397:50:12,  7.19s/it, loss=0.0918, lr=3.61e-06, step=723]Training:   0%|          | 724/200000 [1:27:17<397:50:12,  7.19s/it, loss=0.1591, lr=3.62e-06, step=724]Training:   0%|          | 725/200000 [1:27:24<397:41:09,  7.18s/it, loss=0.1591, lr=3.62e-06, step=724]Training:   0%|          | 725/200000 [1:27:24<397:41:09,  7.18s/it, loss=0.1133, lr=3.62e-06, step=725]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 726/200000 [1:27:31<397:44:55,  7.19s/it, loss=0.1133, lr=3.62e-06, step=725]Training:   0%|          | 726/200000 [1:27:31<397:44:55,  7.19s/it, loss=0.1271, lr=3.63e-06, step=726]Training:   0%|          | 727/200000 [1:27:38<397:49:20,  7.19s/it, loss=0.1271, lr=3.63e-06, step=726]Training:   0%|          | 727/200000 [1:27:38<397:49:20,  7.19s/it, loss=0.1167, lr=3.63e-06, step=727]Training:   0%|          | 728/200000 [1:27:46<398:05:36,  7.19s/it, loss=0.1167, lr=3.63e-06, step=727]Training:   0%|          | 728/200000 [1:27:46<398:05:36,  7.19s/it, loss=0.1117, lr=3.64e-06, step=728]Training:   0%|          | 729/200000 [1:27:53<397:48:37,  7.19s/it, loss=0.1117, lr=3.64e-06, step=728]Training:   0%|          | 729/200000 [1:27:53<397:48:37,  7.19s/it, loss=0.1149, lr=3.64e-06, step=729]Training:   0%|          | 730/200000 [1:28:00<397:33:56,  7.18s/it, loss=0.1149, lr=3.64e-06, step=729]Training:   0%|          | 730/200000 [1:28:00<397:33:56,  7.18s/it, loss=0.1322, lr=3.65e-06, step=730]Training:   0%|          | 731/200000 [1:28:07<397:19:23,  7.18s/it, loss=0.1322, lr=3.65e-06, step=730]Training:   0%|          | 731/200000 [1:28:07<397:19:23,  7.18s/it, loss=0.1200, lr=3.65e-06, step=731]Training:   0%|          | 732/200000 [1:28:14<397:25:22,  7.18s/it, loss=0.1200, lr=3.65e-06, step=731]Training:   0%|          | 732/200000 [1:28:14<397:25:22,  7.18s/it, loss=0.0936, lr=3.66e-06, step=732]Training:   0%|          | 733/200000 [1:28:21<397:25:19,  7.18s/it, loss=0.0936, lr=3.66e-06, step=732]Training:   0%|          | 733/200000 [1:28:21<397:25:19,  7.18s/it, loss=0.1383, lr=3.66e-06, step=733]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 734/200000 [1:28:29<397:31:04,  7.18s/it, loss=0.1383, lr=3.66e-06, step=733]Training:   0%|          | 734/200000 [1:28:29<397:31:04,  7.18s/it, loss=0.1471, lr=3.67e-06, step=734]Training:   0%|          | 735/200000 [1:28:36<397:33:03,  7.18s/it, loss=0.1471, lr=3.67e-06, step=734]Training:   0%|          | 735/200000 [1:28:36<397:33:03,  7.18s/it, loss=0.0915, lr=3.67e-06, step=735]Training:   0%|          | 736/200000 [1:28:43<397:58:51,  7.19s/it, loss=0.0915, lr=3.67e-06, step=735]Training:   0%|          | 736/200000 [1:28:43<397:58:51,  7.19s/it, loss=0.1231, lr=3.68e-06, step=736]Training:   0%|          | 737/200000 [1:28:50<397:42:23,  7.19s/it, loss=0.1231, lr=3.68e-06, step=736]Training:   0%|          | 737/200000 [1:28:50<397:42:23,  7.19s/it, loss=0.1163, lr=3.68e-06, step=737]Training:   0%|          | 738/200000 [1:28:57<397:33:15,  7.18s/it, loss=0.1163, lr=3.68e-06, step=737]Training:   0%|          | 738/200000 [1:28:57<397:33:15,  7.18s/it, loss=0.0938, lr=3.69e-06, step=738]Training:   0%|          | 739/200000 [1:29:05<397:50:19,  7.19s/it, loss=0.0938, lr=3.69e-06, step=738]Training:   0%|          | 739/200000 [1:29:05<397:50:19,  7.19s/it, loss=0.1111, lr=3.69e-06, step=739]Training:   0%|          | 740/200000 [1:29:12<398:08:09,  7.19s/it, loss=0.1111, lr=3.69e-06, step=739]Training:   0%|          | 740/200000 [1:29:12<398:08:09,  7.19s/it, loss=0.1156, lr=3.70e-06, step=740]Training:   0%|          | 741/200000 [1:29:19<400:10:55,  7.23s/it, loss=0.1156, lr=3.70e-06, step=740]Training:   0%|          | 741/200000 [1:29:19<400:10:55,  7.23s/it, loss=0.1018, lr=3.70e-06, step=741]Training:   0%|          | 742/200000 [1:29:26<399:07:53,  7.21s/it, loss=0.1018, lr=3.70e-06, step=741]Training:   0%|          | 742/200000 [1:29:26<399:07:53,  7.21s/it, loss=0.1228, lr=3.71e-06, step=742]Training:   0%|          | 743/200000 [1:29:33<398:27:57,  7.20s/it, loss=0.1228, lr=3.71e-06, step=742]Training:   0%|          | 743/200000 [1:29:33<398:27:57,  7.20s/it, loss=0.1015, lr=3.71e-06, step=743]Training:   0%|          | 744/200000 [1:29:41<397:48:17,  7.19s/it, loss=0.1015, lr=3.71e-06, step=743]Training:   0%|          | 744/200000 [1:29:41<397:48:17,  7.19s/it, loss=0.1102, lr=3.72e-06, step=744]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 745/200000 [1:29:48<398:13:53,  7.19s/it, loss=0.1102, lr=3.72e-06, step=744]Training:   0%|          | 745/200000 [1:29:48<398:13:53,  7.19s/it, loss=0.1290, lr=3.72e-06, step=745]Training:   0%|          | 746/200000 [1:29:55<397:48:13,  7.19s/it, loss=0.1290, lr=3.72e-06, step=745]Training:   0%|          | 746/200000 [1:29:55<397:48:13,  7.19s/it, loss=0.2100, lr=3.73e-06, step=746]Training:   0%|          | 747/200000 [1:30:02<398:03:35,  7.19s/it, loss=0.2100, lr=3.73e-06, step=746]Training:   0%|          | 747/200000 [1:30:02<398:03:35,  7.19s/it, loss=0.0967, lr=3.73e-06, step=747]Training:   0%|          | 748/200000 [1:30:09<397:41:00,  7.19s/it, loss=0.0967, lr=3.73e-06, step=747]Training:   0%|          | 748/200000 [1:30:09<397:41:00,  7.19s/it, loss=0.0945, lr=3.74e-06, step=748]Training:   0%|          | 749/200000 [1:30:17<398:09:57,  7.19s/it, loss=0.0945, lr=3.74e-06, step=748]Training:   0%|          | 749/200000 [1:30:17<398:09:57,  7.19s/it, loss=0.1072, lr=3.74e-06, step=749]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 750/200000 [1:30:24<398:01:48,  7.19s/it, loss=0.1072, lr=3.74e-06, step=749]Training:   0%|          | 750/200000 [1:30:24<398:01:48,  7.19s/it, loss=0.0942, lr=3.75e-06, step=750]Training:   0%|          | 751/200000 [1:30:31<397:33:50,  7.18s/it, loss=0.0942, lr=3.75e-06, step=750]Training:   0%|          | 751/200000 [1:30:31<397:33:50,  7.18s/it, loss=0.1334, lr=3.75e-06, step=751]Training:   0%|          | 752/200000 [1:30:38<397:23:02,  7.18s/it, loss=0.1334, lr=3.75e-06, step=751]Training:   0%|          | 752/200000 [1:30:38<397:23:02,  7.18s/it, loss=0.1001, lr=3.76e-06, step=752]Training:   0%|          | 753/200000 [1:30:45<397:06:10,  7.17s/it, loss=0.1001, lr=3.76e-06, step=752]Training:   0%|          | 753/200000 [1:30:45<397:06:10,  7.17s/it, loss=0.1695, lr=3.76e-06, step=753]Training:   0%|          | 754/200000 [1:30:52<397:01:42,  7.17s/it, loss=0.1695, lr=3.76e-06, step=753]Training:   0%|          | 754/200000 [1:30:52<397:01:42,  7.17s/it, loss=0.1022, lr=3.77e-06, step=754]Training:   0%|          | 755/200000 [1:31:00<397:21:56,  7.18s/it, loss=0.1022, lr=3.77e-06, step=754]Training:   0%|          | 755/200000 [1:31:00<397:21:56,  7.18s/it, loss=0.0981, lr=3.77e-06, step=755]Training:   0%|          | 756/200000 [1:31:07<396:12:24,  7.16s/it, loss=0.0981, lr=3.77e-06, step=755]Training:   0%|          | 756/200000 [1:31:07<396:12:24,  7.16s/it, loss=0.1071, lr=3.78e-06, step=756]Training:   0%|          | 757/200000 [1:31:14<396:45:03,  7.17s/it, loss=0.1071, lr=3.78e-06, step=756]Training:   0%|          | 757/200000 [1:31:14<396:45:03,  7.17s/it, loss=0.1286, lr=3.78e-06, step=757]Training:   0%|          | 758/200000 [1:31:21<397:00:05,  7.17s/it, loss=0.1286, lr=3.78e-06, step=757]Training:   0%|          | 758/200000 [1:31:21<397:00:05,  7.17s/it, loss=0.0868, lr=3.79e-06, step=758]Training:   0%|          | 759/200000 [1:31:28<396:50:04,  7.17s/it, loss=0.0868, lr=3.79e-06, step=758]Training:   0%|          | 759/200000 [1:31:28<396:50:04,  7.17s/it, loss=0.0920, lr=3.79e-06, step=759]Training:   0%|          | 760/200000 [1:31:35<397:17:13,  7.18s/it, loss=0.0920, lr=3.79e-06, step=759]Training:   0%|          | 760/200000 [1:31:35<397:17:13,  7.18s/it, loss=0.1985, lr=3.80e-06, step=760]Training:   0%|          | 761/200000 [1:31:43<396:55:10,  7.17s/it, loss=0.1985, lr=3.80e-06, step=760]Training:   0%|          | 761/200000 [1:31:43<396:55:10,  7.17s/it, loss=0.1168, lr=3.80e-06, step=761]Training:   0%|          | 762/200000 [1:31:50<399:02:05,  7.21s/it, loss=0.1168, lr=3.80e-06, step=761]Training:   0%|          | 762/200000 [1:31:50<399:02:05,  7.21s/it, loss=0.1104, lr=3.81e-06, step=762]Training:   0%|          | 763/200000 [1:31:57<398:57:42,  7.21s/it, loss=0.1104, lr=3.81e-06, step=762]Training:   0%|          | 763/200000 [1:31:57<398:57:42,  7.21s/it, loss=0.1954, lr=3.81e-06, step=763]Training:   0%|          | 764/200000 [1:32:04<398:17:26,  7.20s/it, loss=0.1954, lr=3.81e-06, step=763]Training:   0%|          | 764/200000 [1:32:04<398:17:26,  7.20s/it, loss=0.0902, lr=3.82e-06, step=764]Training:   0%|          | 765/200000 [1:32:11<397:59:50,  7.19s/it, loss=0.0902, lr=3.82e-06, step=764]Training:   0%|          | 765/200000 [1:32:11<397:59:50,  7.19s/it, loss=0.0851, lr=3.82e-06, step=765]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 766/200000 [1:32:19<397:28:12,  7.18s/it, loss=0.0851, lr=3.82e-06, step=765]Training:   0%|          | 766/200000 [1:32:19<397:28:12,  7.18s/it, loss=0.1372, lr=3.83e-06, step=766]Training:   0%|          | 767/200000 [1:32:26<398:08:43,  7.19s/it, loss=0.1372, lr=3.83e-06, step=766]Training:   0%|          | 767/200000 [1:32:26<398:08:43,  7.19s/it, loss=0.1158, lr=3.83e-06, step=767]Training:   0%|          | 768/200000 [1:32:33<397:41:12,  7.19s/it, loss=0.1158, lr=3.83e-06, step=767]Training:   0%|          | 768/200000 [1:32:33<397:41:12,  7.19s/it, loss=0.1089, lr=3.84e-06, step=768]Training:   0%|          | 769/200000 [1:32:40<397:15:34,  7.18s/it, loss=0.1089, lr=3.84e-06, step=768]Training:   0%|          | 769/200000 [1:32:40<397:15:34,  7.18s/it, loss=0.0819, lr=3.84e-06, step=769]Training:   0%|          | 770/200000 [1:32:47<397:14:51,  7.18s/it, loss=0.0819, lr=3.84e-06, step=769]Training:   0%|          | 770/200000 [1:32:47<397:14:51,  7.18s/it, loss=0.1563, lr=3.85e-06, step=770]Training:   0%|          | 771/200000 [1:32:54<397:13:18,  7.18s/it, loss=0.1563, lr=3.85e-06, step=770]Training:   0%|          | 771/200000 [1:32:54<397:13:18,  7.18s/it, loss=0.1159, lr=3.85e-06, step=771]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 772/200000 [1:33:02<398:53:17,  7.21s/it, loss=0.1159, lr=3.85e-06, step=771]Training:   0%|          | 772/200000 [1:33:02<398:53:17,  7.21s/it, loss=0.2036, lr=3.86e-06, step=772]Training:   0%|          | 773/200000 [1:33:09<398:35:57,  7.20s/it, loss=0.2036, lr=3.86e-06, step=772]Training:   0%|          | 773/200000 [1:33:09<398:35:57,  7.20s/it, loss=0.1078, lr=3.86e-06, step=773]Training:   0%|          | 774/200000 [1:33:16<398:49:46,  7.21s/it, loss=0.1078, lr=3.86e-06, step=773]Training:   0%|          | 774/200000 [1:33:16<398:49:46,  7.21s/it, loss=0.1005, lr=3.87e-06, step=774]Training:   0%|          | 775/200000 [1:33:23<398:24:30,  7.20s/it, loss=0.1005, lr=3.87e-06, step=774]Training:   0%|          | 775/200000 [1:33:23<398:24:30,  7.20s/it, loss=0.1412, lr=3.87e-06, step=775]Training:   0%|          | 776/200000 [1:33:31<398:13:07,  7.20s/it, loss=0.1412, lr=3.87e-06, step=775]Training:   0%|          | 776/200000 [1:33:31<398:13:07,  7.20s/it, loss=0.1007, lr=3.88e-06, step=776]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 777/200000 [1:33:38<398:19:44,  7.20s/it, loss=0.1007, lr=3.88e-06, step=776]Training:   0%|          | 777/200000 [1:33:38<398:19:44,  7.20s/it, loss=0.2429, lr=3.88e-06, step=777]Training:   0%|          | 778/200000 [1:33:45<397:53:09,  7.19s/it, loss=0.2429, lr=3.88e-06, step=777]Training:   0%|          | 778/200000 [1:33:45<397:53:09,  7.19s/it, loss=0.0953, lr=3.89e-06, step=778]Training:   0%|          | 779/200000 [1:33:52<397:31:46,  7.18s/it, loss=0.0953, lr=3.89e-06, step=778]Training:   0%|          | 779/200000 [1:33:52<397:31:46,  7.18s/it, loss=0.1115, lr=3.89e-06, step=779]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 780/200000 [1:33:59<397:32:10,  7.18s/it, loss=0.1115, lr=3.89e-06, step=779]Training:   0%|          | 780/200000 [1:33:59<397:32:10,  7.18s/it, loss=0.2043, lr=3.90e-06, step=780]Training:   0%|          | 781/200000 [1:34:06<397:31:23,  7.18s/it, loss=0.2043, lr=3.90e-06, step=780]Training:   0%|          | 781/200000 [1:34:06<397:31:23,  7.18s/it, loss=0.0847, lr=3.90e-06, step=781]Training:   0%|          | 782/200000 [1:34:14<397:53:59,  7.19s/it, loss=0.0847, lr=3.90e-06, step=781]Training:   0%|          | 782/200000 [1:34:14<397:53:59,  7.19s/it, loss=0.1289, lr=3.91e-06, step=782]Training:   0%|          | 783/200000 [1:34:21<398:28:47,  7.20s/it, loss=0.1289, lr=3.91e-06, step=782]Training:   0%|          | 783/200000 [1:34:21<398:28:47,  7.20s/it, loss=0.1818, lr=3.91e-06, step=783]Training:   0%|          | 784/200000 [1:34:28<398:03:48,  7.19s/it, loss=0.1818, lr=3.91e-06, step=783]Training:   0%|          | 784/200000 [1:34:28<398:03:48,  7.19s/it, loss=0.1213, lr=3.92e-06, step=784]Training:   0%|          | 785/200000 [1:34:35<397:20:55,  7.18s/it, loss=0.1213, lr=3.92e-06, step=784]Training:   0%|          | 785/200000 [1:34:35<397:20:55,  7.18s/it, loss=0.1066, lr=3.92e-06, step=785]Training:   0%|          | 786/200000 [1:34:42<397:01:54,  7.17s/it, loss=0.1066, lr=3.92e-06, step=785]Training:   0%|          | 786/200000 [1:34:42<397:01:54,  7.17s/it, loss=0.1166, lr=3.93e-06, step=786]Training:   0%|          | 787/200000 [1:34:50<397:17:12,  7.18s/it, loss=0.1166, lr=3.93e-06, step=786]Training:   0%|          | 787/200000 [1:34:50<397:17:12,  7.18s/it, loss=0.0842, lr=3.93e-06, step=787]Training:   0%|          | 788/200000 [1:34:57<397:18:16,  7.18s/it, loss=0.0842, lr=3.93e-06, step=787]Training:   0%|          | 788/200000 [1:34:57<397:18:16,  7.18s/it, loss=0.0994, lr=3.94e-06, step=788]Training:   0%|          | 789/200000 [1:35:04<397:19:28,  7.18s/it, loss=0.0994, lr=3.94e-06, step=788]Training:   0%|          | 789/200000 [1:35:04<397:19:28,  7.18s/it, loss=0.1147, lr=3.94e-06, step=789]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 790/200000 [1:35:11<397:47:00,  7.19s/it, loss=0.1147, lr=3.94e-06, step=789]Training:   0%|          | 790/200000 [1:35:11<397:47:00,  7.19s/it, loss=0.1029, lr=3.95e-06, step=790]Training:   0%|          | 791/200000 [1:35:18<397:58:01,  7.19s/it, loss=0.1029, lr=3.95e-06, step=790]Training:   0%|          | 791/200000 [1:35:18<397:58:01,  7.19s/it, loss=0.1311, lr=3.95e-06, step=791]Training:   0%|          | 792/200000 [1:35:25<397:22:24,  7.18s/it, loss=0.1311, lr=3.95e-06, step=791]Training:   0%|          | 792/200000 [1:35:25<397:22:24,  7.18s/it, loss=0.1146, lr=3.96e-06, step=792]Training:   0%|          | 793/200000 [1:35:33<397:08:14,  7.18s/it, loss=0.1146, lr=3.96e-06, step=792]Training:   0%|          | 793/200000 [1:35:33<397:08:14,  7.18s/it, loss=0.1076, lr=3.96e-06, step=793]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 794/200000 [1:35:40<397:14:45,  7.18s/it, loss=0.1076, lr=3.96e-06, step=793]Training:   0%|          | 794/200000 [1:35:40<397:14:45,  7.18s/it, loss=0.1417, lr=3.97e-06, step=794]Training:   0%|          | 795/200000 [1:35:47<397:29:36,  7.18s/it, loss=0.1417, lr=3.97e-06, step=794]Training:   0%|          | 795/200000 [1:35:47<397:29:36,  7.18s/it, loss=0.0974, lr=3.97e-06, step=795]Training:   0%|          | 796/200000 [1:35:54<397:33:10,  7.18s/it, loss=0.0974, lr=3.97e-06, step=795]Training:   0%|          | 796/200000 [1:35:54<397:33:10,  7.18s/it, loss=0.0999, lr=3.98e-06, step=796]Training:   0%|          | 797/200000 [1:36:01<397:21:17,  7.18s/it, loss=0.0999, lr=3.98e-06, step=796]Training:   0%|          | 797/200000 [1:36:01<397:21:17,  7.18s/it, loss=0.1025, lr=3.98e-06, step=797]Training:   0%|          | 798/200000 [1:36:09<397:31:15,  7.18s/it, loss=0.1025, lr=3.98e-06, step=797]Training:   0%|          | 798/200000 [1:36:09<397:31:15,  7.18s/it, loss=0.1400, lr=3.99e-06, step=798]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 799/200000 [1:36:16<397:31:58,  7.18s/it, loss=0.1400, lr=3.99e-06, step=798]Training:   0%|          | 799/200000 [1:36:16<397:31:58,  7.18s/it, loss=0.1007, lr=3.99e-06, step=799]Training:   0%|          | 800/200000 [1:36:23<397:39:09,  7.19s/it, loss=0.1007, lr=3.99e-06, step=799]Training:   0%|          | 800/200000 [1:36:23<397:39:09,  7.19s/it, loss=0.1232, lr=4.00e-06, step=800]21:32:58.971 [I] step=800 loss=0.1203 lr=3.76e-06 grad_norm=0.81 time=718.6s                      (486094:train_pytorch.py:582)
Training:   0%|          | 801/200000 [1:36:30<397:58:42,  7.19s/it, loss=0.1232, lr=4.00e-06, step=800]Training:   0%|          | 801/200000 [1:36:30<397:58:42,  7.19s/it, loss=0.1084, lr=4.00e-06, step=801]Training:   0%|          | 802/200000 [1:36:37<397:38:10,  7.19s/it, loss=0.1084, lr=4.00e-06, step=801]Training:   0%|          | 802/200000 [1:36:37<397:38:10,  7.19s/it, loss=0.1139, lr=4.01e-06, step=802]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 803/200000 [1:36:45<397:51:45,  7.19s/it, loss=0.1139, lr=4.01e-06, step=802]Training:   0%|          | 803/200000 [1:36:45<397:51:45,  7.19s/it, loss=0.0846, lr=4.01e-06, step=803]Training:   0%|          | 804/200000 [1:36:52<399:36:44,  7.22s/it, loss=0.0846, lr=4.01e-06, step=803]Training:   0%|          | 804/200000 [1:36:52<399:36:44,  7.22s/it, loss=0.0972, lr=4.02e-06, step=804]Training:   0%|          | 805/200000 [1:36:59<398:44:42,  7.21s/it, loss=0.0972, lr=4.02e-06, step=804]Training:   0%|          | 805/200000 [1:36:59<398:44:42,  7.21s/it, loss=0.1203, lr=4.02e-06, step=805]Training:   0%|          | 806/200000 [1:37:06<398:07:37,  7.20s/it, loss=0.1203, lr=4.02e-06, step=805]Training:   0%|          | 806/200000 [1:37:06<398:07:37,  7.20s/it, loss=0.1134, lr=4.03e-06, step=806]Training:   0%|          | 807/200000 [1:37:13<397:46:51,  7.19s/it, loss=0.1134, lr=4.03e-06, step=806]Training:   0%|          | 807/200000 [1:37:13<397:46:51,  7.19s/it, loss=0.1136, lr=4.03e-06, step=807]Training:   0%|          | 808/200000 [1:37:21<397:52:31,  7.19s/it, loss=0.1136, lr=4.03e-06, step=807]Training:   0%|          | 808/200000 [1:37:21<397:52:31,  7.19s/it, loss=0.1056, lr=4.04e-06, step=808]Training:   0%|          | 809/200000 [1:37:28<398:17:21,  7.20s/it, loss=0.1056, lr=4.04e-06, step=808]Training:   0%|          | 809/200000 [1:37:28<398:17:21,  7.20s/it, loss=0.1151, lr=4.04e-06, step=809]Training:   0%|          | 810/200000 [1:37:35<398:06:43,  7.20s/it, loss=0.1151, lr=4.04e-06, step=809]Training:   0%|          | 810/200000 [1:37:35<398:06:43,  7.20s/it, loss=0.0958, lr=4.05e-06, step=810]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 811/200000 [1:37:42<397:47:34,  7.19s/it, loss=0.0958, lr=4.05e-06, step=810]Training:   0%|          | 811/200000 [1:37:42<397:47:34,  7.19s/it, loss=0.1149, lr=4.05e-06, step=811]Training:   0%|          | 812/200000 [1:37:49<397:57:19,  7.19s/it, loss=0.1149, lr=4.05e-06, step=811]Training:   0%|          | 812/200000 [1:37:49<397:57:19,  7.19s/it, loss=0.1037, lr=4.06e-06, step=812]Training:   0%|          | 813/200000 [1:37:56<396:46:05,  7.17s/it, loss=0.1037, lr=4.06e-06, step=812]Training:   0%|          | 813/200000 [1:37:56<396:46:05,  7.17s/it, loss=0.1336, lr=4.06e-06, step=813]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 814/200000 [1:38:04<397:09:13,  7.18s/it, loss=0.1336, lr=4.06e-06, step=813]Training:   0%|          | 814/200000 [1:38:04<397:09:13,  7.18s/it, loss=0.1200, lr=4.07e-06, step=814]Training:   0%|          | 815/200000 [1:38:11<396:57:06,  7.17s/it, loss=0.1200, lr=4.07e-06, step=814]Training:   0%|          | 815/200000 [1:38:11<396:57:06,  7.17s/it, loss=0.1177, lr=4.07e-06, step=815]Training:   0%|          | 816/200000 [1:38:18<397:16:17,  7.18s/it, loss=0.1177, lr=4.07e-06, step=815]Training:   0%|          | 816/200000 [1:38:18<397:16:17,  7.18s/it, loss=0.1719, lr=4.08e-06, step=816]Training:   0%|          | 817/200000 [1:38:25<397:04:45,  7.18s/it, loss=0.1719, lr=4.08e-06, step=816]Training:   0%|          | 817/200000 [1:38:25<397:04:45,  7.18s/it, loss=0.1009, lr=4.08e-06, step=817]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 818/200000 [1:38:32<397:47:12,  7.19s/it, loss=0.1009, lr=4.08e-06, step=817]Training:   0%|          | 818/200000 [1:38:32<397:47:12,  7.19s/it, loss=0.0947, lr=4.09e-06, step=818]Training:   0%|          | 819/200000 [1:38:40<397:44:26,  7.19s/it, loss=0.0947, lr=4.09e-06, step=818]Training:   0%|          | 819/200000 [1:38:40<397:44:26,  7.19s/it, loss=0.1130, lr=4.09e-06, step=819]Training:   0%|          | 820/200000 [1:38:47<397:56:14,  7.19s/it, loss=0.1130, lr=4.09e-06, step=819]Training:   0%|          | 820/200000 [1:38:47<397:56:14,  7.19s/it, loss=0.1634, lr=4.10e-06, step=820]Training:   0%|          | 821/200000 [1:38:54<397:34:29,  7.19s/it, loss=0.1634, lr=4.10e-06, step=820]Training:   0%|          | 821/200000 [1:38:54<397:34:29,  7.19s/it, loss=0.1069, lr=4.10e-06, step=821]Training:   0%|          | 822/200000 [1:39:01<397:14:40,  7.18s/it, loss=0.1069, lr=4.10e-06, step=821]Training:   0%|          | 822/200000 [1:39:01<397:14:40,  7.18s/it, loss=0.0896, lr=4.11e-06, step=822]Training:   0%|          | 823/200000 [1:39:08<397:06:11,  7.18s/it, loss=0.0896, lr=4.11e-06, step=822]Training:   0%|          | 823/200000 [1:39:08<397:06:11,  7.18s/it, loss=0.1121, lr=4.11e-06, step=823]Training:   0%|          | 824/200000 [1:39:15<397:22:34,  7.18s/it, loss=0.1121, lr=4.11e-06, step=823]Training:   0%|          | 824/200000 [1:39:15<397:22:34,  7.18s/it, loss=0.1242, lr=4.12e-06, step=824]Training:   0%|          | 825/200000 [1:39:23<399:00:39,  7.21s/it, loss=0.1242, lr=4.12e-06, step=824]Training:   0%|          | 825/200000 [1:39:23<399:00:39,  7.21s/it, loss=0.1371, lr=4.12e-06, step=825]Training:   0%|          | 826/200000 [1:39:30<398:43:47,  7.21s/it, loss=0.1371, lr=4.12e-06, step=825]Training:   0%|          | 826/200000 [1:39:30<398:43:47,  7.21s/it, loss=0.0879, lr=4.13e-06, step=826]Training:   0%|          | 827/200000 [1:39:37<398:33:37,  7.20s/it, loss=0.0879, lr=4.13e-06, step=826]Training:   0%|          | 827/200000 [1:39:37<398:33:37,  7.20s/it, loss=0.1183, lr=4.13e-06, step=827]Training:   0%|          | 828/200000 [1:39:44<398:30:28,  7.20s/it, loss=0.1183, lr=4.13e-06, step=827]Training:   0%|          | 828/200000 [1:39:44<398:30:28,  7.20s/it, loss=0.1166, lr=4.14e-06, step=828]Training:   0%|          | 829/200000 [1:39:52<397:48:36,  7.19s/it, loss=0.1166, lr=4.14e-06, step=828]Training:   0%|          | 829/200000 [1:39:52<397:48:36,  7.19s/it, loss=0.1098, lr=4.14e-06, step=829]Training:   0%|          | 830/200000 [1:39:59<397:29:34,  7.18s/it, loss=0.1098, lr=4.14e-06, step=829]Training:   0%|          | 830/200000 [1:39:59<397:29:34,  7.18s/it, loss=0.2226, lr=4.15e-06, step=830]Training:   0%|          | 831/200000 [1:40:06<397:13:37,  7.18s/it, loss=0.2226, lr=4.15e-06, step=830]Training:   0%|          | 831/200000 [1:40:06<397:13:37,  7.18s/it, loss=0.1875, lr=4.15e-06, step=831]Training:   0%|          | 832/200000 [1:40:13<396:59:50,  7.18s/it, loss=0.1875, lr=4.15e-06, step=831]Training:   0%|          | 832/200000 [1:40:13<396:59:50,  7.18s/it, loss=0.1079, lr=4.16e-06, step=832]Training:   0%|          | 833/200000 [1:40:20<397:09:02,  7.18s/it, loss=0.1079, lr=4.16e-06, step=832]Training:   0%|          | 833/200000 [1:40:20<397:09:02,  7.18s/it, loss=0.1234, lr=4.16e-06, step=833]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 834/200000 [1:40:27<397:33:31,  7.19s/it, loss=0.1234, lr=4.16e-06, step=833]Training:   0%|          | 834/200000 [1:40:27<397:33:31,  7.19s/it, loss=0.0909, lr=4.17e-06, step=834]Training:   0%|          | 835/200000 [1:40:35<397:50:56,  7.19s/it, loss=0.0909, lr=4.17e-06, step=834]Training:   0%|          | 835/200000 [1:40:35<397:50:56,  7.19s/it, loss=0.1114, lr=4.17e-06, step=835]Training:   0%|          | 836/200000 [1:40:42<397:31:50,  7.19s/it, loss=0.1114, lr=4.17e-06, step=835]Training:   0%|          | 836/200000 [1:40:42<397:31:50,  7.19s/it, loss=0.1046, lr=4.18e-06, step=836]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 837/200000 [1:40:49<397:47:07,  7.19s/it, loss=0.1046, lr=4.18e-06, step=836]Training:   0%|          | 837/200000 [1:40:49<397:47:07,  7.19s/it, loss=0.1021, lr=4.18e-06, step=837]Training:   0%|          | 838/200000 [1:40:56<397:36:53,  7.19s/it, loss=0.1021, lr=4.18e-06, step=837]Training:   0%|          | 838/200000 [1:40:56<397:36:53,  7.19s/it, loss=0.0704, lr=4.19e-06, step=838]Training:   0%|          | 839/200000 [1:41:03<397:15:52,  7.18s/it, loss=0.0704, lr=4.19e-06, step=838]Training:   0%|          | 839/200000 [1:41:03<397:15:52,  7.18s/it, loss=0.0850, lr=4.19e-06, step=839]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 840/200000 [1:41:11<397:34:01,  7.19s/it, loss=0.0850, lr=4.19e-06, step=839]Training:   0%|          | 840/200000 [1:41:11<397:34:01,  7.19s/it, loss=0.1325, lr=4.20e-06, step=840]Training:   0%|          | 841/200000 [1:41:18<398:01:49,  7.19s/it, loss=0.1325, lr=4.20e-06, step=840]Training:   0%|          | 841/200000 [1:41:18<398:01:49,  7.19s/it, loss=0.1065, lr=4.20e-06, step=841]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 842/200000 [1:41:25<398:02:07,  7.19s/it, loss=0.1065, lr=4.20e-06, step=841]Training:   0%|          | 842/200000 [1:41:25<398:02:07,  7.19s/it, loss=0.1160, lr=4.21e-06, step=842]Training:   0%|          | 843/200000 [1:41:32<398:01:25,  7.19s/it, loss=0.1160, lr=4.21e-06, step=842]Training:   0%|          | 843/200000 [1:41:32<398:01:25,  7.19s/it, loss=0.0946, lr=4.21e-06, step=843]Training:   0%|          | 844/200000 [1:41:39<398:23:19,  7.20s/it, loss=0.0946, lr=4.21e-06, step=843]Training:   0%|          | 844/200000 [1:41:39<398:23:19,  7.20s/it, loss=0.1283, lr=4.22e-06, step=844]Training:   0%|          | 845/200000 [1:41:47<398:15:37,  7.20s/it, loss=0.1283, lr=4.22e-06, step=844]Training:   0%|          | 845/200000 [1:41:47<398:15:37,  7.20s/it, loss=0.1097, lr=4.22e-06, step=845]Training:   0%|          | 846/200000 [1:41:54<398:45:24,  7.21s/it, loss=0.1097, lr=4.22e-06, step=845]Training:   0%|          | 846/200000 [1:41:54<398:45:24,  7.21s/it, loss=0.1659, lr=4.23e-06, step=846]Training:   0%|          | 847/200000 [1:42:01<398:17:29,  7.20s/it, loss=0.1659, lr=4.23e-06, step=846]Training:   0%|          | 847/200000 [1:42:01<398:17:29,  7.20s/it, loss=0.1570, lr=4.23e-06, step=847]Training:   0%|          | 848/200000 [1:42:08<398:14:13,  7.20s/it, loss=0.1570, lr=4.23e-06, step=847]Training:   0%|          | 848/200000 [1:42:08<398:14:13,  7.20s/it, loss=0.0976, lr=4.24e-06, step=848]Training:   0%|          | 849/200000 [1:42:15<398:18:13,  7.20s/it, loss=0.0976, lr=4.24e-06, step=848]Training:   0%|          | 849/200000 [1:42:15<398:18:13,  7.20s/it, loss=0.1308, lr=4.24e-06, step=849]Training:   0%|          | 850/200000 [1:42:23<398:11:22,  7.20s/it, loss=0.1308, lr=4.24e-06, step=849]Training:   0%|          | 850/200000 [1:42:23<398:11:22,  7.20s/it, loss=0.0985, lr=4.25e-06, step=850]Training:   0%|          | 851/200000 [1:42:30<397:49:24,  7.19s/it, loss=0.0985, lr=4.25e-06, step=850]Training:   0%|          | 851/200000 [1:42:30<397:49:24,  7.19s/it, loss=0.1331, lr=4.25e-06, step=851]Training:   0%|          | 852/200000 [1:42:37<396:44:48,  7.17s/it, loss=0.1331, lr=4.25e-06, step=851]Training:   0%|          | 852/200000 [1:42:37<396:44:48,  7.17s/it, loss=0.1339, lr=4.26e-06, step=852]Training:   0%|          | 853/200000 [1:42:44<396:47:52,  7.17s/it, loss=0.1339, lr=4.26e-06, step=852]Training:   0%|          | 853/200000 [1:42:44<396:47:52,  7.17s/it, loss=0.1530, lr=4.26e-06, step=853]Training:   0%|          | 854/200000 [1:42:51<396:48:57,  7.17s/it, loss=0.1530, lr=4.26e-06, step=853]Training:   0%|          | 854/200000 [1:42:51<396:48:57,  7.17s/it, loss=0.1245, lr=4.27e-06, step=854]Training:   0%|          | 855/200000 [1:42:58<396:51:14,  7.17s/it, loss=0.1245, lr=4.27e-06, step=854]Training:   0%|          | 855/200000 [1:42:58<396:51:14,  7.17s/it, loss=0.0820, lr=4.27e-06, step=855]Training:   0%|          | 856/200000 [1:43:06<397:00:57,  7.18s/it, loss=0.0820, lr=4.27e-06, step=855]Training:   0%|          | 856/200000 [1:43:06<397:00:57,  7.18s/it, loss=0.1020, lr=4.28e-06, step=856]Training:   0%|          | 857/200000 [1:43:13<397:02:58,  7.18s/it, loss=0.1020, lr=4.28e-06, step=856]Training:   0%|          | 857/200000 [1:43:13<397:02:58,  7.18s/it, loss=0.0853, lr=4.28e-06, step=857]Training:   0%|          | 858/200000 [1:43:20<397:01:35,  7.18s/it, loss=0.0853, lr=4.28e-06, step=857]Training:   0%|          | 858/200000 [1:43:20<397:01:35,  7.18s/it, loss=0.0999, lr=4.29e-06, step=858]Training:   0%|          | 859/200000 [1:43:27<397:25:32,  7.18s/it, loss=0.0999, lr=4.29e-06, step=858]Training:   0%|          | 859/200000 [1:43:27<397:25:32,  7.18s/it, loss=0.0824, lr=4.29e-06, step=859]Training:   0%|          | 860/200000 [1:43:34<397:07:52,  7.18s/it, loss=0.0824, lr=4.29e-06, step=859]Training:   0%|          | 860/200000 [1:43:34<397:07:52,  7.18s/it, loss=0.0917, lr=4.30e-06, step=860]Training:   0%|          | 861/200000 [1:43:41<397:09:17,  7.18s/it, loss=0.0917, lr=4.30e-06, step=860]Training:   0%|          | 861/200000 [1:43:41<397:09:17,  7.18s/it, loss=0.2362, lr=4.30e-06, step=861]Training:   0%|          | 862/200000 [1:43:49<396:57:47,  7.18s/it, loss=0.2362, lr=4.30e-06, step=861]Training:   0%|          | 862/200000 [1:43:49<396:57:47,  7.18s/it, loss=0.0992, lr=4.31e-06, step=862]Training:   0%|          | 863/200000 [1:43:56<397:08:47,  7.18s/it, loss=0.0992, lr=4.31e-06, step=862]Training:   0%|          | 863/200000 [1:43:56<397:08:47,  7.18s/it, loss=0.1194, lr=4.31e-06, step=863]Training:   0%|          | 864/200000 [1:44:03<397:31:29,  7.19s/it, loss=0.1194, lr=4.31e-06, step=863]Training:   0%|          | 864/200000 [1:44:03<397:31:29,  7.19s/it, loss=0.1146, lr=4.32e-06, step=864]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 865/200000 [1:44:10<397:09:29,  7.18s/it, loss=0.1146, lr=4.32e-06, step=864]Training:   0%|          | 865/200000 [1:44:10<397:09:29,  7.18s/it, loss=0.0930, lr=4.32e-06, step=865]Training:   0%|          | 866/200000 [1:44:17<397:03:37,  7.18s/it, loss=0.0930, lr=4.32e-06, step=865]Training:   0%|          | 866/200000 [1:44:17<397:03:37,  7.18s/it, loss=0.1494, lr=4.33e-06, step=866]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 867/200000 [1:44:25<398:55:22,  7.21s/it, loss=0.1494, lr=4.33e-06, step=866]Training:   0%|          | 867/200000 [1:44:25<398:55:22,  7.21s/it, loss=0.1440, lr=4.33e-06, step=867]Training:   0%|          | 868/200000 [1:44:32<398:43:37,  7.21s/it, loss=0.1440, lr=4.33e-06, step=867]Training:   0%|          | 868/200000 [1:44:32<398:43:37,  7.21s/it, loss=0.0856, lr=4.34e-06, step=868]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 869/200000 [1:44:39<398:16:45,  7.20s/it, loss=0.0856, lr=4.34e-06, step=868]Training:   0%|          | 869/200000 [1:44:39<398:16:45,  7.20s/it, loss=0.1436, lr=4.34e-06, step=869]Training:   0%|          | 870/200000 [1:44:46<398:12:13,  7.20s/it, loss=0.1436, lr=4.34e-06, step=869]Training:   0%|          | 870/200000 [1:44:46<398:12:13,  7.20s/it, loss=0.1015, lr=4.35e-06, step=870]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 871/200000 [1:44:53<397:33:03,  7.19s/it, loss=0.1015, lr=4.35e-06, step=870]Training:   0%|          | 871/200000 [1:44:53<397:33:03,  7.19s/it, loss=0.1169, lr=4.35e-06, step=871]Training:   0%|          | 872/200000 [1:45:01<397:40:57,  7.19s/it, loss=0.1169, lr=4.35e-06, step=871]Training:   0%|          | 872/200000 [1:45:01<397:40:57,  7.19s/it, loss=0.1078, lr=4.36e-06, step=872]Training:   0%|          | 873/200000 [1:45:08<397:13:43,  7.18s/it, loss=0.1078, lr=4.36e-06, step=872]Training:   0%|          | 873/200000 [1:45:08<397:13:43,  7.18s/it, loss=0.0953, lr=4.36e-06, step=873]Training:   0%|          | 874/200000 [1:45:15<397:10:26,  7.18s/it, loss=0.0953, lr=4.36e-06, step=873]Training:   0%|          | 874/200000 [1:45:15<397:10:26,  7.18s/it, loss=0.1054, lr=4.37e-06, step=874]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 875/200000 [1:45:22<397:28:58,  7.19s/it, loss=0.1054, lr=4.37e-06, step=874]Training:   0%|          | 875/200000 [1:45:22<397:28:58,  7.19s/it, loss=0.0769, lr=4.37e-06, step=875]Training:   0%|          | 876/200000 [1:45:29<397:50:56,  7.19s/it, loss=0.0769, lr=4.37e-06, step=875]Training:   0%|          | 876/200000 [1:45:29<397:50:56,  7.19s/it, loss=0.1252, lr=4.38e-06, step=876]Training:   0%|          | 877/200000 [1:45:37<397:25:49,  7.19s/it, loss=0.1252, lr=4.38e-06, step=876]Training:   0%|          | 877/200000 [1:45:37<397:25:49,  7.19s/it, loss=0.1335, lr=4.38e-06, step=877]Training:   0%|          | 878/200000 [1:45:44<397:50:00,  7.19s/it, loss=0.1335, lr=4.38e-06, step=877]Training:   0%|          | 878/200000 [1:45:44<397:50:00,  7.19s/it, loss=0.0943, lr=4.39e-06, step=878]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 879/200000 [1:45:51<397:46:13,  7.19s/it, loss=0.0943, lr=4.39e-06, step=878]Training:   0%|          | 879/200000 [1:45:51<397:46:13,  7.19s/it, loss=0.2592, lr=4.39e-06, step=879]Training:   0%|          | 880/200000 [1:45:58<397:24:33,  7.18s/it, loss=0.2592, lr=4.39e-06, step=879]Training:   0%|          | 880/200000 [1:45:58<397:24:33,  7.18s/it, loss=0.0925, lr=4.40e-06, step=880]Training:   0%|          | 881/200000 [1:46:05<397:27:39,  7.19s/it, loss=0.0925, lr=4.40e-06, step=880]Training:   0%|          | 881/200000 [1:46:05<397:27:39,  7.19s/it, loss=0.0841, lr=4.40e-06, step=881]Training:   0%|          | 882/200000 [1:46:12<397:27:30,  7.19s/it, loss=0.0841, lr=4.40e-06, step=881]Training:   0%|          | 882/200000 [1:46:12<397:27:30,  7.19s/it, loss=0.1167, lr=4.41e-06, step=882]Training:   0%|          | 883/200000 [1:46:20<396:55:01,  7.18s/it, loss=0.1167, lr=4.41e-06, step=882]Training:   0%|          | 883/200000 [1:46:20<396:55:01,  7.18s/it, loss=0.1387, lr=4.41e-06, step=883]Training:   0%|          | 884/200000 [1:46:27<396:52:30,  7.18s/it, loss=0.1387, lr=4.41e-06, step=883]Training:   0%|          | 884/200000 [1:46:27<396:52:30,  7.18s/it, loss=0.1533, lr=4.42e-06, step=884]Training:   0%|          | 885/200000 [1:46:34<396:33:52,  7.17s/it, loss=0.1533, lr=4.42e-06, step=884]Training:   0%|          | 885/200000 [1:46:34<396:33:52,  7.17s/it, loss=0.1522, lr=4.42e-06, step=885]WARNING:root:Token length (53) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 886/200000 [1:46:41<396:58:36,  7.18s/it, loss=0.1522, lr=4.42e-06, step=885]Training:   0%|          | 886/200000 [1:46:41<396:58:36,  7.18s/it, loss=0.1157, lr=4.43e-06, step=886]Training:   0%|          | 887/200000 [1:46:48<396:35:32,  7.17s/it, loss=0.1157, lr=4.43e-06, step=886]Training:   0%|          | 887/200000 [1:46:48<396:35:32,  7.17s/it, loss=0.0994, lr=4.43e-06, step=887]Training:   0%|          | 888/200000 [1:46:56<398:55:58,  7.21s/it, loss=0.0994, lr=4.43e-06, step=887]Training:   0%|          | 888/200000 [1:46:56<398:55:58,  7.21s/it, loss=0.1187, lr=4.44e-06, step=888]Training:   0%|          | 889/200000 [1:47:03<398:17:16,  7.20s/it, loss=0.1187, lr=4.44e-06, step=888]Training:   0%|          | 889/200000 [1:47:03<398:17:16,  7.20s/it, loss=0.0912, lr=4.44e-06, step=889]Training:   0%|          | 890/200000 [1:47:10<398:22:19,  7.20s/it, loss=0.0912, lr=4.44e-06, step=889]Training:   0%|          | 890/200000 [1:47:10<398:22:19,  7.20s/it, loss=0.1054, lr=4.45e-06, step=890]Training:   0%|          | 891/200000 [1:47:17<397:57:56,  7.20s/it, loss=0.1054, lr=4.45e-06, step=890]Training:   0%|          | 891/200000 [1:47:17<397:57:56,  7.20s/it, loss=0.0935, lr=4.45e-06, step=891]Training:   0%|          | 892/200000 [1:47:24<397:50:03,  7.19s/it, loss=0.0935, lr=4.45e-06, step=891]Training:   0%|          | 892/200000 [1:47:24<397:50:03,  7.19s/it, loss=0.1197, lr=4.46e-06, step=892]Training:   0%|          | 893/200000 [1:47:32<397:49:10,  7.19s/it, loss=0.1197, lr=4.46e-06, step=892]Training:   0%|          | 893/200000 [1:47:32<397:49:10,  7.19s/it, loss=0.0704, lr=4.46e-06, step=893]Training:   0%|          | 894/200000 [1:47:39<397:40:27,  7.19s/it, loss=0.0704, lr=4.46e-06, step=893]Training:   0%|          | 894/200000 [1:47:39<397:40:27,  7.19s/it, loss=0.1025, lr=4.47e-06, step=894]Training:   0%|          | 895/200000 [1:47:46<397:36:03,  7.19s/it, loss=0.1025, lr=4.47e-06, step=894]Training:   0%|          | 895/200000 [1:47:46<397:36:03,  7.19s/it, loss=0.0878, lr=4.47e-06, step=895]Training:   0%|          | 896/200000 [1:47:53<397:36:55,  7.19s/it, loss=0.0878, lr=4.47e-06, step=895]Training:   0%|          | 896/200000 [1:47:53<397:36:55,  7.19s/it, loss=0.1102, lr=4.48e-06, step=896]Training:   0%|          | 897/200000 [1:48:00<397:22:22,  7.18s/it, loss=0.1102, lr=4.48e-06, step=896]Training:   0%|          | 897/200000 [1:48:00<397:22:22,  7.18s/it, loss=0.1084, lr=4.48e-06, step=897]Training:   0%|          | 898/200000 [1:48:07<397:24:13,  7.19s/it, loss=0.1084, lr=4.48e-06, step=897]Training:   0%|          | 898/200000 [1:48:07<397:24:13,  7.19s/it, loss=0.1046, lr=4.49e-06, step=898]Training:   0%|          | 899/200000 [1:48:15<397:21:21,  7.18s/it, loss=0.1046, lr=4.49e-06, step=898]Training:   0%|          | 899/200000 [1:48:15<397:21:21,  7.18s/it, loss=0.1094, lr=4.49e-06, step=899]Training:   0%|          | 900/200000 [1:48:22<397:27:30,  7.19s/it, loss=0.1094, lr=4.49e-06, step=899]Training:   0%|          | 900/200000 [1:48:22<397:27:30,  7.19s/it, loss=0.1352, lr=4.50e-06, step=900]21:44:57.841 [I] step=900 loss=0.1173 lr=4.26e-06 grad_norm=0.81 time=718.9s                      (486094:train_pytorch.py:582)
Training:   0%|          | 901/200000 [1:48:29<397:58:46,  7.20s/it, loss=0.1352, lr=4.50e-06, step=900]Training:   0%|          | 901/200000 [1:48:29<397:58:46,  7.20s/it, loss=0.1907, lr=4.50e-06, step=901]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 902/200000 [1:48:36<397:53:44,  7.19s/it, loss=0.1907, lr=4.50e-06, step=901]Training:   0%|          | 902/200000 [1:48:36<397:53:44,  7.19s/it, loss=0.0870, lr=4.51e-06, step=902]Training:   0%|          | 903/200000 [1:48:43<397:24:31,  7.19s/it, loss=0.0870, lr=4.51e-06, step=902]Training:   0%|          | 903/200000 [1:48:43<397:24:31,  7.19s/it, loss=0.1041, lr=4.51e-06, step=903]Training:   0%|          | 904/200000 [1:48:51<397:10:09,  7.18s/it, loss=0.1041, lr=4.51e-06, step=903]Training:   0%|          | 904/200000 [1:48:51<397:10:09,  7.18s/it, loss=0.1201, lr=4.52e-06, step=904]Training:   0%|          | 905/200000 [1:48:58<397:10:16,  7.18s/it, loss=0.1201, lr=4.52e-06, step=904]Training:   0%|          | 905/200000 [1:48:58<397:10:16,  7.18s/it, loss=0.1000, lr=4.52e-06, step=905]Training:   0%|          | 906/200000 [1:49:05<397:28:02,  7.19s/it, loss=0.1000, lr=4.52e-06, step=905]Training:   0%|          | 906/200000 [1:49:05<397:28:02,  7.19s/it, loss=0.1037, lr=4.53e-06, step=906]Training:   0%|          | 907/200000 [1:49:12<397:24:13,  7.19s/it, loss=0.1037, lr=4.53e-06, step=906]Training:   0%|          | 907/200000 [1:49:12<397:24:13,  7.19s/it, loss=0.1051, lr=4.53e-06, step=907]Training:   0%|          | 908/200000 [1:49:19<397:15:50,  7.18s/it, loss=0.1051, lr=4.53e-06, step=907]Training:   0%|          | 908/200000 [1:49:19<397:15:50,  7.18s/it, loss=0.0962, lr=4.54e-06, step=908]Training:   0%|          | 909/200000 [1:49:27<398:28:32,  7.21s/it, loss=0.0962, lr=4.54e-06, step=908]Training:   0%|          | 909/200000 [1:49:27<398:28:32,  7.21s/it, loss=0.1349, lr=4.54e-06, step=909]Training:   0%|          | 910/200000 [1:49:34<398:04:55,  7.20s/it, loss=0.1349, lr=4.54e-06, step=909]Training:   0%|          | 910/200000 [1:49:34<398:04:55,  7.20s/it, loss=0.0961, lr=4.55e-06, step=910]Training:   0%|          | 911/200000 [1:49:41<397:36:40,  7.19s/it, loss=0.0961, lr=4.55e-06, step=910]Training:   0%|          | 911/200000 [1:49:41<397:36:40,  7.19s/it, loss=0.1187, lr=4.55e-06, step=911]Training:   0%|          | 912/200000 [1:49:48<397:28:29,  7.19s/it, loss=0.1187, lr=4.55e-06, step=911]Training:   0%|          | 912/200000 [1:49:48<397:28:29,  7.19s/it, loss=0.1016, lr=4.56e-06, step=912]Training:   0%|          | 913/200000 [1:49:55<397:22:46,  7.19s/it, loss=0.1016, lr=4.56e-06, step=912]Training:   0%|          | 913/200000 [1:49:55<397:22:46,  7.19s/it, loss=0.1130, lr=4.56e-06, step=913]Training:   0%|          | 914/200000 [1:50:02<397:44:17,  7.19s/it, loss=0.1130, lr=4.56e-06, step=913]Training:   0%|          | 914/200000 [1:50:02<397:44:17,  7.19s/it, loss=0.0896, lr=4.57e-06, step=914]Training:   0%|          | 915/200000 [1:50:10<397:11:47,  7.18s/it, loss=0.0896, lr=4.57e-06, step=914]Training:   0%|          | 915/200000 [1:50:10<397:11:47,  7.18s/it, loss=0.1004, lr=4.57e-06, step=915]Training:   0%|          | 916/200000 [1:50:17<397:11:22,  7.18s/it, loss=0.1004, lr=4.57e-06, step=915]Training:   0%|          | 916/200000 [1:50:17<397:11:22,  7.18s/it, loss=0.1140, lr=4.58e-06, step=916]Training:   0%|          | 917/200000 [1:50:24<397:10:51,  7.18s/it, loss=0.1140, lr=4.58e-06, step=916]Training:   0%|          | 917/200000 [1:50:24<397:10:51,  7.18s/it, loss=0.0789, lr=4.58e-06, step=917]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 918/200000 [1:50:31<397:46:07,  7.19s/it, loss=0.0789, lr=4.58e-06, step=917]Training:   0%|          | 918/200000 [1:50:31<397:46:07,  7.19s/it, loss=0.1220, lr=4.59e-06, step=918]Training:   0%|          | 919/200000 [1:50:38<397:04:19,  7.18s/it, loss=0.1220, lr=4.59e-06, step=918]Training:   0%|          | 919/200000 [1:50:38<397:04:19,  7.18s/it, loss=0.0889, lr=4.59e-06, step=919]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 920/200000 [1:50:46<396:51:28,  7.18s/it, loss=0.0889, lr=4.59e-06, step=919]Training:   0%|          | 920/200000 [1:50:46<396:51:28,  7.18s/it, loss=0.1236, lr=4.60e-06, step=920]Training:   0%|          | 921/200000 [1:50:53<396:55:32,  7.18s/it, loss=0.1236, lr=4.60e-06, step=920]Training:   0%|          | 921/200000 [1:50:53<396:55:32,  7.18s/it, loss=0.1078, lr=4.60e-06, step=921]Training:   0%|          | 922/200000 [1:51:00<397:15:16,  7.18s/it, loss=0.1078, lr=4.60e-06, step=921]Training:   0%|          | 922/200000 [1:51:00<397:15:16,  7.18s/it, loss=0.0873, lr=4.61e-06, step=922]Training:   0%|          | 923/200000 [1:51:07<397:11:47,  7.18s/it, loss=0.0873, lr=4.61e-06, step=922]Training:   0%|          | 923/200000 [1:51:07<397:11:47,  7.18s/it, loss=0.0719, lr=4.61e-06, step=923]Training:   0%|          | 924/200000 [1:51:14<397:10:38,  7.18s/it, loss=0.0719, lr=4.61e-06, step=923]Training:   0%|          | 924/200000 [1:51:14<397:10:38,  7.18s/it, loss=0.0816, lr=4.62e-06, step=924]Training:   0%|          | 925/200000 [1:51:21<397:08:58,  7.18s/it, loss=0.0816, lr=4.62e-06, step=924]Training:   0%|          | 925/200000 [1:51:21<397:08:58,  7.18s/it, loss=0.1045, lr=4.62e-06, step=925]Training:   0%|          | 926/200000 [1:51:29<397:25:12,  7.19s/it, loss=0.1045, lr=4.62e-06, step=925]Training:   0%|          | 926/200000 [1:51:29<397:25:12,  7.19s/it, loss=0.0976, lr=4.63e-06, step=926]Training:   0%|          | 927/200000 [1:51:36<397:37:23,  7.19s/it, loss=0.0976, lr=4.63e-06, step=926]Training:   0%|          | 927/200000 [1:51:36<397:37:23,  7.19s/it, loss=0.0898, lr=4.63e-06, step=927]Training:   0%|          | 928/200000 [1:51:43<397:20:22,  7.19s/it, loss=0.0898, lr=4.63e-06, step=927]Training:   0%|          | 928/200000 [1:51:43<397:20:22,  7.19s/it, loss=0.1891, lr=4.64e-06, step=928]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 929/200000 [1:51:50<397:24:05,  7.19s/it, loss=0.1891, lr=4.64e-06, step=928]Training:   0%|          | 929/200000 [1:51:50<397:24:05,  7.19s/it, loss=0.2005, lr=4.64e-06, step=929]Training:   0%|          | 930/200000 [1:51:58<399:19:14,  7.22s/it, loss=0.2005, lr=4.64e-06, step=929]Training:   0%|          | 930/200000 [1:51:58<399:19:14,  7.22s/it, loss=0.1495, lr=4.65e-06, step=930]Training:   0%|          | 931/200000 [1:52:05<398:50:25,  7.21s/it, loss=0.1495, lr=4.65e-06, step=930]Training:   0%|          | 931/200000 [1:52:05<398:50:25,  7.21s/it, loss=0.1171, lr=4.65e-06, step=931]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 932/200000 [1:52:12<398:09:06,  7.20s/it, loss=0.1171, lr=4.65e-06, step=931]Training:   0%|          | 932/200000 [1:52:12<398:09:06,  7.20s/it, loss=0.0838, lr=4.66e-06, step=932]Training:   0%|          | 933/200000 [1:52:19<398:04:38,  7.20s/it, loss=0.0838, lr=4.66e-06, step=932]Training:   0%|          | 933/200000 [1:52:19<398:04:38,  7.20s/it, loss=0.1095, lr=4.66e-06, step=933]Training:   0%|          | 934/200000 [1:52:26<397:32:05,  7.19s/it, loss=0.1095, lr=4.66e-06, step=933]Training:   0%|          | 934/200000 [1:52:26<397:32:05,  7.19s/it, loss=0.0924, lr=4.67e-06, step=934]Training:   0%|          | 935/200000 [1:52:33<397:12:28,  7.18s/it, loss=0.0924, lr=4.67e-06, step=934]Training:   0%|          | 935/200000 [1:52:33<397:12:28,  7.18s/it, loss=0.1115, lr=4.67e-06, step=935]Training:   0%|          | 936/200000 [1:52:41<397:27:53,  7.19s/it, loss=0.1115, lr=4.67e-06, step=935]Training:   0%|          | 936/200000 [1:52:41<397:27:53,  7.19s/it, loss=0.1141, lr=4.68e-06, step=936]Training:   0%|          | 937/200000 [1:52:48<397:27:11,  7.19s/it, loss=0.1141, lr=4.68e-06, step=936]Training:   0%|          | 937/200000 [1:52:48<397:27:11,  7.19s/it, loss=0.0851, lr=4.68e-06, step=937]Training:   0%|          | 938/200000 [1:52:55<397:22:52,  7.19s/it, loss=0.0851, lr=4.68e-06, step=937]Training:   0%|          | 938/200000 [1:52:55<397:22:52,  7.19s/it, loss=0.1010, lr=4.69e-06, step=938]Training:   0%|          | 939/200000 [1:53:02<397:19:32,  7.19s/it, loss=0.1010, lr=4.69e-06, step=938]Training:   0%|          | 939/200000 [1:53:02<397:19:32,  7.19s/it, loss=0.0899, lr=4.69e-06, step=939]Training:   0%|          | 940/200000 [1:53:09<397:08:28,  7.18s/it, loss=0.0899, lr=4.69e-06, step=939]Training:   0%|          | 940/200000 [1:53:09<397:08:28,  7.18s/it, loss=0.0913, lr=4.70e-06, step=940]Training:   0%|          | 941/200000 [1:53:17<396:59:16,  7.18s/it, loss=0.0913, lr=4.70e-06, step=940]Training:   0%|          | 941/200000 [1:53:17<396:59:16,  7.18s/it, loss=0.1698, lr=4.70e-06, step=941]Training:   0%|          | 942/200000 [1:53:24<397:35:42,  7.19s/it, loss=0.1698, lr=4.70e-06, step=941]Training:   0%|          | 942/200000 [1:53:24<397:35:42,  7.19s/it, loss=0.1002, lr=4.71e-06, step=942]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 943/200000 [1:53:31<397:44:42,  7.19s/it, loss=0.1002, lr=4.71e-06, step=942]Training:   0%|          | 943/200000 [1:53:31<397:44:42,  7.19s/it, loss=0.1075, lr=4.71e-06, step=943]Training:   0%|          | 944/200000 [1:53:38<397:29:14,  7.19s/it, loss=0.1075, lr=4.71e-06, step=943]Training:   0%|          | 944/200000 [1:53:38<397:29:14,  7.19s/it, loss=0.0764, lr=4.72e-06, step=944]Training:   0%|          | 945/200000 [1:53:45<397:35:55,  7.19s/it, loss=0.0764, lr=4.72e-06, step=944]Training:   0%|          | 945/200000 [1:53:45<397:35:55,  7.19s/it, loss=0.1170, lr=4.72e-06, step=945]Training:   0%|          | 946/200000 [1:53:53<397:35:02,  7.19s/it, loss=0.1170, lr=4.72e-06, step=945]Training:   0%|          | 946/200000 [1:53:53<397:35:02,  7.19s/it, loss=0.1127, lr=4.73e-06, step=946]Training:   0%|          | 947/200000 [1:54:00<397:13:51,  7.18s/it, loss=0.1127, lr=4.73e-06, step=946]Training:   0%|          | 947/200000 [1:54:00<397:13:51,  7.18s/it, loss=0.0805, lr=4.73e-06, step=947]Training:   0%|          | 948/200000 [1:54:07<396:50:24,  7.18s/it, loss=0.0805, lr=4.73e-06, step=947]Training:   0%|          | 948/200000 [1:54:07<396:50:24,  7.18s/it, loss=0.0757, lr=4.74e-06, step=948]Training:   0%|          | 949/200000 [1:54:14<397:08:58,  7.18s/it, loss=0.0757, lr=4.74e-06, step=948]Training:   0%|          | 949/200000 [1:54:14<397:08:58,  7.18s/it, loss=0.1525, lr=4.74e-06, step=949]Training:   0%|          | 950/200000 [1:54:21<396:57:19,  7.18s/it, loss=0.1525, lr=4.74e-06, step=949]Training:   0%|          | 950/200000 [1:54:21<396:57:19,  7.18s/it, loss=0.0970, lr=4.75e-06, step=950]Training:   0%|          | 951/200000 [1:54:29<399:10:52,  7.22s/it, loss=0.0970, lr=4.75e-06, step=950]Training:   0%|          | 951/200000 [1:54:29<399:10:52,  7.22s/it, loss=0.0777, lr=4.75e-06, step=951]Training:   0%|          | 952/200000 [1:54:36<398:57:24,  7.22s/it, loss=0.0777, lr=4.75e-06, step=951]Training:   0%|          | 952/200000 [1:54:36<398:57:24,  7.22s/it, loss=0.1577, lr=4.76e-06, step=952]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 953/200000 [1:54:43<398:45:10,  7.21s/it, loss=0.1577, lr=4.76e-06, step=952]Training:   0%|          | 953/200000 [1:54:43<398:45:10,  7.21s/it, loss=0.0784, lr=4.76e-06, step=953]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 954/200000 [1:54:50<398:16:55,  7.20s/it, loss=0.0784, lr=4.76e-06, step=953]Training:   0%|          | 954/200000 [1:54:50<398:16:55,  7.20s/it, loss=0.1034, lr=4.77e-06, step=954]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 955/200000 [1:54:57<397:51:55,  7.20s/it, loss=0.1034, lr=4.77e-06, step=954]Training:   0%|          | 955/200000 [1:54:57<397:51:55,  7.20s/it, loss=0.1154, lr=4.77e-06, step=955]Training:   0%|          | 956/200000 [1:55:04<397:51:59,  7.20s/it, loss=0.1154, lr=4.77e-06, step=955]Training:   0%|          | 956/200000 [1:55:04<397:51:59,  7.20s/it, loss=0.0779, lr=4.78e-06, step=956]Training:   0%|          | 957/200000 [1:55:12<397:58:05,  7.20s/it, loss=0.0779, lr=4.78e-06, step=956]Training:   0%|          | 957/200000 [1:55:12<397:58:05,  7.20s/it, loss=0.0938, lr=4.78e-06, step=957]Training:   0%|          | 958/200000 [1:55:19<398:14:04,  7.20s/it, loss=0.0938, lr=4.78e-06, step=957]Training:   0%|          | 958/200000 [1:55:19<398:14:04,  7.20s/it, loss=0.0899, lr=4.79e-06, step=958]Training:   0%|          | 959/200000 [1:55:26<397:55:16,  7.20s/it, loss=0.0899, lr=4.79e-06, step=958]Training:   0%|          | 959/200000 [1:55:26<397:55:16,  7.20s/it, loss=0.0856, lr=4.79e-06, step=959]Training:   0%|          | 960/200000 [1:55:33<397:40:55,  7.19s/it, loss=0.0856, lr=4.79e-06, step=959]Training:   0%|          | 960/200000 [1:55:33<397:40:55,  7.19s/it, loss=0.0801, lr=4.80e-06, step=960]Training:   0%|          | 961/200000 [1:55:40<397:46:14,  7.19s/it, loss=0.0801, lr=4.80e-06, step=960]Training:   0%|          | 961/200000 [1:55:40<397:46:14,  7.19s/it, loss=0.1651, lr=4.80e-06, step=961]Training:   0%|          | 962/200000 [1:55:48<397:31:38,  7.19s/it, loss=0.1651, lr=4.80e-06, step=961]Training:   0%|          | 962/200000 [1:55:48<397:31:38,  7.19s/it, loss=0.1186, lr=4.81e-06, step=962]Training:   0%|          | 963/200000 [1:55:55<397:27:11,  7.19s/it, loss=0.1186, lr=4.81e-06, step=962]Training:   0%|          | 963/200000 [1:55:55<397:27:11,  7.19s/it, loss=0.0721, lr=4.81e-06, step=963]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 964/200000 [1:56:02<397:39:27,  7.19s/it, loss=0.0721, lr=4.81e-06, step=963]Training:   0%|          | 964/200000 [1:56:02<397:39:27,  7.19s/it, loss=0.0948, lr=4.82e-06, step=964]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 965/200000 [1:56:09<397:17:26,  7.19s/it, loss=0.0948, lr=4.82e-06, step=964]Training:   0%|          | 965/200000 [1:56:09<397:17:26,  7.19s/it, loss=0.0911, lr=4.82e-06, step=965]Training:   0%|          | 966/200000 [1:56:16<397:05:06,  7.18s/it, loss=0.0911, lr=4.82e-06, step=965]Training:   0%|          | 966/200000 [1:56:16<397:05:06,  7.18s/it, loss=0.1822, lr=4.83e-06, step=966]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 967/200000 [1:56:24<397:05:13,  7.18s/it, loss=0.1822, lr=4.83e-06, step=966]Training:   0%|          | 967/200000 [1:56:24<397:05:13,  7.18s/it, loss=0.0947, lr=4.83e-06, step=967]Training:   0%|          | 968/200000 [1:56:31<397:11:57,  7.18s/it, loss=0.0947, lr=4.83e-06, step=967]Training:   0%|          | 968/200000 [1:56:31<397:11:57,  7.18s/it, loss=0.1472, lr=4.84e-06, step=968]Training:   0%|          | 969/200000 [1:56:38<396:50:47,  7.18s/it, loss=0.1472, lr=4.84e-06, step=968]Training:   0%|          | 969/200000 [1:56:38<396:50:47,  7.18s/it, loss=0.0875, lr=4.84e-06, step=969]Training:   0%|          | 970/200000 [1:56:45<396:47:48,  7.18s/it, loss=0.0875, lr=4.84e-06, step=969]Training:   0%|          | 970/200000 [1:56:45<396:47:48,  7.18s/it, loss=0.1050, lr=4.85e-06, step=970]Training:   0%|          | 971/200000 [1:56:52<397:14:52,  7.19s/it, loss=0.1050, lr=4.85e-06, step=970]Training:   0%|          | 971/200000 [1:56:52<397:14:52,  7.19s/it, loss=0.1103, lr=4.85e-06, step=971]Training:   0%|          | 972/200000 [1:57:00<397:45:06,  7.19s/it, loss=0.1103, lr=4.85e-06, step=971]Training:   0%|          | 972/200000 [1:57:00<397:45:06,  7.19s/it, loss=0.1398, lr=4.86e-06, step=972]Training:   0%|          | 973/200000 [1:57:07<397:28:49,  7.19s/it, loss=0.1398, lr=4.86e-06, step=972]Training:   0%|          | 973/200000 [1:57:07<397:28:49,  7.19s/it, loss=0.1001, lr=4.86e-06, step=973]Training:   0%|          | 974/200000 [1:57:14<397:38:40,  7.19s/it, loss=0.1001, lr=4.86e-06, step=973]Training:   0%|          | 974/200000 [1:57:14<397:38:40,  7.19s/it, loss=0.1025, lr=4.87e-06, step=974]Training:   0%|          | 975/200000 [1:57:21<397:55:33,  7.20s/it, loss=0.1025, lr=4.87e-06, step=974]Training:   0%|          | 975/200000 [1:57:21<397:55:33,  7.20s/it, loss=0.1026, lr=4.87e-06, step=975]Training:   0%|          | 976/200000 [1:57:28<397:34:34,  7.19s/it, loss=0.1026, lr=4.87e-06, step=975]Training:   0%|          | 976/200000 [1:57:28<397:34:34,  7.19s/it, loss=0.0970, lr=4.88e-06, step=976]Training:   0%|          | 977/200000 [1:57:35<397:34:17,  7.19s/it, loss=0.0970, lr=4.88e-06, step=976]Training:   0%|          | 977/200000 [1:57:35<397:34:17,  7.19s/it, loss=0.1107, lr=4.88e-06, step=977]Training:   0%|          | 978/200000 [1:57:43<396:53:31,  7.18s/it, loss=0.1107, lr=4.88e-06, step=977]Training:   0%|          | 978/200000 [1:57:43<396:53:31,  7.18s/it, loss=0.2590, lr=4.89e-06, step=978]Training:   0%|          | 979/200000 [1:57:50<397:23:49,  7.19s/it, loss=0.2590, lr=4.89e-06, step=978]Training:   0%|          | 979/200000 [1:57:50<397:23:49,  7.19s/it, loss=0.1171, lr=4.89e-06, step=979]Training:   0%|          | 980/200000 [1:57:57<397:35:16,  7.19s/it, loss=0.1171, lr=4.89e-06, step=979]Training:   0%|          | 980/200000 [1:57:57<397:35:16,  7.19s/it, loss=0.1076, lr=4.90e-06, step=980]Training:   0%|          | 981/200000 [1:58:04<397:27:08,  7.19s/it, loss=0.1076, lr=4.90e-06, step=980]Training:   0%|          | 981/200000 [1:58:04<397:27:08,  7.19s/it, loss=0.1069, lr=4.90e-06, step=981]Training:   0%|          | 982/200000 [1:58:11<397:18:18,  7.19s/it, loss=0.1069, lr=4.90e-06, step=981]Training:   0%|          | 982/200000 [1:58:11<397:18:18,  7.19s/it, loss=0.1585, lr=4.91e-06, step=982]Training:   0%|          | 983/200000 [1:58:19<397:14:51,  7.19s/it, loss=0.1585, lr=4.91e-06, step=982]Training:   0%|          | 983/200000 [1:58:19<397:14:51,  7.19s/it, loss=0.1206, lr=4.91e-06, step=983]Training:   0%|          | 984/200000 [1:58:26<396:50:03,  7.18s/it, loss=0.1206, lr=4.91e-06, step=983]Training:   0%|          | 984/200000 [1:58:26<396:50:03,  7.18s/it, loss=0.1423, lr=4.92e-06, step=984]Training:   0%|          | 985/200000 [1:58:33<397:04:36,  7.18s/it, loss=0.1423, lr=4.92e-06, step=984]Training:   0%|          | 985/200000 [1:58:33<397:04:36,  7.18s/it, loss=0.0982, lr=4.92e-06, step=985]Training:   0%|          | 986/200000 [1:58:40<396:56:25,  7.18s/it, loss=0.0982, lr=4.92e-06, step=985]Training:   0%|          | 986/200000 [1:58:40<396:56:25,  7.18s/it, loss=0.1064, lr=4.93e-06, step=986]Training:   0%|          | 987/200000 [1:58:47<396:45:54,  7.18s/it, loss=0.1064, lr=4.93e-06, step=986]Training:   0%|          | 987/200000 [1:58:47<396:45:54,  7.18s/it, loss=0.0969, lr=4.93e-06, step=987]Training:   0%|          | 988/200000 [1:58:54<397:08:04,  7.18s/it, loss=0.0969, lr=4.93e-06, step=987]Training:   0%|          | 988/200000 [1:58:54<397:08:04,  7.18s/it, loss=0.1072, lr=4.94e-06, step=988]Training:   0%|          | 989/200000 [1:59:02<397:11:00,  7.18s/it, loss=0.1072, lr=4.94e-06, step=988]Training:   0%|          | 989/200000 [1:59:02<397:11:00,  7.18s/it, loss=0.0944, lr=4.94e-06, step=989]Training:   0%|          | 990/200000 [1:59:09<397:36:31,  7.19s/it, loss=0.0944, lr=4.94e-06, step=989]Training:   0%|          | 990/200000 [1:59:09<397:36:31,  7.19s/it, loss=0.0968, lr=4.95e-06, step=990]Training:   0%|          | 991/200000 [1:59:16<397:21:28,  7.19s/it, loss=0.0968, lr=4.95e-06, step=990]Training:   0%|          | 991/200000 [1:59:16<397:21:28,  7.19s/it, loss=0.1268, lr=4.95e-06, step=991]Training:   0%|          | 992/200000 [1:59:23<397:22:34,  7.19s/it, loss=0.1268, lr=4.95e-06, step=991]Training:   0%|          | 992/200000 [1:59:23<397:22:34,  7.19s/it, loss=0.1291, lr=4.96e-06, step=992]Training:   0%|          | 993/200000 [1:59:31<399:39:12,  7.23s/it, loss=0.1291, lr=4.96e-06, step=992]Training:   0%|          | 993/200000 [1:59:31<399:39:12,  7.23s/it, loss=0.0885, lr=4.96e-06, step=993]Training:   0%|          | 994/200000 [1:59:38<399:11:44,  7.22s/it, loss=0.0885, lr=4.96e-06, step=993]Training:   0%|          | 994/200000 [1:59:38<399:11:44,  7.22s/it, loss=0.1053, lr=4.97e-06, step=994]Training:   0%|          | 995/200000 [1:59:45<398:35:29,  7.21s/it, loss=0.1053, lr=4.97e-06, step=994]Training:   0%|          | 995/200000 [1:59:45<398:35:29,  7.21s/it, loss=0.0850, lr=4.97e-06, step=995]Training:   0%|          | 996/200000 [1:59:52<398:21:38,  7.21s/it, loss=0.0850, lr=4.97e-06, step=995]Training:   0%|          | 996/200000 [1:59:52<398:21:38,  7.21s/it, loss=0.1088, lr=4.98e-06, step=996]Training:   0%|          | 997/200000 [1:59:59<398:23:11,  7.21s/it, loss=0.1088, lr=4.98e-06, step=996]Training:   0%|          | 997/200000 [1:59:59<398:23:11,  7.21s/it, loss=0.1010, lr=4.98e-06, step=997]Training:   0%|          | 998/200000 [2:00:07<398:21:20,  7.21s/it, loss=0.1010, lr=4.98e-06, step=997]Training:   0%|          | 998/200000 [2:00:07<398:21:20,  7.21s/it, loss=0.2331, lr=4.99e-06, step=998]Training:   0%|          | 999/200000 [2:00:14<398:03:19,  7.20s/it, loss=0.2331, lr=4.99e-06, step=998]Training:   0%|          | 999/200000 [2:00:14<398:03:19,  7.20s/it, loss=0.1037, lr=4.99e-06, step=999]Training:   0%|          | 1000/200000 [2:00:21<397:52:50,  7.20s/it, loss=0.1037, lr=4.99e-06, step=999]Training:   0%|          | 1000/200000 [2:00:21<397:52:50,  7.20s/it, loss=0.0907, lr=5.00e-06, step=1000]21:56:56.941 [I] step=1000 loss=0.1100 lr=4.76e-06 grad_norm=0.80 time=719.1s                     (486094:train_pytorch.py:582)
Training:   1%|          | 1001/200000 [2:00:28<397:56:48,  7.20s/it, loss=0.0907, lr=5.00e-06, step=1000]Training:   1%|          | 1001/200000 [2:00:28<397:56:48,  7.20s/it, loss=0.0709, lr=5.00e-06, step=1001]Training:   1%|          | 1002/200000 [2:00:35<397:53:39,  7.20s/it, loss=0.0709, lr=5.00e-06, step=1001]Training:   1%|          | 1002/200000 [2:00:35<397:53:39,  7.20s/it, loss=0.0868, lr=5.01e-06, step=1002]Training:   1%|          | 1003/200000 [2:00:43<397:47:41,  7.20s/it, loss=0.0868, lr=5.01e-06, step=1002]Training:   1%|          | 1003/200000 [2:00:43<397:47:41,  7.20s/it, loss=0.1302, lr=5.01e-06, step=1003]Training:   1%|          | 1004/200000 [2:00:50<398:02:15,  7.20s/it, loss=0.1302, lr=5.01e-06, step=1003]Training:   1%|          | 1004/200000 [2:00:50<398:02:15,  7.20s/it, loss=0.0770, lr=5.02e-06, step=1004]Training:   1%|          | 1005/200000 [2:00:57<398:12:32,  7.20s/it, loss=0.0770, lr=5.02e-06, step=1004]Training:   1%|          | 1005/200000 [2:00:57<398:12:32,  7.20s/it, loss=0.1024, lr=5.02e-06, step=1005]Training:   1%|          | 1006/200000 [2:01:04<398:08:46,  7.20s/it, loss=0.1024, lr=5.02e-06, step=1005]Training:   1%|          | 1006/200000 [2:01:04<398:08:46,  7.20s/it, loss=0.1580, lr=5.03e-06, step=1006]Training:   1%|          | 1007/200000 [2:01:11<397:34:11,  7.19s/it, loss=0.1580, lr=5.03e-06, step=1006]Training:   1%|          | 1007/200000 [2:01:11<397:34:11,  7.19s/it, loss=0.1083, lr=5.03e-06, step=1007]Training:   1%|          | 1008/200000 [2:01:19<397:23:17,  7.19s/it, loss=0.1083, lr=5.03e-06, step=1007]Training:   1%|          | 1008/200000 [2:01:19<397:23:17,  7.19s/it, loss=0.0944, lr=5.04e-06, step=1008]Training:   1%|          | 1009/200000 [2:01:26<397:33:44,  7.19s/it, loss=0.0944, lr=5.04e-06, step=1008]Training:   1%|          | 1009/200000 [2:01:26<397:33:44,  7.19s/it, loss=0.1363, lr=5.04e-06, step=1009]Training:   1%|          | 1010/200000 [2:01:33<396:59:06,  7.18s/it, loss=0.1363, lr=5.04e-06, step=1009]Training:   1%|          | 1010/200000 [2:01:33<396:59:06,  7.18s/it, loss=0.0797, lr=5.05e-06, step=1010]Training:   1%|          | 1011/200000 [2:01:40<397:08:31,  7.18s/it, loss=0.0797, lr=5.05e-06, step=1010]Training:   1%|          | 1011/200000 [2:01:40<397:08:31,  7.18s/it, loss=0.0696, lr=5.05e-06, step=1011]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1012/200000 [2:01:47<397:41:13,  7.19s/it, loss=0.0696, lr=5.05e-06, step=1011]Training:   1%|          | 1012/200000 [2:01:47<397:41:13,  7.19s/it, loss=0.1071, lr=5.06e-06, step=1012]Training:   1%|          | 1013/200000 [2:01:54<397:00:07,  7.18s/it, loss=0.1071, lr=5.06e-06, step=1012]Training:   1%|          | 1013/200000 [2:01:54<397:00:07,  7.18s/it, loss=0.1514, lr=5.06e-06, step=1013]Training:   1%|          | 1014/200000 [2:02:02<399:23:57,  7.23s/it, loss=0.1514, lr=5.06e-06, step=1013]Training:   1%|          | 1014/200000 [2:02:02<399:23:57,  7.23s/it, loss=0.1030, lr=5.07e-06, step=1014]Training:   1%|          | 1015/200000 [2:02:09<398:45:28,  7.21s/it, loss=0.1030, lr=5.07e-06, step=1014]Training:   1%|          | 1015/200000 [2:02:09<398:45:28,  7.21s/it, loss=0.1323, lr=5.07e-06, step=1015]Training:   1%|          | 1016/200000 [2:02:16<398:26:09,  7.21s/it, loss=0.1323, lr=5.07e-06, step=1015]Training:   1%|          | 1016/200000 [2:02:16<398:26:09,  7.21s/it, loss=0.1078, lr=5.08e-06, step=1016]Training:   1%|          | 1017/200000 [2:02:23<398:21:14,  7.21s/it, loss=0.1078, lr=5.08e-06, step=1016]Training:   1%|          | 1017/200000 [2:02:23<398:21:14,  7.21s/it, loss=0.0913, lr=5.08e-06, step=1017]Training:   1%|          | 1018/200000 [2:02:31<398:02:22,  7.20s/it, loss=0.0913, lr=5.08e-06, step=1017]Training:   1%|          | 1018/200000 [2:02:31<398:02:22,  7.20s/it, loss=0.0914, lr=5.09e-06, step=1018]Training:   1%|          | 1019/200000 [2:02:38<398:07:11,  7.20s/it, loss=0.0914, lr=5.09e-06, step=1018]Training:   1%|          | 1019/200000 [2:02:38<398:07:11,  7.20s/it, loss=0.1548, lr=5.09e-06, step=1019]Training:   1%|          | 1020/200000 [2:02:45<397:34:01,  7.19s/it, loss=0.1548, lr=5.09e-06, step=1019]Training:   1%|          | 1020/200000 [2:02:45<397:34:01,  7.19s/it, loss=0.1139, lr=5.10e-06, step=1020]Training:   1%|          | 1021/200000 [2:02:52<397:03:59,  7.18s/it, loss=0.1139, lr=5.10e-06, step=1020]Training:   1%|          | 1021/200000 [2:02:52<397:03:59,  7.18s/it, loss=0.1011, lr=5.10e-06, step=1021]Training:   1%|          | 1022/200000 [2:02:59<397:12:07,  7.19s/it, loss=0.1011, lr=5.10e-06, step=1021]Training:   1%|          | 1022/200000 [2:02:59<397:12:07,  7.19s/it, loss=0.1619, lr=5.11e-06, step=1022]Training:   1%|          | 1023/200000 [2:03:06<396:56:37,  7.18s/it, loss=0.1619, lr=5.11e-06, step=1022]Training:   1%|          | 1023/200000 [2:03:06<396:56:37,  7.18s/it, loss=0.0861, lr=5.11e-06, step=1023]Training:   1%|          | 1024/200000 [2:03:14<396:41:38,  7.18s/it, loss=0.0861, lr=5.11e-06, step=1023]Training:   1%|          | 1024/200000 [2:03:14<396:41:38,  7.18s/it, loss=0.1168, lr=5.12e-06, step=1024]Training:   1%|          | 1025/200000 [2:03:21<396:20:47,  7.17s/it, loss=0.1168, lr=5.12e-06, step=1024]Training:   1%|          | 1025/200000 [2:03:21<396:20:47,  7.17s/it, loss=0.0669, lr=5.12e-06, step=1025]Training:   1%|          | 1026/200000 [2:03:28<396:05:22,  7.17s/it, loss=0.0669, lr=5.12e-06, step=1025]Training:   1%|          | 1026/200000 [2:03:28<396:05:22,  7.17s/it, loss=0.0893, lr=5.13e-06, step=1026]Training:   1%|          | 1027/200000 [2:03:35<397:55:23,  7.20s/it, loss=0.0893, lr=5.13e-06, step=1026]Training:   1%|          | 1027/200000 [2:03:35<397:55:23,  7.20s/it, loss=0.1056, lr=5.13e-06, step=1027]Training:   1%|          | 1028/200000 [2:03:42<397:28:34,  7.19s/it, loss=0.1056, lr=5.13e-06, step=1027]Training:   1%|          | 1028/200000 [2:03:42<397:28:34,  7.19s/it, loss=0.0801, lr=5.14e-06, step=1028]Training:   1%|          | 1029/200000 [2:03:50<397:11:09,  7.19s/it, loss=0.0801, lr=5.14e-06, step=1028]Training:   1%|          | 1029/200000 [2:03:50<397:11:09,  7.19s/it, loss=0.1119, lr=5.14e-06, step=1029]Training:   1%|          | 1030/200000 [2:03:57<397:04:43,  7.18s/it, loss=0.1119, lr=5.14e-06, step=1029]Training:   1%|          | 1030/200000 [2:03:57<397:04:43,  7.18s/it, loss=0.0806, lr=5.15e-06, step=1030]Training:   1%|          | 1031/200000 [2:04:04<396:51:27,  7.18s/it, loss=0.0806, lr=5.15e-06, step=1030]Training:   1%|          | 1031/200000 [2:04:04<396:51:27,  7.18s/it, loss=0.0978, lr=5.15e-06, step=1031]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1032/200000 [2:04:11<397:04:39,  7.18s/it, loss=0.0978, lr=5.15e-06, step=1031]Training:   1%|          | 1032/200000 [2:04:11<397:04:39,  7.18s/it, loss=0.1144, lr=5.16e-06, step=1032]Training:   1%|          | 1033/200000 [2:04:18<396:28:15,  7.17s/it, loss=0.1144, lr=5.16e-06, step=1032]Training:   1%|          | 1033/200000 [2:04:18<396:28:15,  7.17s/it, loss=0.1053, lr=5.16e-06, step=1033]Training:   1%|          | 1034/200000 [2:04:25<395:20:07,  7.15s/it, loss=0.1053, lr=5.16e-06, step=1033]Training:   1%|          | 1034/200000 [2:04:25<395:20:07,  7.15s/it, loss=0.2053, lr=5.17e-06, step=1034]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1035/200000 [2:04:33<396:14:35,  7.17s/it, loss=0.2053, lr=5.17e-06, step=1034]Training:   1%|          | 1035/200000 [2:04:33<396:14:35,  7.17s/it, loss=0.0897, lr=5.17e-06, step=1035]Training:   1%|          | 1036/200000 [2:04:40<396:57:01,  7.18s/it, loss=0.0897, lr=5.17e-06, step=1035]Training:   1%|          | 1036/200000 [2:04:40<396:57:01,  7.18s/it, loss=0.1576, lr=5.18e-06, step=1036]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1037/200000 [2:04:47<396:44:54,  7.18s/it, loss=0.1576, lr=5.18e-06, step=1036]Training:   1%|          | 1037/200000 [2:04:47<396:44:54,  7.18s/it, loss=0.0916, lr=5.18e-06, step=1037]Training:   1%|          | 1038/200000 [2:04:54<396:42:04,  7.18s/it, loss=0.0916, lr=5.18e-06, step=1037]Training:   1%|          | 1038/200000 [2:04:54<396:42:04,  7.18s/it, loss=0.0907, lr=5.19e-06, step=1038]Training:   1%|          | 1039/200000 [2:05:01<396:35:09,  7.18s/it, loss=0.0907, lr=5.19e-06, step=1038]Training:   1%|          | 1039/200000 [2:05:01<396:35:09,  7.18s/it, loss=0.1082, lr=5.19e-06, step=1039]Training:   1%|          | 1040/200000 [2:05:08<396:55:08,  7.18s/it, loss=0.1082, lr=5.19e-06, step=1039]Training:   1%|          | 1040/200000 [2:05:08<396:55:08,  7.18s/it, loss=0.1144, lr=5.20e-06, step=1040]Training:   1%|          | 1041/200000 [2:05:16<396:35:57,  7.18s/it, loss=0.1144, lr=5.20e-06, step=1040]Training:   1%|          | 1041/200000 [2:05:16<396:35:57,  7.18s/it, loss=0.1013, lr=5.20e-06, step=1041]Training:   1%|          | 1042/200000 [2:05:23<396:28:36,  7.17s/it, loss=0.1013, lr=5.20e-06, step=1041]Training:   1%|          | 1042/200000 [2:05:23<396:28:36,  7.17s/it, loss=0.0983, lr=5.21e-06, step=1042]Training:   1%|          | 1043/200000 [2:05:30<396:21:42,  7.17s/it, loss=0.0983, lr=5.21e-06, step=1042]Training:   1%|          | 1043/200000 [2:05:30<396:21:42,  7.17s/it, loss=0.1406, lr=5.21e-06, step=1043]Training:   1%|          | 1044/200000 [2:05:37<396:23:57,  7.17s/it, loss=0.1406, lr=5.21e-06, step=1043]Training:   1%|          | 1044/200000 [2:05:37<396:23:57,  7.17s/it, loss=0.1028, lr=5.22e-06, step=1044]Training:   1%|          | 1045/200000 [2:05:44<396:55:21,  7.18s/it, loss=0.1028, lr=5.22e-06, step=1044]Training:   1%|          | 1045/200000 [2:05:44<396:55:21,  7.18s/it, loss=0.1108, lr=5.22e-06, step=1045]Training:   1%|          | 1046/200000 [2:05:52<397:10:21,  7.19s/it, loss=0.1108, lr=5.22e-06, step=1045]Training:   1%|          | 1046/200000 [2:05:52<397:10:21,  7.19s/it, loss=0.0977, lr=5.23e-06, step=1046]Training:   1%|          | 1047/200000 [2:05:59<397:25:01,  7.19s/it, loss=0.0977, lr=5.23e-06, step=1046]Training:   1%|          | 1047/200000 [2:05:59<397:25:01,  7.19s/it, loss=0.0900, lr=5.23e-06, step=1047]Training:   1%|          | 1048/200000 [2:06:06<397:29:19,  7.19s/it, loss=0.0900, lr=5.23e-06, step=1047]Training:   1%|          | 1048/200000 [2:06:06<397:29:19,  7.19s/it, loss=0.1409, lr=5.24e-06, step=1048]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1049/200000 [2:06:13<397:09:52,  7.19s/it, loss=0.1409, lr=5.24e-06, step=1048]Training:   1%|          | 1049/200000 [2:06:13<397:09:52,  7.19s/it, loss=0.1463, lr=5.24e-06, step=1049]Training:   1%|          | 1050/200000 [2:06:20<397:15:26,  7.19s/it, loss=0.1463, lr=5.24e-06, step=1049]Training:   1%|          | 1050/200000 [2:06:20<397:15:26,  7.19s/it, loss=0.1057, lr=5.25e-06, step=1050]Training:   1%|          | 1051/200000 [2:06:28<397:28:24,  7.19s/it, loss=0.1057, lr=5.25e-06, step=1050]Training:   1%|          | 1051/200000 [2:06:28<397:28:24,  7.19s/it, loss=0.0935, lr=5.25e-06, step=1051]Training:   1%|          | 1052/200000 [2:06:35<397:06:32,  7.19s/it, loss=0.0935, lr=5.25e-06, step=1051]Training:   1%|          | 1052/200000 [2:06:35<397:06:32,  7.19s/it, loss=0.1227, lr=5.26e-06, step=1052]Training:   1%|          | 1053/200000 [2:06:42<396:50:21,  7.18s/it, loss=0.1227, lr=5.26e-06, step=1052]Training:   1%|          | 1053/200000 [2:06:42<396:50:21,  7.18s/it, loss=0.1056, lr=5.26e-06, step=1053]Training:   1%|          | 1054/200000 [2:06:49<397:00:46,  7.18s/it, loss=0.1056, lr=5.26e-06, step=1053]Training:   1%|          | 1054/200000 [2:06:49<397:00:46,  7.18s/it, loss=0.0879, lr=5.27e-06, step=1054]Training:   1%|          | 1055/200000 [2:06:56<397:27:23,  7.19s/it, loss=0.0879, lr=5.27e-06, step=1054]Training:   1%|          | 1055/200000 [2:06:56<397:27:23,  7.19s/it, loss=0.1318, lr=5.27e-06, step=1055]Training:   1%|          | 1056/200000 [2:07:04<399:20:10,  7.23s/it, loss=0.1318, lr=5.27e-06, step=1055]Training:   1%|          | 1056/200000 [2:07:04<399:20:10,  7.23s/it, loss=0.0899, lr=5.28e-06, step=1056]Training:   1%|          | 1057/200000 [2:07:11<398:31:20,  7.21s/it, loss=0.0899, lr=5.28e-06, step=1056]Training:   1%|          | 1057/200000 [2:07:11<398:31:20,  7.21s/it, loss=0.1028, lr=5.28e-06, step=1057]Training:   1%|          | 1058/200000 [2:07:18<397:51:33,  7.20s/it, loss=0.1028, lr=5.28e-06, step=1057]Training:   1%|          | 1058/200000 [2:07:18<397:51:33,  7.20s/it, loss=0.1085, lr=5.29e-06, step=1058]Training:   1%|          | 1059/200000 [2:07:25<398:15:33,  7.21s/it, loss=0.1085, lr=5.29e-06, step=1058]Training:   1%|          | 1059/200000 [2:07:25<398:15:33,  7.21s/it, loss=0.1103, lr=5.29e-06, step=1059]Training:   1%|          | 1060/200000 [2:07:32<397:42:31,  7.20s/it, loss=0.1103, lr=5.29e-06, step=1059]Training:   1%|          | 1060/200000 [2:07:32<397:42:31,  7.20s/it, loss=0.0983, lr=5.30e-06, step=1060]Training:   1%|          | 1061/200000 [2:07:40<398:05:43,  7.20s/it, loss=0.0983, lr=5.30e-06, step=1060]Training:   1%|          | 1061/200000 [2:07:40<398:05:43,  7.20s/it, loss=0.1015, lr=5.30e-06, step=1061]Training:   1%|          | 1062/200000 [2:07:47<397:27:05,  7.19s/it, loss=0.1015, lr=5.30e-06, step=1061]Training:   1%|          | 1062/200000 [2:07:47<397:27:05,  7.19s/it, loss=0.1229, lr=5.31e-06, step=1062]Training:   1%|          | 1063/200000 [2:07:54<397:28:25,  7.19s/it, loss=0.1229, lr=5.31e-06, step=1062]Training:   1%|          | 1063/200000 [2:07:54<397:28:25,  7.19s/it, loss=0.0959, lr=5.31e-06, step=1063]Training:   1%|          | 1064/200000 [2:08:01<397:16:40,  7.19s/it, loss=0.0959, lr=5.31e-06, step=1063]Training:   1%|          | 1064/200000 [2:08:01<397:16:40,  7.19s/it, loss=0.0760, lr=5.32e-06, step=1064]Training:   1%|          | 1065/200000 [2:08:08<397:23:33,  7.19s/it, loss=0.0760, lr=5.32e-06, step=1064]Training:   1%|          | 1065/200000 [2:08:08<397:23:33,  7.19s/it, loss=0.1098, lr=5.32e-06, step=1065]Training:   1%|          | 1066/200000 [2:08:15<397:30:01,  7.19s/it, loss=0.1098, lr=5.32e-06, step=1065]Training:   1%|          | 1066/200000 [2:08:15<397:30:01,  7.19s/it, loss=0.0762, lr=5.33e-06, step=1066]Training:   1%|          | 1067/200000 [2:08:23<397:16:38,  7.19s/it, loss=0.0762, lr=5.33e-06, step=1066]Training:   1%|          | 1067/200000 [2:08:23<397:16:38,  7.19s/it, loss=0.0798, lr=5.33e-06, step=1067]Training:   1%|          | 1068/200000 [2:08:30<397:19:42,  7.19s/it, loss=0.0798, lr=5.33e-06, step=1067]Training:   1%|          | 1068/200000 [2:08:30<397:19:42,  7.19s/it, loss=0.1349, lr=5.34e-06, step=1068]Training:   1%|          | 1069/200000 [2:08:37<397:44:20,  7.20s/it, loss=0.1349, lr=5.34e-06, step=1068]Training:   1%|          | 1069/200000 [2:08:37<397:44:20,  7.20s/it, loss=0.1095, lr=5.34e-06, step=1069]Training:   1%|          | 1070/200000 [2:08:44<397:14:11,  7.19s/it, loss=0.1095, lr=5.34e-06, step=1069]Training:   1%|          | 1070/200000 [2:08:44<397:14:11,  7.19s/it, loss=0.1117, lr=5.35e-06, step=1070]Training:   1%|          | 1071/200000 [2:08:51<397:07:50,  7.19s/it, loss=0.1117, lr=5.35e-06, step=1070]Training:   1%|          | 1071/200000 [2:08:51<397:07:50,  7.19s/it, loss=0.1821, lr=5.35e-06, step=1071]Training:   1%|          | 1072/200000 [2:08:59<396:51:05,  7.18s/it, loss=0.1821, lr=5.35e-06, step=1071]Training:   1%|          | 1072/200000 [2:08:59<396:51:05,  7.18s/it, loss=0.0898, lr=5.36e-06, step=1072]Training:   1%|          | 1073/200000 [2:09:06<397:11:22,  7.19s/it, loss=0.0898, lr=5.36e-06, step=1072]Training:   1%|          | 1073/200000 [2:09:06<397:11:22,  7.19s/it, loss=0.0766, lr=5.36e-06, step=1073]Training:   1%|          | 1074/200000 [2:09:13<396:14:02,  7.17s/it, loss=0.0766, lr=5.36e-06, step=1073]Training:   1%|          | 1074/200000 [2:09:13<396:14:02,  7.17s/it, loss=0.0933, lr=5.37e-06, step=1074]Training:   1%|          | 1075/200000 [2:09:20<396:15:58,  7.17s/it, loss=0.0933, lr=5.37e-06, step=1074]Training:   1%|          | 1075/200000 [2:09:20<396:15:58,  7.17s/it, loss=0.0951, lr=5.37e-06, step=1075]Training:   1%|          | 1076/200000 [2:09:27<396:16:07,  7.17s/it, loss=0.0951, lr=5.37e-06, step=1075]Training:   1%|          | 1076/200000 [2:09:27<396:16:07,  7.17s/it, loss=0.1441, lr=5.38e-06, step=1076]Training:   1%|          | 1077/200000 [2:09:35<398:44:00,  7.22s/it, loss=0.1441, lr=5.38e-06, step=1076]Training:   1%|          | 1077/200000 [2:09:35<398:44:00,  7.22s/it, loss=0.0846, lr=5.38e-06, step=1077]Training:   1%|          | 1078/200000 [2:09:42<398:22:24,  7.21s/it, loss=0.0846, lr=5.38e-06, step=1077]Training:   1%|          | 1078/200000 [2:09:42<398:22:24,  7.21s/it, loss=0.0980, lr=5.39e-06, step=1078]Training:   1%|          | 1079/200000 [2:09:49<397:57:25,  7.20s/it, loss=0.0980, lr=5.39e-06, step=1078]Training:   1%|          | 1079/200000 [2:09:49<397:57:25,  7.20s/it, loss=0.0819, lr=5.39e-06, step=1079]Training:   1%|          | 1080/200000 [2:09:56<397:20:24,  7.19s/it, loss=0.0819, lr=5.39e-06, step=1079]Training:   1%|          | 1080/200000 [2:09:56<397:20:24,  7.19s/it, loss=0.0702, lr=5.40e-06, step=1080]Training:   1%|          | 1081/200000 [2:10:03<396:48:11,  7.18s/it, loss=0.0702, lr=5.40e-06, step=1080]Training:   1%|          | 1081/200000 [2:10:03<396:48:11,  7.18s/it, loss=0.0985, lr=5.40e-06, step=1081]Training:   1%|          | 1082/200000 [2:10:10<397:09:17,  7.19s/it, loss=0.0985, lr=5.40e-06, step=1081]Training:   1%|          | 1082/200000 [2:10:10<397:09:17,  7.19s/it, loss=0.0987, lr=5.41e-06, step=1082]Training:   1%|          | 1083/200000 [2:10:18<396:29:57,  7.18s/it, loss=0.0987, lr=5.41e-06, step=1082]Training:   1%|          | 1083/200000 [2:10:18<396:29:57,  7.18s/it, loss=0.2277, lr=5.41e-06, step=1083]Training:   1%|          | 1084/200000 [2:10:25<396:40:30,  7.18s/it, loss=0.2277, lr=5.41e-06, step=1083]Training:   1%|          | 1084/200000 [2:10:25<396:40:30,  7.18s/it, loss=0.0796, lr=5.42e-06, step=1084]Training:   1%|          | 1085/200000 [2:10:32<396:20:54,  7.17s/it, loss=0.0796, lr=5.42e-06, step=1084]Training:   1%|          | 1085/200000 [2:10:32<396:20:54,  7.17s/it, loss=0.0934, lr=5.42e-06, step=1085]Training:   1%|          | 1086/200000 [2:10:39<396:08:21,  7.17s/it, loss=0.0934, lr=5.42e-06, step=1085]Training:   1%|          | 1086/200000 [2:10:39<396:08:21,  7.17s/it, loss=0.1117, lr=5.43e-06, step=1086]Training:   1%|          | 1087/200000 [2:10:46<396:25:53,  7.17s/it, loss=0.1117, lr=5.43e-06, step=1086]Training:   1%|          | 1087/200000 [2:10:46<396:25:53,  7.17s/it, loss=0.1230, lr=5.43e-06, step=1087]Training:   1%|          | 1088/200000 [2:10:53<396:23:30,  7.17s/it, loss=0.1230, lr=5.43e-06, step=1087]Training:   1%|          | 1088/200000 [2:10:53<396:23:30,  7.17s/it, loss=0.0751, lr=5.44e-06, step=1088]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1089/200000 [2:11:01<396:09:45,  7.17s/it, loss=0.0751, lr=5.44e-06, step=1088]Training:   1%|          | 1089/200000 [2:11:01<396:09:45,  7.17s/it, loss=0.1274, lr=5.44e-06, step=1089]Training:   1%|          | 1090/200000 [2:11:08<396:36:01,  7.18s/it, loss=0.1274, lr=5.44e-06, step=1089]Training:   1%|          | 1090/200000 [2:11:08<396:36:01,  7.18s/it, loss=0.0675, lr=5.45e-06, step=1090]Training:   1%|          | 1091/200000 [2:11:15<396:32:21,  7.18s/it, loss=0.0675, lr=5.45e-06, step=1090]Training:   1%|          | 1091/200000 [2:11:15<396:32:21,  7.18s/it, loss=0.0792, lr=5.45e-06, step=1091]Training:   1%|          | 1092/200000 [2:11:22<396:20:19,  7.17s/it, loss=0.0792, lr=5.45e-06, step=1091]Training:   1%|          | 1092/200000 [2:11:22<396:20:19,  7.17s/it, loss=0.1102, lr=5.46e-06, step=1092]Training:   1%|          | 1093/200000 [2:11:29<396:16:13,  7.17s/it, loss=0.1102, lr=5.46e-06, step=1092]Training:   1%|          | 1093/200000 [2:11:29<396:16:13,  7.17s/it, loss=0.0888, lr=5.46e-06, step=1093]Training:   1%|          | 1094/200000 [2:11:37<396:19:55,  7.17s/it, loss=0.0888, lr=5.46e-06, step=1093]Training:   1%|          | 1094/200000 [2:11:37<396:19:55,  7.17s/it, loss=0.0831, lr=5.47e-06, step=1094]Training:   1%|          | 1095/200000 [2:11:44<396:51:31,  7.18s/it, loss=0.0831, lr=5.47e-06, step=1094]Training:   1%|          | 1095/200000 [2:11:44<396:51:31,  7.18s/it, loss=0.0963, lr=5.47e-06, step=1095]Training:   1%|          | 1096/200000 [2:11:51<396:40:50,  7.18s/it, loss=0.0963, lr=5.47e-06, step=1095]Training:   1%|          | 1096/200000 [2:11:51<396:40:50,  7.18s/it, loss=0.0895, lr=5.48e-06, step=1096]Training:   1%|          | 1097/200000 [2:11:58<396:34:12,  7.18s/it, loss=0.0895, lr=5.48e-06, step=1096]Training:   1%|          | 1097/200000 [2:11:58<396:34:12,  7.18s/it, loss=0.0924, lr=5.48e-06, step=1097]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1098/200000 [2:12:05<397:46:06,  7.20s/it, loss=0.0924, lr=5.48e-06, step=1097]Training:   1%|          | 1098/200000 [2:12:05<397:46:06,  7.20s/it, loss=0.1391, lr=5.49e-06, step=1098]Training:   1%|          | 1099/200000 [2:12:13<398:06:11,  7.21s/it, loss=0.1391, lr=5.49e-06, step=1098]Training:   1%|          | 1099/200000 [2:12:13<398:06:11,  7.21s/it, loss=0.1140, lr=5.49e-06, step=1099]Training:   1%|          | 1100/200000 [2:12:20<398:00:49,  7.20s/it, loss=0.1140, lr=5.49e-06, step=1099]Training:   1%|          | 1100/200000 [2:12:20<398:00:49,  7.20s/it, loss=0.1108, lr=5.50e-06, step=1100]22:08:55.729 [I] step=1100 loss=0.1078 lr=5.26e-06 grad_norm=0.81 time=718.8s                     (486094:train_pytorch.py:582)
Training:   1%|          | 1101/200000 [2:12:27<397:50:17,  7.20s/it, loss=0.1108, lr=5.50e-06, step=1100]Training:   1%|          | 1101/200000 [2:12:27<397:50:17,  7.20s/it, loss=0.1855, lr=5.50e-06, step=1101]Training:   1%|          | 1102/200000 [2:12:34<397:46:32,  7.20s/it, loss=0.1855, lr=5.50e-06, step=1101]Training:   1%|          | 1102/200000 [2:12:34<397:46:32,  7.20s/it, loss=0.1179, lr=5.51e-06, step=1102]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1103/200000 [2:12:41<397:54:52,  7.20s/it, loss=0.1179, lr=5.51e-06, step=1102]Training:   1%|          | 1103/200000 [2:12:41<397:54:52,  7.20s/it, loss=0.0756, lr=5.51e-06, step=1103]Training:   1%|          | 1104/200000 [2:12:49<397:25:15,  7.19s/it, loss=0.0756, lr=5.51e-06, step=1103]Training:   1%|          | 1104/200000 [2:12:49<397:25:15,  7.19s/it, loss=0.0772, lr=5.52e-06, step=1104]Training:   1%|          | 1105/200000 [2:12:56<397:18:16,  7.19s/it, loss=0.0772, lr=5.52e-06, step=1104]Training:   1%|          | 1105/200000 [2:12:56<397:18:16,  7.19s/it, loss=0.0844, lr=5.52e-06, step=1105]Training:   1%|          | 1106/200000 [2:13:03<397:21:43,  7.19s/it, loss=0.0844, lr=5.52e-06, step=1105]Training:   1%|          | 1106/200000 [2:13:03<397:21:43,  7.19s/it, loss=0.1013, lr=5.53e-06, step=1106]Training:   1%|          | 1107/200000 [2:13:10<397:04:30,  7.19s/it, loss=0.1013, lr=5.53e-06, step=1106]Training:   1%|          | 1107/200000 [2:13:10<397:04:30,  7.19s/it, loss=0.0998, lr=5.53e-06, step=1107]Training:   1%|          | 1108/200000 [2:13:17<397:29:39,  7.19s/it, loss=0.0998, lr=5.53e-06, step=1107]Training:   1%|          | 1108/200000 [2:13:17<397:29:39,  7.19s/it, loss=0.1026, lr=5.54e-06, step=1108]Training:   1%|          | 1109/200000 [2:13:24<397:19:43,  7.19s/it, loss=0.1026, lr=5.54e-06, step=1108]Training:   1%|          | 1109/200000 [2:13:24<397:19:43,  7.19s/it, loss=0.0756, lr=5.54e-06, step=1109]Training:   1%|          | 1110/200000 [2:13:32<397:04:30,  7.19s/it, loss=0.0756, lr=5.54e-06, step=1109]Training:   1%|          | 1110/200000 [2:13:32<397:04:30,  7.19s/it, loss=0.2019, lr=5.55e-06, step=1110]Training:   1%|          | 1111/200000 [2:13:39<397:29:08,  7.19s/it, loss=0.2019, lr=5.55e-06, step=1110]Training:   1%|          | 1111/200000 [2:13:39<397:29:08,  7.19s/it, loss=0.1084, lr=5.55e-06, step=1111]Training:   1%|          | 1112/200000 [2:13:46<396:46:58,  7.18s/it, loss=0.1084, lr=5.55e-06, step=1111]Training:   1%|          | 1112/200000 [2:13:46<396:46:58,  7.18s/it, loss=0.0953, lr=5.56e-06, step=1112]Training:   1%|          | 1113/200000 [2:13:53<397:01:32,  7.19s/it, loss=0.0953, lr=5.56e-06, step=1112]Training:   1%|          | 1113/200000 [2:13:53<397:01:32,  7.19s/it, loss=0.0779, lr=5.56e-06, step=1113]Training:   1%|          | 1114/200000 [2:14:00<397:02:35,  7.19s/it, loss=0.0779, lr=5.56e-06, step=1113]Training:   1%|          | 1114/200000 [2:14:00<397:02:35,  7.19s/it, loss=0.1195, lr=5.57e-06, step=1114]Training:   1%|          | 1115/200000 [2:14:08<396:57:02,  7.19s/it, loss=0.1195, lr=5.57e-06, step=1114]Training:   1%|          | 1115/200000 [2:14:08<396:57:02,  7.19s/it, loss=0.0759, lr=5.57e-06, step=1115]Training:   1%|          | 1116/200000 [2:14:15<397:09:33,  7.19s/it, loss=0.0759, lr=5.57e-06, step=1115]Training:   1%|          | 1116/200000 [2:14:15<397:09:33,  7.19s/it, loss=0.0694, lr=5.58e-06, step=1116]Training:   1%|          | 1117/200000 [2:14:22<396:47:32,  7.18s/it, loss=0.0694, lr=5.58e-06, step=1116]Training:   1%|          | 1117/200000 [2:14:22<396:47:32,  7.18s/it, loss=0.0968, lr=5.58e-06, step=1117]Training:   1%|          | 1118/200000 [2:14:29<397:09:22,  7.19s/it, loss=0.0968, lr=5.58e-06, step=1117]Training:   1%|          | 1118/200000 [2:14:29<397:09:22,  7.19s/it, loss=0.0884, lr=5.59e-06, step=1118]Training:   1%|          | 1119/200000 [2:14:36<399:04:48,  7.22s/it, loss=0.0884, lr=5.59e-06, step=1118]Training:   1%|          | 1119/200000 [2:14:36<399:04:48,  7.22s/it, loss=0.0803, lr=5.59e-06, step=1119]Training:   1%|          | 1120/200000 [2:14:44<398:30:39,  7.21s/it, loss=0.0803, lr=5.59e-06, step=1119]Training:   1%|          | 1120/200000 [2:14:44<398:30:39,  7.21s/it, loss=0.0833, lr=5.60e-06, step=1120]Training:   1%|          | 1121/200000 [2:14:51<397:47:18,  7.20s/it, loss=0.0833, lr=5.60e-06, step=1120]Training:   1%|          | 1121/200000 [2:14:51<397:47:18,  7.20s/it, loss=0.1686, lr=5.60e-06, step=1121]Training:   1%|          | 1122/200000 [2:14:58<397:38:00,  7.20s/it, loss=0.1686, lr=5.60e-06, step=1121]Training:   1%|          | 1122/200000 [2:14:58<397:38:00,  7.20s/it, loss=0.0849, lr=5.61e-06, step=1122]Training:   1%|          | 1123/200000 [2:15:05<397:12:02,  7.19s/it, loss=0.0849, lr=5.61e-06, step=1122]Training:   1%|          | 1123/200000 [2:15:05<397:12:02,  7.19s/it, loss=0.1366, lr=5.61e-06, step=1123]Training:   1%|          | 1124/200000 [2:15:12<397:11:28,  7.19s/it, loss=0.1366, lr=5.61e-06, step=1123]Training:   1%|          | 1124/200000 [2:15:12<397:11:28,  7.19s/it, loss=0.0957, lr=5.62e-06, step=1124]Training:   1%|          | 1125/200000 [2:15:20<397:32:56,  7.20s/it, loss=0.0957, lr=5.62e-06, step=1124]Training:   1%|          | 1125/200000 [2:15:20<397:32:56,  7.20s/it, loss=0.1276, lr=5.62e-06, step=1125]Training:   1%|          | 1126/200000 [2:15:27<397:29:15,  7.20s/it, loss=0.1276, lr=5.62e-06, step=1125]Training:   1%|          | 1126/200000 [2:15:27<397:29:15,  7.20s/it, loss=0.0714, lr=5.63e-06, step=1126]Training:   1%|          | 1127/200000 [2:15:34<397:09:23,  7.19s/it, loss=0.0714, lr=5.63e-06, step=1126]Training:   1%|          | 1127/200000 [2:15:34<397:09:23,  7.19s/it, loss=0.0886, lr=5.63e-06, step=1127]Training:   1%|          | 1128/200000 [2:15:41<397:27:57,  7.19s/it, loss=0.0886, lr=5.63e-06, step=1127]Training:   1%|          | 1128/200000 [2:15:41<397:27:57,  7.19s/it, loss=0.0826, lr=5.64e-06, step=1128]Training:   1%|          | 1129/200000 [2:15:48<397:01:37,  7.19s/it, loss=0.0826, lr=5.64e-06, step=1128]Training:   1%|          | 1129/200000 [2:15:48<397:01:37,  7.19s/it, loss=0.0880, lr=5.64e-06, step=1129]Training:   1%|          | 1130/200000 [2:15:56<397:10:35,  7.19s/it, loss=0.0880, lr=5.64e-06, step=1129]Training:   1%|          | 1130/200000 [2:15:56<397:10:35,  7.19s/it, loss=0.0751, lr=5.65e-06, step=1130]Training:   1%|          | 1131/200000 [2:16:03<397:13:28,  7.19s/it, loss=0.0751, lr=5.65e-06, step=1130]Training:   1%|          | 1131/200000 [2:16:03<397:13:28,  7.19s/it, loss=0.0741, lr=5.65e-06, step=1131]Training:   1%|          | 1132/200000 [2:16:10<397:20:51,  7.19s/it, loss=0.0741, lr=5.65e-06, step=1131]Training:   1%|          | 1132/200000 [2:16:10<397:20:51,  7.19s/it, loss=0.0900, lr=5.66e-06, step=1132]Training:   1%|          | 1133/200000 [2:16:17<396:49:49,  7.18s/it, loss=0.0900, lr=5.66e-06, step=1132]Training:   1%|          | 1133/200000 [2:16:17<396:49:49,  7.18s/it, loss=0.1459, lr=5.66e-06, step=1133]Training:   1%|          | 1134/200000 [2:16:24<396:42:41,  7.18s/it, loss=0.1459, lr=5.66e-06, step=1133]Training:   1%|          | 1134/200000 [2:16:24<396:42:41,  7.18s/it, loss=0.0762, lr=5.67e-06, step=1134]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1135/200000 [2:16:31<396:32:45,  7.18s/it, loss=0.0762, lr=5.67e-06, step=1134]Training:   1%|          | 1135/200000 [2:16:31<396:32:45,  7.18s/it, loss=0.1182, lr=5.67e-06, step=1135]Training:   1%|          | 1136/200000 [2:16:39<396:46:51,  7.18s/it, loss=0.1182, lr=5.67e-06, step=1135]Training:   1%|          | 1136/200000 [2:16:39<396:46:51,  7.18s/it, loss=0.1168, lr=5.68e-06, step=1136]Training:   1%|          | 1137/200000 [2:16:46<396:19:15,  7.17s/it, loss=0.1168, lr=5.68e-06, step=1136]Training:   1%|          | 1137/200000 [2:16:46<396:19:15,  7.17s/it, loss=0.1014, lr=5.68e-06, step=1137]Training:   1%|          | 1138/200000 [2:16:53<396:35:27,  7.18s/it, loss=0.1014, lr=5.68e-06, step=1137]Training:   1%|          | 1138/200000 [2:16:53<396:35:27,  7.18s/it, loss=0.0683, lr=5.69e-06, step=1138]Training:   1%|          | 1139/200000 [2:17:00<396:15:23,  7.17s/it, loss=0.0683, lr=5.69e-06, step=1138]Training:   1%|          | 1139/200000 [2:17:00<396:15:23,  7.17s/it, loss=0.0956, lr=5.69e-06, step=1139]Training:   1%|          | 1140/200000 [2:17:07<398:29:57,  7.21s/it, loss=0.0956, lr=5.69e-06, step=1139]Training:   1%|          | 1140/200000 [2:17:07<398:29:57,  7.21s/it, loss=0.0944, lr=5.70e-06, step=1140]Training:   1%|          | 1141/200000 [2:17:15<397:53:09,  7.20s/it, loss=0.0944, lr=5.70e-06, step=1140]Training:   1%|          | 1141/200000 [2:17:15<397:53:09,  7.20s/it, loss=0.1328, lr=5.70e-06, step=1141]Training:   1%|          | 1142/200000 [2:17:22<397:33:36,  7.20s/it, loss=0.1328, lr=5.70e-06, step=1141]Training:   1%|          | 1142/200000 [2:17:22<397:33:36,  7.20s/it, loss=0.0897, lr=5.71e-06, step=1142]Training:   1%|          | 1143/200000 [2:17:29<397:22:45,  7.19s/it, loss=0.0897, lr=5.71e-06, step=1142]Training:   1%|          | 1143/200000 [2:17:29<397:22:45,  7.19s/it, loss=0.1397, lr=5.71e-06, step=1143]Training:   1%|          | 1144/200000 [2:17:36<397:34:36,  7.20s/it, loss=0.1397, lr=5.71e-06, step=1143]Training:   1%|          | 1144/200000 [2:17:36<397:34:36,  7.20s/it, loss=0.0866, lr=5.72e-06, step=1144]Training:   1%|          | 1145/200000 [2:17:43<397:39:59,  7.20s/it, loss=0.0866, lr=5.72e-06, step=1144]Training:   1%|          | 1145/200000 [2:17:43<397:39:59,  7.20s/it, loss=0.1234, lr=5.72e-06, step=1145]Training:   1%|          | 1146/200000 [2:17:51<397:33:44,  7.20s/it, loss=0.1234, lr=5.72e-06, step=1145]Training:   1%|          | 1146/200000 [2:17:51<397:33:44,  7.20s/it, loss=0.0917, lr=5.73e-06, step=1146]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1147/200000 [2:17:58<397:34:46,  7.20s/it, loss=0.0917, lr=5.73e-06, step=1146]Training:   1%|          | 1147/200000 [2:17:58<397:34:46,  7.20s/it, loss=0.1038, lr=5.73e-06, step=1147]Training:   1%|          | 1148/200000 [2:18:05<397:06:59,  7.19s/it, loss=0.1038, lr=5.73e-06, step=1147]Training:   1%|          | 1148/200000 [2:18:05<397:06:59,  7.19s/it, loss=0.0948, lr=5.74e-06, step=1148]Training:   1%|          | 1149/200000 [2:18:12<397:35:46,  7.20s/it, loss=0.0948, lr=5.74e-06, step=1148]Training:   1%|          | 1149/200000 [2:18:12<397:35:46,  7.20s/it, loss=0.1389, lr=5.74e-06, step=1149]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1150/200000 [2:18:19<397:42:26,  7.20s/it, loss=0.1389, lr=5.74e-06, step=1149]Training:   1%|          | 1150/200000 [2:18:19<397:42:26,  7.20s/it, loss=0.0755, lr=5.75e-06, step=1150]Training:   1%|          | 1151/200000 [2:18:27<397:28:52,  7.20s/it, loss=0.0755, lr=5.75e-06, step=1150]Training:   1%|          | 1151/200000 [2:18:27<397:28:52,  7.20s/it, loss=0.0693, lr=5.75e-06, step=1151]Training:   1%|          | 1152/200000 [2:18:34<397:45:25,  7.20s/it, loss=0.0693, lr=5.75e-06, step=1151]Training:   1%|          | 1152/200000 [2:18:34<397:45:25,  7.20s/it, loss=0.0757, lr=5.76e-06, step=1152]Training:   1%|          | 1153/200000 [2:18:41<397:56:00,  7.20s/it, loss=0.0757, lr=5.76e-06, step=1152]Training:   1%|          | 1153/200000 [2:18:41<397:56:00,  7.20s/it, loss=0.1025, lr=5.76e-06, step=1153]Training:   1%|          | 1154/200000 [2:18:48<397:20:58,  7.19s/it, loss=0.1025, lr=5.76e-06, step=1153]Training:   1%|          | 1154/200000 [2:18:48<397:20:58,  7.19s/it, loss=0.1093, lr=5.77e-06, step=1154]Training:   1%|          | 1155/200000 [2:18:55<396:53:32,  7.19s/it, loss=0.1093, lr=5.77e-06, step=1154]Training:   1%|          | 1155/200000 [2:18:55<396:53:32,  7.19s/it, loss=0.0919, lr=5.77e-06, step=1155]Training:   1%|          | 1156/200000 [2:19:03<396:59:19,  7.19s/it, loss=0.0919, lr=5.77e-06, step=1155]Training:   1%|          | 1156/200000 [2:19:03<396:59:19,  7.19s/it, loss=0.0717, lr=5.78e-06, step=1156]Training:   1%|          | 1157/200000 [2:19:10<397:15:40,  7.19s/it, loss=0.0717, lr=5.78e-06, step=1156]Training:   1%|          | 1157/200000 [2:19:10<397:15:40,  7.19s/it, loss=0.1479, lr=5.78e-06, step=1157]Training:   1%|          | 1158/200000 [2:19:17<397:19:33,  7.19s/it, loss=0.1479, lr=5.78e-06, step=1157]Training:   1%|          | 1158/200000 [2:19:17<397:19:33,  7.19s/it, loss=0.0702, lr=5.79e-06, step=1158]Training:   1%|          | 1159/200000 [2:19:24<397:31:51,  7.20s/it, loss=0.0702, lr=5.79e-06, step=1158]Training:   1%|          | 1159/200000 [2:19:24<397:31:51,  7.20s/it, loss=0.0845, lr=5.79e-06, step=1159]Training:   1%|          | 1160/200000 [2:19:31<397:07:42,  7.19s/it, loss=0.0845, lr=5.79e-06, step=1159]Training:   1%|          | 1160/200000 [2:19:31<397:07:42,  7.19s/it, loss=0.2572, lr=5.80e-06, step=1160]Training:   1%|          | 1161/200000 [2:19:39<398:00:13,  7.21s/it, loss=0.2572, lr=5.80e-06, step=1160]Training:   1%|          | 1161/200000 [2:19:39<398:00:13,  7.21s/it, loss=0.0900, lr=5.80e-06, step=1161]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1162/200000 [2:19:46<397:33:47,  7.20s/it, loss=0.0900, lr=5.80e-06, step=1161]Training:   1%|          | 1162/200000 [2:19:46<397:33:47,  7.20s/it, loss=0.0772, lr=5.81e-06, step=1162]Training:   1%|          | 1163/200000 [2:19:53<396:58:40,  7.19s/it, loss=0.0772, lr=5.81e-06, step=1162]Training:   1%|          | 1163/200000 [2:19:53<396:58:40,  7.19s/it, loss=0.0911, lr=5.81e-06, step=1163]Training:   1%|          | 1164/200000 [2:20:00<396:50:12,  7.18s/it, loss=0.0911, lr=5.81e-06, step=1163]Training:   1%|          | 1164/200000 [2:20:00<396:50:12,  7.18s/it, loss=0.0795, lr=5.82e-06, step=1164]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1165/200000 [2:20:07<397:05:04,  7.19s/it, loss=0.0795, lr=5.82e-06, step=1164]Training:   1%|          | 1165/200000 [2:20:07<397:05:04,  7.19s/it, loss=0.1607, lr=5.82e-06, step=1165]Training:   1%|          | 1166/200000 [2:20:14<397:19:08,  7.19s/it, loss=0.1607, lr=5.82e-06, step=1165]Training:   1%|          | 1166/200000 [2:20:14<397:19:08,  7.19s/it, loss=0.0852, lr=5.83e-06, step=1166]Training:   1%|          | 1167/200000 [2:20:22<397:26:17,  7.20s/it, loss=0.0852, lr=5.83e-06, step=1166]Training:   1%|          | 1167/200000 [2:20:22<397:26:17,  7.20s/it, loss=0.0918, lr=5.83e-06, step=1167]Training:   1%|          | 1168/200000 [2:20:29<397:22:36,  7.19s/it, loss=0.0918, lr=5.83e-06, step=1167]Training:   1%|          | 1168/200000 [2:20:29<397:22:36,  7.19s/it, loss=0.0958, lr=5.84e-06, step=1168]Training:   1%|          | 1169/200000 [2:20:36<396:54:41,  7.19s/it, loss=0.0958, lr=5.84e-06, step=1168]Training:   1%|          | 1169/200000 [2:20:36<396:54:41,  7.19s/it, loss=0.1491, lr=5.84e-06, step=1169]Training:   1%|          | 1170/200000 [2:20:43<397:09:35,  7.19s/it, loss=0.1491, lr=5.84e-06, step=1169]Training:   1%|          | 1170/200000 [2:20:43<397:09:35,  7.19s/it, loss=0.1154, lr=5.85e-06, step=1170]Training:   1%|          | 1171/200000 [2:20:50<396:47:49,  7.18s/it, loss=0.1154, lr=5.85e-06, step=1170]Training:   1%|          | 1171/200000 [2:20:50<396:47:49,  7.18s/it, loss=0.0844, lr=5.85e-06, step=1171]Training:   1%|          | 1172/200000 [2:20:58<396:42:40,  7.18s/it, loss=0.0844, lr=5.85e-06, step=1171]Training:   1%|          | 1172/200000 [2:20:58<396:42:40,  7.18s/it, loss=0.0692, lr=5.86e-06, step=1172]Training:   1%|          | 1173/200000 [2:21:05<396:43:29,  7.18s/it, loss=0.0692, lr=5.86e-06, step=1172]Training:   1%|          | 1173/200000 [2:21:05<396:43:29,  7.18s/it, loss=0.0853, lr=5.86e-06, step=1173]Training:   1%|          | 1174/200000 [2:21:12<396:57:23,  7.19s/it, loss=0.0853, lr=5.86e-06, step=1173]Training:   1%|          | 1174/200000 [2:21:12<396:57:23,  7.19s/it, loss=0.0970, lr=5.87e-06, step=1174]Training:   1%|          | 1175/200000 [2:21:19<396:58:25,  7.19s/it, loss=0.0970, lr=5.87e-06, step=1174]Training:   1%|          | 1175/200000 [2:21:19<396:58:25,  7.19s/it, loss=0.1029, lr=5.87e-06, step=1175]Training:   1%|          | 1176/200000 [2:21:26<397:20:42,  7.19s/it, loss=0.1029, lr=5.87e-06, step=1175]Training:   1%|          | 1176/200000 [2:21:26<397:20:42,  7.19s/it, loss=0.0972, lr=5.88e-06, step=1176]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1177/200000 [2:21:34<396:53:35,  7.19s/it, loss=0.0972, lr=5.88e-06, step=1176]Training:   1%|          | 1177/200000 [2:21:34<396:53:35,  7.19s/it, loss=0.1026, lr=5.88e-06, step=1177]Training:   1%|          | 1178/200000 [2:21:41<397:12:24,  7.19s/it, loss=0.1026, lr=5.88e-06, step=1177]Training:   1%|          | 1178/200000 [2:21:41<397:12:24,  7.19s/it, loss=0.0925, lr=5.89e-06, step=1178]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1179/200000 [2:21:48<397:04:50,  7.19s/it, loss=0.0925, lr=5.89e-06, step=1178]Training:   1%|          | 1179/200000 [2:21:48<397:04:50,  7.19s/it, loss=0.0704, lr=5.89e-06, step=1179]Training:   1%|          | 1180/200000 [2:21:55<396:44:16,  7.18s/it, loss=0.0704, lr=5.89e-06, step=1179]Training:   1%|          | 1180/200000 [2:21:55<396:44:16,  7.18s/it, loss=0.0956, lr=5.90e-06, step=1180]Training:   1%|          | 1181/200000 [2:22:02<396:58:23,  7.19s/it, loss=0.0956, lr=5.90e-06, step=1180]Training:   1%|          | 1181/200000 [2:22:02<396:58:23,  7.19s/it, loss=0.1121, lr=5.90e-06, step=1181]Training:   1%|          | 1182/200000 [2:22:10<398:55:49,  7.22s/it, loss=0.1121, lr=5.90e-06, step=1181]Training:   1%|          | 1182/200000 [2:22:10<398:55:49,  7.22s/it, loss=0.0695, lr=5.91e-06, step=1182]Training:   1%|          | 1183/200000 [2:22:17<397:53:50,  7.20s/it, loss=0.0695, lr=5.91e-06, step=1182]Training:   1%|          | 1183/200000 [2:22:17<397:53:50,  7.20s/it, loss=0.1069, lr=5.91e-06, step=1183]Training:   1%|          | 1184/200000 [2:22:24<397:24:05,  7.20s/it, loss=0.1069, lr=5.91e-06, step=1183]Training:   1%|          | 1184/200000 [2:22:24<397:24:05,  7.20s/it, loss=0.0763, lr=5.92e-06, step=1184]Training:   1%|          | 1185/200000 [2:22:31<397:31:24,  7.20s/it, loss=0.0763, lr=5.92e-06, step=1184]Training:   1%|          | 1185/200000 [2:22:31<397:31:24,  7.20s/it, loss=0.1291, lr=5.92e-06, step=1185]Training:   1%|          | 1186/200000 [2:22:38<397:06:49,  7.19s/it, loss=0.1291, lr=5.92e-06, step=1185]Training:   1%|          | 1186/200000 [2:22:38<397:06:49,  7.19s/it, loss=0.1348, lr=5.93e-06, step=1186]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1187/200000 [2:22:45<396:54:44,  7.19s/it, loss=0.1348, lr=5.93e-06, step=1186]Training:   1%|          | 1187/200000 [2:22:45<396:54:44,  7.19s/it, loss=0.0741, lr=5.93e-06, step=1187]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1188/200000 [2:22:53<397:08:26,  7.19s/it, loss=0.0741, lr=5.93e-06, step=1187]Training:   1%|          | 1188/200000 [2:22:53<397:08:26,  7.19s/it, loss=0.0778, lr=5.94e-06, step=1188]Training:   1%|          | 1189/200000 [2:23:00<396:38:58,  7.18s/it, loss=0.0778, lr=5.94e-06, step=1188]Training:   1%|          | 1189/200000 [2:23:00<396:38:58,  7.18s/it, loss=0.0717, lr=5.94e-06, step=1189]Training:   1%|          | 1190/200000 [2:23:07<396:58:38,  7.19s/it, loss=0.0717, lr=5.94e-06, step=1189]Training:   1%|          | 1190/200000 [2:23:07<396:58:38,  7.19s/it, loss=0.1384, lr=5.95e-06, step=1190]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1191/200000 [2:23:14<397:01:22,  7.19s/it, loss=0.1384, lr=5.95e-06, step=1190]Training:   1%|          | 1191/200000 [2:23:14<397:01:22,  7.19s/it, loss=0.1306, lr=5.95e-06, step=1191]Training:   1%|          | 1192/200000 [2:23:21<396:40:46,  7.18s/it, loss=0.1306, lr=5.95e-06, step=1191]Training:   1%|          | 1192/200000 [2:23:21<396:40:46,  7.18s/it, loss=0.0947, lr=5.96e-06, step=1192]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1193/200000 [2:23:29<396:30:19,  7.18s/it, loss=0.0947, lr=5.96e-06, step=1192]Training:   1%|          | 1193/200000 [2:23:29<396:30:19,  7.18s/it, loss=0.1161, lr=5.96e-06, step=1193]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1194/200000 [2:23:36<396:46:22,  7.18s/it, loss=0.1161, lr=5.96e-06, step=1193]Training:   1%|          | 1194/200000 [2:23:36<396:46:22,  7.18s/it, loss=0.0792, lr=5.97e-06, step=1194]Training:   1%|          | 1195/200000 [2:23:43<397:26:15,  7.20s/it, loss=0.0792, lr=5.97e-06, step=1194]Training:   1%|          | 1195/200000 [2:23:43<397:26:15,  7.20s/it, loss=0.1211, lr=5.97e-06, step=1195]Training:   1%|          | 1196/200000 [2:23:50<397:18:14,  7.19s/it, loss=0.1211, lr=5.97e-06, step=1195]Training:   1%|          | 1196/200000 [2:23:50<397:18:14,  7.19s/it, loss=0.1480, lr=5.98e-06, step=1196]Training:   1%|          | 1197/200000 [2:23:57<397:30:03,  7.20s/it, loss=0.1480, lr=5.98e-06, step=1196]Training:   1%|          | 1197/200000 [2:23:57<397:30:03,  7.20s/it, loss=0.0637, lr=5.98e-06, step=1197]Training:   1%|          | 1198/200000 [2:24:05<397:13:44,  7.19s/it, loss=0.0637, lr=5.98e-06, step=1197]Training:   1%|          | 1198/200000 [2:24:05<397:13:44,  7.19s/it, loss=0.0872, lr=5.99e-06, step=1198]Training:   1%|          | 1199/200000 [2:24:12<397:28:33,  7.20s/it, loss=0.0872, lr=5.99e-06, step=1198]Training:   1%|          | 1199/200000 [2:24:12<397:28:33,  7.20s/it, loss=0.0918, lr=5.99e-06, step=1199]Training:   1%|          | 1200/200000 [2:24:19<397:27:51,  7.20s/it, loss=0.0918, lr=5.99e-06, step=1199]Training:   1%|          | 1200/200000 [2:24:19<397:27:51,  7.20s/it, loss=0.0847, lr=6.00e-06, step=1200]22:20:54.939 [I] step=1200 loss=0.1004 lr=5.76e-06 grad_norm=0.79 time=719.2s                     (486094:train_pytorch.py:582)
Training:   1%|          | 1201/200000 [2:24:26<397:13:44,  7.19s/it, loss=0.0847, lr=6.00e-06, step=1200]Training:   1%|          | 1201/200000 [2:24:26<397:13:44,  7.19s/it, loss=0.1125, lr=6.00e-06, step=1201]Training:   1%|          | 1202/200000 [2:24:33<396:58:35,  7.19s/it, loss=0.1125, lr=6.00e-06, step=1201]Training:   1%|          | 1202/200000 [2:24:33<396:58:35,  7.19s/it, loss=0.0964, lr=6.01e-06, step=1202]Training:   1%|          | 1203/200000 [2:24:41<398:29:50,  7.22s/it, loss=0.0964, lr=6.01e-06, step=1202]Training:   1%|          | 1203/200000 [2:24:41<398:29:50,  7.22s/it, loss=0.0686, lr=6.01e-06, step=1203]Training:   1%|          | 1204/200000 [2:24:48<398:24:11,  7.21s/it, loss=0.0686, lr=6.01e-06, step=1203]Training:   1%|          | 1204/200000 [2:24:48<398:24:11,  7.21s/it, loss=0.0651, lr=6.02e-06, step=1204]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1205/200000 [2:24:55<397:41:57,  7.20s/it, loss=0.0651, lr=6.02e-06, step=1204]Training:   1%|          | 1205/200000 [2:24:55<397:41:57,  7.20s/it, loss=0.0893, lr=6.02e-06, step=1205]Training:   1%|          | 1206/200000 [2:25:02<396:55:34,  7.19s/it, loss=0.0893, lr=6.02e-06, step=1205]Training:   1%|          | 1206/200000 [2:25:02<396:55:34,  7.19s/it, loss=0.1044, lr=6.03e-06, step=1206]Training:   1%|          | 1207/200000 [2:25:09<396:52:35,  7.19s/it, loss=0.1044, lr=6.03e-06, step=1206]Training:   1%|          | 1207/200000 [2:25:09<396:52:35,  7.19s/it, loss=0.1123, lr=6.03e-06, step=1207]Training:   1%|          | 1208/200000 [2:25:17<397:15:39,  7.19s/it, loss=0.1123, lr=6.03e-06, step=1207]Training:   1%|          | 1208/200000 [2:25:17<397:15:39,  7.19s/it, loss=0.0800, lr=6.04e-06, step=1208]Training:   1%|          | 1209/200000 [2:25:24<397:26:59,  7.20s/it, loss=0.0800, lr=6.04e-06, step=1208]Training:   1%|          | 1209/200000 [2:25:24<397:26:59,  7.20s/it, loss=0.0685, lr=6.04e-06, step=1209]Training:   1%|          | 1210/200000 [2:25:31<396:54:40,  7.19s/it, loss=0.0685, lr=6.04e-06, step=1209]Training:   1%|          | 1210/200000 [2:25:31<396:54:40,  7.19s/it, loss=0.0702, lr=6.05e-06, step=1210]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1211/200000 [2:25:38<397:01:11,  7.19s/it, loss=0.0702, lr=6.05e-06, step=1210]Training:   1%|          | 1211/200000 [2:25:38<397:01:11,  7.19s/it, loss=0.0855, lr=6.05e-06, step=1211]Training:   1%|          | 1212/200000 [2:25:45<397:12:10,  7.19s/it, loss=0.0855, lr=6.05e-06, step=1211]Training:   1%|          | 1212/200000 [2:25:45<397:12:10,  7.19s/it, loss=0.0892, lr=6.06e-06, step=1212]Training:   1%|          | 1213/200000 [2:25:52<396:34:28,  7.18s/it, loss=0.0892, lr=6.06e-06, step=1212]Training:   1%|          | 1213/200000 [2:25:52<396:34:28,  7.18s/it, loss=0.0813, lr=6.06e-06, step=1213]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1214/200000 [2:26:00<396:45:06,  7.19s/it, loss=0.0813, lr=6.06e-06, step=1213]Training:   1%|          | 1214/200000 [2:26:00<396:45:06,  7.19s/it, loss=0.1301, lr=6.07e-06, step=1214]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1215/200000 [2:26:07<396:54:49,  7.19s/it, loss=0.1301, lr=6.07e-06, step=1214]Training:   1%|          | 1215/200000 [2:26:07<396:54:49,  7.19s/it, loss=0.0866, lr=6.07e-06, step=1215]Training:   1%|          | 1216/200000 [2:26:14<396:46:30,  7.19s/it, loss=0.0866, lr=6.07e-06, step=1215]Training:   1%|          | 1216/200000 [2:26:14<396:46:30,  7.19s/it, loss=0.1590, lr=6.08e-06, step=1216]Training:   1%|          | 1217/200000 [2:26:21<396:42:09,  7.18s/it, loss=0.1590, lr=6.08e-06, step=1216]Training:   1%|          | 1217/200000 [2:26:21<396:42:09,  7.18s/it, loss=0.0833, lr=6.08e-06, step=1217]Training:   1%|          | 1218/200000 [2:26:28<396:52:56,  7.19s/it, loss=0.0833, lr=6.08e-06, step=1217]Training:   1%|          | 1218/200000 [2:26:28<396:52:56,  7.19s/it, loss=0.0985, lr=6.09e-06, step=1218]Training:   1%|          | 1219/200000 [2:26:36<397:02:39,  7.19s/it, loss=0.0985, lr=6.09e-06, step=1218]Training:   1%|          | 1219/200000 [2:26:36<397:02:39,  7.19s/it, loss=0.0903, lr=6.09e-06, step=1219]Training:   1%|          | 1220/200000 [2:26:43<396:35:26,  7.18s/it, loss=0.0903, lr=6.09e-06, step=1219]Training:   1%|          | 1220/200000 [2:26:43<396:35:26,  7.18s/it, loss=0.0865, lr=6.10e-06, step=1220]Training:   1%|          | 1221/200000 [2:26:50<397:03:14,  7.19s/it, loss=0.0865, lr=6.10e-06, step=1220]Training:   1%|          | 1221/200000 [2:26:50<397:03:14,  7.19s/it, loss=0.0844, lr=6.10e-06, step=1221]Training:   1%|          | 1222/200000 [2:26:57<397:14:13,  7.19s/it, loss=0.0844, lr=6.10e-06, step=1221]Training:   1%|          | 1222/200000 [2:26:57<397:14:13,  7.19s/it, loss=0.1680, lr=6.11e-06, step=1222]Training:   1%|          | 1223/200000 [2:27:04<397:19:47,  7.20s/it, loss=0.1680, lr=6.11e-06, step=1222]Training:   1%|          | 1223/200000 [2:27:04<397:19:47,  7.20s/it, loss=0.0677, lr=6.11e-06, step=1223]Training:   1%|          | 1224/200000 [2:27:12<397:51:27,  7.21s/it, loss=0.0677, lr=6.11e-06, step=1223]Training:   1%|          | 1224/200000 [2:27:12<397:51:27,  7.21s/it, loss=0.1538, lr=6.12e-06, step=1224]Training:   1%|          | 1225/200000 [2:27:19<397:40:26,  7.20s/it, loss=0.1538, lr=6.12e-06, step=1224]Training:   1%|          | 1225/200000 [2:27:19<397:40:26,  7.20s/it, loss=0.0938, lr=6.12e-06, step=1225]Training:   1%|          | 1226/200000 [2:27:26<397:20:38,  7.20s/it, loss=0.0938, lr=6.12e-06, step=1225]Training:   1%|          | 1226/200000 [2:27:26<397:20:38,  7.20s/it, loss=0.0876, lr=6.13e-06, step=1226]Training:   1%|          | 1227/200000 [2:27:33<396:52:15,  7.19s/it, loss=0.0876, lr=6.13e-06, step=1226]Training:   1%|          | 1227/200000 [2:27:33<396:52:15,  7.19s/it, loss=0.0819, lr=6.13e-06, step=1227]Training:   1%|          | 1228/200000 [2:27:40<396:51:55,  7.19s/it, loss=0.0819, lr=6.13e-06, step=1227]Training:   1%|          | 1228/200000 [2:27:40<396:51:55,  7.19s/it, loss=0.0635, lr=6.14e-06, step=1228]Training:   1%|          | 1229/200000 [2:27:48<396:34:58,  7.18s/it, loss=0.0635, lr=6.14e-06, step=1228]Training:   1%|          | 1229/200000 [2:27:48<396:34:58,  7.18s/it, loss=0.0977, lr=6.14e-06, step=1229]Training:   1%|          | 1230/200000 [2:27:55<396:28:54,  7.18s/it, loss=0.0977, lr=6.14e-06, step=1229]Training:   1%|          | 1230/200000 [2:27:55<396:28:54,  7.18s/it, loss=0.0893, lr=6.15e-06, step=1230]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1231/200000 [2:28:02<396:44:51,  7.19s/it, loss=0.0893, lr=6.15e-06, step=1230]Training:   1%|          | 1231/200000 [2:28:02<396:44:51,  7.19s/it, loss=0.0993, lr=6.15e-06, step=1231]Training:   1%|          | 1232/200000 [2:28:09<396:23:02,  7.18s/it, loss=0.0993, lr=6.15e-06, step=1231]Training:   1%|          | 1232/200000 [2:28:09<396:23:02,  7.18s/it, loss=0.0913, lr=6.16e-06, step=1232]Training:   1%|          | 1233/200000 [2:28:16<396:59:22,  7.19s/it, loss=0.0913, lr=6.16e-06, step=1232]Training:   1%|          | 1233/200000 [2:28:16<396:59:22,  7.19s/it, loss=0.1065, lr=6.16e-06, step=1233]Training:   1%|          | 1234/200000 [2:28:23<397:06:47,  7.19s/it, loss=0.1065, lr=6.16e-06, step=1233]Training:   1%|          | 1234/200000 [2:28:23<397:06:47,  7.19s/it, loss=0.1013, lr=6.17e-06, step=1234]Training:   1%|          | 1235/200000 [2:28:31<396:32:57,  7.18s/it, loss=0.1013, lr=6.17e-06, step=1234]Training:   1%|          | 1235/200000 [2:28:31<396:32:57,  7.18s/it, loss=0.1081, lr=6.17e-06, step=1235]Training:   1%|          | 1236/200000 [2:28:38<396:55:08,  7.19s/it, loss=0.1081, lr=6.17e-06, step=1235]Training:   1%|          | 1236/200000 [2:28:38<396:55:08,  7.19s/it, loss=0.0770, lr=6.18e-06, step=1236]Training:   1%|          | 1237/200000 [2:28:45<396:42:10,  7.19s/it, loss=0.0770, lr=6.18e-06, step=1236]Training:   1%|          | 1237/200000 [2:28:45<396:42:10,  7.19s/it, loss=0.1087, lr=6.18e-06, step=1237]Training:   1%|          | 1238/200000 [2:28:52<396:38:38,  7.18s/it, loss=0.1087, lr=6.18e-06, step=1237]Training:   1%|          | 1238/200000 [2:28:52<396:38:38,  7.18s/it, loss=0.1931, lr=6.19e-06, step=1238]Training:   1%|          | 1239/200000 [2:28:59<396:52:01,  7.19s/it, loss=0.1931, lr=6.19e-06, step=1238]Training:   1%|          | 1239/200000 [2:28:59<396:52:01,  7.19s/it, loss=0.1052, lr=6.19e-06, step=1239]Training:   1%|          | 1240/200000 [2:29:07<396:32:14,  7.18s/it, loss=0.1052, lr=6.19e-06, step=1239]Training:   1%|          | 1240/200000 [2:29:07<396:32:14,  7.18s/it, loss=0.0710, lr=6.20e-06, step=1240]Training:   1%|          | 1241/200000 [2:29:14<396:21:15,  7.18s/it, loss=0.0710, lr=6.20e-06, step=1240]Training:   1%|          | 1241/200000 [2:29:14<396:21:15,  7.18s/it, loss=0.1135, lr=6.20e-06, step=1241]Training:   1%|          | 1242/200000 [2:29:21<396:21:30,  7.18s/it, loss=0.1135, lr=6.20e-06, step=1241]Training:   1%|          | 1242/200000 [2:29:21<396:21:30,  7.18s/it, loss=0.1247, lr=6.21e-06, step=1242]Training:   1%|          | 1243/200000 [2:29:28<396:47:11,  7.19s/it, loss=0.1247, lr=6.21e-06, step=1242]Training:   1%|          | 1243/200000 [2:29:28<396:47:11,  7.19s/it, loss=0.0932, lr=6.21e-06, step=1243]Training:   1%|          | 1244/200000 [2:29:35<396:41:18,  7.19s/it, loss=0.0932, lr=6.21e-06, step=1243]Training:   1%|          | 1244/200000 [2:29:35<396:41:18,  7.19s/it, loss=0.0760, lr=6.22e-06, step=1244]Training:   1%|          | 1245/200000 [2:29:43<399:22:03,  7.23s/it, loss=0.0760, lr=6.22e-06, step=1244]Training:   1%|          | 1245/200000 [2:29:43<399:22:03,  7.23s/it, loss=0.0680, lr=6.22e-06, step=1245]Training:   1%|          | 1246/200000 [2:29:50<398:50:55,  7.22s/it, loss=0.0680, lr=6.22e-06, step=1245]Training:   1%|          | 1246/200000 [2:29:50<398:50:55,  7.22s/it, loss=0.0671, lr=6.23e-06, step=1246]Training:   1%|          | 1247/200000 [2:29:57<398:00:13,  7.21s/it, loss=0.0671, lr=6.23e-06, step=1246]Training:   1%|          | 1247/200000 [2:29:57<398:00:13,  7.21s/it, loss=0.1106, lr=6.23e-06, step=1247]Training:   1%|          | 1248/200000 [2:30:04<397:27:19,  7.20s/it, loss=0.1106, lr=6.23e-06, step=1247]Training:   1%|          | 1248/200000 [2:30:04<397:27:19,  7.20s/it, loss=0.1099, lr=6.24e-06, step=1248]Training:   1%|          | 1249/200000 [2:30:11<397:05:15,  7.19s/it, loss=0.1099, lr=6.24e-06, step=1248]Training:   1%|          | 1249/200000 [2:30:11<397:05:15,  7.19s/it, loss=0.0673, lr=6.24e-06, step=1249]Training:   1%|          | 1250/200000 [2:30:19<396:47:57,  7.19s/it, loss=0.0673, lr=6.24e-06, step=1249]Training:   1%|          | 1250/200000 [2:30:19<396:47:57,  7.19s/it, loss=0.0908, lr=6.25e-06, step=1250]Training:   1%|          | 1251/200000 [2:30:26<397:09:25,  7.19s/it, loss=0.0908, lr=6.25e-06, step=1250]Training:   1%|          | 1251/200000 [2:30:26<397:09:25,  7.19s/it, loss=0.1042, lr=6.25e-06, step=1251]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1252/200000 [2:30:33<396:59:14,  7.19s/it, loss=0.1042, lr=6.25e-06, step=1251]Training:   1%|          | 1252/200000 [2:30:33<396:59:14,  7.19s/it, loss=0.0859, lr=6.26e-06, step=1252]Training:   1%|          | 1253/200000 [2:30:40<396:58:27,  7.19s/it, loss=0.0859, lr=6.26e-06, step=1252]Training:   1%|          | 1253/200000 [2:30:40<396:58:27,  7.19s/it, loss=0.0890, lr=6.26e-06, step=1253]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1254/200000 [2:30:47<396:37:20,  7.18s/it, loss=0.0890, lr=6.26e-06, step=1253]Training:   1%|          | 1254/200000 [2:30:47<396:37:20,  7.18s/it, loss=0.0858, lr=6.27e-06, step=1254]Training:   1%|          | 1255/200000 [2:30:55<397:15:21,  7.20s/it, loss=0.0858, lr=6.27e-06, step=1254]Training:   1%|          | 1255/200000 [2:30:55<397:15:21,  7.20s/it, loss=0.0840, lr=6.27e-06, step=1255]Training:   1%|          | 1256/200000 [2:31:02<397:11:13,  7.19s/it, loss=0.0840, lr=6.27e-06, step=1255]Training:   1%|          | 1256/200000 [2:31:02<397:11:13,  7.19s/it, loss=0.0798, lr=6.28e-06, step=1256]Training:   1%|          | 1257/200000 [2:31:09<397:13:42,  7.20s/it, loss=0.0798, lr=6.28e-06, step=1256]Training:   1%|          | 1257/200000 [2:31:09<397:13:42,  7.20s/it, loss=0.0562, lr=6.28e-06, step=1257]Training:   1%|          | 1258/200000 [2:31:16<396:48:03,  7.19s/it, loss=0.0562, lr=6.28e-06, step=1257]Training:   1%|          | 1258/200000 [2:31:16<396:48:03,  7.19s/it, loss=0.0777, lr=6.29e-06, step=1258]Training:   1%|          | 1259/200000 [2:31:23<396:40:01,  7.19s/it, loss=0.0777, lr=6.29e-06, step=1258]Training:   1%|          | 1259/200000 [2:31:23<396:40:01,  7.19s/it, loss=0.1026, lr=6.29e-06, step=1259]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1260/200000 [2:31:30<397:07:28,  7.19s/it, loss=0.1026, lr=6.29e-06, step=1259]Training:   1%|          | 1260/200000 [2:31:30<397:07:28,  7.19s/it, loss=0.0806, lr=6.30e-06, step=1260]Training:   1%|          | 1261/200000 [2:31:38<396:59:10,  7.19s/it, loss=0.0806, lr=6.30e-06, step=1260]Training:   1%|          | 1261/200000 [2:31:38<396:59:10,  7.19s/it, loss=0.1443, lr=6.30e-06, step=1261]Training:   1%|          | 1262/200000 [2:31:45<396:50:20,  7.19s/it, loss=0.1443, lr=6.30e-06, step=1261]Training:   1%|          | 1262/200000 [2:31:45<396:50:20,  7.19s/it, loss=0.0895, lr=6.31e-06, step=1262]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1263/200000 [2:31:52<396:37:00,  7.18s/it, loss=0.0895, lr=6.31e-06, step=1262]Training:   1%|          | 1263/200000 [2:31:52<396:37:00,  7.18s/it, loss=0.1588, lr=6.31e-06, step=1263]Training:   1%|          | 1264/200000 [2:31:59<396:17:32,  7.18s/it, loss=0.1588, lr=6.31e-06, step=1263]Training:   1%|          | 1264/200000 [2:31:59<396:17:32,  7.18s/it, loss=0.0728, lr=6.32e-06, step=1264]Training:   1%|          | 1265/200000 [2:32:06<396:31:50,  7.18s/it, loss=0.0728, lr=6.32e-06, step=1264]Training:   1%|          | 1265/200000 [2:32:06<396:31:50,  7.18s/it, loss=0.0875, lr=6.32e-06, step=1265]Training:   1%|          | 1266/200000 [2:32:14<398:29:54,  7.22s/it, loss=0.0875, lr=6.32e-06, step=1265]Training:   1%|          | 1266/200000 [2:32:14<398:29:54,  7.22s/it, loss=0.1241, lr=6.33e-06, step=1266]Training:   1%|          | 1267/200000 [2:32:21<398:19:29,  7.22s/it, loss=0.1241, lr=6.33e-06, step=1266]Training:   1%|          | 1267/200000 [2:32:21<398:19:29,  7.22s/it, loss=0.1035, lr=6.33e-06, step=1267]Training:   1%|          | 1268/200000 [2:32:28<396:46:30,  7.19s/it, loss=0.1035, lr=6.33e-06, step=1267]Training:   1%|          | 1268/200000 [2:32:28<396:46:30,  7.19s/it, loss=0.0960, lr=6.34e-06, step=1268]Training:   1%|          | 1269/200000 [2:32:35<396:34:20,  7.18s/it, loss=0.0960, lr=6.34e-06, step=1268]Training:   1%|          | 1269/200000 [2:32:35<396:34:20,  7.18s/it, loss=0.2161, lr=6.34e-06, step=1269]Training:   1%|          | 1270/200000 [2:32:42<396:54:51,  7.19s/it, loss=0.2161, lr=6.34e-06, step=1269]Training:   1%|          | 1270/200000 [2:32:42<396:54:51,  7.19s/it, loss=0.0954, lr=6.35e-06, step=1270]Training:   1%|          | 1271/200000 [2:32:50<396:40:38,  7.19s/it, loss=0.0954, lr=6.35e-06, step=1270]Training:   1%|          | 1271/200000 [2:32:50<396:40:38,  7.19s/it, loss=0.1790, lr=6.35e-06, step=1271]Training:   1%|          | 1272/200000 [2:32:57<396:48:39,  7.19s/it, loss=0.1790, lr=6.35e-06, step=1271]Training:   1%|          | 1272/200000 [2:32:57<396:48:39,  7.19s/it, loss=0.0831, lr=6.36e-06, step=1272]Training:   1%|          | 1273/200000 [2:33:04<396:56:59,  7.19s/it, loss=0.0831, lr=6.36e-06, step=1272]Training:   1%|          | 1273/200000 [2:33:04<396:56:59,  7.19s/it, loss=0.1090, lr=6.36e-06, step=1273]Training:   1%|          | 1274/200000 [2:33:11<397:28:27,  7.20s/it, loss=0.1090, lr=6.36e-06, step=1273]Training:   1%|          | 1274/200000 [2:33:11<397:28:27,  7.20s/it, loss=0.0720, lr=6.37e-06, step=1274]Training:   1%|          | 1275/200000 [2:33:18<396:50:40,  7.19s/it, loss=0.0720, lr=6.37e-06, step=1274]Training:   1%|          | 1275/200000 [2:33:18<396:50:40,  7.19s/it, loss=0.0893, lr=6.37e-06, step=1275]Training:   1%|          | 1276/200000 [2:33:25<396:23:37,  7.18s/it, loss=0.0893, lr=6.37e-06, step=1275]Training:   1%|          | 1276/200000 [2:33:25<396:23:37,  7.18s/it, loss=0.0794, lr=6.38e-06, step=1276]Training:   1%|          | 1277/200000 [2:33:33<396:35:38,  7.18s/it, loss=0.0794, lr=6.38e-06, step=1276]Training:   1%|          | 1277/200000 [2:33:33<396:35:38,  7.18s/it, loss=0.0853, lr=6.38e-06, step=1277]Training:   1%|          | 1278/200000 [2:33:40<396:16:11,  7.18s/it, loss=0.0853, lr=6.38e-06, step=1277]Training:   1%|          | 1278/200000 [2:33:40<396:16:11,  7.18s/it, loss=0.0876, lr=6.39e-06, step=1278]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1279/200000 [2:33:47<396:03:47,  7.18s/it, loss=0.0876, lr=6.39e-06, step=1278]Training:   1%|          | 1279/200000 [2:33:47<396:03:47,  7.18s/it, loss=0.2081, lr=6.39e-06, step=1279]Training:   1%|          | 1280/200000 [2:33:54<396:29:22,  7.18s/it, loss=0.2081, lr=6.39e-06, step=1279]Training:   1%|          | 1280/200000 [2:33:54<396:29:22,  7.18s/it, loss=0.0896, lr=6.40e-06, step=1280]Training:   1%|          | 1281/200000 [2:34:01<396:12:34,  7.18s/it, loss=0.0896, lr=6.40e-06, step=1280]Training:   1%|          | 1281/200000 [2:34:01<396:12:34,  7.18s/it, loss=0.0926, lr=6.40e-06, step=1281]Training:   1%|          | 1282/200000 [2:34:09<396:35:37,  7.18s/it, loss=0.0926, lr=6.40e-06, step=1281]Training:   1%|          | 1282/200000 [2:34:09<396:35:37,  7.18s/it, loss=0.0649, lr=6.41e-06, step=1282]Training:   1%|          | 1283/200000 [2:34:16<397:19:38,  7.20s/it, loss=0.0649, lr=6.41e-06, step=1282]Training:   1%|          | 1283/200000 [2:34:16<397:19:38,  7.20s/it, loss=0.0850, lr=6.41e-06, step=1283]Training:   1%|          | 1284/200000 [2:34:23<397:09:49,  7.20s/it, loss=0.0850, lr=6.41e-06, step=1283]Training:   1%|          | 1284/200000 [2:34:23<397:09:49,  7.20s/it, loss=0.1108, lr=6.42e-06, step=1284]Training:   1%|          | 1285/200000 [2:34:30<396:45:57,  7.19s/it, loss=0.1108, lr=6.42e-06, step=1284]Training:   1%|          | 1285/200000 [2:34:30<396:45:57,  7.19s/it, loss=0.0759, lr=6.42e-06, step=1285]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1286/200000 [2:34:37<397:13:43,  7.20s/it, loss=0.0759, lr=6.42e-06, step=1285]Training:   1%|          | 1286/200000 [2:34:37<397:13:43,  7.20s/it, loss=0.1006, lr=6.43e-06, step=1286]Training:   1%|          | 1287/200000 [2:34:45<398:23:39,  7.22s/it, loss=0.1006, lr=6.43e-06, step=1286]Training:   1%|          | 1287/200000 [2:34:45<398:23:39,  7.22s/it, loss=0.1295, lr=6.43e-06, step=1287]Training:   1%|          | 1288/200000 [2:34:52<398:02:59,  7.21s/it, loss=0.1295, lr=6.43e-06, step=1287]Training:   1%|          | 1288/200000 [2:34:52<398:02:59,  7.21s/it, loss=0.0893, lr=6.44e-06, step=1288]Training:   1%|          | 1289/200000 [2:34:59<397:34:03,  7.20s/it, loss=0.0893, lr=6.44e-06, step=1288]Training:   1%|          | 1289/200000 [2:34:59<397:34:03,  7.20s/it, loss=0.0832, lr=6.44e-06, step=1289]Training:   1%|          | 1290/200000 [2:35:06<397:24:34,  7.20s/it, loss=0.0832, lr=6.44e-06, step=1289]Training:   1%|          | 1290/200000 [2:35:06<397:24:34,  7.20s/it, loss=0.1785, lr=6.45e-06, step=1290]Training:   1%|          | 1291/200000 [2:35:13<396:54:52,  7.19s/it, loss=0.1785, lr=6.45e-06, step=1290]Training:   1%|          | 1291/200000 [2:35:13<396:54:52,  7.19s/it, loss=0.0710, lr=6.45e-06, step=1291]Training:   1%|          | 1292/200000 [2:35:21<396:34:20,  7.18s/it, loss=0.0710, lr=6.45e-06, step=1291]Training:   1%|          | 1292/200000 [2:35:21<396:34:20,  7.18s/it, loss=0.0738, lr=6.46e-06, step=1292]Training:   1%|          | 1293/200000 [2:35:28<396:51:14,  7.19s/it, loss=0.0738, lr=6.46e-06, step=1292]Training:   1%|          | 1293/200000 [2:35:28<396:51:14,  7.19s/it, loss=0.0866, lr=6.46e-06, step=1293]Training:   1%|          | 1294/200000 [2:35:35<396:33:10,  7.18s/it, loss=0.0866, lr=6.46e-06, step=1293]Training:   1%|          | 1294/200000 [2:35:35<396:33:10,  7.18s/it, loss=0.0964, lr=6.47e-06, step=1294]Training:   1%|          | 1295/200000 [2:35:42<396:26:59,  7.18s/it, loss=0.0964, lr=6.47e-06, step=1294]Training:   1%|          | 1295/200000 [2:35:42<396:26:59,  7.18s/it, loss=0.0964, lr=6.47e-06, step=1295]Training:   1%|          | 1296/200000 [2:35:49<396:13:56,  7.18s/it, loss=0.0964, lr=6.47e-06, step=1295]Training:   1%|          | 1296/200000 [2:35:49<396:13:56,  7.18s/it, loss=0.0960, lr=6.48e-06, step=1296]Training:   1%|          | 1297/200000 [2:35:56<396:26:28,  7.18s/it, loss=0.0960, lr=6.48e-06, step=1296]Training:   1%|          | 1297/200000 [2:35:56<396:26:28,  7.18s/it, loss=0.0831, lr=6.48e-06, step=1297]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1298/200000 [2:36:04<396:16:59,  7.18s/it, loss=0.0831, lr=6.48e-06, step=1297]Training:   1%|          | 1298/200000 [2:36:04<396:16:59,  7.18s/it, loss=0.0828, lr=6.49e-06, step=1298]Training:   1%|          | 1299/200000 [2:36:11<396:02:58,  7.18s/it, loss=0.0828, lr=6.49e-06, step=1298]Training:   1%|          | 1299/200000 [2:36:11<396:02:58,  7.18s/it, loss=0.0982, lr=6.49e-06, step=1299]Training:   1%|          | 1300/200000 [2:36:18<395:46:11,  7.17s/it, loss=0.0982, lr=6.49e-06, step=1299]Training:   1%|          | 1300/200000 [2:36:18<395:46:11,  7.17s/it, loss=0.0871, lr=6.50e-06, step=1300]22:32:53.966 [I] step=1300 loss=0.0979 lr=6.26e-06 grad_norm=0.83 time=719.0s                     (486094:train_pytorch.py:582)
Training:   1%|          | 1301/200000 [2:36:25<396:11:50,  7.18s/it, loss=0.0871, lr=6.50e-06, step=1300]Training:   1%|          | 1301/200000 [2:36:25<396:11:50,  7.18s/it, loss=0.0751, lr=6.50e-06, step=1301]Training:   1%|          | 1302/200000 [2:36:32<396:14:54,  7.18s/it, loss=0.0751, lr=6.50e-06, step=1301]Training:   1%|          | 1302/200000 [2:36:32<396:14:54,  7.18s/it, loss=0.1318, lr=6.51e-06, step=1302]Training:   1%|          | 1303/200000 [2:36:40<396:22:45,  7.18s/it, loss=0.1318, lr=6.51e-06, step=1302]Training:   1%|          | 1303/200000 [2:36:40<396:22:45,  7.18s/it, loss=0.0856, lr=6.51e-06, step=1303]Training:   1%|          | 1304/200000 [2:36:47<396:21:23,  7.18s/it, loss=0.0856, lr=6.51e-06, step=1303]Training:   1%|          | 1304/200000 [2:36:47<396:21:23,  7.18s/it, loss=0.0896, lr=6.52e-06, step=1304]Training:   1%|          | 1305/200000 [2:36:54<396:34:31,  7.19s/it, loss=0.0896, lr=6.52e-06, step=1304]Training:   1%|          | 1305/200000 [2:36:54<396:34:31,  7.19s/it, loss=0.0635, lr=6.52e-06, step=1305]Training:   1%|          | 1306/200000 [2:37:01<396:20:58,  7.18s/it, loss=0.0635, lr=6.52e-06, step=1305]Training:   1%|          | 1306/200000 [2:37:01<396:20:58,  7.18s/it, loss=0.0754, lr=6.53e-06, step=1306]Training:   1%|          | 1307/200000 [2:37:08<396:40:04,  7.19s/it, loss=0.0754, lr=6.53e-06, step=1306]Training:   1%|          | 1307/200000 [2:37:08<396:40:04,  7.19s/it, loss=0.1108, lr=6.53e-06, step=1307]Training:   1%|          | 1308/200000 [2:37:16<399:33:41,  7.24s/it, loss=0.1108, lr=6.53e-06, step=1307]Training:   1%|          | 1308/200000 [2:37:16<399:33:41,  7.24s/it, loss=0.0800, lr=6.54e-06, step=1308]Training:   1%|          | 1309/200000 [2:37:23<398:49:42,  7.23s/it, loss=0.0800, lr=6.54e-06, step=1308]Training:   1%|          | 1309/200000 [2:37:23<398:49:42,  7.23s/it, loss=0.0776, lr=6.54e-06, step=1309]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1310/200000 [2:37:30<397:46:25,  7.21s/it, loss=0.0776, lr=6.54e-06, step=1309]Training:   1%|          | 1310/200000 [2:37:30<397:46:25,  7.21s/it, loss=0.0667, lr=6.55e-06, step=1310]Training:   1%|          | 1311/200000 [2:37:37<396:57:34,  7.19s/it, loss=0.0667, lr=6.55e-06, step=1310]Training:   1%|          | 1311/200000 [2:37:37<396:57:34,  7.19s/it, loss=0.1109, lr=6.55e-06, step=1311]Training:   1%|          | 1312/200000 [2:37:44<397:23:42,  7.20s/it, loss=0.1109, lr=6.55e-06, step=1311]Training:   1%|          | 1312/200000 [2:37:44<397:23:42,  7.20s/it, loss=0.0827, lr=6.56e-06, step=1312]Training:   1%|          | 1313/200000 [2:37:52<397:12:28,  7.20s/it, loss=0.0827, lr=6.56e-06, step=1312]Training:   1%|          | 1313/200000 [2:37:52<397:12:28,  7.20s/it, loss=0.0778, lr=6.56e-06, step=1313]Training:   1%|          | 1314/200000 [2:37:59<397:20:37,  7.20s/it, loss=0.0778, lr=6.56e-06, step=1313]Training:   1%|          | 1314/200000 [2:37:59<397:20:37,  7.20s/it, loss=0.0649, lr=6.57e-06, step=1314]Training:   1%|          | 1315/200000 [2:38:06<396:46:08,  7.19s/it, loss=0.0649, lr=6.57e-06, step=1314]Training:   1%|          | 1315/200000 [2:38:06<396:46:08,  7.19s/it, loss=0.1049, lr=6.57e-06, step=1315]Training:   1%|          | 1316/200000 [2:38:13<396:59:03,  7.19s/it, loss=0.1049, lr=6.57e-06, step=1315]Training:   1%|          | 1316/200000 [2:38:13<396:59:03,  7.19s/it, loss=0.0885, lr=6.58e-06, step=1316]Training:   1%|          | 1317/200000 [2:38:20<396:24:23,  7.18s/it, loss=0.0885, lr=6.58e-06, step=1316]Training:   1%|          | 1317/200000 [2:38:20<396:24:23,  7.18s/it, loss=0.0973, lr=6.58e-06, step=1317]Training:   1%|          | 1318/200000 [2:38:28<396:40:52,  7.19s/it, loss=0.0973, lr=6.58e-06, step=1317]Training:   1%|          | 1318/200000 [2:38:28<396:40:52,  7.19s/it, loss=0.1129, lr=6.59e-06, step=1318]Training:   1%|          | 1319/200000 [2:38:35<396:54:50,  7.19s/it, loss=0.1129, lr=6.59e-06, step=1318]Training:   1%|          | 1319/200000 [2:38:35<396:54:50,  7.19s/it, loss=0.0747, lr=6.59e-06, step=1319]Training:   1%|          | 1320/200000 [2:38:42<396:45:26,  7.19s/it, loss=0.0747, lr=6.59e-06, step=1319]Training:   1%|          | 1320/200000 [2:38:42<396:45:26,  7.19s/it, loss=0.0960, lr=6.60e-06, step=1320]Training:   1%|          | 1321/200000 [2:38:49<396:52:26,  7.19s/it, loss=0.0960, lr=6.60e-06, step=1320]Training:   1%|          | 1321/200000 [2:38:49<396:52:26,  7.19s/it, loss=0.0805, lr=6.60e-06, step=1321]Training:   1%|          | 1322/200000 [2:38:56<397:01:36,  7.19s/it, loss=0.0805, lr=6.60e-06, step=1321]Training:   1%|          | 1322/200000 [2:38:56<397:01:36,  7.19s/it, loss=0.0751, lr=6.61e-06, step=1322]Training:   1%|          | 1323/200000 [2:39:03<396:46:59,  7.19s/it, loss=0.0751, lr=6.61e-06, step=1322]Training:   1%|          | 1323/200000 [2:39:03<396:46:59,  7.19s/it, loss=0.0893, lr=6.61e-06, step=1323]Training:   1%|          | 1324/200000 [2:39:11<397:04:17,  7.19s/it, loss=0.0893, lr=6.61e-06, step=1323]Training:   1%|          | 1324/200000 [2:39:11<397:04:17,  7.19s/it, loss=0.0795, lr=6.62e-06, step=1324]Training:   1%|          | 1325/200000 [2:39:18<397:09:18,  7.20s/it, loss=0.0795, lr=6.62e-06, step=1324]Training:   1%|          | 1325/200000 [2:39:18<397:09:18,  7.20s/it, loss=0.0971, lr=6.62e-06, step=1325]Training:   1%|          | 1326/200000 [2:39:25<396:47:14,  7.19s/it, loss=0.0971, lr=6.62e-06, step=1325]Training:   1%|          | 1326/200000 [2:39:25<396:47:14,  7.19s/it, loss=0.0682, lr=6.63e-06, step=1326]Training:   1%|          | 1327/200000 [2:39:32<396:59:22,  7.19s/it, loss=0.0682, lr=6.63e-06, step=1326]Training:   1%|          | 1327/200000 [2:39:32<396:59:22,  7.19s/it, loss=0.1147, lr=6.63e-06, step=1327]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1328/200000 [2:39:39<396:31:20,  7.19s/it, loss=0.1147, lr=6.63e-06, step=1327]Training:   1%|          | 1328/200000 [2:39:39<396:31:20,  7.19s/it, loss=0.0765, lr=6.64e-06, step=1328]Training:   1%|          | 1329/200000 [2:39:47<398:29:07,  7.22s/it, loss=0.0765, lr=6.64e-06, step=1328]Training:   1%|          | 1329/200000 [2:39:47<398:29:07,  7.22s/it, loss=0.0855, lr=6.64e-06, step=1329]Training:   1%|          | 1330/200000 [2:39:54<398:20:30,  7.22s/it, loss=0.0855, lr=6.64e-06, step=1329]Training:   1%|          | 1330/200000 [2:39:54<398:20:30,  7.22s/it, loss=0.0975, lr=6.65e-06, step=1330]Training:   1%|          | 1331/200000 [2:40:01<397:31:00,  7.20s/it, loss=0.0975, lr=6.65e-06, step=1330]Training:   1%|          | 1331/200000 [2:40:01<397:31:00,  7.20s/it, loss=0.3000, lr=6.65e-06, step=1331]Training:   1%|          | 1332/200000 [2:40:08<397:27:46,  7.20s/it, loss=0.3000, lr=6.65e-06, step=1331]Training:   1%|          | 1332/200000 [2:40:08<397:27:46,  7.20s/it, loss=0.0681, lr=6.66e-06, step=1332]Training:   1%|          | 1333/200000 [2:40:15<397:22:08,  7.20s/it, loss=0.0681, lr=6.66e-06, step=1332]Training:   1%|          | 1333/200000 [2:40:15<397:22:08,  7.20s/it, loss=0.1221, lr=6.66e-06, step=1333]Training:   1%|          | 1334/200000 [2:40:23<397:26:09,  7.20s/it, loss=0.1221, lr=6.66e-06, step=1333]Training:   1%|          | 1334/200000 [2:40:23<397:26:09,  7.20s/it, loss=0.0829, lr=6.67e-06, step=1334]Training:   1%|          | 1335/200000 [2:40:30<397:07:18,  7.20s/it, loss=0.0829, lr=6.67e-06, step=1334]Training:   1%|          | 1335/200000 [2:40:30<397:07:18,  7.20s/it, loss=0.1048, lr=6.67e-06, step=1335]Training:   1%|          | 1336/200000 [2:40:37<397:00:57,  7.19s/it, loss=0.1048, lr=6.67e-06, step=1335]Training:   1%|          | 1336/200000 [2:40:37<397:00:57,  7.19s/it, loss=0.1068, lr=6.68e-06, step=1336]Training:   1%|          | 1337/200000 [2:40:44<396:51:19,  7.19s/it, loss=0.1068, lr=6.68e-06, step=1336]Training:   1%|          | 1337/200000 [2:40:44<396:51:19,  7.19s/it, loss=0.0557, lr=6.68e-06, step=1337]Training:   1%|          | 1338/200000 [2:40:51<396:52:03,  7.19s/it, loss=0.0557, lr=6.68e-06, step=1337]Training:   1%|          | 1338/200000 [2:40:51<396:52:03,  7.19s/it, loss=0.1364, lr=6.69e-06, step=1338]Training:   1%|          | 1339/200000 [2:40:59<396:23:04,  7.18s/it, loss=0.1364, lr=6.69e-06, step=1338]Training:   1%|          | 1339/200000 [2:40:59<396:23:04,  7.18s/it, loss=0.0632, lr=6.69e-06, step=1339]Training:   1%|          | 1340/200000 [2:41:06<396:38:26,  7.19s/it, loss=0.0632, lr=6.69e-06, step=1339]Training:   1%|          | 1340/200000 [2:41:06<396:38:26,  7.19s/it, loss=0.0761, lr=6.70e-06, step=1340]Training:   1%|          | 1341/200000 [2:41:13<396:22:29,  7.18s/it, loss=0.0761, lr=6.70e-06, step=1340]Training:   1%|          | 1341/200000 [2:41:13<396:22:29,  7.18s/it, loss=0.0732, lr=6.70e-06, step=1341]Training:   1%|          | 1342/200000 [2:41:20<396:37:45,  7.19s/it, loss=0.0732, lr=6.70e-06, step=1341]Training:   1%|          | 1342/200000 [2:41:20<396:37:45,  7.19s/it, loss=0.1163, lr=6.71e-06, step=1342]Training:   1%|          | 1343/200000 [2:41:27<396:49:54,  7.19s/it, loss=0.1163, lr=6.71e-06, step=1342]Training:   1%|          | 1343/200000 [2:41:27<396:49:54,  7.19s/it, loss=0.0722, lr=6.71e-06, step=1343]Training:   1%|          | 1344/200000 [2:41:35<396:49:18,  7.19s/it, loss=0.0722, lr=6.71e-06, step=1343]Training:   1%|          | 1344/200000 [2:41:35<396:49:18,  7.19s/it, loss=0.1960, lr=6.72e-06, step=1344]Training:   1%|          | 1345/200000 [2:41:42<396:54:55,  7.19s/it, loss=0.1960, lr=6.72e-06, step=1344]Training:   1%|          | 1345/200000 [2:41:42<396:54:55,  7.19s/it, loss=0.0885, lr=6.72e-06, step=1345]Training:   1%|          | 1346/200000 [2:41:49<396:43:53,  7.19s/it, loss=0.0885, lr=6.72e-06, step=1345]Training:   1%|          | 1346/200000 [2:41:49<396:43:53,  7.19s/it, loss=0.0737, lr=6.73e-06, step=1346]Training:   1%|          | 1347/200000 [2:41:56<396:34:05,  7.19s/it, loss=0.0737, lr=6.73e-06, step=1346]Training:   1%|          | 1347/200000 [2:41:56<396:34:05,  7.19s/it, loss=0.0580, lr=6.73e-06, step=1347]Training:   1%|          | 1348/200000 [2:42:03<396:37:34,  7.19s/it, loss=0.0580, lr=6.73e-06, step=1347]Training:   1%|          | 1348/200000 [2:42:03<396:37:34,  7.19s/it, loss=0.0799, lr=6.74e-06, step=1348]Training:   1%|          | 1349/200000 [2:42:10<396:13:20,  7.18s/it, loss=0.0799, lr=6.74e-06, step=1348]Training:   1%|          | 1349/200000 [2:42:10<396:13:20,  7.18s/it, loss=0.0680, lr=6.74e-06, step=1349]Training:   1%|          | 1350/200000 [2:42:18<396:42:58,  7.19s/it, loss=0.0680, lr=6.74e-06, step=1349]Training:   1%|          | 1350/200000 [2:42:18<396:42:58,  7.19s/it, loss=0.0900, lr=6.75e-06, step=1350]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1351/200000 [2:42:25<396:18:53,  7.18s/it, loss=0.0900, lr=6.75e-06, step=1350]Training:   1%|          | 1351/200000 [2:42:25<396:18:53,  7.18s/it, loss=0.0996, lr=6.75e-06, step=1351]Training:   1%|          | 1352/200000 [2:42:32<396:39:55,  7.19s/it, loss=0.0996, lr=6.75e-06, step=1351]Training:   1%|          | 1352/200000 [2:42:32<396:39:55,  7.19s/it, loss=0.1162, lr=6.76e-06, step=1352]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1353/200000 [2:42:39<396:37:30,  7.19s/it, loss=0.1162, lr=6.76e-06, step=1352]Training:   1%|          | 1353/200000 [2:42:39<396:37:30,  7.19s/it, loss=0.1297, lr=6.76e-06, step=1353]Training:   1%|          | 1354/200000 [2:42:46<397:01:49,  7.20s/it, loss=0.1297, lr=6.76e-06, step=1353]Training:   1%|          | 1354/200000 [2:42:46<397:01:49,  7.20s/it, loss=0.0826, lr=6.77e-06, step=1354]Training:   1%|          | 1355/200000 [2:42:54<396:49:33,  7.19s/it, loss=0.0826, lr=6.77e-06, step=1354]Training:   1%|          | 1355/200000 [2:42:54<396:49:33,  7.19s/it, loss=0.0864, lr=6.77e-06, step=1355]Training:   1%|          | 1356/200000 [2:43:01<396:59:40,  7.19s/it, loss=0.0864, lr=6.77e-06, step=1355]Training:   1%|          | 1356/200000 [2:43:01<396:59:40,  7.19s/it, loss=0.0733, lr=6.78e-06, step=1356]Training:   1%|          | 1357/200000 [2:43:08<396:33:57,  7.19s/it, loss=0.0733, lr=6.78e-06, step=1356]Training:   1%|          | 1357/200000 [2:43:08<396:33:57,  7.19s/it, loss=0.0543, lr=6.78e-06, step=1357]Training:   1%|          | 1358/200000 [2:43:15<396:27:57,  7.19s/it, loss=0.0543, lr=6.78e-06, step=1357]Training:   1%|          | 1358/200000 [2:43:15<396:27:57,  7.19s/it, loss=0.1093, lr=6.79e-06, step=1358]Training:   1%|          | 1359/200000 [2:43:22<396:36:01,  7.19s/it, loss=0.1093, lr=6.79e-06, step=1358]Training:   1%|          | 1359/200000 [2:43:22<396:36:01,  7.19s/it, loss=0.0893, lr=6.79e-06, step=1359]Training:   1%|          | 1360/200000 [2:43:30<396:09:58,  7.18s/it, loss=0.0893, lr=6.79e-06, step=1359]Training:   1%|          | 1360/200000 [2:43:30<396:09:58,  7.18s/it, loss=0.0612, lr=6.80e-06, step=1360]Training:   1%|          | 1361/200000 [2:43:37<396:25:24,  7.18s/it, loss=0.0612, lr=6.80e-06, step=1360]Training:   1%|          | 1361/200000 [2:43:37<396:25:24,  7.18s/it, loss=0.0993, lr=6.80e-06, step=1361]Training:   1%|          | 1362/200000 [2:43:44<396:40:19,  7.19s/it, loss=0.0993, lr=6.80e-06, step=1361]Training:   1%|          | 1362/200000 [2:43:44<396:40:19,  7.19s/it, loss=0.0690, lr=6.81e-06, step=1362]Training:   1%|          | 1363/200000 [2:43:51<396:50:31,  7.19s/it, loss=0.0690, lr=6.81e-06, step=1362]Training:   1%|          | 1363/200000 [2:43:51<396:50:31,  7.19s/it, loss=0.1010, lr=6.81e-06, step=1363]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1364/200000 [2:43:58<396:58:05,  7.19s/it, loss=0.1010, lr=6.81e-06, step=1363]Training:   1%|          | 1364/200000 [2:43:58<396:58:05,  7.19s/it, loss=0.0886, lr=6.82e-06, step=1364]Training:   1%|          | 1365/200000 [2:44:05<395:17:51,  7.16s/it, loss=0.0886, lr=6.82e-06, step=1364]Training:   1%|          | 1365/200000 [2:44:05<395:17:51,  7.16s/it, loss=0.1227, lr=6.82e-06, step=1365]Training:   1%|          | 1366/200000 [2:44:13<395:17:15,  7.16s/it, loss=0.1227, lr=6.82e-06, step=1365]Training:   1%|          | 1366/200000 [2:44:13<395:17:15,  7.16s/it, loss=0.0675, lr=6.83e-06, step=1366]Training:   1%|          | 1367/200000 [2:44:20<395:21:29,  7.17s/it, loss=0.0675, lr=6.83e-06, step=1366]Training:   1%|          | 1367/200000 [2:44:20<395:21:29,  7.17s/it, loss=0.0916, lr=6.83e-06, step=1367]Training:   1%|          | 1368/200000 [2:44:27<395:58:16,  7.18s/it, loss=0.0916, lr=6.83e-06, step=1367]Training:   1%|          | 1368/200000 [2:44:27<395:58:16,  7.18s/it, loss=0.0740, lr=6.84e-06, step=1368]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1369/200000 [2:44:34<396:14:05,  7.18s/it, loss=0.0740, lr=6.84e-06, step=1368]Training:   1%|          | 1369/200000 [2:44:34<396:14:05,  7.18s/it, loss=0.0766, lr=6.84e-06, step=1369]Training:   1%|          | 1370/200000 [2:44:41<395:53:47,  7.18s/it, loss=0.0766, lr=6.84e-06, step=1369]Training:   1%|          | 1370/200000 [2:44:41<395:53:47,  7.18s/it, loss=0.0830, lr=6.85e-06, step=1370]Training:   1%|          | 1371/200000 [2:44:49<398:33:42,  7.22s/it, loss=0.0830, lr=6.85e-06, step=1370]Training:   1%|          | 1371/200000 [2:44:49<398:33:42,  7.22s/it, loss=0.1093, lr=6.85e-06, step=1371]Training:   1%|          | 1372/200000 [2:44:56<397:46:51,  7.21s/it, loss=0.1093, lr=6.85e-06, step=1371]Training:   1%|          | 1372/200000 [2:44:56<397:46:51,  7.21s/it, loss=0.1460, lr=6.86e-06, step=1372]Training:   1%|          | 1373/200000 [2:45:03<396:54:49,  7.19s/it, loss=0.1460, lr=6.86e-06, step=1372]Training:   1%|          | 1373/200000 [2:45:03<396:54:49,  7.19s/it, loss=0.0725, lr=6.86e-06, step=1373]Training:   1%|          | 1374/200000 [2:45:10<396:22:11,  7.18s/it, loss=0.0725, lr=6.86e-06, step=1373]Training:   1%|          | 1374/200000 [2:45:10<396:22:11,  7.18s/it, loss=0.0831, lr=6.87e-06, step=1374]Training:   1%|          | 1375/200000 [2:45:17<396:44:56,  7.19s/it, loss=0.0831, lr=6.87e-06, step=1374]Training:   1%|          | 1375/200000 [2:45:17<396:44:56,  7.19s/it, loss=0.1097, lr=6.87e-06, step=1375]Training:   1%|          | 1376/200000 [2:45:25<396:52:35,  7.19s/it, loss=0.1097, lr=6.87e-06, step=1375]Training:   1%|          | 1376/200000 [2:45:25<396:52:35,  7.19s/it, loss=0.0862, lr=6.88e-06, step=1376]Training:   1%|          | 1377/200000 [2:45:32<396:35:21,  7.19s/it, loss=0.0862, lr=6.88e-06, step=1376]Training:   1%|          | 1377/200000 [2:45:32<396:35:21,  7.19s/it, loss=0.0829, lr=6.88e-06, step=1377]Training:   1%|          | 1378/200000 [2:45:39<396:34:16,  7.19s/it, loss=0.0829, lr=6.88e-06, step=1377]Training:   1%|          | 1378/200000 [2:45:39<396:34:16,  7.19s/it, loss=0.0854, lr=6.89e-06, step=1378]Training:   1%|          | 1379/200000 [2:45:46<396:15:41,  7.18s/it, loss=0.0854, lr=6.89e-06, step=1378]Training:   1%|          | 1379/200000 [2:45:46<396:15:41,  7.18s/it, loss=0.0829, lr=6.89e-06, step=1379]Training:   1%|          | 1380/200000 [2:45:53<396:01:35,  7.18s/it, loss=0.0829, lr=6.89e-06, step=1379]Training:   1%|          | 1380/200000 [2:45:53<396:01:35,  7.18s/it, loss=0.0717, lr=6.90e-06, step=1380]Training:   1%|          | 1381/200000 [2:46:00<394:42:18,  7.15s/it, loss=0.0717, lr=6.90e-06, step=1380]Training:   1%|          | 1381/200000 [2:46:00<394:42:18,  7.15s/it, loss=0.0854, lr=6.90e-06, step=1381]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1382/200000 [2:46:08<395:05:54,  7.16s/it, loss=0.0854, lr=6.90e-06, step=1381]Training:   1%|          | 1382/200000 [2:46:08<395:05:54,  7.16s/it, loss=0.0851, lr=6.91e-06, step=1382]Training:   1%|          | 1383/200000 [2:46:15<395:36:40,  7.17s/it, loss=0.0851, lr=6.91e-06, step=1382]Training:   1%|          | 1383/200000 [2:46:15<395:36:40,  7.17s/it, loss=0.1003, lr=6.91e-06, step=1383]Training:   1%|          | 1384/200000 [2:46:22<396:04:06,  7.18s/it, loss=0.1003, lr=6.91e-06, step=1383]Training:   1%|          | 1384/200000 [2:46:22<396:04:06,  7.18s/it, loss=0.0967, lr=6.92e-06, step=1384]Training:   1%|          | 1385/200000 [2:46:29<396:11:33,  7.18s/it, loss=0.0967, lr=6.92e-06, step=1384]Training:   1%|          | 1385/200000 [2:46:29<396:11:33,  7.18s/it, loss=0.0709, lr=6.92e-06, step=1385]Training:   1%|          | 1386/200000 [2:46:36<396:08:59,  7.18s/it, loss=0.0709, lr=6.92e-06, step=1385]Training:   1%|          | 1386/200000 [2:46:36<396:08:59,  7.18s/it, loss=0.0812, lr=6.93e-06, step=1386]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1387/200000 [2:46:43<395:56:49,  7.18s/it, loss=0.0812, lr=6.93e-06, step=1386]Training:   1%|          | 1387/200000 [2:46:43<395:56:49,  7.18s/it, loss=0.0577, lr=6.93e-06, step=1387]Training:   1%|          | 1388/200000 [2:46:51<396:24:36,  7.19s/it, loss=0.0577, lr=6.93e-06, step=1387]Training:   1%|          | 1388/200000 [2:46:51<396:24:36,  7.19s/it, loss=0.0910, lr=6.94e-06, step=1388]Training:   1%|          | 1389/200000 [2:46:58<396:16:10,  7.18s/it, loss=0.0910, lr=6.94e-06, step=1388]Training:   1%|          | 1389/200000 [2:46:58<396:16:10,  7.18s/it, loss=0.0965, lr=6.94e-06, step=1389]Training:   1%|          | 1390/200000 [2:47:05<396:06:17,  7.18s/it, loss=0.0965, lr=6.94e-06, step=1389]Training:   1%|          | 1390/200000 [2:47:05<396:06:17,  7.18s/it, loss=0.0877, lr=6.95e-06, step=1390]Training:   1%|          | 1391/200000 [2:47:12<396:08:21,  7.18s/it, loss=0.0877, lr=6.95e-06, step=1390]Training:   1%|          | 1391/200000 [2:47:12<396:08:21,  7.18s/it, loss=0.0557, lr=6.95e-06, step=1391]Training:   1%|          | 1392/200000 [2:47:20<398:09:08,  7.22s/it, loss=0.0557, lr=6.95e-06, step=1391]Training:   1%|          | 1392/200000 [2:47:20<398:09:08,  7.22s/it, loss=0.2249, lr=6.96e-06, step=1392]Training:   1%|          | 1393/200000 [2:47:27<397:36:19,  7.21s/it, loss=0.2249, lr=6.96e-06, step=1392]Training:   1%|          | 1393/200000 [2:47:27<397:36:19,  7.21s/it, loss=0.0708, lr=6.96e-06, step=1393]Training:   1%|          | 1394/200000 [2:47:34<397:09:21,  7.20s/it, loss=0.0708, lr=6.96e-06, step=1393]Training:   1%|          | 1394/200000 [2:47:34<397:09:21,  7.20s/it, loss=0.0742, lr=6.97e-06, step=1394]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1395/200000 [2:47:41<396:26:12,  7.19s/it, loss=0.0742, lr=6.97e-06, step=1394]Training:   1%|          | 1395/200000 [2:47:41<396:26:12,  7.19s/it, loss=0.0920, lr=6.97e-06, step=1395]Training:   1%|          | 1396/200000 [2:47:48<396:25:07,  7.19s/it, loss=0.0920, lr=6.97e-06, step=1395]Training:   1%|          | 1396/200000 [2:47:48<396:25:07,  7.19s/it, loss=0.0761, lr=6.98e-06, step=1396]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1397/200000 [2:47:55<396:37:05,  7.19s/it, loss=0.0761, lr=6.98e-06, step=1396]Training:   1%|          | 1397/200000 [2:47:55<396:37:05,  7.19s/it, loss=0.0709, lr=6.98e-06, step=1397]Training:   1%|          | 1398/200000 [2:48:03<396:25:55,  7.19s/it, loss=0.0709, lr=6.98e-06, step=1397]Training:   1%|          | 1398/200000 [2:48:03<396:25:55,  7.19s/it, loss=0.0692, lr=6.99e-06, step=1398]Training:   1%|          | 1399/200000 [2:48:10<396:20:51,  7.18s/it, loss=0.0692, lr=6.99e-06, step=1398]Training:   1%|          | 1399/200000 [2:48:10<396:20:51,  7.18s/it, loss=0.0735, lr=6.99e-06, step=1399]Training:   1%|          | 1400/200000 [2:48:17<395:50:05,  7.18s/it, loss=0.0735, lr=6.99e-06, step=1399]Training:   1%|          | 1400/200000 [2:48:17<395:50:05,  7.18s/it, loss=0.1034, lr=7.00e-06, step=1400]22:44:52.916 [I] step=1400 loss=0.0915 lr=6.76e-06 grad_norm=0.83 time=718.9s                     (486094:train_pytorch.py:582)
Training:   1%|          | 1401/200000 [2:48:24<396:14:38,  7.18s/it, loss=0.1034, lr=7.00e-06, step=1400]Training:   1%|          | 1401/200000 [2:48:24<396:14:38,  7.18s/it, loss=0.0903, lr=7.00e-06, step=1401]Training:   1%|          | 1402/200000 [2:48:31<396:21:22,  7.18s/it, loss=0.0903, lr=7.00e-06, step=1401]Training:   1%|          | 1402/200000 [2:48:31<396:21:22,  7.18s/it, loss=0.0706, lr=7.01e-06, step=1402]Training:   1%|          | 1403/200000 [2:48:39<396:35:41,  7.19s/it, loss=0.0706, lr=7.01e-06, step=1402]Training:   1%|          | 1403/200000 [2:48:39<396:35:41,  7.19s/it, loss=0.0759, lr=7.01e-06, step=1403]Training:   1%|          | 1404/200000 [2:48:46<396:46:05,  7.19s/it, loss=0.0759, lr=7.01e-06, step=1403]Training:   1%|          | 1404/200000 [2:48:46<396:46:05,  7.19s/it, loss=0.0631, lr=7.02e-06, step=1404]Training:   1%|          | 1405/200000 [2:48:53<396:32:15,  7.19s/it, loss=0.0631, lr=7.02e-06, step=1404]Training:   1%|          | 1405/200000 [2:48:53<396:32:15,  7.19s/it, loss=0.0816, lr=7.02e-06, step=1405]Training:   1%|          | 1406/200000 [2:49:00<396:26:34,  7.19s/it, loss=0.0816, lr=7.02e-06, step=1405]Training:   1%|          | 1406/200000 [2:49:00<396:26:34,  7.19s/it, loss=0.1010, lr=7.03e-06, step=1406]Training:   1%|          | 1407/200000 [2:49:07<396:52:22,  7.19s/it, loss=0.1010, lr=7.03e-06, step=1406]Training:   1%|          | 1407/200000 [2:49:07<396:52:22,  7.19s/it, loss=0.0613, lr=7.03e-06, step=1407]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
slurmstepd: error: *** JOB 342343 ON dgx-40 CANCELLED AT 2026-01-30T22:45:39 ***
Fri Jan 30 22:45:45 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H800                    On  | 00000000:1B:00.0 Off |                    0 |
| N/A   28C    P0              75W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H800                    On  | 00000000:43:00.0 Off |                    0 |
| N/A   20C    P0              74W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H800                    On  | 00000000:52:00.0 Off |                    0 |
| N/A   22C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H800                    On  | 00000000:61:00.0 Off |                    0 |
| N/A   27C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H800                    On  | 00000000:9D:00.0 Off |                    0 |
| N/A   20C    P0              66W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H800                    On  | 00000000:C3:00.0 Off |                    0 |
| N/A   19C    P0              65W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H800                    On  | 00000000:D1:00.0 Off |                    0 |
| N/A   20C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H800                    On  | 00000000:DF:00.0 Off |                    0 |
| N/A   22C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
W0130 22:46:16.406000 694990 site-packages/torch/distributed/run.py:774] 
W0130 22:46:16.406000 694990 site-packages/torch/distributed/run.py:774] *****************************************
W0130 22:46:16.406000 694990 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0130 22:46:16.406000 694990 site-packages/torch/distributed/run.py:774] *****************************************
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
Traceback (most recent call last):
  File "/project/peilab/wzj/RoboTwin/policy/openpi_test/scripts/train_pytorch.py", line 34, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
E0130 22:46:16.887000 694990 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 695600) of binary: /home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/bin/python3.10
Traceback (most recent call last):
  File "/home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rouyangaa/anaconda3/envs/RoboTwin-pi0-1/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_pytorch.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 695601)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 695602)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 695603)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 695604)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 695605)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 695606)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 695607)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-30_22:46:16
  host      : dgx-40.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 695600)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Fri Jan 30 22:50:41 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H800                    On  | 00000000:1B:00.0 Off |                    0 |
| N/A   18C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H800                    On  | 00000000:43:00.0 Off |                    0 |
| N/A   19C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H800                    On  | 00000000:52:00.0 Off |                    0 |
| N/A   21C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H800                    On  | 00000000:61:00.0 Off |                    0 |
| N/A   19C    P0              67W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H800                    On  | 00000000:9D:00.0 Off |                    0 |
| N/A   19C    P0              66W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H800                    On  | 00000000:C3:00.0 Off |                    0 |
| N/A   18C    P0              65W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H800                    On  | 00000000:D1:00.0 Off |                    0 |
| N/A   19C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H800                    On  | 00000000:DF:00.0 Off |                    0 |
| N/A   21C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
W0130 22:50:49.732000 701601 .venv/lib/python3.11/site-packages/torch/distributed/run.py:766] 
W0130 22:50:49.732000 701601 .venv/lib/python3.11/site-packages/torch/distributed/run.py:766] *****************************************
W0130 22:50:49.732000 701601 .venv/lib/python3.11/site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0130 22:50:49.732000 701601 .venv/lib/python3.11/site-packages/torch/distributed/run.py:766] *****************************************
22:51:22.939 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701682:train_pytorch.py:340)
22:51:22.939 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701682:train_pytorch.py:354)
22:51:23.017 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701675:train_pytorch.py:340)
22:51:23.022 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701682:config.py:196)
22:51:23.023 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439ea9b10>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701682:data_loader.py:243)
22:51:23.207 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701676:train_pytorch.py:340)
22:51:23.207 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701676:train_pytorch.py:354)
22:51:23.281 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701676:config.py:196)
22:51:23.282 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x15543b4ec510>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701676:data_loader.py:243)
22:51:23.763 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701680:train_pytorch.py:340)
22:51:23.763 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701680:train_pytorch.py:354)
22:51:23.803 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701677:train_pytorch.py:340)
22:51:23.803 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701677:train_pytorch.py:354)
22:51:23.821 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701679:train_pytorch.py:340)
22:51:23.821 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701679:train_pytorch.py:354)
22:51:23.836 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701680:config.py:196)
22:51:23.837 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439fae050>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701680:data_loader.py:243)
wandb: Currently logged in as: 1364904434 (1364904434-hong-kong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
22:51:23.845 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701681:train_pytorch.py:340)
22:51:23.846 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701681:train_pytorch.py:354)
22:51:23.875 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701677:config.py:196)
22:51:23.876 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x15543a011650>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701677:data_loader.py:243)
22:51:23.880 [I] Created experiment checkpoint directory: /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot (701683:train_pytorch.py:340)
22:51:23.880 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701683:train_pytorch.py:354)
22:51:23.900 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701679:config.py:196)
22:51:23.901 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439ea0d10>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701679:data_loader.py:243)
22:51:23.922 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701681:config.py:196)
22:51:23.923 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439f05ad0>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701681:data_loader.py:243)
22:51:23.966 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701683:config.py:196)
22:51:23.967 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439d17850>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701683:data_loader.py:243)
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /project/peilab/wzj/RoboTwin/policy/openpi_test/wandb/run-20260130_225123-wucq8j3m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robotwin_aloha_lerobot
wandb: ‚≠êÔ∏è View project at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi
wandb: üöÄ View run at https://wandb.ai/1364904434-hong-kong-university-of-science-and-technology/openpi/runs/wucq8j3m
22:51:25.328 [I] Using batch size per GPU: 16 (total batch size across 8 GPUs: 128)               (701675:train_pytorch.py:354)
22:51:25.395 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701675:config.py:196)
22:51:25.396 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439d05fd0>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701675:data_loader.py:243)
22:51:34.973 [I] local_batch_size: 16                                                             (701682:data_loader.py:324)
22:51:35.242 [I] local_batch_size: 16                                                             (701680:data_loader.py:324)
22:51:35.337 [I] local_batch_size: 16                                                             (701681:data_loader.py:324)
22:51:35.376 [I] local_batch_size: 16                                                             (701675:data_loader.py:324)
22:51:35.443 [I] local_batch_size: 16                                                             (701683:data_loader.py:324)
22:51:35.474 [I] local_batch_size: 16                                                             (701676:data_loader.py:324)
22:51:35.510 [I] local_batch_size: 16                                                             (701677:data_loader.py:324)
22:51:35.789 [I] local_batch_size: 16                                                             (701679:data_loader.py:324)
INFO:2026-01-30 22:51:46,859:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:46.859 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701680:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,860:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:46.860 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701682:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,861:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:46.861 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701680:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,861:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:46.861 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701682:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,893:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:46.893 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701676:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,894:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:46.894 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701676:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,895:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:46.895 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701677:xla_bridge.py:925)
INFO:2026-01-30 22:51:46,896:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:46.896 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701677:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,003:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:47.003 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701683:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,004:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:47.004 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701683:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,035:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:47.035 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701681:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,036:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:47.036 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701681:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,071:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:47.071 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701679:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,072:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:47.072 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701679:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,439:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
22:51:47.439 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (701675:xla_bridge.py:925)
INFO:2026-01-30 22:51:47,440:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
22:51:47.440 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (701675:xla_bridge.py:925)
22:51:47.504 [I] Loaded norm stats from /project/peilab/wzj/RoboTwin/policy/openpi_test/assets/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot_repo (701675:config.py:196)
22:51:47.505 [I] data_config: DataConfig(repo_id='robotwin_aloha_lerobot_repo', asset_id='robotwin_aloha_lerobot_repo', norm_stats={'state': NormStats(mean=array([-0.2224945 ,  1.08982718,  0.79591465, -0.33272886,  0.05979836,
       -0.043853  ,  0.67307353,  0.23255804,  1.10730827,  0.82398069,
       -0.34208655, -0.015279  ,  0.0103482 ,  0.67311168]), std=array([0.36667523, 0.99401045, 0.78608114, 0.67225939, 0.24997029,
       0.55710292, 0.45222378, 0.32254505, 1.01875401, 0.808204  ,
       0.72012764, 0.26058581, 0.61368692, 0.45168629]), q01=array([-9.65091895e-01, -3.16402118e-04, -1.74276372e-03, -1.59232093e+00,
       -4.36648487e-01, -2.19382558e+00,  0.00000000e+00, -1.56521035e-01,
       -2.51359060e-03, -2.38050565e-03, -1.70236679e+00, -1.03065560e+00,
       -1.66630766e+00,  0.00000000e+00]), q99=array([0.17045697, 2.57765113, 2.4680983 , 1.32222104, 1.21582126,
       1.43892573, 0.9998    , 1.06100315, 2.59984096, 2.49145158,
       1.30731568, 1.05090197, 2.09641071, 0.9998    ])), 'actions': NormStats(mean=array([-0.01524173,  0.05534578,  0.04061704, -0.00830928,  0.00520263,
       -0.0159315 ,  0.64439434,  0.02754116,  0.08904913,  0.06243737,
       -0.0118403 , -0.00198644,  0.01282268,  0.64564937]), std=array([0.21512882, 0.52675635, 0.42676991, 0.36585233, 0.14483902,
       0.30340073, 0.46301985, 0.189355  , 0.51227379, 0.4247252 ,
       0.34619212, 0.14758518, 0.32021213, 0.46215528]), q01=array([-0.65610172, -1.76358445, -1.37108001, -1.14166522, -0.47777414,
       -1.18612817,  0.        , -0.58630055, -1.5966792 , -1.23936615,
       -1.15867711, -0.66592155, -0.98992321,  0.        ]), q99=array([0.72178027, 2.0680415 , 1.67058788, 1.34100596, 0.61617074,
       0.86893194, 0.9998    , 0.66833033, 2.11176138, 1.71022064,
       1.24400797, 0.50260132, 1.21457384, 0.9998    ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x155439db8710>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (701675:data_loader.py:243)
22:51:57.322 [I] local_batch_size: 16                                                             (701675:data_loader.py:324)
22:52:23.223 [I] Cleared sample batch and data loader from memory                                 (701675:train_pytorch.py:390)
22:52:38.376 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701682:pi0_pytorch.py:133)
22:52:38.377 [I] Enabled gradient checkpointing for memory optimization                           (701682:train_pytorch.py:414)
22:52:38.377 [I] Enabled memory optimizations for 8+ GPU training                                 (701682:train_pytorch.py:430)
22:52:38.457 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701683:pi0_pytorch.py:133)
22:52:38.457 [I] Enabled gradient checkpointing for memory optimization                           (701683:train_pytorch.py:414)
22:52:38.457 [I] Enabled memory optimizations for 8+ GPU training                                 (701683:train_pytorch.py:430)
22:52:39.113 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701677:pi0_pytorch.py:133)
22:52:39.113 [I] Enabled gradient checkpointing for memory optimization                           (701677:train_pytorch.py:414)
22:52:39.113 [I] Enabled memory optimizations for 8+ GPU training                                 (701677:train_pytorch.py:430)
22:52:39.169 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701680:pi0_pytorch.py:133)
22:52:39.169 [I] Enabled gradient checkpointing for memory optimization                           (701680:train_pytorch.py:414)
22:52:39.169 [I] Enabled memory optimizations for 8+ GPU training                                 (701680:train_pytorch.py:430)
22:52:39.243 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701679:pi0_pytorch.py:133)
22:52:39.243 [I] Enabled gradient checkpointing for memory optimization                           (701679:train_pytorch.py:414)
22:52:39.243 [I] Enabled memory optimizations for 8+ GPU training                                 (701679:train_pytorch.py:430)
22:52:39.376 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701681:pi0_pytorch.py:133)
22:52:39.376 [I] Enabled gradient checkpointing for memory optimization                           (701681:train_pytorch.py:414)
22:52:39.376 [I] Enabled memory optimizations for 8+ GPU training                                 (701681:train_pytorch.py:430)
22:52:39.399 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701676:pi0_pytorch.py:133)
22:52:39.399 [I] Enabled gradient checkpointing for memory optimization                           (701676:train_pytorch.py:414)
22:52:39.399 [I] Enabled memory optimizations for 8+ GPU training                                 (701676:train_pytorch.py:430)
22:53:05.583 [I] Enabled gradient checkpointing for PI0Pytorch model                              (701675:pi0_pytorch.py:133)
22:53:05.583 [I] Enabled gradient checkpointing for memory optimization                           (701675:train_pytorch.py:414)
22:53:05.584 [I] Step 0 (after_model_creation): GPU memory - allocated: 7.02GB, reserved: 7.09GB, free: 0.07GB, peak_allocated: 7.02GB, peak_reserved: 7.09GB | DDP: rank=0, world_size=8 (701675:train_pytorch.py:304)
22:53:05.584 [I] Enabled memory optimizations for 8+ GPU training                                 (701675:train_pytorch.py:430)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.631 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701677:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.634 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701676:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.642 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701682:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.646 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701679:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.651 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701680:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.656 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701675:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.682 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701681:train_pytorch.py:443)
/project/peilab/wzj/RoboTwin/policy/openpi_test/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2348: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
22:53:10.745 [I] Loading weights from: /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701683:train_pytorch.py:443)
22:53:12.235 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701682:train_pytorch.py:449)
22:53:13.304 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701677:train_pytorch.py:449)
22:53:13.304 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701679:train_pytorch.py:449)
22:53:13.304 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701676:train_pytorch.py:449)
22:53:13.304 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701675:train_pytorch.py:449)
22:53:13.305 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701680:train_pytorch.py:449)
22:53:13.305 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701681:train_pytorch.py:449)
22:53:13.305 [I] Loaded PyTorch weights from /project/peilab/wzj/RoboTwin/policy/openpi_test/base_model_pytorch/pi0_base_pytorch (701683:train_pytorch.py:449)
22:53:13.309 [I] Running on: dgx-40 | world_size=8                                                (701675:train_pytorch.py:486)
22:53:13.309 [I] Training config: batch_size=128, effective_batch_size=16, num_train_steps=200000 (701675:train_pytorch.py:489)
22:53:13.309 [I] Memory optimizations: gradient_checkpointing=True                                (701675:train_pytorch.py:492)
22:53:13.309 [I] LR schedule: warmup=10000, peak_lr=5.00e-05, decay_steps=1000000, end_lr=5.00e-05 (701675:train_pytorch.py:493)
22:53:13.309 [I] Optimizer: AdamW, weight_decay=1e-10, clip_norm=1.0                              (701675:train_pytorch.py:496)
22:53:13.309 [I] EMA is not supported for PyTorch training                                        (701675:train_pytorch.py:499)
22:53:13.309 [I] Training precision: bfloat16                                                     (701675:train_pytorch.py:500)
Training:   0%|          | 0/200000 [00:00<?, ?it/s]22:54:14.945 [I] Step 0 (after_backward): GPU memory - allocated: 14.13GB, reserved: 22.36GB, free: 8.22GB, peak_allocated: 24.85GB, peak_reserved: 25.36GB | DDP: rank=0, world_size=8 (701675:train_pytorch.py:304)
22:54:37.716 [I] step=0 loss=0.1354 lr=5.00e-09 grad_norm=2.87 time=84.4s                         (701675:train_pytorch.py:582)
Training:   0%|          | 1/200000 [01:24<4689:15:42, 84.41s/it]Training:   0%|          | 1/200000 [01:24<4689:15:42, 84.41s/it, loss=0.1354, lr=5.00e-09, step=1]22:54:39.000 [I] Step 1 (after_backward): GPU memory - allocated: 26.70GB, reserved: 36.08GB, free: 9.39GB, peak_allocated: 32.98GB, peak_reserved: 36.08GB | DDP: rank=0, world_size=8 (701675:train_pytorch.py:304)
Training:   0%|          | 2/200000 [01:26<1986:40:33, 35.76s/it, loss=0.1354, lr=5.00e-09, step=1]Training:   0%|          | 2/200000 [01:26<1986:40:33, 35.76s/it, loss=0.1453, lr=1.00e-08, step=2]22:54:40.403 [I] Step 2 (after_backward): GPU memory - allocated: 26.70GB, reserved: 36.08GB, free: 9.39GB, peak_allocated: 32.98GB, peak_reserved: 36.08GB | DDP: rank=0, world_size=8 (701675:train_pytorch.py:304)
Training:   0%|          | 3/200000 [01:27<1106:46:15, 19.92s/it, loss=0.1453, lr=1.00e-08, step=2]Training:   0%|          | 3/200000 [01:27<1106:46:15, 19.92s/it, loss=0.2544, lr=1.50e-08, step=3]22:54:41.477 [I] Step 3 (after_backward): GPU memory - allocated: 26.70GB, reserved: 36.08GB, free: 9.39GB, peak_allocated: 32.99GB, peak_reserved: 36.08GB | DDP: rank=0, world_size=8 (701675:train_pytorch.py:304)
Training:   0%|          | 4/200000 [01:28<693:21:30, 12.48s/it, loss=0.2544, lr=1.50e-08, step=3] Training:   0%|          | 4/200000 [01:28<693:21:30, 12.48s/it, loss=0.2310, lr=2.00e-08, step=4]22:54:42.609 [I] Step 4 (after_backward): GPU memory - allocated: 26.70GB, reserved: 36.08GB, free: 9.39GB, peak_allocated: 32.99GB, peak_reserved: 36.08GB | DDP: rank=0, world_size=8 (701675:train_pytorch.py:304)
Training:   0%|          | 5/200000 [01:29<471:49:19,  8.49s/it, loss=0.2310, lr=2.00e-08, step=4]Training:   0%|          | 5/200000 [01:29<471:49:19,  8.49s/it, loss=0.3443, lr=2.50e-08, step=5]Training:   0%|          | 6/200000 [01:31<336:45:22,  6.06s/it, loss=0.3443, lr=2.50e-08, step=5]Training:   0%|          | 6/200000 [01:31<336:45:22,  6.06s/it, loss=0.1541, lr=3.00e-08, step=6]Training:   0%|          | 7/200000 [01:32<246:08:02,  4.43s/it, loss=0.1541, lr=3.00e-08, step=6]Training:   0%|          | 7/200000 [01:32<246:08:02,  4.43s/it, loss=0.4014, lr=3.50e-08, step=7]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 8/200000 [01:33<195:18:30,  3.52s/it, loss=0.4014, lr=3.50e-08, step=7]Training:   0%|          | 8/200000 [01:33<195:18:30,  3.52s/it, loss=0.1218, lr=4.00e-08, step=8]Training:   0%|          | 9/200000 [01:35<158:38:07,  2.86s/it, loss=0.1218, lr=4.00e-08, step=8]Training:   0%|          | 9/200000 [01:35<158:38:07,  2.86s/it, loss=0.3977, lr=4.50e-08, step=9]Training:   0%|          | 10/200000 [01:36<128:05:28,  2.31s/it, loss=0.3977, lr=4.50e-08, step=9]Training:   0%|          | 10/200000 [01:36<128:05:28,  2.31s/it, loss=0.3405, lr=5.00e-08, step=10]Training:   0%|          | 11/200000 [01:37<113:16:48,  2.04s/it, loss=0.3405, lr=5.00e-08, step=10]Training:   0%|          | 11/200000 [01:37<113:16:48,  2.04s/it, loss=0.1998, lr=5.50e-08, step=11]Training:   0%|          | 12/200000 [01:38<97:03:04,  1.75s/it, loss=0.1998, lr=5.50e-08, step=11] Training:   0%|          | 12/200000 [01:38<97:03:04,  1.75s/it, loss=0.2788, lr=6.00e-08, step=12]Training:   0%|          | 13/200000 [01:40<90:42:58,  1.63s/it, loss=0.2788, lr=6.00e-08, step=12]Training:   0%|          | 13/200000 [01:40<90:42:58,  1.63s/it, loss=0.4794, lr=6.50e-08, step=13]Training:   0%|          | 14/200000 [01:41<81:24:31,  1.47s/it, loss=0.4794, lr=6.50e-08, step=13]Training:   0%|          | 14/200000 [01:41<81:24:31,  1.47s/it, loss=0.3434, lr=7.00e-08, step=14]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 15/200000 [01:42<78:54:54,  1.42s/it, loss=0.3434, lr=7.00e-08, step=14]Training:   0%|          | 15/200000 [01:42<78:54:54,  1.42s/it, loss=0.1873, lr=7.50e-08, step=15]Training:   0%|          | 16/200000 [01:43<73:09:54,  1.32s/it, loss=0.1873, lr=7.50e-08, step=15]Training:   0%|          | 16/200000 [01:43<73:09:54,  1.32s/it, loss=0.3678, lr=8.00e-08, step=16]Training:   0%|          | 17/200000 [01:44<74:54:23,  1.35s/it, loss=0.3678, lr=8.00e-08, step=16]Training:   0%|          | 17/200000 [01:44<74:54:23,  1.35s/it, loss=0.1698, lr=8.50e-08, step=17]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 18/200000 [01:46<76:18:42,  1.37s/it, loss=0.1698, lr=8.50e-08, step=17]Training:   0%|          | 18/200000 [01:46<76:18:42,  1.37s/it, loss=0.1900, lr=9.00e-08, step=18]Training:   0%|          | 19/200000 [01:47<71:21:03,  1.28s/it, loss=0.1900, lr=9.00e-08, step=18]Training:   0%|          | 19/200000 [01:47<71:21:03,  1.28s/it, loss=0.1636, lr=9.50e-08, step=19]Training:   0%|          | 20/200000 [01:48<74:14:15,  1.34s/it, loss=0.1636, lr=9.50e-08, step=19]Training:   0%|          | 20/200000 [01:48<74:14:15,  1.34s/it, loss=0.5031, lr=1.00e-07, step=20]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 21/200000 [01:49<69:51:56,  1.26s/it, loss=0.5031, lr=1.00e-07, step=20]Training:   0%|          | 21/200000 [01:49<69:51:56,  1.26s/it, loss=0.2895, lr=1.05e-07, step=21]Training:   0%|          | 22/200000 [01:51<73:03:51,  1.32s/it, loss=0.2895, lr=1.05e-07, step=21]Training:   0%|          | 22/200000 [01:51<73:03:51,  1.32s/it, loss=0.2553, lr=1.10e-07, step=22]Training:   0%|          | 23/200000 [01:52<69:07:17,  1.24s/it, loss=0.2553, lr=1.10e-07, step=22]Training:   0%|          | 23/200000 [01:52<69:07:17,  1.24s/it, loss=0.3572, lr=1.15e-07, step=23]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 24/200000 [01:53<72:27:58,  1.30s/it, loss=0.3572, lr=1.15e-07, step=23]Training:   0%|          | 24/200000 [01:53<72:27:58,  1.30s/it, loss=0.6004, lr=1.20e-07, step=24]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 25/200000 [01:54<68:36:43,  1.24s/it, loss=0.6004, lr=1.20e-07, step=24]Training:   0%|          | 25/200000 [01:55<68:36:43,  1.24s/it, loss=0.2691, lr=1.25e-07, step=25]Training:   0%|          | 26/200000 [01:56<70:44:10,  1.27s/it, loss=0.2691, lr=1.25e-07, step=25]Training:   0%|          | 26/200000 [01:56<70:44:10,  1.27s/it, loss=0.2478, lr=1.30e-07, step=26]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 27/200000 [01:57<71:37:40,  1.29s/it, loss=0.2478, lr=1.30e-07, step=26]Training:   0%|          | 27/200000 [01:57<71:37:40,  1.29s/it, loss=0.2107, lr=1.35e-07, step=27]Training:   0%|          | 28/200000 [01:58<68:03:12,  1.23s/it, loss=0.2107, lr=1.35e-07, step=27]Training:   0%|          | 28/200000 [01:58<68:03:12,  1.23s/it, loss=0.2926, lr=1.40e-07, step=28]Training:   0%|          | 29/200000 [02:00<72:13:35,  1.30s/it, loss=0.2926, lr=1.40e-07, step=28]Training:   0%|          | 29/200000 [02:00<72:13:35,  1.30s/it, loss=0.2787, lr=1.45e-07, step=29]Training:   0%|          | 30/200000 [02:01<68:28:47,  1.23s/it, loss=0.2787, lr=1.45e-07, step=29]Training:   0%|          | 30/200000 [02:01<68:28:47,  1.23s/it, loss=0.2407, lr=1.50e-07, step=30]Training:   0%|          | 31/200000 [02:02<73:18:10,  1.32s/it, loss=0.2407, lr=1.50e-07, step=30]Training:   0%|          | 31/200000 [02:02<73:18:10,  1.32s/it, loss=0.2091, lr=1.55e-07, step=31]Training:   0%|          | 32/200000 [02:04<72:53:39,  1.31s/it, loss=0.2091, lr=1.55e-07, step=31]Training:   0%|          | 32/200000 [02:04<72:53:39,  1.31s/it, loss=0.3468, lr=1.60e-07, step=32]Training:   0%|          | 33/200000 [02:05<68:57:09,  1.24s/it, loss=0.3468, lr=1.60e-07, step=32]Training:   0%|          | 33/200000 [02:05<68:57:09,  1.24s/it, loss=0.1764, lr=1.65e-07, step=33]Training:   0%|          | 34/200000 [02:06<71:43:06,  1.29s/it, loss=0.1764, lr=1.65e-07, step=33]Training:   0%|          | 34/200000 [02:06<71:43:06,  1.29s/it, loss=0.1706, lr=1.70e-07, step=34]Training:   0%|          | 35/200000 [02:07<68:06:11,  1.23s/it, loss=0.1706, lr=1.70e-07, step=34]Training:   0%|          | 35/200000 [02:07<68:06:11,  1.23s/it, loss=0.1687, lr=1.75e-07, step=35]Training:   0%|          | 36/200000 [02:08<65:36:10,  1.18s/it, loss=0.1687, lr=1.75e-07, step=35]Training:   0%|          | 36/200000 [02:08<65:36:10,  1.18s/it, loss=0.1489, lr=1.80e-07, step=36]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 37/200000 [02:10<69:59:30,  1.26s/it, loss=0.1489, lr=1.80e-07, step=36]Training:   0%|          | 37/200000 [02:10<69:59:30,  1.26s/it, loss=0.1580, lr=1.85e-07, step=37]Training:   0%|          | 38/200000 [02:11<72:17:17,  1.30s/it, loss=0.1580, lr=1.85e-07, step=37]Training:   0%|          | 38/200000 [02:11<72:17:17,  1.30s/it, loss=0.2383, lr=1.90e-07, step=38]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 39/200000 [02:12<73:36:18,  1.33s/it, loss=0.2383, lr=1.90e-07, step=38]Training:   0%|          | 39/200000 [02:12<73:36:18,  1.33s/it, loss=0.1249, lr=1.95e-07, step=39]Training:   0%|          | 40/200000 [02:14<69:30:04,  1.25s/it, loss=0.1249, lr=1.95e-07, step=39]Training:   0%|          | 40/200000 [02:14<69:30:04,  1.25s/it, loss=0.1916, lr=2.00e-07, step=40]Training:   0%|          | 41/200000 [02:15<72:51:09,  1.31s/it, loss=0.1916, lr=2.00e-07, step=40]Training:   0%|          | 41/200000 [02:15<72:51:09,  1.31s/it, loss=0.3376, lr=2.05e-07, step=41]Training:   0%|          | 42/200000 [02:16<69:00:44,  1.24s/it, loss=0.3376, lr=2.05e-07, step=41]Training:   0%|          | 42/200000 [02:16<69:00:44,  1.24s/it, loss=0.2866, lr=2.10e-07, step=42]Training:   0%|          | 43/200000 [02:18<73:01:15,  1.31s/it, loss=0.2866, lr=2.10e-07, step=42]Training:   0%|          | 43/200000 [02:18<73:01:15,  1.31s/it, loss=0.1470, lr=2.15e-07, step=43]Training:   0%|          | 44/200000 [02:19<69:05:49,  1.24s/it, loss=0.1470, lr=2.15e-07, step=43]Training:   0%|          | 44/200000 [02:19<69:05:49,  1.24s/it, loss=0.2266, lr=2.20e-07, step=44]Training:   0%|          | 45/200000 [02:20<72:45:55,  1.31s/it, loss=0.2266, lr=2.20e-07, step=44]Training:   0%|          | 45/200000 [02:20<72:45:55,  1.31s/it, loss=0.1543, lr=2.25e-07, step=45]Training:   0%|          | 46/200000 [02:21<68:52:26,  1.24s/it, loss=0.1543, lr=2.25e-07, step=45]Training:   0%|          | 46/200000 [02:21<68:52:26,  1.24s/it, loss=0.2842, lr=2.30e-07, step=46]Training:   0%|          | 47/200000 [02:23<70:09:53,  1.26s/it, loss=0.2842, lr=2.30e-07, step=46]Training:   0%|          | 47/200000 [02:23<70:09:53,  1.26s/it, loss=0.1872, lr=2.35e-07, step=47]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 48/200000 [02:24<71:24:44,  1.29s/it, loss=0.1872, lr=2.35e-07, step=47]Training:   0%|          | 48/200000 [02:24<71:24:44,  1.29s/it, loss=0.2995, lr=2.40e-07, step=48]Training:   0%|          | 49/200000 [02:25<67:54:55,  1.22s/it, loss=0.2995, lr=2.40e-07, step=48]Training:   0%|          | 49/200000 [02:25<67:54:55,  1.22s/it, loss=0.1426, lr=2.45e-07, step=49]Training:   0%|          | 50/200000 [02:26<73:28:50,  1.32s/it, loss=0.1426, lr=2.45e-07, step=49]Training:   0%|          | 50/200000 [02:26<73:28:50,  1.32s/it, loss=0.2352, lr=2.50e-07, step=50]Training:   0%|          | 51/200000 [02:28<69:26:33,  1.25s/it, loss=0.2352, lr=2.50e-07, step=50]Training:   0%|          | 51/200000 [02:28<69:26:33,  1.25s/it, loss=0.1115, lr=2.55e-07, step=51]Training:   0%|          | 52/200000 [02:29<73:57:14,  1.33s/it, loss=0.1115, lr=2.55e-07, step=51]Training:   0%|          | 52/200000 [02:29<73:57:14,  1.33s/it, loss=0.3166, lr=2.60e-07, step=52]Training:   0%|          | 53/200000 [02:31<75:21:52,  1.36s/it, loss=0.3166, lr=2.60e-07, step=52]Training:   0%|          | 53/200000 [02:31<75:21:52,  1.36s/it, loss=0.1272, lr=2.65e-07, step=53]Training:   0%|          | 54/200000 [02:32<70:41:18,  1.27s/it, loss=0.1272, lr=2.65e-07, step=53]Training:   0%|          | 54/200000 [02:32<70:41:18,  1.27s/it, loss=0.2664, lr=2.70e-07, step=54]Training:   0%|          | 55/200000 [02:33<72:37:26,  1.31s/it, loss=0.2664, lr=2.70e-07, step=54]Training:   0%|          | 55/200000 [02:33<72:37:26,  1.31s/it, loss=0.1297, lr=2.75e-07, step=55]Training:   0%|          | 56/200000 [02:34<68:45:17,  1.24s/it, loss=0.1297, lr=2.75e-07, step=55]Training:   0%|          | 56/200000 [02:34<68:45:17,  1.24s/it, loss=0.4008, lr=2.80e-07, step=56]Training:   0%|          | 57/200000 [02:35<66:03:59,  1.19s/it, loss=0.4008, lr=2.80e-07, step=56]Training:   0%|          | 57/200000 [02:35<66:03:59,  1.19s/it, loss=0.3467, lr=2.85e-07, step=57]Training:   0%|          | 58/200000 [02:37<70:00:16,  1.26s/it, loss=0.3467, lr=2.85e-07, step=57]Training:   0%|          | 58/200000 [02:37<70:00:16,  1.26s/it, loss=0.3996, lr=2.90e-07, step=58]Training:   0%|          | 59/200000 [02:38<71:20:34,  1.28s/it, loss=0.3996, lr=2.90e-07, step=58]Training:   0%|          | 59/200000 [02:38<71:20:34,  1.28s/it, loss=0.2399, lr=2.95e-07, step=59]Training:   0%|          | 60/200000 [02:39<73:24:21,  1.32s/it, loss=0.2399, lr=2.95e-07, step=59]Training:   0%|          | 60/200000 [02:39<73:24:21,  1.32s/it, loss=0.2153, lr=3.00e-07, step=60]Training:   0%|          | 61/200000 [02:40<69:16:07,  1.25s/it, loss=0.2153, lr=3.00e-07, step=60]Training:   0%|          | 61/200000 [02:40<69:16:07,  1.25s/it, loss=0.1757, lr=3.05e-07, step=61]Training:   0%|          | 62/200000 [02:42<72:45:42,  1.31s/it, loss=0.1757, lr=3.05e-07, step=61]Training:   0%|          | 62/200000 [02:42<72:45:42,  1.31s/it, loss=0.5530, lr=3.10e-07, step=62]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 63/200000 [02:43<73:13:17,  1.32s/it, loss=0.5530, lr=3.10e-07, step=62]Training:   0%|          | 63/200000 [02:43<73:13:17,  1.32s/it, loss=0.2592, lr=3.15e-07, step=63]Training:   0%|          | 64/200000 [02:44<69:10:20,  1.25s/it, loss=0.2592, lr=3.15e-07, step=63]Training:   0%|          | 64/200000 [02:44<69:10:20,  1.25s/it, loss=0.2725, lr=3.20e-07, step=64]Training:   0%|          | 65/200000 [02:45<66:20:25,  1.19s/it, loss=0.2725, lr=3.20e-07, step=64]Training:   0%|          | 65/200000 [02:45<66:20:25,  1.19s/it, loss=0.2556, lr=3.25e-07, step=65]Training:   0%|          | 66/200000 [02:47<74:26:13,  1.34s/it, loss=0.2556, lr=3.25e-07, step=65]Training:   0%|          | 66/200000 [02:47<74:26:13,  1.34s/it, loss=0.1464, lr=3.30e-07, step=66]Training:   0%|          | 67/200000 [02:48<70:02:27,  1.26s/it, loss=0.1464, lr=3.30e-07, step=66]Training:   0%|          | 67/200000 [02:48<70:02:27,  1.26s/it, loss=0.2983, lr=3.35e-07, step=67]Training:   0%|          | 68/200000 [02:49<70:53:27,  1.28s/it, loss=0.2983, lr=3.35e-07, step=67]Training:   0%|          | 68/200000 [02:49<70:53:27,  1.28s/it, loss=0.1296, lr=3.40e-07, step=68]Training:   0%|          | 69/200000 [02:50<67:34:16,  1.22s/it, loss=0.1296, lr=3.40e-07, step=68]Training:   0%|          | 69/200000 [02:50<67:34:16,  1.22s/it, loss=0.1966, lr=3.45e-07, step=69]Training:   0%|          | 70/200000 [02:52<70:56:53,  1.28s/it, loss=0.1966, lr=3.45e-07, step=69]Training:   0%|          | 70/200000 [02:52<70:56:53,  1.28s/it, loss=0.2845, lr=3.50e-07, step=70]Training:   0%|          | 71/200000 [02:53<73:45:35,  1.33s/it, loss=0.2845, lr=3.50e-07, step=70]Training:   0%|          | 71/200000 [02:53<73:45:35,  1.33s/it, loss=0.2418, lr=3.55e-07, step=71]Training:   0%|          | 72/200000 [02:54<69:30:48,  1.25s/it, loss=0.2418, lr=3.55e-07, step=71]Training:   0%|          | 72/200000 [02:54<69:30:48,  1.25s/it, loss=0.1319, lr=3.60e-07, step=72]Training:   0%|          | 73/200000 [02:56<72:31:54,  1.31s/it, loss=0.1319, lr=3.60e-07, step=72]Training:   0%|          | 73/200000 [02:56<72:31:54,  1.31s/it, loss=0.1927, lr=3.65e-07, step=73]Training:   0%|          | 74/200000 [02:57<73:40:56,  1.33s/it, loss=0.1927, lr=3.65e-07, step=73]Training:   0%|          | 74/200000 [02:57<73:40:56,  1.33s/it, loss=0.2975, lr=3.70e-07, step=74]Training:   0%|          | 75/200000 [02:58<69:31:47,  1.25s/it, loss=0.2975, lr=3.70e-07, step=74]Training:   0%|          | 75/200000 [02:58<69:31:47,  1.25s/it, loss=0.2397, lr=3.75e-07, step=75]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 76/200000 [03:00<72:22:29,  1.30s/it, loss=0.2397, lr=3.75e-07, step=75]Training:   0%|          | 76/200000 [03:00<72:22:29,  1.30s/it, loss=0.2519, lr=3.80e-07, step=76]Training:   0%|          | 77/200000 [03:01<75:21:20,  1.36s/it, loss=0.2519, lr=3.80e-07, step=76]Training:   0%|          | 77/200000 [03:01<75:21:20,  1.36s/it, loss=0.3778, lr=3.85e-07, step=77]Training:   0%|          | 78/200000 [03:02<70:42:49,  1.27s/it, loss=0.3778, lr=3.85e-07, step=77]Training:   0%|          | 78/200000 [03:02<70:42:49,  1.27s/it, loss=0.7810, lr=3.90e-07, step=78]Training:   0%|          | 79/200000 [03:04<72:20:19,  1.30s/it, loss=0.7810, lr=3.90e-07, step=78]Training:   0%|          | 79/200000 [03:04<72:20:19,  1.30s/it, loss=0.1674, lr=3.95e-07, step=79]Training:   0%|          | 80/200000 [03:05<72:54:49,  1.31s/it, loss=0.1674, lr=3.95e-07, step=79]Training:   0%|          | 80/200000 [03:05<72:54:49,  1.31s/it, loss=0.1772, lr=4.00e-07, step=80]Training:   0%|          | 81/200000 [03:06<69:00:22,  1.24s/it, loss=0.1772, lr=4.00e-07, step=80]Training:   0%|          | 81/200000 [03:06<69:00:22,  1.24s/it, loss=0.1569, lr=4.05e-07, step=81]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 82/200000 [03:08<74:16:58,  1.34s/it, loss=0.1569, lr=4.05e-07, step=81]Training:   0%|          | 82/200000 [03:08<74:16:58,  1.34s/it, loss=0.1933, lr=4.10e-07, step=82]Training:   0%|          | 83/200000 [03:09<77:31:00,  1.40s/it, loss=0.1933, lr=4.10e-07, step=82]Training:   0%|          | 83/200000 [03:09<77:31:00,  1.40s/it, loss=0.1091, lr=4.15e-07, step=83]Training:   0%|          | 84/200000 [03:10<72:10:10,  1.30s/it, loss=0.1091, lr=4.15e-07, step=83]Training:   0%|          | 84/200000 [03:10<72:10:10,  1.30s/it, loss=0.1705, lr=4.20e-07, step=84]Training:   0%|          | 85/200000 [03:12<75:18:52,  1.36s/it, loss=0.1705, lr=4.20e-07, step=84]Training:   0%|          | 85/200000 [03:12<75:18:52,  1.36s/it, loss=0.2376, lr=4.25e-07, step=85]Training:   0%|          | 86/200000 [03:13<76:24:17,  1.38s/it, loss=0.2376, lr=4.25e-07, step=85]Training:   0%|          | 86/200000 [03:13<76:24:17,  1.38s/it, loss=0.3546, lr=4.30e-07, step=86]Training:   0%|          | 87/200000 [03:15<76:59:42,  1.39s/it, loss=0.3546, lr=4.30e-07, step=86]Training:   0%|          | 87/200000 [03:15<76:59:42,  1.39s/it, loss=0.3553, lr=4.35e-07, step=87]Training:   0%|          | 88/200000 [03:16<71:50:06,  1.29s/it, loss=0.3553, lr=4.35e-07, step=87]Training:   0%|          | 88/200000 [03:16<71:50:06,  1.29s/it, loss=0.1603, lr=4.40e-07, step=88]Training:   0%|          | 89/200000 [03:17<68:14:27,  1.23s/it, loss=0.1603, lr=4.40e-07, step=88]Training:   0%|          | 89/200000 [03:17<68:14:27,  1.23s/it, loss=0.3004, lr=4.45e-07, step=89]Training:   0%|          | 90/200000 [03:18<71:47:34,  1.29s/it, loss=0.3004, lr=4.45e-07, step=89]Training:   0%|          | 90/200000 [03:18<71:47:34,  1.29s/it, loss=0.2299, lr=4.50e-07, step=90]Training:   0%|          | 91/200000 [03:20<74:13:48,  1.34s/it, loss=0.2299, lr=4.50e-07, step=90]Training:   0%|          | 91/200000 [03:20<74:13:48,  1.34s/it, loss=0.1521, lr=4.55e-07, step=91]Training:   0%|          | 92/200000 [03:21<77:55:10,  1.40s/it, loss=0.1521, lr=4.55e-07, step=91]Training:   0%|          | 92/200000 [03:21<77:55:10,  1.40s/it, loss=0.1534, lr=4.60e-07, step=92]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 93/200000 [03:23<79:29:47,  1.43s/it, loss=0.1534, lr=4.60e-07, step=92]Training:   0%|          | 93/200000 [03:23<79:29:47,  1.43s/it, loss=0.1748, lr=4.65e-07, step=93]Training:   0%|          | 94/200000 [03:24<73:36:29,  1.33s/it, loss=0.1748, lr=4.65e-07, step=93]Training:   0%|          | 94/200000 [03:24<73:36:29,  1.33s/it, loss=0.2126, lr=4.70e-07, step=94]Training:   0%|          | 95/200000 [03:25<74:15:38,  1.34s/it, loss=0.2126, lr=4.70e-07, step=94]Training:   0%|          | 95/200000 [03:25<74:15:38,  1.34s/it, loss=0.4069, lr=4.75e-07, step=95]Training:   0%|          | 96/200000 [03:26<74:23:53,  1.34s/it, loss=0.4069, lr=4.75e-07, step=95]Training:   0%|          | 96/200000 [03:26<74:23:53,  1.34s/it, loss=0.3907, lr=4.80e-07, step=96]Training:   0%|          | 97/200000 [03:28<70:00:30,  1.26s/it, loss=0.3907, lr=4.80e-07, step=96]Training:   0%|          | 97/200000 [03:28<70:00:30,  1.26s/it, loss=0.1094, lr=4.85e-07, step=97]Training:   0%|          | 98/200000 [03:29<73:30:16,  1.32s/it, loss=0.1094, lr=4.85e-07, step=97]Training:   0%|          | 98/200000 [03:29<73:30:16,  1.32s/it, loss=0.2481, lr=4.90e-07, step=98]Training:   0%|          | 99/200000 [03:30<69:24:06,  1.25s/it, loss=0.2481, lr=4.90e-07, step=98]Training:   0%|          | 99/200000 [03:30<69:24:06,  1.25s/it, loss=0.2268, lr=4.95e-07, step=99]Training:   0%|          | 100/200000 [03:31<71:17:45,  1.28s/it, loss=0.2268, lr=4.95e-07, step=99]Training:   0%|          | 100/200000 [03:31<71:17:45,  1.28s/it, loss=0.2475, lr=5.00e-07, step=100]22:56:46.560 [I] step=100 loss=0.2532 lr=2.57e-07 grad_norm=4.30 time=128.8s                      (701675:train_pytorch.py:582)
Training:   0%|          | 101/200000 [03:33<72:05:34,  1.30s/it, loss=0.2475, lr=5.00e-07, step=100]Training:   0%|          | 101/200000 [03:33<72:05:34,  1.30s/it, loss=0.1979, lr=5.05e-07, step=101]Training:   0%|          | 102/200000 [03:34<74:52:34,  1.35s/it, loss=0.1979, lr=5.05e-07, step=101]Training:   0%|          | 102/200000 [03:34<74:52:34,  1.35s/it, loss=0.2758, lr=5.10e-07, step=102]Training:   0%|          | 103/200000 [03:36<75:37:00,  1.36s/it, loss=0.2758, lr=5.10e-07, step=102]Training:   0%|          | 103/200000 [03:36<75:37:00,  1.36s/it, loss=0.1728, lr=5.15e-07, step=103]Training:   0%|          | 104/200000 [03:37<70:53:54,  1.28s/it, loss=0.1728, lr=5.15e-07, step=103]Training:   0%|          | 104/200000 [03:37<70:53:54,  1.28s/it, loss=0.1443, lr=5.20e-07, step=104]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 105/200000 [03:38<67:49:30,  1.22s/it, loss=0.1443, lr=5.20e-07, step=104]Training:   0%|          | 105/200000 [03:38<67:49:30,  1.22s/it, loss=0.3002, lr=5.25e-07, step=105]Training:   0%|          | 106/200000 [03:39<71:29:22,  1.29s/it, loss=0.3002, lr=5.25e-07, step=105]Training:   0%|          | 106/200000 [03:39<71:29:22,  1.29s/it, loss=0.1657, lr=5.30e-07, step=106]Training:   0%|          | 107/200000 [03:41<72:42:00,  1.31s/it, loss=0.1657, lr=5.30e-07, step=106]Training:   0%|          | 107/200000 [03:41<72:42:00,  1.31s/it, loss=0.1256, lr=5.35e-07, step=107]Training:   0%|          | 108/200000 [03:42<73:50:12,  1.33s/it, loss=0.1256, lr=5.35e-07, step=107]Training:   0%|          | 108/200000 [03:42<73:50:12,  1.33s/it, loss=0.2927, lr=5.40e-07, step=108]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 109/200000 [03:43<69:58:58,  1.26s/it, loss=0.2927, lr=5.40e-07, step=108]Training:   0%|          | 109/200000 [03:43<69:58:58,  1.26s/it, loss=0.2310, lr=5.45e-07, step=109]Training:   0%|          | 110/200000 [03:44<66:54:14,  1.20s/it, loss=0.2310, lr=5.45e-07, step=109]Training:   0%|          | 110/200000 [03:44<66:54:14,  1.20s/it, loss=0.1856, lr=5.50e-07, step=110]Training:   0%|          | 111/200000 [03:46<70:41:45,  1.27s/it, loss=0.1856, lr=5.50e-07, step=110]Training:   0%|          | 111/200000 [03:46<70:41:45,  1.27s/it, loss=0.3131, lr=5.55e-07, step=111]Training:   0%|          | 112/200000 [03:47<72:04:36,  1.30s/it, loss=0.3131, lr=5.55e-07, step=111]Training:   0%|          | 112/200000 [03:47<72:04:36,  1.30s/it, loss=0.2779, lr=5.60e-07, step=112]Training:   0%|          | 113/200000 [03:48<75:31:28,  1.36s/it, loss=0.2779, lr=5.60e-07, step=112]Training:   0%|          | 113/200000 [03:48<75:31:28,  1.36s/it, loss=0.2780, lr=5.65e-07, step=113]Training:   0%|          | 114/200000 [03:50<76:49:22,  1.38s/it, loss=0.2780, lr=5.65e-07, step=113]Training:   0%|          | 114/200000 [03:50<76:49:22,  1.38s/it, loss=0.1873, lr=5.70e-07, step=114]Training:   0%|          | 115/200000 [03:51<72:03:16,  1.30s/it, loss=0.1873, lr=5.70e-07, step=114]Training:   0%|          | 115/200000 [03:51<72:03:16,  1.30s/it, loss=0.4602, lr=5.75e-07, step=115]Training:   0%|          | 116/200000 [03:52<72:49:39,  1.31s/it, loss=0.4602, lr=5.75e-07, step=115]Training:   0%|          | 116/200000 [03:52<72:49:39,  1.31s/it, loss=0.3044, lr=5.80e-07, step=116]Training:   0%|          | 117/200000 [03:53<69:11:51,  1.25s/it, loss=0.3044, lr=5.80e-07, step=116]Training:   0%|          | 117/200000 [03:53<69:11:51,  1.25s/it, loss=0.1554, lr=5.85e-07, step=117]Training:   0%|          | 118/200000 [03:55<72:26:40,  1.30s/it, loss=0.1554, lr=5.85e-07, step=117]Training:   0%|          | 118/200000 [03:55<72:26:40,  1.30s/it, loss=0.0921, lr=5.90e-07, step=118]Training:   0%|          | 119/200000 [03:56<73:52:07,  1.33s/it, loss=0.0921, lr=5.90e-07, step=118]Training:   0%|          | 119/200000 [03:56<73:52:07,  1.33s/it, loss=0.1940, lr=5.95e-07, step=119]Training:   0%|          | 120/200000 [03:57<69:42:11,  1.26s/it, loss=0.1940, lr=5.95e-07, step=119]Training:   0%|          | 120/200000 [03:57<69:42:11,  1.26s/it, loss=0.3213, lr=6.00e-07, step=120]Training:   0%|          | 121/200000 [03:59<70:44:08,  1.27s/it, loss=0.3213, lr=6.00e-07, step=120]Training:   0%|          | 121/200000 [03:59<70:44:08,  1.27s/it, loss=0.7760, lr=6.05e-07, step=121]Training:   0%|          | 122/200000 [04:00<67:27:58,  1.22s/it, loss=0.7760, lr=6.05e-07, step=121]Training:   0%|          | 122/200000 [04:00<67:27:58,  1.22s/it, loss=0.2152, lr=6.10e-07, step=122]Training:   0%|          | 123/200000 [04:01<71:05:28,  1.28s/it, loss=0.2152, lr=6.10e-07, step=122]Training:   0%|          | 123/200000 [04:01<71:05:28,  1.28s/it, loss=0.3047, lr=6.15e-07, step=123]Training:   0%|          | 124/200000 [04:03<75:47:30,  1.37s/it, loss=0.3047, lr=6.15e-07, step=123]Training:   0%|          | 124/200000 [04:03<75:47:30,  1.37s/it, loss=0.1102, lr=6.20e-07, step=124]Training:   0%|          | 125/200000 [04:04<78:04:34,  1.41s/it, loss=0.1102, lr=6.20e-07, step=124]Training:   0%|          | 125/200000 [04:04<78:04:34,  1.41s/it, loss=0.2600, lr=6.25e-07, step=125]Training:   0%|          | 126/200000 [04:05<72:37:04,  1.31s/it, loss=0.2600, lr=6.25e-07, step=125]Training:   0%|          | 126/200000 [04:05<72:37:04,  1.31s/it, loss=0.1183, lr=6.30e-07, step=126]Training:   0%|          | 127/200000 [04:07<73:58:06,  1.33s/it, loss=0.1183, lr=6.30e-07, step=126]Training:   0%|          | 127/200000 [04:07<73:58:06,  1.33s/it, loss=0.5383, lr=6.35e-07, step=127]Training:   0%|          | 128/200000 [04:08<73:50:32,  1.33s/it, loss=0.5383, lr=6.35e-07, step=127]Training:   0%|          | 128/200000 [04:08<73:50:32,  1.33s/it, loss=0.1557, lr=6.40e-07, step=128]Training:   0%|          | 129/200000 [04:09<69:36:28,  1.25s/it, loss=0.1557, lr=6.40e-07, step=128]Training:   0%|          | 129/200000 [04:09<69:36:28,  1.25s/it, loss=0.3354, lr=6.45e-07, step=129]Training:   0%|          | 130/200000 [04:11<72:47:57,  1.31s/it, loss=0.3354, lr=6.45e-07, step=129]Training:   0%|          | 130/200000 [04:11<72:47:57,  1.31s/it, loss=0.5388, lr=6.50e-07, step=130]Training:   0%|          | 131/200000 [04:12<68:53:08,  1.24s/it, loss=0.5388, lr=6.50e-07, step=130]Training:   0%|          | 131/200000 [04:12<68:53:08,  1.24s/it, loss=0.3127, lr=6.55e-07, step=131]Training:   0%|          | 132/200000 [04:13<70:59:53,  1.28s/it, loss=0.3127, lr=6.55e-07, step=131]Training:   0%|          | 132/200000 [04:13<70:59:53,  1.28s/it, loss=0.0789, lr=6.60e-07, step=132]Training:   0%|          | 133/200000 [04:14<71:57:01,  1.30s/it, loss=0.0789, lr=6.60e-07, step=132]Training:   0%|          | 133/200000 [04:14<71:57:01,  1.30s/it, loss=0.1970, lr=6.65e-07, step=133]Training:   0%|          | 134/200000 [04:16<72:58:56,  1.31s/it, loss=0.1970, lr=6.65e-07, step=133]Training:   0%|          | 134/200000 [04:16<72:58:56,  1.31s/it, loss=0.1925, lr=6.70e-07, step=134]Training:   0%|          | 135/200000 [04:17<73:34:44,  1.33s/it, loss=0.1925, lr=6.70e-07, step=134]Training:   0%|          | 135/200000 [04:17<73:34:44,  1.33s/it, loss=0.3000, lr=6.75e-07, step=135]Training:   0%|          | 136/200000 [04:18<69:29:21,  1.25s/it, loss=0.3000, lr=6.75e-07, step=135]Training:   0%|          | 136/200000 [04:18<69:29:21,  1.25s/it, loss=0.2451, lr=6.80e-07, step=136]Training:   0%|          | 137/200000 [04:19<66:39:41,  1.20s/it, loss=0.2451, lr=6.80e-07, step=136]Training:   0%|          | 137/200000 [04:19<66:39:41,  1.20s/it, loss=0.3328, lr=6.85e-07, step=137]Training:   0%|          | 138/200000 [04:21<70:36:22,  1.27s/it, loss=0.3328, lr=6.85e-07, step=137]Training:   0%|          | 138/200000 [04:21<70:36:22,  1.27s/it, loss=0.2919, lr=6.90e-07, step=138]Training:   0%|          | 139/200000 [04:22<72:25:23,  1.30s/it, loss=0.2919, lr=6.90e-07, step=138]Training:   0%|          | 139/200000 [04:22<72:25:23,  1.30s/it, loss=0.1289, lr=6.95e-07, step=139]Training:   0%|          | 140/200000 [04:23<74:06:06,  1.33s/it, loss=0.1289, lr=6.95e-07, step=139]Training:   0%|          | 140/200000 [04:23<74:06:06,  1.33s/it, loss=0.1407, lr=7.00e-07, step=140]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 141/200000 [04:24<69:48:19,  1.26s/it, loss=0.1407, lr=7.00e-07, step=140]Training:   0%|          | 141/200000 [04:24<69:48:19,  1.26s/it, loss=0.2002, lr=7.05e-07, step=141]Training:   0%|          | 142/200000 [04:26<66:45:01,  1.20s/it, loss=0.2002, lr=7.05e-07, step=141]Training:   0%|          | 142/200000 [04:26<66:45:01,  1.20s/it, loss=0.2192, lr=7.10e-07, step=142]Training:   0%|          | 143/200000 [04:27<71:29:20,  1.29s/it, loss=0.2192, lr=7.10e-07, step=142]Training:   0%|          | 143/200000 [04:27<71:29:20,  1.29s/it, loss=0.5775, lr=7.15e-07, step=143]Training:   0%|          | 144/200000 [04:28<73:26:42,  1.32s/it, loss=0.5775, lr=7.15e-07, step=143]Training:   0%|          | 144/200000 [04:28<73:26:42,  1.32s/it, loss=0.2202, lr=7.20e-07, step=144]Training:   0%|          | 145/200000 [04:30<77:23:56,  1.39s/it, loss=0.2202, lr=7.20e-07, step=144]Training:   0%|          | 145/200000 [04:30<77:23:56,  1.39s/it, loss=0.1776, lr=7.25e-07, step=145]Training:   0%|          | 146/200000 [04:31<79:09:43,  1.43s/it, loss=0.1776, lr=7.25e-07, step=145]Training:   0%|          | 146/200000 [04:31<79:09:43,  1.43s/it, loss=0.3086, lr=7.30e-07, step=146]Training:   0%|          | 147/200000 [04:33<73:17:24,  1.32s/it, loss=0.3086, lr=7.30e-07, step=146]Training:   0%|          | 147/200000 [04:33<73:17:24,  1.32s/it, loss=0.1047, lr=7.35e-07, step=147]Training:   0%|          | 148/200000 [04:34<73:52:14,  1.33s/it, loss=0.1047, lr=7.35e-07, step=147]Training:   0%|          | 148/200000 [04:34<73:52:14,  1.33s/it, loss=0.2258, lr=7.40e-07, step=148]Training:   0%|          | 149/200000 [04:35<74:12:55,  1.34s/it, loss=0.2258, lr=7.40e-07, step=148]Training:   0%|          | 149/200000 [04:35<74:12:55,  1.34s/it, loss=0.2150, lr=7.45e-07, step=149]Training:   0%|          | 150/200000 [04:36<69:51:42,  1.26s/it, loss=0.2150, lr=7.45e-07, step=149]Training:   0%|          | 150/200000 [04:36<69:51:42,  1.26s/it, loss=0.2372, lr=7.50e-07, step=150]Training:   0%|          | 151/200000 [04:38<72:13:41,  1.30s/it, loss=0.2372, lr=7.50e-07, step=150]Training:   0%|          | 151/200000 [04:38<72:13:41,  1.30s/it, loss=0.1704, lr=7.55e-07, step=151]Training:   0%|          | 152/200000 [04:39<68:29:22,  1.23s/it, loss=0.1704, lr=7.55e-07, step=151]Training:   0%|          | 152/200000 [04:39<68:29:22,  1.23s/it, loss=0.4528, lr=7.60e-07, step=152]Training:   0%|          | 153/200000 [04:40<69:59:21,  1.26s/it, loss=0.4528, lr=7.60e-07, step=152]Training:   0%|          | 153/200000 [04:40<69:59:21,  1.26s/it, loss=0.1545, lr=7.65e-07, step=153]Training:   0%|          | 154/200000 [04:41<71:24:48,  1.29s/it, loss=0.1545, lr=7.65e-07, step=153]Training:   0%|          | 154/200000 [04:41<71:24:48,  1.29s/it, loss=0.2332, lr=7.70e-07, step=154]Training:   0%|          | 155/200000 [04:43<75:47:47,  1.37s/it, loss=0.2332, lr=7.70e-07, step=154]Training:   0%|          | 155/200000 [04:43<75:47:47,  1.37s/it, loss=0.2551, lr=7.75e-07, step=155]Training:   0%|          | 156/200000 [04:45<77:39:05,  1.40s/it, loss=0.2551, lr=7.75e-07, step=155]Training:   0%|          | 156/200000 [04:45<77:39:05,  1.40s/it, loss=0.1089, lr=7.80e-07, step=156]Training:   0%|          | 157/200000 [04:46<72:14:00,  1.30s/it, loss=0.1089, lr=7.80e-07, step=156]Training:   0%|          | 157/200000 [04:46<72:14:00,  1.30s/it, loss=0.4956, lr=7.85e-07, step=157]Training:   0%|          | 158/200000 [04:47<68:30:27,  1.23s/it, loss=0.4956, lr=7.85e-07, step=157]Training:   0%|          | 158/200000 [04:47<68:30:27,  1.23s/it, loss=0.1473, lr=7.90e-07, step=158]Training:   0%|          | 159/200000 [04:48<71:28:21,  1.29s/it, loss=0.1473, lr=7.90e-07, step=158]Training:   0%|          | 159/200000 [04:48<71:28:21,  1.29s/it, loss=0.2314, lr=7.95e-07, step=159]Training:   0%|          | 160/200000 [04:49<67:56:33,  1.22s/it, loss=0.2314, lr=7.95e-07, step=159]Training:   0%|          | 160/200000 [04:49<67:56:33,  1.22s/it, loss=0.2443, lr=8.00e-07, step=160]Training:   0%|          | 161/200000 [04:51<70:58:33,  1.28s/it, loss=0.2443, lr=8.00e-07, step=160]Training:   0%|          | 161/200000 [04:51<70:58:33,  1.28s/it, loss=0.6325, lr=8.05e-07, step=161]Training:   0%|          | 162/200000 [04:52<67:38:21,  1.22s/it, loss=0.6325, lr=8.05e-07, step=161]Training:   0%|          | 162/200000 [04:52<67:38:21,  1.22s/it, loss=0.2299, lr=8.10e-07, step=162]Training:   0%|          | 163/200000 [04:53<65:18:58,  1.18s/it, loss=0.2299, lr=8.10e-07, step=162]Training:   0%|          | 163/200000 [04:53<65:18:58,  1.18s/it, loss=0.1861, lr=8.15e-07, step=163]Training:   0%|          | 164/200000 [04:54<70:09:16,  1.26s/it, loss=0.1861, lr=8.15e-07, step=163]Training:   0%|          | 164/200000 [04:54<70:09:16,  1.26s/it, loss=0.1483, lr=8.20e-07, step=164]Training:   0%|          | 165/200000 [04:56<72:41:56,  1.31s/it, loss=0.1483, lr=8.20e-07, step=164]Training:   0%|          | 165/200000 [04:56<72:41:56,  1.31s/it, loss=0.2379, lr=8.25e-07, step=165]Training:   0%|          | 166/200000 [04:57<73:55:23,  1.33s/it, loss=0.2379, lr=8.25e-07, step=165]Training:   0%|          | 166/200000 [04:57<73:55:23,  1.33s/it, loss=0.1732, lr=8.30e-07, step=166]Training:   0%|          | 167/200000 [04:58<69:38:39,  1.25s/it, loss=0.1732, lr=8.30e-07, step=166]Training:   0%|          | 167/200000 [04:58<69:38:39,  1.25s/it, loss=0.1457, lr=8.35e-07, step=167]Training:   0%|          | 168/200000 [04:59<66:45:16,  1.20s/it, loss=0.1457, lr=8.35e-07, step=167]Training:   0%|          | 168/200000 [04:59<66:45:16,  1.20s/it, loss=0.1743, lr=8.40e-07, step=168]Training:   0%|          | 169/200000 [05:00<68:31:18,  1.23s/it, loss=0.1743, lr=8.40e-07, step=168]Training:   0%|          | 169/200000 [05:00<68:31:18,  1.23s/it, loss=0.1637, lr=8.45e-07, step=169]Training:   0%|          | 170/200000 [05:02<70:14:55,  1.27s/it, loss=0.1637, lr=8.45e-07, step=169]Training:   0%|          | 170/200000 [05:02<70:14:55,  1.27s/it, loss=0.1817, lr=8.50e-07, step=170]Training:   0%|          | 171/200000 [05:03<67:07:16,  1.21s/it, loss=0.1817, lr=8.50e-07, step=170]Training:   0%|          | 171/200000 [05:03<67:07:16,  1.21s/it, loss=0.1010, lr=8.55e-07, step=171]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 172/200000 [05:04<69:47:17,  1.26s/it, loss=0.1010, lr=8.55e-07, step=171]Training:   0%|          | 172/200000 [05:04<69:47:17,  1.26s/it, loss=0.1946, lr=8.60e-07, step=172]Training:   0%|          | 173/200000 [05:05<66:46:16,  1.20s/it, loss=0.1946, lr=8.60e-07, step=172]Training:   0%|          | 173/200000 [05:05<66:46:16,  1.20s/it, loss=0.1740, lr=8.65e-07, step=173]Training:   0%|          | 174/200000 [05:07<68:43:23,  1.24s/it, loss=0.1740, lr=8.65e-07, step=173]Training:   0%|          | 174/200000 [05:07<68:43:23,  1.24s/it, loss=0.2910, lr=8.70e-07, step=174]Training:   0%|          | 175/200000 [05:08<66:02:53,  1.19s/it, loss=0.2910, lr=8.70e-07, step=174]Training:   0%|          | 175/200000 [05:08<66:02:53,  1.19s/it, loss=0.1438, lr=8.75e-07, step=175]Training:   0%|          | 176/200000 [05:09<70:43:33,  1.27s/it, loss=0.1438, lr=8.75e-07, step=175]Training:   0%|          | 176/200000 [05:09<70:43:33,  1.27s/it, loss=0.1688, lr=8.80e-07, step=176]Training:   0%|          | 177/200000 [05:11<72:55:02,  1.31s/it, loss=0.1688, lr=8.80e-07, step=176]Training:   0%|          | 177/200000 [05:11<72:55:02,  1.31s/it, loss=0.1198, lr=8.85e-07, step=177]Training:   0%|          | 178/200000 [05:12<68:59:37,  1.24s/it, loss=0.1198, lr=8.85e-07, step=177]Training:   0%|          | 178/200000 [05:12<68:59:37,  1.24s/it, loss=0.1509, lr=8.90e-07, step=178]Training:   0%|          | 179/200000 [05:13<66:14:08,  1.19s/it, loss=0.1509, lr=8.90e-07, step=178]Training:   0%|          | 179/200000 [05:13<66:14:08,  1.19s/it, loss=0.2716, lr=8.95e-07, step=179]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 180/200000 [05:14<69:03:46,  1.24s/it, loss=0.2716, lr=8.95e-07, step=179]Training:   0%|          | 180/200000 [05:14<69:03:46,  1.24s/it, loss=0.2154, lr=9.00e-07, step=180]Training:   0%|          | 181/200000 [05:15<71:19:56,  1.29s/it, loss=0.2154, lr=9.00e-07, step=180]Training:   0%|          | 181/200000 [05:15<71:19:56,  1.29s/it, loss=0.0814, lr=9.05e-07, step=181]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 182/200000 [05:17<67:53:15,  1.22s/it, loss=0.0814, lr=9.05e-07, step=181]Training:   0%|          | 182/200000 [05:17<67:53:15,  1.22s/it, loss=0.1790, lr=9.10e-07, step=182]Training:   0%|          | 183/200000 [05:18<70:57:32,  1.28s/it, loss=0.1790, lr=9.10e-07, step=182]Training:   0%|          | 183/200000 [05:18<70:57:32,  1.28s/it, loss=0.4307, lr=9.15e-07, step=183]Training:   0%|          | 184/200000 [05:19<67:34:06,  1.22s/it, loss=0.4307, lr=9.15e-07, step=183]Training:   0%|          | 184/200000 [05:19<67:34:06,  1.22s/it, loss=0.1660, lr=9.20e-07, step=184]Training:   0%|          | 185/200000 [05:20<70:05:54,  1.26s/it, loss=0.1660, lr=9.20e-07, step=184]Training:   0%|          | 185/200000 [05:20<70:05:54,  1.26s/it, loss=0.1277, lr=9.25e-07, step=185]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 186/200000 [05:22<71:12:39,  1.28s/it, loss=0.1277, lr=9.25e-07, step=185]Training:   0%|          | 186/200000 [05:22<71:12:39,  1.28s/it, loss=0.1336, lr=9.30e-07, step=186]Training:   0%|          | 187/200000 [05:23<75:29:00,  1.36s/it, loss=0.1336, lr=9.30e-07, step=186]Training:   0%|          | 187/200000 [05:23<75:29:00,  1.36s/it, loss=0.0716, lr=9.35e-07, step=187]Training:   0%|          | 188/200000 [05:25<77:22:36,  1.39s/it, loss=0.0716, lr=9.35e-07, step=187]Training:   0%|          | 188/200000 [05:25<77:22:36,  1.39s/it, loss=0.2307, lr=9.40e-07, step=188]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 189/200000 [05:26<72:04:21,  1.30s/it, loss=0.2307, lr=9.40e-07, step=188]Training:   0%|          | 189/200000 [05:26<72:04:21,  1.30s/it, loss=0.1768, lr=9.45e-07, step=189]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 190/200000 [05:27<68:20:50,  1.23s/it, loss=0.1768, lr=9.45e-07, step=189]Training:   0%|          | 190/200000 [05:27<68:20:50,  1.23s/it, loss=0.2677, lr=9.50e-07, step=190]Training:   0%|          | 191/200000 [05:28<71:01:43,  1.28s/it, loss=0.2677, lr=9.50e-07, step=190]Training:   0%|          | 191/200000 [05:28<71:01:43,  1.28s/it, loss=0.2315, lr=9.55e-07, step=191]Training:   0%|          | 192/200000 [05:29<67:37:05,  1.22s/it, loss=0.2315, lr=9.55e-07, step=191]Training:   0%|          | 192/200000 [05:29<67:37:05,  1.22s/it, loss=0.1124, lr=9.60e-07, step=192]Training:   0%|          | 193/200000 [05:31<70:49:25,  1.28s/it, loss=0.1124, lr=9.60e-07, step=192]Training:   0%|          | 193/200000 [05:31<70:49:25,  1.28s/it, loss=0.1047, lr=9.65e-07, step=193]Training:   0%|          | 194/200000 [05:32<67:32:04,  1.22s/it, loss=0.1047, lr=9.65e-07, step=193]Training:   0%|          | 194/200000 [05:32<67:32:04,  1.22s/it, loss=0.2555, lr=9.70e-07, step=194]Training:   0%|          | 195/200000 [05:33<65:14:47,  1.18s/it, loss=0.2555, lr=9.70e-07, step=194]Training:   0%|          | 195/200000 [05:33<65:14:47,  1.18s/it, loss=0.1590, lr=9.75e-07, step=195]Training:   0%|          | 196/200000 [05:34<69:55:06,  1.26s/it, loss=0.1590, lr=9.75e-07, step=195]Training:   0%|          | 196/200000 [05:34<69:55:06,  1.26s/it, loss=0.1560, lr=9.80e-07, step=196]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 197/200000 [05:36<72:48:16,  1.31s/it, loss=0.1560, lr=9.80e-07, step=196]Training:   0%|          | 197/200000 [05:36<72:48:16,  1.31s/it, loss=0.1950, lr=9.85e-07, step=197]Training:   0%|          | 198/200000 [05:37<74:20:48,  1.34s/it, loss=0.1950, lr=9.85e-07, step=197]Training:   0%|          | 198/200000 [05:37<74:20:48,  1.34s/it, loss=0.1581, lr=9.90e-07, step=198]Training:   0%|          | 199/200000 [05:38<69:55:57,  1.26s/it, loss=0.1581, lr=9.90e-07, step=198]Training:   0%|          | 199/200000 [05:38<69:55:57,  1.26s/it, loss=0.0919, lr=9.95e-07, step=199]Training:   0%|          | 200/200000 [05:39<66:49:58,  1.20s/it, loss=0.0919, lr=9.95e-07, step=199]Training:   0%|          | 200/200000 [05:39<66:49:58,  1.20s/it, loss=0.2146, lr=1.00e-06, step=200]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
22:58:54.569 [I] step=200 loss=0.2297 lr=7.57e-07 grad_norm=3.33 time=128.0s                      (701675:train_pytorch.py:582)
Training:   0%|          | 201/200000 [05:41<69:29:11,  1.25s/it, loss=0.2146, lr=1.00e-06, step=200]Training:   0%|          | 201/200000 [05:41<69:29:11,  1.25s/it, loss=0.2889, lr=1.00e-06, step=201]Training:   0%|          | 202/200000 [05:42<71:29:15,  1.29s/it, loss=0.2889, lr=1.00e-06, step=201]Training:   0%|          | 202/200000 [05:42<71:29:15,  1.29s/it, loss=0.2549, lr=1.01e-06, step=202]Training:   0%|          | 203/200000 [05:43<67:57:46,  1.22s/it, loss=0.2549, lr=1.01e-06, step=202]Training:   0%|          | 203/200000 [05:43<67:57:46,  1.22s/it, loss=0.1446, lr=1.01e-06, step=203]Training:   0%|          | 204/200000 [05:45<71:33:11,  1.29s/it, loss=0.1446, lr=1.01e-06, step=203]Training:   0%|          | 204/200000 [05:45<71:33:11,  1.29s/it, loss=0.2279, lr=1.02e-06, step=204]Training:   0%|          | 205/200000 [05:46<68:04:20,  1.23s/it, loss=0.2279, lr=1.02e-06, step=204]Training:   0%|          | 205/200000 [05:46<68:04:20,  1.23s/it, loss=0.0928, lr=1.02e-06, step=205]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 206/200000 [05:47<69:47:25,  1.26s/it, loss=0.0928, lr=1.02e-06, step=205]Training:   0%|          | 206/200000 [05:47<69:47:25,  1.26s/it, loss=0.1634, lr=1.03e-06, step=206]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 207/200000 [05:48<71:21:52,  1.29s/it, loss=0.1634, lr=1.03e-06, step=206]Training:   0%|          | 207/200000 [05:48<71:21:52,  1.29s/it, loss=0.2363, lr=1.03e-06, step=207]Training:   0%|          | 208/200000 [05:50<74:39:05,  1.35s/it, loss=0.2363, lr=1.03e-06, step=207]Training:   0%|          | 208/200000 [05:50<74:39:05,  1.35s/it, loss=0.1328, lr=1.04e-06, step=208]Training:   0%|          | 209/200000 [05:51<77:13:29,  1.39s/it, loss=0.1328, lr=1.04e-06, step=208]Training:   0%|          | 209/200000 [05:51<77:13:29,  1.39s/it, loss=0.1238, lr=1.04e-06, step=209]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 210/200000 [05:52<71:57:03,  1.30s/it, loss=0.1238, lr=1.04e-06, step=209]Training:   0%|          | 210/200000 [05:52<71:57:03,  1.30s/it, loss=0.1601, lr=1.05e-06, step=210]Training:   0%|          | 211/200000 [05:54<68:19:39,  1.23s/it, loss=0.1601, lr=1.05e-06, step=210]Training:   0%|          | 211/200000 [05:54<68:19:39,  1.23s/it, loss=0.4896, lr=1.05e-06, step=211]Training:   0%|          | 212/200000 [05:55<71:53:11,  1.30s/it, loss=0.4896, lr=1.05e-06, step=211]Training:   0%|          | 212/200000 [05:55<71:53:11,  1.30s/it, loss=0.0952, lr=1.06e-06, step=212]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 213/200000 [05:56<68:16:06,  1.23s/it, loss=0.0952, lr=1.06e-06, step=212]Training:   0%|          | 213/200000 [05:56<68:16:06,  1.23s/it, loss=0.1017, lr=1.06e-06, step=213]Training:   0%|          | 214/200000 [05:57<70:44:57,  1.27s/it, loss=0.1017, lr=1.06e-06, step=213]Training:   0%|          | 214/200000 [05:57<70:44:57,  1.27s/it, loss=0.2739, lr=1.07e-06, step=214]Training:   0%|          | 215/200000 [05:59<67:31:37,  1.22s/it, loss=0.2739, lr=1.07e-06, step=214]Training:   0%|          | 215/200000 [05:59<67:31:37,  1.22s/it, loss=0.1450, lr=1.07e-06, step=215]Training:   0%|          | 216/200000 [06:00<65:12:19,  1.17s/it, loss=0.1450, lr=1.07e-06, step=215]Training:   0%|          | 216/200000 [06:00<65:12:19,  1.17s/it, loss=0.1953, lr=1.08e-06, step=216]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 217/200000 [06:01<69:27:27,  1.25s/it, loss=0.1953, lr=1.08e-06, step=216]Training:   0%|          | 217/200000 [06:01<69:27:27,  1.25s/it, loss=0.1198, lr=1.08e-06, step=217]Training:   0%|          | 218/200000 [06:02<72:07:24,  1.30s/it, loss=0.1198, lr=1.08e-06, step=217]Training:   0%|          | 218/200000 [06:02<72:07:24,  1.30s/it, loss=0.1646, lr=1.09e-06, step=218]Training:   0%|          | 219/200000 [06:04<73:39:05,  1.33s/it, loss=0.1646, lr=1.09e-06, step=218]Training:   0%|          | 219/200000 [06:04<73:39:05,  1.33s/it, loss=0.1549, lr=1.09e-06, step=219]Training:   0%|          | 220/200000 [06:05<69:29:48,  1.25s/it, loss=0.1549, lr=1.09e-06, step=219]Training:   0%|          | 220/200000 [06:05<69:29:48,  1.25s/it, loss=0.0819, lr=1.10e-06, step=220]Training:   0%|          | 221/200000 [06:06<66:34:35,  1.20s/it, loss=0.0819, lr=1.10e-06, step=220]Training:   0%|          | 221/200000 [06:06<66:34:35,  1.20s/it, loss=0.3493, lr=1.10e-06, step=221]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 222/200000 [06:07<69:07:00,  1.25s/it, loss=0.3493, lr=1.10e-06, step=221]Training:   0%|          | 222/200000 [06:07<69:07:00,  1.25s/it, loss=0.1906, lr=1.11e-06, step=222]Training:   0%|          | 223/200000 [06:09<70:01:32,  1.26s/it, loss=0.1906, lr=1.11e-06, step=222]Training:   0%|          | 223/200000 [06:09<70:01:32,  1.26s/it, loss=0.0937, lr=1.11e-06, step=223]Training:   0%|          | 224/200000 [06:10<66:59:34,  1.21s/it, loss=0.0937, lr=1.11e-06, step=223]Training:   0%|          | 224/200000 [06:10<66:59:34,  1.21s/it, loss=0.3112, lr=1.12e-06, step=224]Training:   0%|          | 225/200000 [06:11<69:15:15,  1.25s/it, loss=0.3112, lr=1.12e-06, step=224]Training:   0%|          | 225/200000 [06:11<69:15:15,  1.25s/it, loss=0.2158, lr=1.12e-06, step=225]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 226/200000 [06:12<66:24:59,  1.20s/it, loss=0.2158, lr=1.12e-06, step=225]Training:   0%|          | 226/200000 [06:12<66:24:59,  1.20s/it, loss=0.1046, lr=1.13e-06, step=226]Training:   0%|          | 227/200000 [06:14<69:13:25,  1.25s/it, loss=0.1046, lr=1.13e-06, step=226]Training:   0%|          | 227/200000 [06:14<69:13:25,  1.25s/it, loss=0.3435, lr=1.13e-06, step=227]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 228/200000 [06:15<66:23:53,  1.20s/it, loss=0.3435, lr=1.13e-06, step=227]Training:   0%|          | 228/200000 [06:15<66:23:53,  1.20s/it, loss=0.1337, lr=1.14e-06, step=228]Training:   0%|          | 229/200000 [06:16<70:16:41,  1.27s/it, loss=0.1337, lr=1.14e-06, step=228]Training:   0%|          | 229/200000 [06:16<70:16:41,  1.27s/it, loss=0.1647, lr=1.14e-06, step=229]Training:   0%|          | 230/200000 [06:17<72:45:18,  1.31s/it, loss=0.1647, lr=1.14e-06, step=229]Training:   0%|          | 230/200000 [06:17<72:45:18,  1.31s/it, loss=0.1588, lr=1.15e-06, step=230]Training:   0%|          | 231/200000 [06:19<68:52:30,  1.24s/it, loss=0.1588, lr=1.15e-06, step=230]Training:   0%|          | 231/200000 [06:19<68:52:30,  1.24s/it, loss=0.1023, lr=1.15e-06, step=231]Training:   0%|          | 232/200000 [06:20<66:17:34,  1.19s/it, loss=0.1023, lr=1.15e-06, step=231]Training:   0%|          | 232/200000 [06:20<66:17:34,  1.19s/it, loss=0.1731, lr=1.16e-06, step=232]Training:   0%|          | 233/200000 [06:21<69:07:27,  1.25s/it, loss=0.1731, lr=1.16e-06, step=232]Training:   0%|          | 233/200000 [06:21<69:07:27,  1.25s/it, loss=0.5670, lr=1.16e-06, step=233]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 234/200000 [06:22<71:31:51,  1.29s/it, loss=0.5670, lr=1.16e-06, step=233]Training:   0%|          | 234/200000 [06:22<71:31:51,  1.29s/it, loss=0.8824, lr=1.17e-06, step=234]Training:   0%|          | 235/200000 [06:23<67:58:45,  1.23s/it, loss=0.8824, lr=1.17e-06, step=234]Training:   0%|          | 235/200000 [06:23<67:58:45,  1.23s/it, loss=0.2027, lr=1.17e-06, step=235]Training:   0%|          | 236/200000 [06:25<71:44:52,  1.29s/it, loss=0.2027, lr=1.17e-06, step=235]Training:   0%|          | 236/200000 [06:25<71:44:52,  1.29s/it, loss=0.0598, lr=1.18e-06, step=236]Training:   0%|          | 237/200000 [06:26<68:07:59,  1.23s/it, loss=0.0598, lr=1.18e-06, step=236]Training:   0%|          | 237/200000 [06:26<68:07:59,  1.23s/it, loss=0.0886, lr=1.18e-06, step=237]Training:   0%|          | 238/200000 [06:27<70:39:19,  1.27s/it, loss=0.0886, lr=1.18e-06, step=237]Training:   0%|          | 238/200000 [06:27<70:39:19,  1.27s/it, loss=0.1394, lr=1.19e-06, step=238]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 239/200000 [06:29<71:37:28,  1.29s/it, loss=0.1394, lr=1.19e-06, step=238]Training:   0%|          | 239/200000 [06:29<71:37:28,  1.29s/it, loss=0.2264, lr=1.19e-06, step=239]Training:   0%|          | 240/200000 [06:30<75:10:31,  1.35s/it, loss=0.2264, lr=1.19e-06, step=239]Training:   0%|          | 240/200000 [06:30<75:10:31,  1.35s/it, loss=0.1324, lr=1.20e-06, step=240]Training:   0%|          | 241/200000 [06:32<77:23:42,  1.39s/it, loss=0.1324, lr=1.20e-06, step=240]Training:   0%|          | 241/200000 [06:32<77:23:42,  1.39s/it, loss=0.1255, lr=1.20e-06, step=241]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 242/200000 [06:33<72:05:52,  1.30s/it, loss=0.1255, lr=1.20e-06, step=241]Training:   0%|          | 242/200000 [06:33<72:05:52,  1.30s/it, loss=0.2164, lr=1.21e-06, step=242]Training:   0%|          | 243/200000 [06:34<68:24:37,  1.23s/it, loss=0.2164, lr=1.21e-06, step=242]Training:   0%|          | 243/200000 [06:34<68:24:37,  1.23s/it, loss=0.1933, lr=1.21e-06, step=243]Training:   0%|          | 244/200000 [06:35<71:50:01,  1.29s/it, loss=0.1933, lr=1.21e-06, step=243]Training:   0%|          | 244/200000 [06:35<71:50:01,  1.29s/it, loss=0.0659, lr=1.22e-06, step=244]Training:   0%|          | 245/200000 [06:36<68:14:44,  1.23s/it, loss=0.0659, lr=1.22e-06, step=244]Training:   0%|          | 245/200000 [06:36<68:14:44,  1.23s/it, loss=0.1728, lr=1.22e-06, step=245]Training:   0%|          | 246/200000 [06:38<71:18:36,  1.29s/it, loss=0.1728, lr=1.22e-06, step=245]Training:   0%|          | 246/200000 [06:38<71:18:36,  1.29s/it, loss=0.2052, lr=1.23e-06, step=246]Training:   0%|          | 247/200000 [06:39<67:51:19,  1.22s/it, loss=0.2052, lr=1.23e-06, step=246]Training:   0%|          | 247/200000 [06:39<67:51:19,  1.22s/it, loss=0.1349, lr=1.23e-06, step=247]Training:   0%|          | 248/200000 [06:40<65:25:38,  1.18s/it, loss=0.1349, lr=1.23e-06, step=247]Training:   0%|          | 248/200000 [06:40<65:25:38,  1.18s/it, loss=0.1070, lr=1.24e-06, step=248]Training:   0%|          | 249/200000 [06:41<69:48:24,  1.26s/it, loss=0.1070, lr=1.24e-06, step=248]Training:   0%|          | 249/200000 [06:41<69:48:24,  1.26s/it, loss=0.1366, lr=1.24e-06, step=249]Training:   0%|          | 250/200000 [06:43<73:28:10,  1.32s/it, loss=0.1366, lr=1.24e-06, step=249]Training:   0%|          | 250/200000 [06:43<73:28:10,  1.32s/it, loss=0.3004, lr=1.25e-06, step=250]Training:   0%|          | 251/200000 [06:44<74:38:36,  1.35s/it, loss=0.3004, lr=1.25e-06, step=250]Training:   0%|          | 251/200000 [06:44<74:38:36,  1.35s/it, loss=0.2855, lr=1.25e-06, step=251]Training:   0%|          | 252/200000 [06:45<70:16:21,  1.27s/it, loss=0.2855, lr=1.25e-06, step=251]Training:   0%|          | 252/200000 [06:45<70:16:21,  1.27s/it, loss=0.2089, lr=1.26e-06, step=252]Training:   0%|          | 253/200000 [06:46<67:04:39,  1.21s/it, loss=0.2089, lr=1.26e-06, step=252]Training:   0%|          | 253/200000 [06:46<67:04:39,  1.21s/it, loss=0.1276, lr=1.26e-06, step=253]Training:   0%|          | 254/200000 [06:48<69:37:47,  1.25s/it, loss=0.1276, lr=1.26e-06, step=253]Training:   0%|          | 254/200000 [06:48<69:37:47,  1.25s/it, loss=0.0930, lr=1.27e-06, step=254]Training:   0%|          | 255/200000 [06:49<71:45:17,  1.29s/it, loss=0.0930, lr=1.27e-06, step=254]Training:   0%|          | 255/200000 [06:49<71:45:17,  1.29s/it, loss=0.1284, lr=1.27e-06, step=255]Training:   0%|          | 256/200000 [06:50<68:14:39,  1.23s/it, loss=0.1284, lr=1.27e-06, step=255]Training:   0%|          | 256/200000 [06:50<68:14:39,  1.23s/it, loss=0.1687, lr=1.28e-06, step=256]Training:   0%|          | 257/200000 [06:52<71:21:00,  1.29s/it, loss=0.1687, lr=1.28e-06, step=256]Training:   0%|          | 257/200000 [06:52<71:21:00,  1.29s/it, loss=0.1235, lr=1.28e-06, step=257]Training:   0%|          | 258/200000 [06:53<67:54:29,  1.22s/it, loss=0.1235, lr=1.28e-06, step=257]Training:   0%|          | 258/200000 [06:53<67:54:29,  1.22s/it, loss=0.1632, lr=1.29e-06, step=258]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 259/200000 [06:54<69:31:28,  1.25s/it, loss=0.1632, lr=1.29e-06, step=258]Training:   0%|          | 259/200000 [06:54<69:31:28,  1.25s/it, loss=0.1835, lr=1.29e-06, step=259]Training:   0%|          | 260/200000 [06:55<70:50:11,  1.28s/it, loss=0.1835, lr=1.29e-06, step=259]Training:   0%|          | 260/200000 [06:55<70:50:11,  1.28s/it, loss=0.2259, lr=1.30e-06, step=260]Training:   0%|          | 261/200000 [06:57<74:17:25,  1.34s/it, loss=0.2259, lr=1.30e-06, step=260]Training:   0%|          | 261/200000 [06:57<74:17:25,  1.34s/it, loss=0.1150, lr=1.30e-06, step=261]Training:   0%|          | 262/200000 [06:58<76:29:39,  1.38s/it, loss=0.1150, lr=1.30e-06, step=261]Training:   0%|          | 262/200000 [06:58<76:29:39,  1.38s/it, loss=0.1300, lr=1.31e-06, step=262]Training:   0%|          | 263/200000 [06:59<71:24:03,  1.29s/it, loss=0.1300, lr=1.31e-06, step=262]Training:   0%|          | 263/200000 [06:59<71:24:03,  1.29s/it, loss=0.1793, lr=1.31e-06, step=263]Training:   0%|          | 264/200000 [07:00<67:57:09,  1.22s/it, loss=0.1793, lr=1.31e-06, step=263]Training:   0%|          | 264/200000 [07:00<67:57:09,  1.22s/it, loss=0.1587, lr=1.32e-06, step=264]Training:   0%|          | 265/200000 [07:02<71:11:31,  1.28s/it, loss=0.1587, lr=1.32e-06, step=264]Training:   0%|          | 265/200000 [07:02<71:11:31,  1.28s/it, loss=0.1094, lr=1.32e-06, step=265]Training:   0%|          | 266/200000 [07:03<67:44:16,  1.22s/it, loss=0.1094, lr=1.32e-06, step=265]Training:   0%|          | 266/200000 [07:03<67:44:16,  1.22s/it, loss=0.3300, lr=1.33e-06, step=266]Training:   0%|          | 267/200000 [07:04<70:08:44,  1.26s/it, loss=0.3300, lr=1.33e-06, step=266]Training:   0%|          | 267/200000 [07:04<70:08:44,  1.26s/it, loss=0.1187, lr=1.33e-06, step=267]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 268/200000 [07:05<66:58:26,  1.21s/it, loss=0.1187, lr=1.33e-06, step=267]Training:   0%|          | 268/200000 [07:05<66:58:26,  1.21s/it, loss=0.1411, lr=1.34e-06, step=268]Training:   0%|          | 269/200000 [07:06<64:49:50,  1.17s/it, loss=0.1411, lr=1.34e-06, step=268]Training:   0%|          | 269/200000 [07:06<64:49:50,  1.17s/it, loss=0.2155, lr=1.34e-06, step=269]Training:   0%|          | 270/200000 [07:08<69:48:33,  1.26s/it, loss=0.2155, lr=1.34e-06, step=269]Training:   0%|          | 270/200000 [07:08<69:48:33,  1.26s/it, loss=0.1147, lr=1.35e-06, step=270]Training:   0%|          | 271/200000 [07:09<72:31:38,  1.31s/it, loss=0.1147, lr=1.35e-06, step=270]Training:   0%|          | 271/200000 [07:09<72:31:38,  1.31s/it, loss=0.2085, lr=1.35e-06, step=271]Training:   0%|          | 272/200000 [07:11<73:48:25,  1.33s/it, loss=0.2085, lr=1.35e-06, step=271]Training:   0%|          | 272/200000 [07:11<73:48:25,  1.33s/it, loss=0.1351, lr=1.36e-06, step=272]Training:   0%|          | 273/200000 [07:12<69:32:38,  1.25s/it, loss=0.1351, lr=1.36e-06, step=272]Training:   0%|          | 273/200000 [07:12<69:32:38,  1.25s/it, loss=0.1480, lr=1.36e-06, step=273]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 274/200000 [07:13<66:34:19,  1.20s/it, loss=0.1480, lr=1.36e-06, step=273]Training:   0%|          | 274/200000 [07:13<66:34:19,  1.20s/it, loss=0.0962, lr=1.37e-06, step=274]Training:   0%|          | 275/200000 [07:14<68:43:20,  1.24s/it, loss=0.0962, lr=1.37e-06, step=274]Training:   0%|          | 275/200000 [07:14<68:43:20,  1.24s/it, loss=0.1456, lr=1.37e-06, step=275]Training:   0%|          | 276/200000 [07:16<70:05:49,  1.26s/it, loss=0.1456, lr=1.37e-06, step=275]Training:   0%|          | 276/200000 [07:16<70:05:49,  1.26s/it, loss=0.2336, lr=1.38e-06, step=276]Training:   0%|          | 277/200000 [07:17<66:56:26,  1.21s/it, loss=0.2336, lr=1.38e-06, step=276]Training:   0%|          | 277/200000 [07:17<66:56:26,  1.21s/it, loss=0.0745, lr=1.38e-06, step=277]Training:   0%|          | 278/200000 [07:18<69:23:30,  1.25s/it, loss=0.0745, lr=1.38e-06, step=277]Training:   0%|          | 278/200000 [07:18<69:23:30,  1.25s/it, loss=0.0670, lr=1.39e-06, step=278]Training:   0%|          | 279/200000 [07:19<66:29:16,  1.20s/it, loss=0.0670, lr=1.39e-06, step=278]Training:   0%|          | 279/200000 [07:19<66:29:16,  1.20s/it, loss=0.3201, lr=1.39e-06, step=279]Training:   0%|          | 280/200000 [07:20<69:10:43,  1.25s/it, loss=0.3201, lr=1.39e-06, step=279]Training:   0%|          | 280/200000 [07:20<69:10:43,  1.25s/it, loss=0.0856, lr=1.40e-06, step=280]Training:   0%|          | 281/200000 [07:21<66:20:25,  1.20s/it, loss=0.0856, lr=1.40e-06, step=280]Training:   0%|          | 281/200000 [07:21<66:20:25,  1.20s/it, loss=0.2228, lr=1.40e-06, step=281]Training:   0%|          | 282/200000 [07:23<70:14:15,  1.27s/it, loss=0.2228, lr=1.40e-06, step=281]Training:   0%|          | 282/200000 [07:23<70:14:15,  1.27s/it, loss=0.0871, lr=1.41e-06, step=282]Training:   0%|          | 283/200000 [07:24<73:00:07,  1.32s/it, loss=0.0871, lr=1.41e-06, step=282]Training:   0%|          | 283/200000 [07:24<73:00:07,  1.32s/it, loss=0.1408, lr=1.41e-06, step=283]Training:   0%|          | 284/200000 [07:25<68:56:57,  1.24s/it, loss=0.1408, lr=1.41e-06, step=283]Training:   0%|          | 284/200000 [07:25<68:56:57,  1.24s/it, loss=0.1401, lr=1.42e-06, step=284]Training:   0%|          | 285/200000 [07:26<66:09:36,  1.19s/it, loss=0.1401, lr=1.42e-06, step=284]Training:   0%|          | 285/200000 [07:26<66:09:36,  1.19s/it, loss=0.1409, lr=1.42e-06, step=285]Training:   0%|          | 286/200000 [07:28<68:57:17,  1.24s/it, loss=0.1409, lr=1.42e-06, step=285]Training:   0%|          | 286/200000 [07:28<68:57:17,  1.24s/it, loss=0.0884, lr=1.43e-06, step=286]Training:   0%|          | 287/200000 [07:29<71:23:09,  1.29s/it, loss=0.0884, lr=1.43e-06, step=286]Training:   0%|          | 287/200000 [07:29<71:23:09,  1.29s/it, loss=0.2790, lr=1.43e-06, step=287]Training:   0%|          | 288/200000 [07:30<67:52:23,  1.22s/it, loss=0.2790, lr=1.43e-06, step=287]Training:   0%|          | 288/200000 [07:30<67:52:23,  1.22s/it, loss=0.1291, lr=1.44e-06, step=288]Training:   0%|          | 289/200000 [07:32<71:39:13,  1.29s/it, loss=0.1291, lr=1.44e-06, step=288]Training:   0%|          | 289/200000 [07:32<71:39:13,  1.29s/it, loss=0.1033, lr=1.44e-06, step=289]Training:   0%|          | 290/200000 [07:33<68:07:39,  1.23s/it, loss=0.1033, lr=1.44e-06, step=289]Training:   0%|          | 290/200000 [07:33<68:07:39,  1.23s/it, loss=0.1197, lr=1.45e-06, step=290]Training:   0%|          | 291/200000 [07:34<70:31:27,  1.27s/it, loss=0.1197, lr=1.45e-06, step=290]Training:   0%|          | 291/200000 [07:34<70:31:27,  1.27s/it, loss=0.1136, lr=1.45e-06, step=291]Training:   0%|          | 292/200000 [07:36<71:38:36,  1.29s/it, loss=0.1136, lr=1.45e-06, step=291]Training:   0%|          | 292/200000 [07:36<71:38:36,  1.29s/it, loss=0.1061, lr=1.46e-06, step=292]Training:   0%|          | 293/200000 [07:37<75:17:33,  1.36s/it, loss=0.1061, lr=1.46e-06, step=292]Training:   0%|          | 293/200000 [07:37<75:17:33,  1.36s/it, loss=0.0665, lr=1.46e-06, step=293]Training:   0%|          | 294/200000 [07:39<77:40:04,  1.40s/it, loss=0.0665, lr=1.46e-06, step=293]Training:   0%|          | 294/200000 [07:39<77:40:04,  1.40s/it, loss=0.1584, lr=1.47e-06, step=294]Training:   0%|          | 295/200000 [07:40<72:18:56,  1.30s/it, loss=0.1584, lr=1.47e-06, step=294]Training:   0%|          | 295/200000 [07:40<72:18:56,  1.30s/it, loss=0.1299, lr=1.47e-06, step=295]Training:   0%|          | 296/200000 [07:41<68:31:06,  1.24s/it, loss=0.1299, lr=1.47e-06, step=295]Training:   0%|          | 296/200000 [07:41<68:31:06,  1.24s/it, loss=0.1535, lr=1.48e-06, step=296]Training:   0%|          | 297/200000 [07:42<71:53:36,  1.30s/it, loss=0.1535, lr=1.48e-06, step=296]Training:   0%|          | 297/200000 [07:42<71:53:36,  1.30s/it, loss=0.0933, lr=1.48e-06, step=297]Training:   0%|          | 298/200000 [07:43<68:13:29,  1.23s/it, loss=0.0933, lr=1.48e-06, step=297]Training:   0%|          | 298/200000 [07:43<68:13:29,  1.23s/it, loss=0.1167, lr=1.49e-06, step=298]Training:   0%|          | 299/200000 [07:45<70:18:36,  1.27s/it, loss=0.1167, lr=1.49e-06, step=298]Training:   0%|          | 299/200000 [07:45<70:18:36,  1.27s/it, loss=0.1709, lr=1.49e-06, step=299]Training:   0%|          | 300/200000 [07:46<67:07:49,  1.21s/it, loss=0.1709, lr=1.49e-06, step=299]Training:   0%|          | 300/200000 [07:46<67:07:49,  1.21s/it, loss=0.1437, lr=1.50e-06, step=300]23:01:00.559 [I] step=300 loss=0.1732 lr=1.26e-06 grad_norm=1.92 time=126.0s                      (701675:train_pytorch.py:582)
Training:   0%|          | 301/200000 [07:47<64:55:11,  1.17s/it, loss=0.1437, lr=1.50e-06, step=300]Training:   0%|          | 301/200000 [07:47<64:55:11,  1.17s/it, loss=0.1241, lr=1.50e-06, step=301]Training:   0%|          | 302/200000 [07:48<69:24:17,  1.25s/it, loss=0.1241, lr=1.50e-06, step=301]Training:   0%|          | 302/200000 [07:48<69:24:17,  1.25s/it, loss=0.0822, lr=1.51e-06, step=302]Training:   0%|          | 303/200000 [07:50<73:03:05,  1.32s/it, loss=0.0822, lr=1.51e-06, step=302]Training:   0%|          | 303/200000 [07:50<73:03:05,  1.32s/it, loss=0.0904, lr=1.51e-06, step=303]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 304/200000 [07:51<74:32:16,  1.34s/it, loss=0.0904, lr=1.51e-06, step=303]Training:   0%|          | 304/200000 [07:51<74:32:16,  1.34s/it, loss=0.0800, lr=1.52e-06, step=304]Training:   0%|          | 305/200000 [07:52<70:09:31,  1.26s/it, loss=0.0800, lr=1.52e-06, step=304]Training:   0%|          | 305/200000 [07:52<70:09:31,  1.26s/it, loss=0.3294, lr=1.52e-06, step=305]Training:   0%|          | 306/200000 [07:53<67:04:46,  1.21s/it, loss=0.3294, lr=1.52e-06, step=305]Training:   0%|          | 306/200000 [07:53<67:04:46,  1.21s/it, loss=0.0606, lr=1.53e-06, step=306]Training:   0%|          | 307/200000 [07:55<69:30:43,  1.25s/it, loss=0.0606, lr=1.53e-06, step=306]Training:   0%|          | 307/200000 [07:55<69:30:43,  1.25s/it, loss=0.1256, lr=1.53e-06, step=307]Training:   0%|          | 308/200000 [07:56<71:16:40,  1.28s/it, loss=0.1256, lr=1.53e-06, step=307]Training:   0%|          | 308/200000 [07:56<71:16:40,  1.28s/it, loss=0.0837, lr=1.54e-06, step=308]Training:   0%|          | 309/200000 [07:57<67:48:27,  1.22s/it, loss=0.0837, lr=1.54e-06, step=308]Training:   0%|          | 309/200000 [07:57<67:48:27,  1.22s/it, loss=0.1161, lr=1.54e-06, step=309]Training:   0%|          | 310/200000 [07:58<71:02:28,  1.28s/it, loss=0.1161, lr=1.54e-06, step=309]Training:   0%|          | 310/200000 [07:58<71:02:28,  1.28s/it, loss=0.1578, lr=1.55e-06, step=310]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 311/200000 [08:00<67:39:33,  1.22s/it, loss=0.1578, lr=1.55e-06, step=310]Training:   0%|          | 311/200000 [08:00<67:39:33,  1.22s/it, loss=0.0873, lr=1.55e-06, step=311]Training:   0%|          | 312/200000 [08:01<69:55:25,  1.26s/it, loss=0.0873, lr=1.55e-06, step=311]Training:   0%|          | 312/200000 [08:01<69:55:25,  1.26s/it, loss=0.5694, lr=1.56e-06, step=312]Training:   0%|          | 313/200000 [08:02<71:10:13,  1.28s/it, loss=0.5694, lr=1.56e-06, step=312]Training:   0%|          | 313/200000 [08:02<71:10:13,  1.28s/it, loss=0.1090, lr=1.56e-06, step=313]Training:   0%|          | 314/200000 [08:04<74:38:38,  1.35s/it, loss=0.1090, lr=1.56e-06, step=313]Training:   0%|          | 314/200000 [08:04<74:38:38,  1.35s/it, loss=0.1584, lr=1.57e-06, step=314]Training:   0%|          | 315/200000 [08:05<77:17:15,  1.39s/it, loss=0.1584, lr=1.57e-06, step=314]Training:   0%|          | 315/200000 [08:05<77:17:15,  1.39s/it, loss=0.1631, lr=1.57e-06, step=315]Training:   0%|          | 316/200000 [08:06<72:00:18,  1.30s/it, loss=0.1631, lr=1.57e-06, step=315]Training:   0%|          | 316/200000 [08:06<72:00:18,  1.30s/it, loss=0.0885, lr=1.58e-06, step=316]Training:   0%|          | 317/200000 [08:07<68:19:57,  1.23s/it, loss=0.0885, lr=1.58e-06, step=316]Training:   0%|          | 317/200000 [08:07<68:19:57,  1.23s/it, loss=0.0968, lr=1.58e-06, step=317]Training:   0%|          | 318/200000 [08:09<70:48:40,  1.28s/it, loss=0.0968, lr=1.58e-06, step=317]Training:   0%|          | 318/200000 [08:09<70:48:40,  1.28s/it, loss=0.3034, lr=1.59e-06, step=318]Training:   0%|          | 319/200000 [08:10<67:29:53,  1.22s/it, loss=0.3034, lr=1.59e-06, step=318]Training:   0%|          | 319/200000 [08:10<67:29:53,  1.22s/it, loss=0.1296, lr=1.59e-06, step=319]Training:   0%|          | 320/200000 [08:11<69:53:32,  1.26s/it, loss=0.1296, lr=1.59e-06, step=319]Training:   0%|          | 320/200000 [08:11<69:53:32,  1.26s/it, loss=0.2365, lr=1.60e-06, step=320]Training:   0%|          | 321/200000 [08:12<66:53:08,  1.21s/it, loss=0.2365, lr=1.60e-06, step=320]Training:   0%|          | 321/200000 [08:12<66:53:08,  1.21s/it, loss=0.4083, lr=1.60e-06, step=321]Training:   0%|          | 322/200000 [08:13<64:42:58,  1.17s/it, loss=0.4083, lr=1.60e-06, step=321]Training:   0%|          | 322/200000 [08:13<64:42:58,  1.17s/it, loss=0.1212, lr=1.61e-06, step=322]WARNING:root:Token length (51) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 323/200000 [08:15<69:08:09,  1.25s/it, loss=0.1212, lr=1.61e-06, step=322]Training:   0%|          | 323/200000 [08:15<69:08:09,  1.25s/it, loss=0.1596, lr=1.61e-06, step=323]Training:   0%|          | 324/200000 [08:16<72:13:33,  1.30s/it, loss=0.1596, lr=1.61e-06, step=323]Training:   0%|          | 324/200000 [08:16<72:13:33,  1.30s/it, loss=0.1213, lr=1.62e-06, step=324]Training:   0%|          | 325/200000 [08:18<73:28:31,  1.32s/it, loss=0.1213, lr=1.62e-06, step=324]Training:   0%|          | 325/200000 [08:18<73:28:31,  1.32s/it, loss=0.1759, lr=1.62e-06, step=325]Training:   0%|          | 326/200000 [08:19<69:21:21,  1.25s/it, loss=0.1759, lr=1.62e-06, step=325]Training:   0%|          | 326/200000 [08:19<69:21:21,  1.25s/it, loss=0.1827, lr=1.63e-06, step=326]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 327/200000 [08:20<66:30:11,  1.20s/it, loss=0.1827, lr=1.63e-06, step=326]Training:   0%|          | 327/200000 [08:20<66:30:11,  1.20s/it, loss=0.1324, lr=1.63e-06, step=327]Training:   0%|          | 328/200000 [08:21<68:23:33,  1.23s/it, loss=0.1324, lr=1.63e-06, step=327]Training:   0%|          | 328/200000 [08:21<68:23:33,  1.23s/it, loss=0.1894, lr=1.64e-06, step=328]Training:   0%|          | 329/200000 [08:22<69:32:57,  1.25s/it, loss=0.1894, lr=1.64e-06, step=328]Training:   0%|          | 329/200000 [08:22<69:32:57,  1.25s/it, loss=0.1010, lr=1.64e-06, step=329]Training:   0%|          | 330/200000 [08:23<66:40:08,  1.20s/it, loss=0.1010, lr=1.64e-06, step=329]Training:   0%|          | 330/200000 [08:23<66:40:08,  1.20s/it, loss=0.5009, lr=1.65e-06, step=330]Training:   0%|          | 331/200000 [08:25<69:06:55,  1.25s/it, loss=0.5009, lr=1.65e-06, step=330]Training:   0%|          | 331/200000 [08:25<69:06:55,  1.25s/it, loss=0.0841, lr=1.65e-06, step=331]Training:   0%|          | 332/200000 [08:26<66:20:11,  1.20s/it, loss=0.0841, lr=1.65e-06, step=331]Training:   0%|          | 332/200000 [08:26<66:20:11,  1.20s/it, loss=0.1714, lr=1.66e-06, step=332]Training:   0%|          | 333/200000 [08:27<69:01:50,  1.24s/it, loss=0.1714, lr=1.66e-06, step=332]Training:   0%|          | 333/200000 [08:27<69:01:50,  1.24s/it, loss=0.1893, lr=1.66e-06, step=333]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 334/200000 [08:28<66:17:49,  1.20s/it, loss=0.1893, lr=1.66e-06, step=333]Training:   0%|          | 334/200000 [08:28<66:17:49,  1.20s/it, loss=0.2278, lr=1.67e-06, step=334]Training:   0%|          | 335/200000 [08:30<70:51:46,  1.28s/it, loss=0.2278, lr=1.67e-06, step=334]Training:   0%|          | 335/200000 [08:30<70:51:46,  1.28s/it, loss=0.1075, lr=1.67e-06, step=335]Training:   0%|          | 336/200000 [08:31<73:41:33,  1.33s/it, loss=0.1075, lr=1.67e-06, step=335]Training:   0%|          | 336/200000 [08:31<73:41:33,  1.33s/it, loss=0.0978, lr=1.68e-06, step=336]Training:   0%|          | 337/200000 [08:32<69:30:19,  1.25s/it, loss=0.0978, lr=1.68e-06, step=336]Training:   0%|          | 337/200000 [08:32<69:30:19,  1.25s/it, loss=0.0777, lr=1.68e-06, step=337]Training:   0%|          | 338/200000 [08:33<66:32:46,  1.20s/it, loss=0.0777, lr=1.68e-06, step=337]Training:   0%|          | 338/200000 [08:33<66:32:46,  1.20s/it, loss=0.1334, lr=1.69e-06, step=338]Training:   0%|          | 339/200000 [08:35<68:45:58,  1.24s/it, loss=0.1334, lr=1.69e-06, step=338]Training:   0%|          | 339/200000 [08:35<68:45:58,  1.24s/it, loss=0.2357, lr=1.69e-06, step=339]Training:   0%|          | 340/200000 [08:36<71:26:56,  1.29s/it, loss=0.2357, lr=1.69e-06, step=339]Training:   0%|          | 340/200000 [08:36<71:26:56,  1.29s/it, loss=0.3558, lr=1.70e-06, step=340]Training:   0%|          | 341/200000 [08:37<67:54:50,  1.22s/it, loss=0.3558, lr=1.70e-06, step=340]Training:   0%|          | 341/200000 [08:37<67:54:50,  1.22s/it, loss=0.2037, lr=1.70e-06, step=341]Training:   0%|          | 342/200000 [08:39<71:32:18,  1.29s/it, loss=0.2037, lr=1.70e-06, step=341]Training:   0%|          | 342/200000 [08:39<71:32:18,  1.29s/it, loss=0.0789, lr=1.71e-06, step=342]Training:   0%|          | 343/200000 [08:40<67:58:04,  1.23s/it, loss=0.0789, lr=1.71e-06, step=342]Training:   0%|          | 343/200000 [08:40<67:58:04,  1.23s/it, loss=0.3512, lr=1.71e-06, step=343]Training:   0%|          | 344/200000 [08:41<71:35:53,  1.29s/it, loss=0.3512, lr=1.71e-06, step=343]Training:   0%|          | 344/200000 [08:41<71:35:53,  1.29s/it, loss=0.0672, lr=1.72e-06, step=344]Training:   0%|          | 345/200000 [08:42<72:24:53,  1.31s/it, loss=0.0672, lr=1.72e-06, step=344]Training:   0%|          | 345/200000 [08:42<72:24:53,  1.31s/it, loss=0.1748, lr=1.72e-06, step=345]Training:   0%|          | 346/200000 [08:44<75:48:47,  1.37s/it, loss=0.1748, lr=1.72e-06, step=345]Training:   0%|          | 346/200000 [08:44<75:48:47,  1.37s/it, loss=0.1341, lr=1.73e-06, step=346]Training:   0%|          | 347/200000 [08:45<78:08:48,  1.41s/it, loss=0.1341, lr=1.73e-06, step=346]Training:   0%|          | 347/200000 [08:45<78:08:48,  1.41s/it, loss=0.1458, lr=1.73e-06, step=347]Training:   0%|          | 348/200000 [08:47<72:35:18,  1.31s/it, loss=0.1458, lr=1.73e-06, step=347]Training:   0%|          | 348/200000 [08:47<72:35:18,  1.31s/it, loss=0.0838, lr=1.74e-06, step=348]Training:   0%|          | 349/200000 [08:48<68:46:48,  1.24s/it, loss=0.0838, lr=1.74e-06, step=348]Training:   0%|          | 349/200000 [08:48<68:46:48,  1.24s/it, loss=0.0950, lr=1.74e-06, step=349]Training:   0%|          | 350/200000 [08:49<72:09:53,  1.30s/it, loss=0.0950, lr=1.74e-06, step=349]Training:   0%|          | 350/200000 [08:49<72:09:53,  1.30s/it, loss=0.0982, lr=1.75e-06, step=350]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 351/200000 [08:50<68:22:59,  1.23s/it, loss=0.0982, lr=1.75e-06, step=350]Training:   0%|          | 351/200000 [08:50<68:22:59,  1.23s/it, loss=0.1580, lr=1.75e-06, step=351]Training:   0%|          | 352/200000 [08:52<70:57:02,  1.28s/it, loss=0.1580, lr=1.75e-06, step=351]Training:   0%|          | 352/200000 [08:52<70:57:02,  1.28s/it, loss=0.2110, lr=1.76e-06, step=352]Training:   0%|          | 353/200000 [08:53<67:31:00,  1.22s/it, loss=0.2110, lr=1.76e-06, step=352]Training:   0%|          | 353/200000 [08:53<67:31:00,  1.22s/it, loss=0.0778, lr=1.76e-06, step=353]Training:   0%|          | 354/200000 [08:54<65:13:36,  1.18s/it, loss=0.0778, lr=1.76e-06, step=353]Training:   0%|          | 354/200000 [08:54<65:13:36,  1.18s/it, loss=0.0811, lr=1.77e-06, step=354]Training:   0%|          | 355/200000 [08:55<69:38:40,  1.26s/it, loss=0.0811, lr=1.77e-06, step=354]Training:   0%|          | 355/200000 [08:55<69:38:40,  1.26s/it, loss=0.2047, lr=1.77e-06, step=355]Training:   0%|          | 356/200000 [08:57<73:13:58,  1.32s/it, loss=0.2047, lr=1.77e-06, step=355]Training:   0%|          | 356/200000 [08:57<73:13:58,  1.32s/it, loss=0.1298, lr=1.78e-06, step=356]Training:   0%|          | 357/200000 [08:58<74:15:34,  1.34s/it, loss=0.1298, lr=1.78e-06, step=356]Training:   0%|          | 357/200000 [08:58<74:15:34,  1.34s/it, loss=0.0943, lr=1.78e-06, step=357]Training:   0%|          | 358/200000 [08:59<69:52:07,  1.26s/it, loss=0.0943, lr=1.78e-06, step=357]Training:   0%|          | 358/200000 [08:59<69:52:07,  1.26s/it, loss=0.1931, lr=1.79e-06, step=358]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 359/200000 [09:00<66:50:29,  1.21s/it, loss=0.1931, lr=1.79e-06, step=358]Training:   0%|          | 359/200000 [09:00<66:50:29,  1.21s/it, loss=0.1037, lr=1.79e-06, step=359]Training:   0%|          | 360/200000 [09:02<69:31:08,  1.25s/it, loss=0.1037, lr=1.79e-06, step=359]Training:   0%|          | 360/200000 [09:02<69:31:08,  1.25s/it, loss=0.1312, lr=1.80e-06, step=360]Training:   0%|          | 361/200000 [09:03<71:33:14,  1.29s/it, loss=0.1312, lr=1.80e-06, step=360]Training:   0%|          | 361/200000 [09:03<71:33:14,  1.29s/it, loss=0.1023, lr=1.80e-06, step=361]Training:   0%|          | 362/200000 [09:04<68:00:10,  1.23s/it, loss=0.1023, lr=1.80e-06, step=361]Training:   0%|          | 362/200000 [09:04<68:00:10,  1.23s/it, loss=0.0687, lr=1.81e-06, step=362]Training:   0%|          | 363/200000 [09:05<70:57:07,  1.28s/it, loss=0.0687, lr=1.81e-06, step=362]Training:   0%|          | 363/200000 [09:05<70:57:07,  1.28s/it, loss=0.0704, lr=1.81e-06, step=363]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 364/200000 [09:06<67:33:43,  1.22s/it, loss=0.0704, lr=1.81e-06, step=363]Training:   0%|          | 364/200000 [09:06<67:33:43,  1.22s/it, loss=0.1788, lr=1.82e-06, step=364]Training:   0%|          | 365/200000 [09:08<68:51:15,  1.24s/it, loss=0.1788, lr=1.82e-06, step=364]Training:   0%|          | 365/200000 [09:08<68:51:15,  1.24s/it, loss=0.0876, lr=1.82e-06, step=365]Training:   0%|          | 366/200000 [09:09<70:29:53,  1.27s/it, loss=0.0876, lr=1.82e-06, step=365]Training:   0%|          | 366/200000 [09:09<70:29:53,  1.27s/it, loss=0.1250, lr=1.83e-06, step=366]Training:   0%|          | 367/200000 [09:11<74:13:06,  1.34s/it, loss=0.1250, lr=1.83e-06, step=366]Training:   0%|          | 367/200000 [09:11<74:13:06,  1.34s/it, loss=0.2027, lr=1.83e-06, step=367]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 368/200000 [09:12<76:57:03,  1.39s/it, loss=0.2027, lr=1.83e-06, step=367]Training:   0%|          | 368/200000 [09:12<76:57:03,  1.39s/it, loss=0.0860, lr=1.84e-06, step=368]Training:   0%|          | 369/200000 [09:13<71:48:39,  1.29s/it, loss=0.0860, lr=1.84e-06, step=368]Training:   0%|          | 369/200000 [09:13<71:48:39,  1.29s/it, loss=0.1563, lr=1.84e-06, step=369]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 370/200000 [09:14<68:20:21,  1.23s/it, loss=0.1563, lr=1.84e-06, step=369]Training:   0%|          | 370/200000 [09:14<68:20:21,  1.23s/it, loss=0.3865, lr=1.85e-06, step=370]Training:   0%|          | 371/200000 [09:16<70:35:20,  1.27s/it, loss=0.3865, lr=1.85e-06, step=370]Training:   0%|          | 371/200000 [09:16<70:35:20,  1.27s/it, loss=0.1006, lr=1.85e-06, step=371]Training:   0%|          | 372/200000 [09:17<67:22:20,  1.21s/it, loss=0.1006, lr=1.85e-06, step=371]Training:   0%|          | 372/200000 [09:17<67:22:20,  1.21s/it, loss=0.0952, lr=1.86e-06, step=372]Training:   0%|          | 373/200000 [09:18<69:55:06,  1.26s/it, loss=0.0952, lr=1.86e-06, step=372]Training:   0%|          | 373/200000 [09:18<69:55:06,  1.26s/it, loss=0.0859, lr=1.86e-06, step=373]Training:   0%|          | 374/200000 [09:19<66:50:06,  1.21s/it, loss=0.0859, lr=1.86e-06, step=373]Training:   0%|          | 374/200000 [09:19<66:50:06,  1.21s/it, loss=0.1196, lr=1.87e-06, step=374]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 375/200000 [09:20<64:53:55,  1.17s/it, loss=0.1196, lr=1.87e-06, step=374]Training:   0%|          | 375/200000 [09:20<64:53:55,  1.17s/it, loss=0.1115, lr=1.87e-06, step=375]Training:   0%|          | 376/200000 [09:22<69:10:44,  1.25s/it, loss=0.1115, lr=1.87e-06, step=375]Training:   0%|          | 376/200000 [09:22<69:10:44,  1.25s/it, loss=0.0870, lr=1.88e-06, step=376]Training:   0%|          | 377/200000 [09:23<72:35:52,  1.31s/it, loss=0.0870, lr=1.88e-06, step=376]Training:   0%|          | 377/200000 [09:23<72:35:52,  1.31s/it, loss=0.0988, lr=1.88e-06, step=377]Training:   0%|          | 378/200000 [09:24<73:33:52,  1.33s/it, loss=0.0988, lr=1.88e-06, step=377]Training:   0%|          | 378/200000 [09:24<73:33:52,  1.33s/it, loss=0.0806, lr=1.89e-06, step=378]Training:   0%|          | 379/200000 [09:26<69:24:39,  1.25s/it, loss=0.0806, lr=1.89e-06, step=378]Training:   0%|          | 379/200000 [09:26<69:24:39,  1.25s/it, loss=0.2321, lr=1.89e-06, step=379]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 380/200000 [09:27<66:28:11,  1.20s/it, loss=0.2321, lr=1.89e-06, step=379]Training:   0%|          | 380/200000 [09:27<66:28:11,  1.20s/it, loss=0.1129, lr=1.90e-06, step=380]Training:   0%|          | 381/200000 [09:28<69:00:31,  1.24s/it, loss=0.1129, lr=1.90e-06, step=380]Training:   0%|          | 381/200000 [09:28<69:00:31,  1.24s/it, loss=0.0986, lr=1.90e-06, step=381]Training:   0%|          | 382/200000 [09:29<70:05:14,  1.26s/it, loss=0.0986, lr=1.90e-06, step=381]Training:   0%|          | 382/200000 [09:29<70:05:14,  1.26s/it, loss=0.3200, lr=1.91e-06, step=382]Training:   0%|          | 383/200000 [09:30<67:00:54,  1.21s/it, loss=0.3200, lr=1.91e-06, step=382]Training:   0%|          | 383/200000 [09:30<67:00:54,  1.21s/it, loss=0.0897, lr=1.91e-06, step=383]Training:   0%|          | 384/200000 [09:32<69:00:06,  1.24s/it, loss=0.0897, lr=1.91e-06, step=383]Training:   0%|          | 384/200000 [09:32<69:00:06,  1.24s/it, loss=0.1097, lr=1.92e-06, step=384]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 385/200000 [09:33<66:14:28,  1.19s/it, loss=0.1097, lr=1.92e-06, step=384]Training:   0%|          | 385/200000 [09:33<66:14:28,  1.19s/it, loss=0.1408, lr=1.92e-06, step=385]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 386/200000 [09:34<68:19:01,  1.23s/it, loss=0.1408, lr=1.92e-06, step=385]Training:   0%|          | 386/200000 [09:34<68:19:01,  1.23s/it, loss=0.1612, lr=1.93e-06, step=386]Training:   0%|          | 387/200000 [09:35<65:46:34,  1.19s/it, loss=0.1612, lr=1.93e-06, step=386]Training:   0%|          | 387/200000 [09:35<65:46:34,  1.19s/it, loss=0.2089, lr=1.93e-06, step=387]Training:   0%|          | 388/200000 [09:37<69:51:58,  1.26s/it, loss=0.2089, lr=1.93e-06, step=387]Training:   0%|          | 388/200000 [09:37<69:51:58,  1.26s/it, loss=0.1764, lr=1.94e-06, step=388]Training:   0%|          | 389/200000 [09:38<72:59:38,  1.32s/it, loss=0.1764, lr=1.94e-06, step=388]Training:   0%|          | 389/200000 [09:38<72:59:38,  1.32s/it, loss=0.1056, lr=1.94e-06, step=389]Training:   0%|          | 390/200000 [09:39<68:58:46,  1.24s/it, loss=0.1056, lr=1.94e-06, step=389]Training:   0%|          | 390/200000 [09:39<68:58:46,  1.24s/it, loss=0.3071, lr=1.95e-06, step=390]Training:   0%|          | 391/200000 [09:40<66:12:04,  1.19s/it, loss=0.3071, lr=1.95e-06, step=390]Training:   0%|          | 391/200000 [09:40<66:12:04,  1.19s/it, loss=0.0929, lr=1.95e-06, step=391]Training:   0%|          | 392/200000 [09:42<68:26:42,  1.23s/it, loss=0.0929, lr=1.95e-06, step=391]Training:   0%|          | 392/200000 [09:42<68:26:42,  1.23s/it, loss=0.1535, lr=1.96e-06, step=392]Training:   0%|          | 393/200000 [09:43<70:49:43,  1.28s/it, loss=0.1535, lr=1.96e-06, step=392]Training:   0%|          | 393/200000 [09:43<70:49:43,  1.28s/it, loss=0.1092, lr=1.96e-06, step=393]Training:   0%|          | 394/200000 [09:44<67:30:25,  1.22s/it, loss=0.1092, lr=1.96e-06, step=393]Training:   0%|          | 394/200000 [09:44<67:30:25,  1.22s/it, loss=0.1099, lr=1.97e-06, step=394]Training:   0%|          | 395/200000 [09:45<70:47:55,  1.28s/it, loss=0.1099, lr=1.97e-06, step=394]Training:   0%|          | 395/200000 [09:45<70:47:55,  1.28s/it, loss=0.1181, lr=1.97e-06, step=395]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 396/200000 [09:46<67:26:56,  1.22s/it, loss=0.1181, lr=1.97e-06, step=395]Training:   0%|          | 396/200000 [09:46<67:26:56,  1.22s/it, loss=0.2090, lr=1.98e-06, step=396]Training:   0%|          | 397/200000 [09:48<69:56:27,  1.26s/it, loss=0.2090, lr=1.98e-06, step=396]Training:   0%|          | 397/200000 [09:48<69:56:27,  1.26s/it, loss=0.1673, lr=1.98e-06, step=397]Training:   0%|          | 398/200000 [09:49<71:46:07,  1.29s/it, loss=0.1673, lr=1.98e-06, step=397]Training:   0%|          | 398/200000 [09:49<71:46:07,  1.29s/it, loss=0.1270, lr=1.99e-06, step=398]Training:   0%|          | 399/200000 [09:51<75:22:09,  1.36s/it, loss=0.1270, lr=1.99e-06, step=398]Training:   0%|          | 399/200000 [09:51<75:22:09,  1.36s/it, loss=0.2944, lr=1.99e-06, step=399]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 400/200000 [09:52<77:20:07,  1.39s/it, loss=0.2944, lr=1.99e-06, step=399]Training:   0%|          | 400/200000 [09:52<77:20:07,  1.39s/it, loss=0.2796, lr=2.00e-06, step=400]23:03:07.102 [I] step=400 loss=0.1574 lr=1.76e-06 grad_norm=1.36 time=126.5s                      (701675:train_pytorch.py:582)
Training:   0%|          | 401/200000 [09:53<72:12:23,  1.30s/it, loss=0.2796, lr=2.00e-06, step=400]Training:   0%|          | 401/200000 [09:53<72:12:23,  1.30s/it, loss=0.1111, lr=2.00e-06, step=401]Training:   0%|          | 402/200000 [09:54<68:27:49,  1.23s/it, loss=0.1111, lr=2.00e-06, step=401]Training:   0%|          | 402/200000 [09:54<68:27:49,  1.23s/it, loss=0.1487, lr=2.01e-06, step=402]Training:   0%|          | 403/200000 [09:56<71:15:02,  1.29s/it, loss=0.1487, lr=2.01e-06, step=402]Training:   0%|          | 403/200000 [09:56<71:15:02,  1.29s/it, loss=0.1435, lr=2.01e-06, step=403]Training:   0%|          | 404/200000 [09:57<67:46:19,  1.22s/it, loss=0.1435, lr=2.01e-06, step=403]Training:   0%|          | 404/200000 [09:57<67:46:19,  1.22s/it, loss=1.0881, lr=2.02e-06, step=404]Training:   0%|          | 405/200000 [09:58<70:49:05,  1.28s/it, loss=1.0881, lr=2.02e-06, step=404]Training:   0%|          | 405/200000 [09:58<70:49:05,  1.28s/it, loss=0.2013, lr=2.02e-06, step=405]Training:   0%|          | 406/200000 [09:59<67:30:34,  1.22s/it, loss=0.2013, lr=2.02e-06, step=405]Training:   0%|          | 406/200000 [09:59<67:30:34,  1.22s/it, loss=0.0820, lr=2.03e-06, step=406]Training:   0%|          | 407/200000 [10:00<65:12:24,  1.18s/it, loss=0.0820, lr=2.03e-06, step=406]Training:   0%|          | 407/200000 [10:00<65:12:24,  1.18s/it, loss=0.1413, lr=2.03e-06, step=407]Training:   0%|          | 408/200000 [10:02<69:38:45,  1.26s/it, loss=0.1413, lr=2.03e-06, step=407]Training:   0%|          | 408/200000 [10:02<69:38:45,  1.26s/it, loss=0.0993, lr=2.04e-06, step=408]Training:   0%|          | 409/200000 [10:03<73:11:30,  1.32s/it, loss=0.0993, lr=2.04e-06, step=408]Training:   0%|          | 409/200000 [10:03<73:11:30,  1.32s/it, loss=0.1522, lr=2.04e-06, step=409]Training:   0%|          | 410/200000 [10:05<74:22:04,  1.34s/it, loss=0.1522, lr=2.04e-06, step=409]Training:   0%|          | 410/200000 [10:05<74:22:04,  1.34s/it, loss=0.2454, lr=2.05e-06, step=410]Training:   0%|          | 411/200000 [10:06<70:00:46,  1.26s/it, loss=0.2454, lr=2.05e-06, step=410]Training:   0%|          | 411/200000 [10:06<70:00:46,  1.26s/it, loss=0.0927, lr=2.05e-06, step=411]Training:   0%|          | 412/200000 [10:07<66:53:27,  1.21s/it, loss=0.0927, lr=2.05e-06, step=411]Training:   0%|          | 412/200000 [10:07<66:53:27,  1.21s/it, loss=0.1468, lr=2.06e-06, step=412]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 413/200000 [10:08<69:29:59,  1.25s/it, loss=0.1468, lr=2.06e-06, step=412]Training:   0%|          | 413/200000 [10:08<69:29:59,  1.25s/it, loss=0.0790, lr=2.06e-06, step=413]Training:   0%|          | 414/200000 [10:10<71:29:22,  1.29s/it, loss=0.0790, lr=2.06e-06, step=413]Training:   0%|          | 414/200000 [10:10<71:29:22,  1.29s/it, loss=0.1227, lr=2.07e-06, step=414]Training:   0%|          | 415/200000 [10:11<68:00:35,  1.23s/it, loss=0.1227, lr=2.07e-06, step=414]Training:   0%|          | 415/200000 [10:11<68:00:35,  1.23s/it, loss=0.0602, lr=2.07e-06, step=415]Training:   0%|          | 416/200000 [10:12<71:00:26,  1.28s/it, loss=0.0602, lr=2.07e-06, step=415]Training:   0%|          | 416/200000 [10:12<71:00:26,  1.28s/it, loss=0.1980, lr=2.08e-06, step=416]Training:   0%|          | 417/200000 [10:13<67:37:40,  1.22s/it, loss=0.1980, lr=2.08e-06, step=416]Training:   0%|          | 417/200000 [10:13<67:37:40,  1.22s/it, loss=0.0600, lr=2.08e-06, step=417]Training:   0%|          | 418/200000 [10:14<69:15:44,  1.25s/it, loss=0.0600, lr=2.08e-06, step=417]Training:   0%|          | 418/200000 [10:14<69:15:44,  1.25s/it, loss=0.1068, lr=2.09e-06, step=418]Training:   0%|          | 419/200000 [10:16<71:09:54,  1.28s/it, loss=0.1068, lr=2.09e-06, step=418]Training:   0%|          | 419/200000 [10:16<71:09:54,  1.28s/it, loss=0.2806, lr=2.09e-06, step=419]Training:   0%|          | 420/200000 [10:17<74:41:22,  1.35s/it, loss=0.2806, lr=2.09e-06, step=419]Training:   0%|          | 420/200000 [10:17<74:41:22,  1.35s/it, loss=0.1129, lr=2.10e-06, step=420]Training:   0%|          | 421/200000 [10:19<77:25:49,  1.40s/it, loss=0.1129, lr=2.10e-06, step=420]Training:   0%|          | 421/200000 [10:19<77:25:49,  1.40s/it, loss=0.2897, lr=2.10e-06, step=421]Training:   0%|          | 422/200000 [10:20<72:03:34,  1.30s/it, loss=0.2897, lr=2.10e-06, step=421]Training:   0%|          | 422/200000 [10:20<72:03:34,  1.30s/it, loss=0.1958, lr=2.11e-06, step=422]Training:   0%|          | 423/200000 [10:21<68:20:41,  1.23s/it, loss=0.1958, lr=2.11e-06, step=422]Training:   0%|          | 423/200000 [10:21<68:20:41,  1.23s/it, loss=0.1734, lr=2.11e-06, step=423]Training:   0%|          | 424/200000 [10:22<70:46:27,  1.28s/it, loss=0.1734, lr=2.11e-06, step=423]Training:   0%|          | 424/200000 [10:22<70:46:27,  1.28s/it, loss=0.0891, lr=2.12e-06, step=424]Training:   0%|          | 425/200000 [10:23<67:28:01,  1.22s/it, loss=0.0891, lr=2.12e-06, step=424]Training:   0%|          | 425/200000 [10:23<67:28:01,  1.22s/it, loss=0.1128, lr=2.12e-06, step=425]Training:   0%|          | 426/200000 [10:25<70:17:24,  1.27s/it, loss=0.1128, lr=2.12e-06, step=425]Training:   0%|          | 426/200000 [10:25<70:17:24,  1.27s/it, loss=0.0954, lr=2.13e-06, step=426]Training:   0%|          | 427/200000 [10:26<67:07:40,  1.21s/it, loss=0.0954, lr=2.13e-06, step=426]Training:   0%|          | 427/200000 [10:26<67:07:40,  1.21s/it, loss=0.1842, lr=2.13e-06, step=427]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 428/200000 [10:27<64:50:48,  1.17s/it, loss=0.1842, lr=2.13e-06, step=427]Training:   0%|          | 428/200000 [10:27<64:50:48,  1.17s/it, loss=0.1546, lr=2.14e-06, step=428]Training:   0%|          | 429/200000 [10:28<69:11:28,  1.25s/it, loss=0.1546, lr=2.14e-06, step=428]Training:   0%|          | 429/200000 [10:28<69:11:28,  1.25s/it, loss=0.1522, lr=2.14e-06, step=429]Training:   0%|          | 430/200000 [10:30<72:45:41,  1.31s/it, loss=0.1522, lr=2.14e-06, step=429]Training:   0%|          | 430/200000 [10:30<72:45:41,  1.31s/it, loss=0.1981, lr=2.15e-06, step=430]Training:   0%|          | 431/200000 [10:31<73:59:36,  1.33s/it, loss=0.1981, lr=2.15e-06, step=430]Training:   0%|          | 431/200000 [10:31<73:59:36,  1.33s/it, loss=0.1516, lr=2.15e-06, step=431]Training:   0%|          | 432/200000 [10:32<69:39:08,  1.26s/it, loss=0.1516, lr=2.15e-06, step=431]Training:   0%|          | 432/200000 [10:32<69:39:08,  1.26s/it, loss=0.0765, lr=2.16e-06, step=432]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 433/200000 [10:33<66:40:06,  1.20s/it, loss=0.0765, lr=2.16e-06, step=432]Training:   0%|          | 433/200000 [10:33<66:40:06,  1.20s/it, loss=0.1398, lr=2.16e-06, step=433]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 434/200000 [10:35<68:59:44,  1.24s/it, loss=0.1398, lr=2.16e-06, step=433]Training:   0%|          | 434/200000 [10:35<68:59:44,  1.24s/it, loss=0.1371, lr=2.17e-06, step=434]Training:   0%|          | 435/200000 [10:36<70:07:17,  1.26s/it, loss=0.1371, lr=2.17e-06, step=434]Training:   0%|          | 435/200000 [10:36<70:07:17,  1.26s/it, loss=0.1089, lr=2.17e-06, step=435]Training:   0%|          | 436/200000 [10:37<66:56:56,  1.21s/it, loss=0.1089, lr=2.17e-06, step=435]Training:   0%|          | 436/200000 [10:37<66:56:56,  1.21s/it, loss=0.1035, lr=2.18e-06, step=436]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 437/200000 [10:39<69:03:50,  1.25s/it, loss=0.1035, lr=2.18e-06, step=436]Training:   0%|          | 437/200000 [10:39<69:03:50,  1.25s/it, loss=0.1309, lr=2.18e-06, step=437]Training:   0%|          | 438/200000 [10:40<66:12:40,  1.19s/it, loss=0.1309, lr=2.18e-06, step=437]Training:   0%|          | 438/200000 [10:40<66:12:40,  1.19s/it, loss=0.1438, lr=2.19e-06, step=438]Training:   0%|          | 439/200000 [10:41<68:52:59,  1.24s/it, loss=0.1438, lr=2.19e-06, step=438]Training:   0%|          | 439/200000 [10:41<68:52:59,  1.24s/it, loss=0.0988, lr=2.19e-06, step=439]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 440/200000 [10:42<66:04:23,  1.19s/it, loss=0.0988, lr=2.19e-06, step=439]Training:   0%|          | 440/200000 [10:42<66:04:23,  1.19s/it, loss=0.1015, lr=2.20e-06, step=440]Training:   0%|          | 441/200000 [10:43<70:59:32,  1.28s/it, loss=0.1015, lr=2.20e-06, step=440]Training:   0%|          | 441/200000 [10:43<70:59:32,  1.28s/it, loss=0.0832, lr=2.20e-06, step=441]Training:   0%|          | 442/200000 [10:45<72:52:34,  1.31s/it, loss=0.0832, lr=2.20e-06, step=441]Training:   0%|          | 442/200000 [10:45<72:52:34,  1.31s/it, loss=0.0837, lr=2.21e-06, step=442]Training:   0%|          | 443/200000 [10:46<68:56:26,  1.24s/it, loss=0.0837, lr=2.21e-06, step=442]Training:   0%|          | 443/200000 [10:46<68:56:26,  1.24s/it, loss=0.4660, lr=2.21e-06, step=443]Training:   0%|          | 444/200000 [10:47<66:11:04,  1.19s/it, loss=0.4660, lr=2.21e-06, step=443]Training:   0%|          | 444/200000 [10:47<66:11:04,  1.19s/it, loss=0.0732, lr=2.22e-06, step=444]Training:   0%|          | 445/200000 [10:48<68:11:04,  1.23s/it, loss=0.0732, lr=2.22e-06, step=444]Training:   0%|          | 445/200000 [10:48<68:11:04,  1.23s/it, loss=0.1229, lr=2.22e-06, step=445]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 446/200000 [10:50<70:57:22,  1.28s/it, loss=0.1229, lr=2.22e-06, step=445]Training:   0%|          | 446/200000 [10:50<70:57:22,  1.28s/it, loss=0.0696, lr=2.23e-06, step=446]Training:   0%|          | 447/200000 [10:51<67:36:27,  1.22s/it, loss=0.0696, lr=2.23e-06, step=446]Training:   0%|          | 447/200000 [10:51<67:36:27,  1.22s/it, loss=0.1686, lr=2.23e-06, step=447]Training:   0%|          | 448/200000 [10:52<70:43:39,  1.28s/it, loss=0.1686, lr=2.23e-06, step=447]Training:   0%|          | 448/200000 [10:52<70:43:39,  1.28s/it, loss=0.0648, lr=2.24e-06, step=448]Training:   0%|          | 449/200000 [10:53<67:25:11,  1.22s/it, loss=0.0648, lr=2.24e-06, step=448]Training:   0%|          | 449/200000 [10:53<67:25:11,  1.22s/it, loss=0.1524, lr=2.24e-06, step=449]Training:   0%|          | 450/200000 [10:55<70:22:15,  1.27s/it, loss=0.1524, lr=2.24e-06, step=449]Training:   0%|          | 450/200000 [10:55<70:22:15,  1.27s/it, loss=0.0754, lr=2.25e-06, step=450]Training:   0%|          | 451/200000 [10:56<71:11:28,  1.28s/it, loss=0.0754, lr=2.25e-06, step=450]Training:   0%|          | 451/200000 [10:56<71:11:28,  1.28s/it, loss=0.2006, lr=2.25e-06, step=451]Training:   0%|          | 452/200000 [10:58<74:57:06,  1.35s/it, loss=0.2006, lr=2.25e-06, step=451]Training:   0%|          | 452/200000 [10:58<74:57:06,  1.35s/it, loss=0.1875, lr=2.26e-06, step=452]Training:   0%|          | 453/200000 [10:59<77:55:32,  1.41s/it, loss=0.1875, lr=2.26e-06, step=452]Training:   0%|          | 453/200000 [10:59<77:55:32,  1.41s/it, loss=0.1967, lr=2.26e-06, step=453]Training:   0%|          | 454/200000 [11:00<72:24:41,  1.31s/it, loss=0.1967, lr=2.26e-06, step=453]Training:   0%|          | 454/200000 [11:00<72:24:41,  1.31s/it, loss=0.2111, lr=2.27e-06, step=454]Training:   0%|          | 455/200000 [11:01<68:35:22,  1.24s/it, loss=0.2111, lr=2.27e-06, step=454]Training:   0%|          | 455/200000 [11:01<68:35:22,  1.24s/it, loss=0.1180, lr=2.27e-06, step=455]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 456/200000 [11:03<71:29:35,  1.29s/it, loss=0.1180, lr=2.27e-06, step=455]Training:   0%|          | 456/200000 [11:03<71:29:35,  1.29s/it, loss=0.0835, lr=2.28e-06, step=456]Training:   0%|          | 457/200000 [11:04<67:53:54,  1.22s/it, loss=0.0835, lr=2.28e-06, step=456]Training:   0%|          | 457/200000 [11:04<67:53:54,  1.22s/it, loss=0.0721, lr=2.28e-06, step=457]Training:   0%|          | 458/200000 [11:05<70:24:31,  1.27s/it, loss=0.0721, lr=2.28e-06, step=457]Training:   0%|          | 458/200000 [11:05<70:24:31,  1.27s/it, loss=0.1943, lr=2.29e-06, step=458]Training:   0%|          | 459/200000 [11:06<67:10:30,  1.21s/it, loss=0.1943, lr=2.29e-06, step=458]Training:   0%|          | 459/200000 [11:06<67:10:30,  1.21s/it, loss=0.0572, lr=2.29e-06, step=459]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 460/200000 [11:07<64:51:30,  1.17s/it, loss=0.0572, lr=2.29e-06, step=459]Training:   0%|          | 460/200000 [11:07<64:51:30,  1.17s/it, loss=0.1060, lr=2.30e-06, step=460]Training:   0%|          | 461/200000 [11:09<69:23:19,  1.25s/it, loss=0.1060, lr=2.30e-06, step=460]Training:   0%|          | 461/200000 [11:09<69:23:19,  1.25s/it, loss=0.3119, lr=2.30e-06, step=461]Training:   0%|          | 462/200000 [11:10<72:33:50,  1.31s/it, loss=0.3119, lr=2.30e-06, step=461]Training:   0%|          | 462/200000 [11:10<72:33:50,  1.31s/it, loss=0.3194, lr=2.31e-06, step=462]Training:   0%|          | 463/200000 [11:11<73:44:59,  1.33s/it, loss=0.3194, lr=2.31e-06, step=462]Training:   0%|          | 463/200000 [11:11<73:44:59,  1.33s/it, loss=0.1633, lr=2.31e-06, step=463]Training:   0%|          | 464/200000 [11:13<69:29:39,  1.25s/it, loss=0.1633, lr=2.31e-06, step=463]Training:   0%|          | 464/200000 [11:13<69:29:39,  1.25s/it, loss=0.2148, lr=2.32e-06, step=464]Training:   0%|          | 465/200000 [11:14<66:35:05,  1.20s/it, loss=0.2148, lr=2.32e-06, step=464]Training:   0%|          | 465/200000 [11:14<66:35:05,  1.20s/it, loss=0.1260, lr=2.32e-06, step=465]Training:   0%|          | 466/200000 [11:15<68:58:09,  1.24s/it, loss=0.1260, lr=2.32e-06, step=465]Training:   0%|          | 466/200000 [11:15<68:58:09,  1.24s/it, loss=0.1228, lr=2.33e-06, step=466]Training:   0%|          | 467/200000 [11:16<71:19:23,  1.29s/it, loss=0.1228, lr=2.33e-06, step=466]Training:   0%|          | 467/200000 [11:16<71:19:23,  1.29s/it, loss=0.0787, lr=2.33e-06, step=467]Training:   0%|          | 468/200000 [11:17<67:46:03,  1.22s/it, loss=0.0787, lr=2.33e-06, step=467]Training:   0%|          | 468/200000 [11:17<67:46:03,  1.22s/it, loss=0.1422, lr=2.34e-06, step=468]Training:   0%|          | 469/200000 [11:19<70:40:57,  1.28s/it, loss=0.1422, lr=2.34e-06, step=468]Training:   0%|          | 469/200000 [11:19<70:40:57,  1.28s/it, loss=0.1765, lr=2.34e-06, step=469]Training:   0%|          | 470/200000 [11:20<67:19:44,  1.21s/it, loss=0.1765, lr=2.34e-06, step=469]Training:   0%|          | 470/200000 [11:20<67:19:44,  1.21s/it, loss=0.1829, lr=2.35e-06, step=470]Training:   0%|          | 471/200000 [11:21<69:24:55,  1.25s/it, loss=0.1829, lr=2.35e-06, step=470]Training:   0%|          | 471/200000 [11:21<69:24:55,  1.25s/it, loss=0.1534, lr=2.35e-06, step=471]Training:   0%|          | 472/200000 [11:23<70:40:34,  1.28s/it, loss=0.1534, lr=2.35e-06, step=471]Training:   0%|          | 472/200000 [11:23<70:40:34,  1.28s/it, loss=0.2122, lr=2.36e-06, step=472]Training:   0%|          | 473/200000 [11:24<74:12:29,  1.34s/it, loss=0.2122, lr=2.36e-06, step=472]Training:   0%|          | 473/200000 [11:24<74:12:29,  1.34s/it, loss=0.0753, lr=2.36e-06, step=473]Training:   0%|          | 474/200000 [11:26<76:29:34,  1.38s/it, loss=0.0753, lr=2.36e-06, step=473]Training:   0%|          | 474/200000 [11:26<76:29:34,  1.38s/it, loss=0.0461, lr=2.37e-06, step=474]Training:   0%|          | 475/200000 [11:27<71:26:06,  1.29s/it, loss=0.0461, lr=2.37e-06, step=474]Training:   0%|          | 475/200000 [11:27<71:26:06,  1.29s/it, loss=0.1261, lr=2.37e-06, step=475]Training:   0%|          | 476/200000 [11:28<67:52:47,  1.22s/it, loss=0.1261, lr=2.37e-06, step=475]Training:   0%|          | 476/200000 [11:28<67:52:47,  1.22s/it, loss=0.0671, lr=2.38e-06, step=476]Training:   0%|          | 477/200000 [11:29<70:47:50,  1.28s/it, loss=0.0671, lr=2.38e-06, step=476]Training:   0%|          | 477/200000 [11:29<70:47:50,  1.28s/it, loss=0.0979, lr=2.38e-06, step=477]Training:   0%|          | 478/200000 [11:30<67:25:03,  1.22s/it, loss=0.0979, lr=2.38e-06, step=477]Training:   0%|          | 478/200000 [11:30<67:25:03,  1.22s/it, loss=0.1887, lr=2.39e-06, step=478]Training:   0%|          | 479/200000 [11:32<70:32:43,  1.27s/it, loss=0.1887, lr=2.39e-06, step=478]Training:   0%|          | 479/200000 [11:32<70:32:43,  1.27s/it, loss=0.1096, lr=2.39e-06, step=479]Training:   0%|          | 480/200000 [11:33<67:13:11,  1.21s/it, loss=0.1096, lr=2.39e-06, step=479]Training:   0%|          | 480/200000 [11:33<67:13:11,  1.21s/it, loss=0.2241, lr=2.40e-06, step=480]Training:   0%|          | 481/200000 [11:34<64:57:06,  1.17s/it, loss=0.2241, lr=2.40e-06, step=480]Training:   0%|          | 481/200000 [11:34<64:57:06,  1.17s/it, loss=0.0772, lr=2.40e-06, step=481]Training:   0%|          | 482/200000 [11:35<69:12:38,  1.25s/it, loss=0.0772, lr=2.40e-06, step=481]Training:   0%|          | 482/200000 [11:35<69:12:38,  1.25s/it, loss=0.1871, lr=2.41e-06, step=482]Training:   0%|          | 483/200000 [11:37<72:41:02,  1.31s/it, loss=0.1871, lr=2.41e-06, step=482]Training:   0%|          | 483/200000 [11:37<72:41:02,  1.31s/it, loss=0.0975, lr=2.41e-06, step=483]Training:   0%|          | 484/200000 [11:38<73:54:51,  1.33s/it, loss=0.0975, lr=2.41e-06, step=483]Training:   0%|          | 484/200000 [11:38<73:54:51,  1.33s/it, loss=0.1328, lr=2.42e-06, step=484]Training:   0%|          | 485/200000 [11:39<69:40:30,  1.26s/it, loss=0.1328, lr=2.42e-06, step=484]Training:   0%|          | 485/200000 [11:39<69:40:30,  1.26s/it, loss=0.1675, lr=2.42e-06, step=485]Training:   0%|          | 486/200000 [11:40<66:39:43,  1.20s/it, loss=0.1675, lr=2.42e-06, step=485]Training:   0%|          | 486/200000 [11:40<66:39:43,  1.20s/it, loss=0.1054, lr=2.43e-06, step=486]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 487/200000 [11:42<69:00:12,  1.25s/it, loss=0.1054, lr=2.43e-06, step=486]Training:   0%|          | 487/200000 [11:42<69:00:12,  1.25s/it, loss=0.1273, lr=2.43e-06, step=487]Training:   0%|          | 488/200000 [11:43<70:08:57,  1.27s/it, loss=0.1273, lr=2.43e-06, step=487]Training:   0%|          | 488/200000 [11:43<70:08:57,  1.27s/it, loss=0.2453, lr=2.44e-06, step=488]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 489/200000 [11:44<66:59:57,  1.21s/it, loss=0.2453, lr=2.44e-06, step=488]Training:   0%|          | 489/200000 [11:44<66:59:57,  1.21s/it, loss=0.0730, lr=2.44e-06, step=489]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 490/200000 [11:45<69:04:03,  1.25s/it, loss=0.0730, lr=2.44e-06, step=489]Training:   0%|          | 490/200000 [11:45<69:04:03,  1.25s/it, loss=0.0607, lr=2.45e-06, step=490]Training:   0%|          | 491/200000 [11:46<66:14:51,  1.20s/it, loss=0.0607, lr=2.45e-06, step=490]Training:   0%|          | 491/200000 [11:46<66:14:51,  1.20s/it, loss=0.0570, lr=2.45e-06, step=491]Training:   0%|          | 492/200000 [11:48<68:35:17,  1.24s/it, loss=0.0570, lr=2.45e-06, step=491]Training:   0%|          | 492/200000 [11:48<68:35:17,  1.24s/it, loss=0.0602, lr=2.46e-06, step=492]Training:   0%|          | 493/200000 [11:49<65:55:45,  1.19s/it, loss=0.0602, lr=2.46e-06, step=492]Training:   0%|          | 493/200000 [11:49<65:55:45,  1.19s/it, loss=0.1015, lr=2.46e-06, step=493]Training:   0%|          | 494/200000 [11:50<70:32:37,  1.27s/it, loss=0.1015, lr=2.46e-06, step=493]Training:   0%|          | 494/200000 [11:50<70:32:37,  1.27s/it, loss=0.1309, lr=2.47e-06, step=494]Training:   0%|          | 495/200000 [11:52<73:32:49,  1.33s/it, loss=0.1309, lr=2.47e-06, step=494]Training:   0%|          | 495/200000 [11:52<73:32:49,  1.33s/it, loss=0.0739, lr=2.47e-06, step=495]Training:   0%|          | 496/200000 [11:53<69:22:07,  1.25s/it, loss=0.0739, lr=2.47e-06, step=495]Training:   0%|          | 496/200000 [11:53<69:22:07,  1.25s/it, loss=0.1008, lr=2.48e-06, step=496]Training:   0%|          | 497/200000 [11:54<66:30:46,  1.20s/it, loss=0.1008, lr=2.48e-06, step=496]Training:   0%|          | 497/200000 [11:54<66:30:46,  1.20s/it, loss=0.0795, lr=2.48e-06, step=497]Training:   0%|          | 498/200000 [11:55<69:10:57,  1.25s/it, loss=0.0795, lr=2.48e-06, step=497]Training:   0%|          | 498/200000 [11:55<69:10:57,  1.25s/it, loss=0.2199, lr=2.49e-06, step=498]Training:   0%|          | 499/200000 [11:57<71:38:03,  1.29s/it, loss=0.2199, lr=2.49e-06, step=498]Training:   0%|          | 499/200000 [11:57<71:38:03,  1.29s/it, loss=0.2587, lr=2.49e-06, step=499]Training:   0%|          | 500/200000 [11:58<67:59:27,  1.23s/it, loss=0.2587, lr=2.49e-06, step=499]Training:   0%|          | 500/200000 [11:58<67:59:27,  1.23s/it, loss=0.0941, lr=2.50e-06, step=500]23:05:12.867 [I] step=500 loss=0.1484 lr=2.26e-06 grad_norm=1.07 time=125.8s                      (701675:train_pytorch.py:582)
Training:   0%|          | 501/200000 [11:59<71:21:02,  1.29s/it, loss=0.0941, lr=2.50e-06, step=500]Training:   0%|          | 501/200000 [11:59<71:21:02,  1.29s/it, loss=0.0680, lr=2.50e-06, step=501]Training:   0%|          | 502/200000 [12:00<67:50:00,  1.22s/it, loss=0.0680, lr=2.50e-06, step=501]Training:   0%|          | 502/200000 [12:00<67:50:00,  1.22s/it, loss=0.1052, lr=2.51e-06, step=502]Training:   0%|          | 503/200000 [12:02<70:16:22,  1.27s/it, loss=0.1052, lr=2.51e-06, step=502]Training:   0%|          | 503/200000 [12:02<70:16:22,  1.27s/it, loss=0.1554, lr=2.51e-06, step=503]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 504/200000 [12:03<71:28:31,  1.29s/it, loss=0.1554, lr=2.51e-06, step=503]Training:   0%|          | 504/200000 [12:03<71:28:31,  1.29s/it, loss=0.0680, lr=2.52e-06, step=504]Training:   0%|          | 505/200000 [12:04<75:06:24,  1.36s/it, loss=0.0680, lr=2.52e-06, step=504]Training:   0%|          | 505/200000 [12:04<75:06:24,  1.36s/it, loss=0.1256, lr=2.52e-06, step=505]Training:   0%|          | 506/200000 [12:06<77:05:59,  1.39s/it, loss=0.1256, lr=2.52e-06, step=505]Training:   0%|          | 506/200000 [12:06<77:05:59,  1.39s/it, loss=0.0964, lr=2.53e-06, step=506]Training:   0%|          | 507/200000 [12:07<71:52:37,  1.30s/it, loss=0.0964, lr=2.53e-06, step=506]Training:   0%|          | 507/200000 [12:07<71:52:37,  1.30s/it, loss=0.0953, lr=2.53e-06, step=507]Training:   0%|          | 508/200000 [12:08<68:13:21,  1.23s/it, loss=0.0953, lr=2.53e-06, step=507]Training:   0%|          | 508/200000 [12:08<68:13:21,  1.23s/it, loss=0.0748, lr=2.54e-06, step=508]Training:   0%|          | 509/200000 [12:09<71:55:36,  1.30s/it, loss=0.0748, lr=2.54e-06, step=508]Training:   0%|          | 509/200000 [12:09<71:55:36,  1.30s/it, loss=0.0764, lr=2.54e-06, step=509]Training:   0%|          | 510/200000 [12:11<68:11:46,  1.23s/it, loss=0.0764, lr=2.54e-06, step=509]Training:   0%|          | 510/200000 [12:11<68:11:46,  1.23s/it, loss=0.1764, lr=2.55e-06, step=510]Training:   0%|          | 511/200000 [12:12<71:14:39,  1.29s/it, loss=0.1764, lr=2.55e-06, step=510]Training:   0%|          | 511/200000 [12:12<71:14:39,  1.29s/it, loss=0.1168, lr=2.55e-06, step=511]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 512/200000 [12:13<67:41:40,  1.22s/it, loss=0.1168, lr=2.55e-06, step=511]Training:   0%|          | 512/200000 [12:13<67:41:40,  1.22s/it, loss=0.1461, lr=2.56e-06, step=512]Training:   0%|          | 513/200000 [12:14<65:16:40,  1.18s/it, loss=0.1461, lr=2.56e-06, step=512]Training:   0%|          | 513/200000 [12:14<65:16:40,  1.18s/it, loss=0.1440, lr=2.56e-06, step=513]Training:   0%|          | 514/200000 [12:16<69:38:23,  1.26s/it, loss=0.1440, lr=2.56e-06, step=513]Training:   0%|          | 514/200000 [12:16<69:38:23,  1.26s/it, loss=0.1519, lr=2.57e-06, step=514]Training:   0%|          | 515/200000 [12:17<73:14:18,  1.32s/it, loss=0.1519, lr=2.57e-06, step=514]Training:   0%|          | 515/200000 [12:17<73:14:18,  1.32s/it, loss=0.1474, lr=2.57e-06, step=515]Training:   0%|          | 516/200000 [12:18<75:12:19,  1.36s/it, loss=0.1474, lr=2.57e-06, step=515]Training:   0%|          | 516/200000 [12:18<75:12:19,  1.36s/it, loss=0.0901, lr=2.58e-06, step=516]Training:   0%|          | 517/200000 [12:20<70:34:06,  1.27s/it, loss=0.0901, lr=2.58e-06, step=516]Training:   0%|          | 517/200000 [12:20<70:34:06,  1.27s/it, loss=0.2404, lr=2.58e-06, step=517]Training:   0%|          | 518/200000 [12:21<67:14:24,  1.21s/it, loss=0.2404, lr=2.58e-06, step=517]Training:   0%|          | 518/200000 [12:21<67:14:24,  1.21s/it, loss=0.1125, lr=2.59e-06, step=518]Training:   0%|          | 519/200000 [12:22<69:18:54,  1.25s/it, loss=0.1125, lr=2.59e-06, step=518]Training:   0%|          | 519/200000 [12:22<69:18:54,  1.25s/it, loss=0.0712, lr=2.59e-06, step=519]Training:   0%|          | 520/200000 [12:23<71:37:36,  1.29s/it, loss=0.0712, lr=2.59e-06, step=519]Training:   0%|          | 520/200000 [12:23<71:37:36,  1.29s/it, loss=0.1282, lr=2.60e-06, step=520]Training:   0%|          | 521/200000 [12:24<68:01:40,  1.23s/it, loss=0.1282, lr=2.60e-06, step=520]Training:   0%|          | 521/200000 [12:24<68:01:40,  1.23s/it, loss=0.1283, lr=2.60e-06, step=521]Training:   0%|          | 522/200000 [12:26<70:49:52,  1.28s/it, loss=0.1283, lr=2.60e-06, step=521]Training:   0%|          | 522/200000 [12:26<70:49:52,  1.28s/it, loss=0.1589, lr=2.61e-06, step=522]Training:   0%|          | 523/200000 [12:27<67:29:05,  1.22s/it, loss=0.1589, lr=2.61e-06, step=522]Training:   0%|          | 523/200000 [12:27<67:29:05,  1.22s/it, loss=0.0595, lr=2.61e-06, step=523]Training:   0%|          | 524/200000 [12:28<69:54:41,  1.26s/it, loss=0.0595, lr=2.61e-06, step=523]Training:   0%|          | 524/200000 [12:28<69:54:41,  1.26s/it, loss=0.1508, lr=2.62e-06, step=524]Training:   0%|          | 525/200000 [12:30<71:04:29,  1.28s/it, loss=0.1508, lr=2.62e-06, step=524]Training:   0%|          | 525/200000 [12:30<71:04:29,  1.28s/it, loss=0.1069, lr=2.62e-06, step=525]Training:   0%|          | 526/200000 [12:31<74:28:53,  1.34s/it, loss=0.1069, lr=2.62e-06, step=525]Training:   0%|          | 526/200000 [12:31<74:28:53,  1.34s/it, loss=0.3049, lr=2.63e-06, step=526]Training:   0%|          | 527/200000 [12:33<77:12:32,  1.39s/it, loss=0.3049, lr=2.63e-06, step=526]Training:   0%|          | 527/200000 [12:33<77:12:32,  1.39s/it, loss=0.2136, lr=2.63e-06, step=527]Training:   0%|          | 528/200000 [12:34<71:55:56,  1.30s/it, loss=0.2136, lr=2.63e-06, step=527]Training:   0%|          | 528/200000 [12:34<71:55:56,  1.30s/it, loss=0.2619, lr=2.64e-06, step=528]Training:   0%|          | 529/200000 [12:35<68:16:00,  1.23s/it, loss=0.2619, lr=2.64e-06, step=528]Training:   0%|          | 529/200000 [12:35<68:16:00,  1.23s/it, loss=0.0593, lr=2.64e-06, step=529]Training:   0%|          | 530/200000 [12:36<71:34:06,  1.29s/it, loss=0.0593, lr=2.64e-06, step=529]Training:   0%|          | 530/200000 [12:36<71:34:06,  1.29s/it, loss=0.0647, lr=2.65e-06, step=530]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 531/200000 [12:37<68:01:45,  1.23s/it, loss=0.0647, lr=2.65e-06, step=530]Training:   0%|          | 531/200000 [12:37<68:01:45,  1.23s/it, loss=0.0921, lr=2.65e-06, step=531]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 532/200000 [12:39<71:00:36,  1.28s/it, loss=0.0921, lr=2.65e-06, step=531]Training:   0%|          | 532/200000 [12:39<71:00:36,  1.28s/it, loss=0.1148, lr=2.66e-06, step=532]Training:   0%|          | 533/200000 [12:40<67:36:07,  1.22s/it, loss=0.1148, lr=2.66e-06, step=532]Training:   0%|          | 533/200000 [12:40<67:36:07,  1.22s/it, loss=0.1359, lr=2.66e-06, step=533]Training:   0%|          | 534/200000 [12:41<65:09:19,  1.18s/it, loss=0.1359, lr=2.66e-06, step=533]Training:   0%|          | 534/200000 [12:41<65:09:19,  1.18s/it, loss=0.1235, lr=2.67e-06, step=534]Training:   0%|          | 535/200000 [12:42<69:20:41,  1.25s/it, loss=0.1235, lr=2.67e-06, step=534]Training:   0%|          | 535/200000 [12:42<69:20:41,  1.25s/it, loss=0.0956, lr=2.67e-06, step=535]Training:   0%|          | 536/200000 [12:44<72:52:28,  1.32s/it, loss=0.0956, lr=2.67e-06, step=535]Training:   0%|          | 536/200000 [12:44<72:52:28,  1.32s/it, loss=0.0905, lr=2.68e-06, step=536]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 537/200000 [12:45<74:53:07,  1.35s/it, loss=0.0905, lr=2.68e-06, step=536]Training:   0%|          | 537/200000 [12:45<74:53:07,  1.35s/it, loss=0.1299, lr=2.68e-06, step=537]Training:   0%|          | 538/200000 [12:46<70:17:54,  1.27s/it, loss=0.1299, lr=2.68e-06, step=537]Training:   0%|          | 538/200000 [12:46<70:17:54,  1.27s/it, loss=0.1156, lr=2.69e-06, step=538]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 539/200000 [12:47<67:09:55,  1.21s/it, loss=0.1156, lr=2.69e-06, step=538]Training:   0%|          | 539/200000 [12:47<67:09:55,  1.21s/it, loss=0.0976, lr=2.69e-06, step=539]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 540/200000 [12:49<69:20:14,  1.25s/it, loss=0.0976, lr=2.69e-06, step=539]Training:   0%|          | 540/200000 [12:49<69:20:14,  1.25s/it, loss=0.1079, lr=2.70e-06, step=540]Training:   0%|          | 541/200000 [12:50<70:23:14,  1.27s/it, loss=0.1079, lr=2.70e-06, step=540]Training:   0%|          | 541/200000 [12:50<70:23:14,  1.27s/it, loss=0.1632, lr=2.70e-06, step=541]Training:   0%|          | 542/200000 [12:51<67:05:30,  1.21s/it, loss=0.1632, lr=2.70e-06, step=541]Training:   0%|          | 542/200000 [12:51<67:05:30,  1.21s/it, loss=0.0787, lr=2.71e-06, step=542]Training:   0%|          | 543/200000 [12:52<68:55:29,  1.24s/it, loss=0.0787, lr=2.71e-06, step=542]Training:   0%|          | 543/200000 [12:52<68:55:29,  1.24s/it, loss=0.1434, lr=2.71e-06, step=543]Training:   0%|          | 544/200000 [12:53<66:06:26,  1.19s/it, loss=0.1434, lr=2.71e-06, step=543]Training:   0%|          | 544/200000 [12:53<66:06:26,  1.19s/it, loss=0.2241, lr=2.72e-06, step=544]Training:   0%|          | 545/200000 [12:55<68:49:37,  1.24s/it, loss=0.2241, lr=2.72e-06, step=544]Training:   0%|          | 545/200000 [12:55<68:49:37,  1.24s/it, loss=0.0794, lr=2.72e-06, step=545]Training:   0%|          | 546/200000 [12:56<66:01:30,  1.19s/it, loss=0.0794, lr=2.72e-06, step=545]Training:   0%|          | 546/200000 [12:56<66:01:30,  1.19s/it, loss=0.0951, lr=2.73e-06, step=546]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 547/200000 [12:57<70:04:36,  1.26s/it, loss=0.0951, lr=2.73e-06, step=546]Training:   0%|          | 547/200000 [12:57<70:04:36,  1.26s/it, loss=0.1621, lr=2.73e-06, step=547]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 548/200000 [12:59<72:39:52,  1.31s/it, loss=0.1621, lr=2.73e-06, step=547]Training:   0%|          | 548/200000 [12:59<72:39:52,  1.31s/it, loss=0.1255, lr=2.74e-06, step=548]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 549/200000 [13:00<68:48:01,  1.24s/it, loss=0.1255, lr=2.74e-06, step=548]Training:   0%|          | 549/200000 [13:00<68:48:01,  1.24s/it, loss=0.0507, lr=2.74e-06, step=549]Training:   0%|          | 550/200000 [13:01<66:00:52,  1.19s/it, loss=0.0507, lr=2.74e-06, step=549]Training:   0%|          | 550/200000 [13:01<66:00:52,  1.19s/it, loss=0.0721, lr=2.75e-06, step=550]Training:   0%|          | 551/200000 [13:02<68:08:10,  1.23s/it, loss=0.0721, lr=2.75e-06, step=550]Training:   0%|          | 551/200000 [13:02<68:08:10,  1.23s/it, loss=0.0586, lr=2.75e-06, step=551]Training:   0%|          | 552/200000 [13:04<70:53:53,  1.28s/it, loss=0.0586, lr=2.75e-06, step=551]Training:   0%|          | 552/200000 [13:04<70:53:53,  1.28s/it, loss=0.0680, lr=2.76e-06, step=552]Training:   0%|          | 553/200000 [13:05<67:28:58,  1.22s/it, loss=0.0680, lr=2.76e-06, step=552]Training:   0%|          | 553/200000 [13:05<67:28:58,  1.22s/it, loss=0.2315, lr=2.76e-06, step=553]Training:   0%|          | 554/200000 [13:06<70:38:09,  1.27s/it, loss=0.2315, lr=2.76e-06, step=553]Training:   0%|          | 554/200000 [13:06<70:38:09,  1.27s/it, loss=0.1445, lr=2.77e-06, step=554]Training:   0%|          | 555/200000 [13:07<67:20:47,  1.22s/it, loss=0.1445, lr=2.77e-06, step=554]Training:   0%|          | 555/200000 [13:07<67:20:47,  1.22s/it, loss=0.4723, lr=2.77e-06, step=555]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 556/200000 [13:08<69:52:09,  1.26s/it, loss=0.4723, lr=2.77e-06, step=555]Training:   0%|          | 556/200000 [13:08<69:52:09,  1.26s/it, loss=0.1100, lr=2.78e-06, step=556]Training:   0%|          | 557/200000 [13:10<71:04:43,  1.28s/it, loss=0.1100, lr=2.78e-06, step=556]Training:   0%|          | 557/200000 [13:10<71:04:43,  1.28s/it, loss=0.1637, lr=2.78e-06, step=557]Training:   0%|          | 558/200000 [13:11<74:47:33,  1.35s/it, loss=0.1637, lr=2.78e-06, step=557]Training:   0%|          | 558/200000 [13:11<74:47:33,  1.35s/it, loss=0.1153, lr=2.79e-06, step=558]Training:   0%|          | 559/200000 [13:13<77:12:33,  1.39s/it, loss=0.1153, lr=2.79e-06, step=558]Training:   0%|          | 559/200000 [13:13<77:12:33,  1.39s/it, loss=0.1084, lr=2.79e-06, step=559]Training:   0%|          | 560/200000 [13:14<71:53:42,  1.30s/it, loss=0.1084, lr=2.79e-06, step=559]Training:   0%|          | 560/200000 [13:14<71:53:42,  1.30s/it, loss=0.0702, lr=2.80e-06, step=560]Training:   0%|          | 561/200000 [13:15<68:14:02,  1.23s/it, loss=0.0702, lr=2.80e-06, step=560]Training:   0%|          | 561/200000 [13:15<68:14:02,  1.23s/it, loss=0.0570, lr=2.80e-06, step=561]Training:   0%|          | 562/200000 [13:16<71:07:09,  1.28s/it, loss=0.0570, lr=2.80e-06, step=561]Training:   0%|          | 562/200000 [13:16<71:07:09,  1.28s/it, loss=0.1310, lr=2.81e-06, step=562]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 563/200000 [13:17<67:41:58,  1.22s/it, loss=0.1310, lr=2.81e-06, step=562]Training:   0%|          | 563/200000 [13:17<67:41:58,  1.22s/it, loss=0.0759, lr=2.81e-06, step=563]Training:   0%|          | 564/200000 [13:19<69:20:12,  1.25s/it, loss=0.0759, lr=2.81e-06, step=563]Training:   0%|          | 564/200000 [13:19<69:20:12,  1.25s/it, loss=0.1256, lr=2.82e-06, step=564]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 565/200000 [13:20<66:24:32,  1.20s/it, loss=0.1256, lr=2.82e-06, step=564]Training:   0%|          | 565/200000 [13:20<66:24:32,  1.20s/it, loss=0.0643, lr=2.82e-06, step=565]Training:   0%|          | 566/200000 [13:21<64:19:38,  1.16s/it, loss=0.0643, lr=2.82e-06, step=565]Training:   0%|          | 566/200000 [13:21<64:19:38,  1.16s/it, loss=0.1008, lr=2.83e-06, step=566]Training:   0%|          | 567/200000 [13:22<68:57:11,  1.24s/it, loss=0.1008, lr=2.83e-06, step=566]Training:   0%|          | 567/200000 [13:22<68:57:11,  1.24s/it, loss=0.1394, lr=2.83e-06, step=567]Training:   0%|          | 568/200000 [13:24<72:38:00,  1.31s/it, loss=0.1394, lr=2.83e-06, step=567]Training:   0%|          | 568/200000 [13:24<72:38:00,  1.31s/it, loss=0.1597, lr=2.84e-06, step=568]Training:   0%|          | 569/200000 [13:25<73:54:26,  1.33s/it, loss=0.1597, lr=2.84e-06, step=568]Training:   0%|          | 569/200000 [13:25<73:54:26,  1.33s/it, loss=0.2765, lr=2.84e-06, step=569]Training:   0%|          | 570/200000 [13:26<69:34:37,  1.26s/it, loss=0.2765, lr=2.84e-06, step=569]Training:   0%|          | 570/200000 [13:26<69:34:37,  1.26s/it, loss=0.1243, lr=2.85e-06, step=570]Training:   0%|          | 571/200000 [13:27<66:37:45,  1.20s/it, loss=0.1243, lr=2.85e-06, step=570]Training:   0%|          | 571/200000 [13:27<66:37:45,  1.20s/it, loss=0.2312, lr=2.85e-06, step=571]Training:   0%|          | 572/200000 [13:29<69:12:09,  1.25s/it, loss=0.2312, lr=2.85e-06, step=571]Training:   0%|          | 572/200000 [13:29<69:12:09,  1.25s/it, loss=0.3311, lr=2.86e-06, step=572]Training:   0%|          | 573/200000 [13:30<71:52:39,  1.30s/it, loss=0.3311, lr=2.86e-06, step=572]Training:   0%|          | 573/200000 [13:30<71:52:39,  1.30s/it, loss=0.0861, lr=2.86e-06, step=573]Training:   0%|          | 574/200000 [13:31<68:11:12,  1.23s/it, loss=0.0861, lr=2.86e-06, step=573]Training:   0%|          | 574/200000 [13:31<68:11:12,  1.23s/it, loss=0.0992, lr=2.87e-06, step=574]Training:   0%|          | 575/200000 [13:33<71:37:55,  1.29s/it, loss=0.0992, lr=2.87e-06, step=574]Training:   0%|          | 575/200000 [13:33<71:37:55,  1.29s/it, loss=0.1088, lr=2.87e-06, step=575]Training:   0%|          | 576/200000 [13:34<68:02:34,  1.23s/it, loss=0.1088, lr=2.87e-06, step=575]Training:   0%|          | 576/200000 [13:34<68:02:34,  1.23s/it, loss=0.0809, lr=2.88e-06, step=576]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 577/200000 [13:35<70:00:13,  1.26s/it, loss=0.0809, lr=2.88e-06, step=576]Training:   0%|          | 577/200000 [13:35<70:00:13,  1.26s/it, loss=0.1257, lr=2.88e-06, step=577]Training:   0%|          | 578/200000 [13:36<71:01:54,  1.28s/it, loss=0.1257, lr=2.88e-06, step=577]Training:   0%|          | 578/200000 [13:36<71:01:54,  1.28s/it, loss=0.0634, lr=2.89e-06, step=578]Training:   0%|          | 579/200000 [13:38<74:22:50,  1.34s/it, loss=0.0634, lr=2.89e-06, step=578]Training:   0%|          | 579/200000 [13:38<74:22:50,  1.34s/it, loss=0.0813, lr=2.89e-06, step=579]Training:   0%|          | 580/200000 [13:39<76:19:21,  1.38s/it, loss=0.0813, lr=2.89e-06, step=579]Training:   0%|          | 580/200000 [13:39<76:19:21,  1.38s/it, loss=0.0794, lr=2.90e-06, step=580]Training:   0%|          | 581/200000 [13:40<71:19:28,  1.29s/it, loss=0.0794, lr=2.90e-06, step=580]Training:   0%|          | 581/200000 [13:40<71:19:28,  1.29s/it, loss=0.1088, lr=2.90e-06, step=581]Training:   0%|          | 582/200000 [13:41<67:48:14,  1.22s/it, loss=0.1088, lr=2.90e-06, step=581]Training:   0%|          | 582/200000 [13:41<67:48:14,  1.22s/it, loss=0.0698, lr=2.91e-06, step=582]Training:   0%|          | 583/200000 [13:43<71:26:23,  1.29s/it, loss=0.0698, lr=2.91e-06, step=582]Training:   0%|          | 583/200000 [13:43<71:26:23,  1.29s/it, loss=0.1156, lr=2.91e-06, step=583]Training:   0%|          | 584/200000 [13:44<67:53:00,  1.23s/it, loss=0.1156, lr=2.91e-06, step=583]Training:   0%|          | 584/200000 [13:44<67:53:00,  1.23s/it, loss=0.0483, lr=2.92e-06, step=584]Training:   0%|          | 585/200000 [13:45<69:09:32,  1.25s/it, loss=0.0483, lr=2.92e-06, step=584]Training:   0%|          | 585/200000 [13:45<69:09:32,  1.25s/it, loss=0.2159, lr=2.92e-06, step=585]Training:   0%|          | 586/200000 [13:46<66:15:54,  1.20s/it, loss=0.2159, lr=2.92e-06, step=585]Training:   0%|          | 586/200000 [13:46<66:15:54,  1.20s/it, loss=0.1178, lr=2.93e-06, step=586]Training:   0%|          | 587/200000 [13:47<64:18:40,  1.16s/it, loss=0.1178, lr=2.93e-06, step=586]Training:   0%|          | 587/200000 [13:47<64:18:40,  1.16s/it, loss=0.0963, lr=2.93e-06, step=587]Training:   0%|          | 588/200000 [13:49<68:45:29,  1.24s/it, loss=0.0963, lr=2.93e-06, step=587]Training:   0%|          | 588/200000 [13:49<68:45:29,  1.24s/it, loss=0.1473, lr=2.94e-06, step=588]Training:   0%|          | 589/200000 [13:50<72:01:15,  1.30s/it, loss=0.1473, lr=2.94e-06, step=588]Training:   0%|          | 589/200000 [13:50<72:01:15,  1.30s/it, loss=0.1327, lr=2.94e-06, step=589]Training:   0%|          | 590/200000 [13:52<73:00:40,  1.32s/it, loss=0.1327, lr=2.94e-06, step=589]Training:   0%|          | 590/200000 [13:52<73:00:40,  1.32s/it, loss=0.1152, lr=2.95e-06, step=590]Training:   0%|          | 591/200000 [13:53<68:58:32,  1.25s/it, loss=0.1152, lr=2.95e-06, step=590]Training:   0%|          | 591/200000 [13:53<68:58:32,  1.25s/it, loss=0.0953, lr=2.95e-06, step=591]Training:   0%|          | 592/200000 [13:54<66:08:12,  1.19s/it, loss=0.0953, lr=2.95e-06, step=591]Training:   0%|          | 592/200000 [13:54<66:08:12,  1.19s/it, loss=0.0747, lr=2.96e-06, step=592]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 593/200000 [13:55<68:36:19,  1.24s/it, loss=0.0747, lr=2.96e-06, step=592]Training:   0%|          | 593/200000 [13:55<68:36:19,  1.24s/it, loss=0.1330, lr=2.96e-06, step=593]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 594/200000 [13:57<70:36:23,  1.27s/it, loss=0.1330, lr=2.96e-06, step=593]Training:   0%|          | 594/200000 [13:57<70:36:23,  1.27s/it, loss=0.1218, lr=2.97e-06, step=594]Training:   0%|          | 595/200000 [13:58<67:21:07,  1.22s/it, loss=0.1218, lr=2.97e-06, step=594]Training:   0%|          | 595/200000 [13:58<67:21:07,  1.22s/it, loss=0.3403, lr=2.97e-06, step=595]Training:   0%|          | 596/200000 [13:59<69:04:14,  1.25s/it, loss=0.3403, lr=2.97e-06, step=595]Training:   0%|          | 596/200000 [13:59<69:04:14,  1.25s/it, loss=0.1142, lr=2.98e-06, step=596]Training:   0%|          | 597/200000 [14:00<66:13:04,  1.20s/it, loss=0.1142, lr=2.98e-06, step=596]Training:   0%|          | 597/200000 [14:00<66:13:04,  1.20s/it, loss=0.1174, lr=2.98e-06, step=597]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 598/200000 [14:01<68:24:55,  1.24s/it, loss=0.1174, lr=2.98e-06, step=597]Training:   0%|          | 598/200000 [14:01<68:24:55,  1.24s/it, loss=0.0912, lr=2.99e-06, step=598]Training:   0%|          | 599/200000 [14:02<65:45:25,  1.19s/it, loss=0.0912, lr=2.99e-06, step=598]Training:   0%|          | 599/200000 [14:02<65:45:25,  1.19s/it, loss=0.0885, lr=2.99e-06, step=599]Training:   0%|          | 600/200000 [14:04<70:18:02,  1.27s/it, loss=0.0885, lr=2.99e-06, step=599]Training:   0%|          | 600/200000 [14:04<70:18:02,  1.27s/it, loss=0.0500, lr=3.00e-06, step=600]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
23:07:19.121 [I] step=600 loss=0.1272 lr=2.76e-06 grad_norm=0.88 time=126.3s                      (701675:train_pytorch.py:582)
Training:   0%|          | 601/200000 [14:05<73:19:08,  1.32s/it, loss=0.0500, lr=3.00e-06, step=600]Training:   0%|          | 601/200000 [14:05<73:19:08,  1.32s/it, loss=0.1110, lr=3.00e-06, step=601]Training:   0%|          | 602/200000 [14:06<69:10:17,  1.25s/it, loss=0.1110, lr=3.00e-06, step=601]Training:   0%|          | 602/200000 [14:06<69:10:17,  1.25s/it, loss=0.0460, lr=3.01e-06, step=602]Training:   0%|          | 603/200000 [14:07<66:19:01,  1.20s/it, loss=0.0460, lr=3.01e-06, step=602]Training:   0%|          | 603/200000 [14:07<66:19:01,  1.20s/it, loss=0.0873, lr=3.01e-06, step=603]Training:   0%|          | 604/200000 [14:09<68:11:25,  1.23s/it, loss=0.0873, lr=3.01e-06, step=603]Training:   0%|          | 604/200000 [14:09<68:11:25,  1.23s/it, loss=0.1382, lr=3.02e-06, step=604]Training:   0%|          | 605/200000 [14:10<71:43:28,  1.29s/it, loss=0.1382, lr=3.02e-06, step=604]Training:   0%|          | 605/200000 [14:10<71:43:28,  1.29s/it, loss=0.0854, lr=3.02e-06, step=605]Training:   0%|          | 606/200000 [14:11<68:02:55,  1.23s/it, loss=0.0854, lr=3.02e-06, step=605]Training:   0%|          | 606/200000 [14:11<68:02:55,  1.23s/it, loss=0.1544, lr=3.03e-06, step=606]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 607/200000 [14:13<71:44:57,  1.30s/it, loss=0.1544, lr=3.03e-06, step=606]Training:   0%|          | 607/200000 [14:13<71:44:57,  1.30s/it, loss=0.1577, lr=3.03e-06, step=607]Training:   0%|          | 608/200000 [14:14<68:05:10,  1.23s/it, loss=0.1577, lr=3.03e-06, step=607]Training:   0%|          | 608/200000 [14:14<68:05:10,  1.23s/it, loss=0.1635, lr=3.04e-06, step=608]Training:   0%|          | 609/200000 [14:15<70:27:01,  1.27s/it, loss=0.1635, lr=3.04e-06, step=608]Training:   0%|          | 609/200000 [14:15<70:27:01,  1.27s/it, loss=0.2215, lr=3.04e-06, step=609]Training:   0%|          | 610/200000 [14:17<71:47:37,  1.30s/it, loss=0.2215, lr=3.04e-06, step=609]Training:   0%|          | 610/200000 [14:17<71:47:37,  1.30s/it, loss=0.2189, lr=3.05e-06, step=610]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 611/200000 [14:18<75:09:46,  1.36s/it, loss=0.2189, lr=3.05e-06, step=610]Training:   0%|          | 611/200000 [14:18<75:09:46,  1.36s/it, loss=0.0662, lr=3.05e-06, step=611]Training:   0%|          | 612/200000 [14:20<77:18:47,  1.40s/it, loss=0.0662, lr=3.05e-06, step=611]Training:   0%|          | 612/200000 [14:20<77:18:47,  1.40s/it, loss=0.1976, lr=3.06e-06, step=612]Training:   0%|          | 613/200000 [14:21<72:13:57,  1.30s/it, loss=0.1976, lr=3.06e-06, step=612]Training:   0%|          | 613/200000 [14:21<72:13:57,  1.30s/it, loss=0.1069, lr=3.06e-06, step=613]Training:   0%|          | 614/200000 [14:22<68:25:32,  1.24s/it, loss=0.1069, lr=3.06e-06, step=613]Training:   0%|          | 614/200000 [14:22<68:25:32,  1.24s/it, loss=0.1046, lr=3.07e-06, step=614]Training:   0%|          | 615/200000 [14:23<71:26:30,  1.29s/it, loss=0.1046, lr=3.07e-06, step=614]Training:   0%|          | 615/200000 [14:23<71:26:30,  1.29s/it, loss=0.2258, lr=3.07e-06, step=615]Training:   0%|          | 616/200000 [14:24<67:52:00,  1.23s/it, loss=0.2258, lr=3.07e-06, step=615]Training:   0%|          | 616/200000 [14:24<67:52:00,  1.23s/it, loss=0.1309, lr=3.08e-06, step=616]Training:   0%|          | 617/200000 [14:26<69:24:09,  1.25s/it, loss=0.1309, lr=3.08e-06, step=616]Training:   0%|          | 617/200000 [14:26<69:24:09,  1.25s/it, loss=0.0686, lr=3.08e-06, step=617]Training:   0%|          | 618/200000 [14:27<66:25:43,  1.20s/it, loss=0.0686, lr=3.08e-06, step=617]Training:   0%|          | 618/200000 [14:27<66:25:43,  1.20s/it, loss=0.1386, lr=3.09e-06, step=618]Training:   0%|          | 619/200000 [14:28<64:38:57,  1.17s/it, loss=0.1386, lr=3.09e-06, step=618]Training:   0%|          | 619/200000 [14:28<64:38:57,  1.17s/it, loss=0.0813, lr=3.09e-06, step=619]Training:   0%|          | 620/200000 [14:29<69:08:25,  1.25s/it, loss=0.0813, lr=3.09e-06, step=619]Training:   0%|          | 620/200000 [14:29<69:08:25,  1.25s/it, loss=0.1084, lr=3.10e-06, step=620]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 621/200000 [14:31<73:31:15,  1.33s/it, loss=0.1084, lr=3.10e-06, step=620]Training:   0%|          | 621/200000 [14:31<73:31:15,  1.33s/it, loss=0.0712, lr=3.10e-06, step=621]Training:   0%|          | 622/200000 [14:32<73:57:11,  1.34s/it, loss=0.0712, lr=3.10e-06, step=621]Training:   0%|          | 622/200000 [14:32<73:57:11,  1.34s/it, loss=0.0892, lr=3.11e-06, step=622]Training:   0%|          | 623/200000 [14:33<69:41:02,  1.26s/it, loss=0.0892, lr=3.11e-06, step=622]Training:   0%|          | 623/200000 [14:33<69:41:02,  1.26s/it, loss=0.0873, lr=3.11e-06, step=623]Training:   0%|          | 624/200000 [14:34<66:38:09,  1.20s/it, loss=0.0873, lr=3.11e-06, step=623]Training:   0%|          | 624/200000 [14:34<66:38:09,  1.20s/it, loss=0.0741, lr=3.12e-06, step=624]Training:   0%|          | 625/200000 [14:35<69:06:40,  1.25s/it, loss=0.0741, lr=3.12e-06, step=624]Training:   0%|          | 625/200000 [14:35<69:06:40,  1.25s/it, loss=0.0806, lr=3.12e-06, step=625]Training:   0%|          | 626/200000 [14:37<71:57:13,  1.30s/it, loss=0.0806, lr=3.12e-06, step=625]Training:   0%|          | 626/200000 [14:37<71:57:13,  1.30s/it, loss=0.0927, lr=3.13e-06, step=626]Training:   0%|          | 627/200000 [14:38<68:16:01,  1.23s/it, loss=0.0927, lr=3.13e-06, step=626]Training:   0%|          | 627/200000 [14:38<68:16:01,  1.23s/it, loss=0.1281, lr=3.13e-06, step=627]Training:   0%|          | 628/200000 [14:39<70:59:58,  1.28s/it, loss=0.1281, lr=3.13e-06, step=627]Training:   0%|          | 628/200000 [14:39<70:59:58,  1.28s/it, loss=0.1009, lr=3.14e-06, step=628]Training:   0%|          | 629/200000 [14:40<67:35:50,  1.22s/it, loss=0.1009, lr=3.14e-06, step=628]Training:   0%|          | 629/200000 [14:40<67:35:50,  1.22s/it, loss=0.0729, lr=3.14e-06, step=629]Training:   0%|          | 630/200000 [14:42<70:01:49,  1.26s/it, loss=0.0729, lr=3.14e-06, step=629]Training:   0%|          | 630/200000 [14:42<70:01:49,  1.26s/it, loss=0.1033, lr=3.15e-06, step=630]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 631/200000 [14:43<71:30:36,  1.29s/it, loss=0.1033, lr=3.15e-06, step=630]Training:   0%|          | 631/200000 [14:43<71:30:36,  1.29s/it, loss=0.1079, lr=3.15e-06, step=631]Training:   0%|          | 632/200000 [14:45<74:41:51,  1.35s/it, loss=0.1079, lr=3.15e-06, step=631]Training:   0%|          | 632/200000 [14:45<74:41:51,  1.35s/it, loss=0.1256, lr=3.16e-06, step=632]Training:   0%|          | 633/200000 [14:46<77:18:18,  1.40s/it, loss=0.1256, lr=3.16e-06, step=632]Training:   0%|          | 633/200000 [14:46<77:18:18,  1.40s/it, loss=0.1045, lr=3.16e-06, step=633]Training:   0%|          | 634/200000 [14:47<71:59:52,  1.30s/it, loss=0.1045, lr=3.16e-06, step=633]Training:   0%|          | 634/200000 [14:47<71:59:52,  1.30s/it, loss=0.0519, lr=3.17e-06, step=634]Training:   0%|          | 635/200000 [14:48<68:18:10,  1.23s/it, loss=0.0519, lr=3.17e-06, step=634]Training:   0%|          | 635/200000 [14:48<68:18:10,  1.23s/it, loss=0.0804, lr=3.17e-06, step=635]Training:   0%|          | 636/200000 [14:50<70:48:54,  1.28s/it, loss=0.0804, lr=3.17e-06, step=635]Training:   0%|          | 636/200000 [14:50<70:48:54,  1.28s/it, loss=0.0941, lr=3.18e-06, step=636]Training:   0%|          | 637/200000 [14:51<67:26:38,  1.22s/it, loss=0.0941, lr=3.18e-06, step=636]Training:   0%|          | 637/200000 [14:51<67:26:38,  1.22s/it, loss=0.1194, lr=3.18e-06, step=637]Training:   0%|          | 638/200000 [14:52<68:53:06,  1.24s/it, loss=0.1194, lr=3.18e-06, step=637]Training:   0%|          | 638/200000 [14:52<68:53:06,  1.24s/it, loss=0.3181, lr=3.19e-06, step=638]Training:   0%|          | 639/200000 [14:53<66:04:24,  1.19s/it, loss=0.3181, lr=3.19e-06, step=638]Training:   0%|          | 639/200000 [14:53<66:04:24,  1.19s/it, loss=0.0842, lr=3.19e-06, step=639]Training:   0%|          | 640/200000 [14:54<64:05:19,  1.16s/it, loss=0.0842, lr=3.19e-06, step=639]Training:   0%|          | 640/200000 [14:54<64:05:19,  1.16s/it, loss=0.1070, lr=3.20e-06, step=640]Training:   0%|          | 641/200000 [14:56<68:31:56,  1.24s/it, loss=0.1070, lr=3.20e-06, step=640]Training:   0%|          | 641/200000 [14:56<68:31:56,  1.24s/it, loss=0.0586, lr=3.20e-06, step=641]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 642/200000 [14:57<72:06:13,  1.30s/it, loss=0.0586, lr=3.20e-06, step=641]Training:   0%|          | 642/200000 [14:57<72:06:13,  1.30s/it, loss=0.1268, lr=3.21e-06, step=642]Training:   0%|          | 643/200000 [14:58<73:34:38,  1.33s/it, loss=0.1268, lr=3.21e-06, step=642]Training:   0%|          | 643/200000 [14:58<73:34:38,  1.33s/it, loss=0.0592, lr=3.21e-06, step=643]Training:   0%|          | 644/200000 [15:00<69:20:25,  1.25s/it, loss=0.0592, lr=3.21e-06, step=643]Training:   0%|          | 644/200000 [15:00<69:20:25,  1.25s/it, loss=0.0619, lr=3.22e-06, step=644]Training:   0%|          | 645/200000 [15:01<66:27:54,  1.20s/it, loss=0.0619, lr=3.22e-06, step=644]Training:   0%|          | 645/200000 [15:01<66:27:54,  1.20s/it, loss=0.1238, lr=3.22e-06, step=645]Training:   0%|          | 646/200000 [15:02<68:34:42,  1.24s/it, loss=0.1238, lr=3.22e-06, step=645]Training:   0%|          | 646/200000 [15:02<68:34:42,  1.24s/it, loss=0.2123, lr=3.23e-06, step=646]Training:   0%|          | 647/200000 [15:03<70:04:23,  1.27s/it, loss=0.2123, lr=3.23e-06, step=646]Training:   0%|          | 647/200000 [15:03<70:04:23,  1.27s/it, loss=0.0706, lr=3.23e-06, step=647]Training:   0%|          | 648/200000 [15:04<66:54:30,  1.21s/it, loss=0.0706, lr=3.23e-06, step=647]Training:   0%|          | 648/200000 [15:04<66:54:30,  1.21s/it, loss=0.1448, lr=3.24e-06, step=648]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 649/200000 [15:06<69:19:49,  1.25s/it, loss=0.1448, lr=3.24e-06, step=648]Training:   0%|          | 649/200000 [15:06<69:19:49,  1.25s/it, loss=0.1280, lr=3.24e-06, step=649]Training:   0%|          | 650/200000 [15:07<66:23:01,  1.20s/it, loss=0.1280, lr=3.24e-06, step=649]Training:   0%|          | 650/200000 [15:07<66:23:01,  1.20s/it, loss=0.2502, lr=3.25e-06, step=650]Training:   0%|          | 651/200000 [15:08<68:28:24,  1.24s/it, loss=0.2502, lr=3.25e-06, step=650]Training:   0%|          | 651/200000 [15:08<68:28:24,  1.24s/it, loss=0.1344, lr=3.25e-06, step=651]Training:   0%|          | 652/200000 [15:09<65:49:06,  1.19s/it, loss=0.1344, lr=3.25e-06, step=651]Training:   0%|          | 652/200000 [15:09<65:49:06,  1.19s/it, loss=0.0626, lr=3.26e-06, step=652]Training:   0%|          | 653/200000 [15:11<69:43:52,  1.26s/it, loss=0.0626, lr=3.26e-06, step=652]Training:   0%|          | 653/200000 [15:11<69:43:52,  1.26s/it, loss=0.1401, lr=3.26e-06, step=653]Training:   0%|          | 654/200000 [15:12<72:03:57,  1.30s/it, loss=0.1401, lr=3.26e-06, step=653]Training:   0%|          | 654/200000 [15:12<72:03:57,  1.30s/it, loss=0.0908, lr=3.27e-06, step=654]Training:   0%|          | 655/200000 [15:13<68:19:46,  1.23s/it, loss=0.0908, lr=3.27e-06, step=654]Training:   0%|          | 655/200000 [15:13<68:19:46,  1.23s/it, loss=0.1119, lr=3.27e-06, step=655]Training:   0%|          | 656/200000 [15:14<65:42:37,  1.19s/it, loss=0.1119, lr=3.27e-06, step=655]Training:   0%|          | 656/200000 [15:14<65:42:37,  1.19s/it, loss=0.1336, lr=3.28e-06, step=656]Training:   0%|          | 657/200000 [15:16<68:03:37,  1.23s/it, loss=0.1336, lr=3.28e-06, step=656]Training:   0%|          | 657/200000 [15:16<68:03:37,  1.23s/it, loss=0.1647, lr=3.28e-06, step=657]Training:   0%|          | 658/200000 [15:17<71:41:59,  1.29s/it, loss=0.1647, lr=3.28e-06, step=657]Training:   0%|          | 658/200000 [15:17<71:41:59,  1.29s/it, loss=0.0762, lr=3.29e-06, step=658]Training:   0%|          | 659/200000 [15:18<68:05:01,  1.23s/it, loss=0.0762, lr=3.29e-06, step=658]Training:   0%|          | 659/200000 [15:18<68:05:01,  1.23s/it, loss=0.1139, lr=3.29e-06, step=659]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 660/200000 [15:19<71:04:14,  1.28s/it, loss=0.1139, lr=3.29e-06, step=659]Training:   0%|          | 660/200000 [15:19<71:04:14,  1.28s/it, loss=0.0974, lr=3.30e-06, step=660]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 661/200000 [15:21<67:38:42,  1.22s/it, loss=0.0974, lr=3.30e-06, step=660]Training:   0%|          | 661/200000 [15:21<67:38:42,  1.22s/it, loss=0.0896, lr=3.30e-06, step=661]Training:   0%|          | 662/200000 [15:22<70:00:13,  1.26s/it, loss=0.0896, lr=3.30e-06, step=661]Training:   0%|          | 662/200000 [15:22<70:00:13,  1.26s/it, loss=0.1600, lr=3.31e-06, step=662]Training:   0%|          | 663/200000 [15:23<71:04:36,  1.28s/it, loss=0.1600, lr=3.31e-06, step=662]Training:   0%|          | 663/200000 [15:23<71:04:36,  1.28s/it, loss=0.0658, lr=3.31e-06, step=663]Training:   0%|          | 664/200000 [15:25<74:45:10,  1.35s/it, loss=0.0658, lr=3.31e-06, step=663]Training:   0%|          | 664/200000 [15:25<74:45:10,  1.35s/it, loss=0.1205, lr=3.32e-06, step=664]Training:   0%|          | 665/200000 [15:26<77:40:50,  1.40s/it, loss=0.1205, lr=3.32e-06, step=664]Training:   0%|          | 665/200000 [15:26<77:40:50,  1.40s/it, loss=0.1119, lr=3.32e-06, step=665]Training:   0%|          | 666/200000 [15:27<72:13:40,  1.30s/it, loss=0.1119, lr=3.32e-06, step=665]Training:   0%|          | 666/200000 [15:27<72:13:40,  1.30s/it, loss=0.1227, lr=3.33e-06, step=666]Training:   0%|          | 667/200000 [15:28<68:28:33,  1.24s/it, loss=0.1227, lr=3.33e-06, step=666]Training:   0%|          | 667/200000 [15:28<68:28:33,  1.24s/it, loss=0.0954, lr=3.33e-06, step=667]Training:   0%|          | 668/200000 [15:30<71:19:03,  1.29s/it, loss=0.0954, lr=3.33e-06, step=667]Training:   0%|          | 668/200000 [15:30<71:19:03,  1.29s/it, loss=0.0865, lr=3.34e-06, step=668]Training:   0%|          | 669/200000 [15:31<67:48:18,  1.22s/it, loss=0.0865, lr=3.34e-06, step=668]Training:   0%|          | 669/200000 [15:31<67:48:18,  1.22s/it, loss=0.1451, lr=3.34e-06, step=669]Training:   0%|          | 670/200000 [15:32<69:23:48,  1.25s/it, loss=0.1451, lr=3.34e-06, step=669]Training:   0%|          | 670/200000 [15:32<69:23:48,  1.25s/it, loss=0.2582, lr=3.35e-06, step=670]Training:   0%|          | 671/200000 [15:33<66:27:04,  1.20s/it, loss=0.2582, lr=3.35e-06, step=670]Training:   0%|          | 671/200000 [15:33<66:27:04,  1.20s/it, loss=0.0835, lr=3.35e-06, step=671]Training:   0%|          | 672/200000 [15:34<64:22:43,  1.16s/it, loss=0.0835, lr=3.35e-06, step=671]Training:   0%|          | 672/200000 [15:34<64:22:43,  1.16s/it, loss=0.0772, lr=3.36e-06, step=672]Training:   0%|          | 673/200000 [15:36<68:54:22,  1.24s/it, loss=0.0772, lr=3.36e-06, step=672]Training:   0%|          | 673/200000 [15:36<68:54:22,  1.24s/it, loss=0.1440, lr=3.36e-06, step=673]Training:   0%|          | 674/200000 [15:37<72:09:20,  1.30s/it, loss=0.1440, lr=3.36e-06, step=673]Training:   0%|          | 674/200000 [15:37<72:09:20,  1.30s/it, loss=0.1031, lr=3.37e-06, step=674]Training:   0%|          | 675/200000 [15:39<73:46:40,  1.33s/it, loss=0.1031, lr=3.37e-06, step=674]Training:   0%|          | 675/200000 [15:39<73:46:40,  1.33s/it, loss=0.0662, lr=3.37e-06, step=675]Training:   0%|          | 676/200000 [15:40<69:29:38,  1.26s/it, loss=0.0662, lr=3.37e-06, step=675]Training:   0%|          | 676/200000 [15:40<69:29:38,  1.26s/it, loss=0.0624, lr=3.38e-06, step=676]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 677/200000 [15:41<66:32:30,  1.20s/it, loss=0.0624, lr=3.38e-06, step=676]Training:   0%|          | 677/200000 [15:41<66:32:30,  1.20s/it, loss=0.1512, lr=3.38e-06, step=677]Training:   0%|          | 678/200000 [15:42<69:06:47,  1.25s/it, loss=0.1512, lr=3.38e-06, step=677]Training:   0%|          | 678/200000 [15:42<69:06:47,  1.25s/it, loss=0.1566, lr=3.39e-06, step=678]Training:   0%|          | 679/200000 [15:44<71:27:09,  1.29s/it, loss=0.1566, lr=3.39e-06, step=678]Training:   0%|          | 679/200000 [15:44<71:27:09,  1.29s/it, loss=0.0796, lr=3.39e-06, step=679]Training:   0%|          | 680/200000 [15:45<67:51:39,  1.23s/it, loss=0.0796, lr=3.39e-06, step=679]Training:   0%|          | 680/200000 [15:45<67:51:39,  1.23s/it, loss=0.0936, lr=3.40e-06, step=680]Training:   0%|          | 681/200000 [15:46<70:48:01,  1.28s/it, loss=0.0936, lr=3.40e-06, step=680]Training:   0%|          | 681/200000 [15:46<70:48:01,  1.28s/it, loss=0.0781, lr=3.40e-06, step=681]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 682/200000 [15:47<67:25:50,  1.22s/it, loss=0.0781, lr=3.40e-06, step=681]Training:   0%|          | 682/200000 [15:47<67:25:50,  1.22s/it, loss=0.0567, lr=3.41e-06, step=682]Training:   0%|          | 683/200000 [15:48<69:21:50,  1.25s/it, loss=0.0567, lr=3.41e-06, step=682]Training:   0%|          | 683/200000 [15:48<69:21:50,  1.25s/it, loss=0.1003, lr=3.41e-06, step=683]Training:   0%|          | 684/200000 [15:50<70:33:48,  1.27s/it, loss=0.1003, lr=3.41e-06, step=683]Training:   0%|          | 684/200000 [15:50<70:33:48,  1.27s/it, loss=0.1334, lr=3.42e-06, step=684]Training:   0%|          | 685/200000 [15:51<73:56:28,  1.34s/it, loss=0.1334, lr=3.42e-06, step=684]Training:   0%|          | 685/200000 [15:51<73:56:28,  1.34s/it, loss=0.2348, lr=3.42e-06, step=685]Training:   0%|          | 686/200000 [15:53<75:54:41,  1.37s/it, loss=0.2348, lr=3.42e-06, step=685]Training:   0%|          | 686/200000 [15:53<75:54:41,  1.37s/it, loss=0.3584, lr=3.43e-06, step=686]Training:   0%|          | 687/200000 [15:54<71:01:50,  1.28s/it, loss=0.3584, lr=3.43e-06, step=686]Training:   0%|          | 687/200000 [15:54<71:01:50,  1.28s/it, loss=0.0882, lr=3.43e-06, step=687]Training:   0%|          | 688/200000 [15:55<67:35:17,  1.22s/it, loss=0.0882, lr=3.43e-06, step=687]Training:   0%|          | 688/200000 [15:55<67:35:17,  1.22s/it, loss=0.1468, lr=3.44e-06, step=688]Training:   0%|          | 689/200000 [15:56<70:59:48,  1.28s/it, loss=0.1468, lr=3.44e-06, step=688]Training:   0%|          | 689/200000 [15:56<70:59:48,  1.28s/it, loss=0.0804, lr=3.44e-06, step=689]Training:   0%|          | 690/200000 [15:57<67:34:04,  1.22s/it, loss=0.0804, lr=3.44e-06, step=689]Training:   0%|          | 690/200000 [15:57<67:34:04,  1.22s/it, loss=0.1094, lr=3.45e-06, step=690]Training:   0%|          | 691/200000 [15:59<69:43:29,  1.26s/it, loss=0.1094, lr=3.45e-06, step=690]Training:   0%|          | 691/200000 [15:59<69:43:29,  1.26s/it, loss=0.1174, lr=3.45e-06, step=691]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 692/200000 [16:00<66:41:07,  1.20s/it, loss=0.1174, lr=3.45e-06, step=691]Training:   0%|          | 692/200000 [16:00<66:41:07,  1.20s/it, loss=0.1210, lr=3.46e-06, step=692]Training:   0%|          | 693/200000 [16:01<64:34:23,  1.17s/it, loss=0.1210, lr=3.46e-06, step=692]Training:   0%|          | 693/200000 [16:01<64:34:23,  1.17s/it, loss=0.1003, lr=3.46e-06, step=693]Training:   0%|          | 694/200000 [16:02<68:53:40,  1.24s/it, loss=0.1003, lr=3.46e-06, step=693]Training:   0%|          | 694/200000 [16:02<68:53:40,  1.24s/it, loss=0.1181, lr=3.47e-06, step=694]Training:   0%|          | 695/200000 [16:04<72:30:51,  1.31s/it, loss=0.1181, lr=3.47e-06, step=694]Training:   0%|          | 695/200000 [16:04<72:30:51,  1.31s/it, loss=0.0928, lr=3.47e-06, step=695]Training:   0%|          | 696/200000 [16:05<73:39:25,  1.33s/it, loss=0.0928, lr=3.47e-06, step=695]Training:   0%|          | 696/200000 [16:05<73:39:25,  1.33s/it, loss=0.0740, lr=3.48e-06, step=696]Training:   0%|          | 697/200000 [16:06<69:27:22,  1.25s/it, loss=0.0740, lr=3.48e-06, step=696]Training:   0%|          | 697/200000 [16:06<69:27:22,  1.25s/it, loss=0.1000, lr=3.48e-06, step=697]Training:   0%|          | 698/200000 [16:07<66:29:01,  1.20s/it, loss=0.1000, lr=3.48e-06, step=697]Training:   0%|          | 698/200000 [16:07<66:29:01,  1.20s/it, loss=0.2150, lr=3.49e-06, step=698]Training:   0%|          | 699/200000 [16:09<68:47:19,  1.24s/it, loss=0.2150, lr=3.49e-06, step=698]Training:   0%|          | 699/200000 [16:09<68:47:19,  1.24s/it, loss=0.1141, lr=3.49e-06, step=699]Training:   0%|          | 700/200000 [16:10<70:44:47,  1.28s/it, loss=0.1141, lr=3.49e-06, step=699]Training:   0%|          | 700/200000 [16:10<70:44:47,  1.28s/it, loss=0.1504, lr=3.50e-06, step=700]23:09:24.841 [I] step=700 loss=0.1188 lr=3.26e-06 grad_norm=0.87 time=125.7s                      (701675:train_pytorch.py:582)
Training:   0%|          | 701/200000 [16:11<67:24:06,  1.22s/it, loss=0.1504, lr=3.50e-06, step=700]Training:   0%|          | 701/200000 [16:11<67:24:06,  1.22s/it, loss=0.0586, lr=3.50e-06, step=701]Training:   0%|          | 702/200000 [16:12<69:09:05,  1.25s/it, loss=0.0586, lr=3.50e-06, step=701]Training:   0%|          | 702/200000 [16:12<69:09:05,  1.25s/it, loss=0.1778, lr=3.51e-06, step=702]Training:   0%|          | 703/200000 [16:13<66:15:56,  1.20s/it, loss=0.1778, lr=3.51e-06, step=702]Training:   0%|          | 703/200000 [16:13<66:15:56,  1.20s/it, loss=0.0580, lr=3.51e-06, step=703]Training:   0%|          | 704/200000 [16:15<69:07:50,  1.25s/it, loss=0.0580, lr=3.51e-06, step=703]Training:   0%|          | 704/200000 [16:15<69:07:50,  1.25s/it, loss=0.0850, lr=3.52e-06, step=704]Training:   0%|          | 705/200000 [16:16<66:14:15,  1.20s/it, loss=0.0850, lr=3.52e-06, step=704]Training:   0%|          | 705/200000 [16:16<66:14:15,  1.20s/it, loss=0.0854, lr=3.52e-06, step=705]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 706/200000 [16:17<70:01:24,  1.26s/it, loss=0.0854, lr=3.52e-06, step=705]Training:   0%|          | 706/200000 [16:17<70:01:24,  1.26s/it, loss=0.1570, lr=3.53e-06, step=706]Training:   0%|          | 707/200000 [16:19<73:00:08,  1.32s/it, loss=0.1570, lr=3.53e-06, step=706]Training:   0%|          | 707/200000 [16:19<73:00:08,  1.32s/it, loss=0.0530, lr=3.53e-06, step=707]Training:   0%|          | 708/200000 [16:20<68:56:37,  1.25s/it, loss=0.0530, lr=3.53e-06, step=707]Training:   0%|          | 708/200000 [16:20<68:56:37,  1.25s/it, loss=0.1734, lr=3.54e-06, step=708]Training:   0%|          | 709/200000 [16:21<66:08:39,  1.19s/it, loss=0.1734, lr=3.54e-06, step=708]Training:   0%|          | 709/200000 [16:21<66:08:39,  1.19s/it, loss=0.0950, lr=3.54e-06, step=709]Training:   0%|          | 710/200000 [16:22<69:06:57,  1.25s/it, loss=0.0950, lr=3.54e-06, step=709]Training:   0%|          | 710/200000 [16:22<69:06:57,  1.25s/it, loss=0.2369, lr=3.55e-06, step=710]Training:   0%|          | 711/200000 [16:24<71:37:25,  1.29s/it, loss=0.2369, lr=3.55e-06, step=710]Training:   0%|          | 711/200000 [16:24<71:37:25,  1.29s/it, loss=0.0558, lr=3.55e-06, step=711]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 712/200000 [16:25<68:00:00,  1.23s/it, loss=0.0558, lr=3.55e-06, step=711]Training:   0%|          | 712/200000 [16:25<68:00:00,  1.23s/it, loss=0.0591, lr=3.56e-06, step=712]Training:   0%|          | 713/200000 [16:26<70:55:05,  1.28s/it, loss=0.0591, lr=3.56e-06, step=712]Training:   0%|          | 713/200000 [16:26<70:55:05,  1.28s/it, loss=0.0874, lr=3.56e-06, step=713]Training:   0%|          | 714/200000 [16:27<67:29:52,  1.22s/it, loss=0.0874, lr=3.56e-06, step=713]Training:   0%|          | 714/200000 [16:27<67:29:52,  1.22s/it, loss=0.0877, lr=3.57e-06, step=714]Training:   0%|          | 715/200000 [16:29<69:55:42,  1.26s/it, loss=0.0877, lr=3.57e-06, step=714]Training:   0%|          | 715/200000 [16:29<69:55:42,  1.26s/it, loss=0.0968, lr=3.57e-06, step=715]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 716/200000 [16:30<71:31:00,  1.29s/it, loss=0.0968, lr=3.57e-06, step=715]Training:   0%|          | 716/200000 [16:30<71:31:00,  1.29s/it, loss=0.0673, lr=3.58e-06, step=716]Training:   0%|          | 717/200000 [16:31<74:58:18,  1.35s/it, loss=0.0673, lr=3.58e-06, step=716]Training:   0%|          | 717/200000 [16:31<74:58:18,  1.35s/it, loss=0.1089, lr=3.58e-06, step=717]Training:   0%|          | 718/200000 [16:33<77:04:43,  1.39s/it, loss=0.1089, lr=3.58e-06, step=717]Training:   0%|          | 718/200000 [16:33<77:04:43,  1.39s/it, loss=0.0499, lr=3.59e-06, step=718]Training:   0%|          | 719/200000 [16:34<71:49:41,  1.30s/it, loss=0.0499, lr=3.59e-06, step=718]Training:   0%|          | 719/200000 [16:34<71:49:41,  1.30s/it, loss=0.0560, lr=3.59e-06, step=719]Training:   0%|          | 720/200000 [16:35<68:09:54,  1.23s/it, loss=0.0560, lr=3.59e-06, step=719]Training:   0%|          | 720/200000 [16:35<68:09:54,  1.23s/it, loss=0.0749, lr=3.60e-06, step=720]Training:   0%|          | 721/200000 [16:36<71:13:01,  1.29s/it, loss=0.0749, lr=3.60e-06, step=720]Training:   0%|          | 721/200000 [16:36<71:13:01,  1.29s/it, loss=0.1156, lr=3.60e-06, step=721]Training:   0%|          | 722/200000 [16:38<67:42:22,  1.22s/it, loss=0.1156, lr=3.60e-06, step=721]Training:   0%|          | 722/200000 [16:38<67:42:22,  1.22s/it, loss=0.0657, lr=3.61e-06, step=722]Training:   0%|          | 723/200000 [16:39<69:36:09,  1.26s/it, loss=0.0657, lr=3.61e-06, step=722]Training:   0%|          | 723/200000 [16:39<69:36:09,  1.26s/it, loss=0.1067, lr=3.61e-06, step=723]Training:   0%|          | 724/200000 [16:40<66:34:01,  1.20s/it, loss=0.1067, lr=3.61e-06, step=723]Training:   0%|          | 724/200000 [16:40<66:34:01,  1.20s/it, loss=0.1193, lr=3.62e-06, step=724]Training:   0%|          | 725/200000 [16:41<64:26:51,  1.16s/it, loss=0.1193, lr=3.62e-06, step=724]Training:   0%|          | 725/200000 [16:41<64:26:51,  1.16s/it, loss=0.1729, lr=3.62e-06, step=725]Training:   0%|          | 726/200000 [16:42<69:01:58,  1.25s/it, loss=0.1729, lr=3.62e-06, step=725]Training:   0%|          | 726/200000 [16:43<69:01:58,  1.25s/it, loss=0.1193, lr=3.63e-06, step=726]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 727/200000 [16:44<72:25:59,  1.31s/it, loss=0.1193, lr=3.63e-06, step=726]Training:   0%|          | 727/200000 [16:44<72:25:59,  1.31s/it, loss=0.0716, lr=3.63e-06, step=727]Training:   0%|          | 728/200000 [16:45<73:46:51,  1.33s/it, loss=0.0716, lr=3.63e-06, step=727]Training:   0%|          | 728/200000 [16:45<73:46:51,  1.33s/it, loss=0.1097, lr=3.64e-06, step=728]Training:   0%|          | 729/200000 [16:46<69:32:29,  1.26s/it, loss=0.1097, lr=3.64e-06, step=728]Training:   0%|          | 729/200000 [16:46<69:32:29,  1.26s/it, loss=0.3117, lr=3.64e-06, step=729]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 730/200000 [16:47<66:36:05,  1.20s/it, loss=0.3117, lr=3.64e-06, step=729]Training:   0%|          | 730/200000 [16:47<66:36:05,  1.20s/it, loss=0.0799, lr=3.65e-06, step=730]Training:   0%|          | 731/200000 [16:49<69:04:46,  1.25s/it, loss=0.0799, lr=3.65e-06, step=730]Training:   0%|          | 731/200000 [16:49<69:04:46,  1.25s/it, loss=0.1716, lr=3.65e-06, step=731]Training:   0%|          | 732/200000 [16:50<71:15:59,  1.29s/it, loss=0.1716, lr=3.65e-06, step=731]Training:   0%|          | 732/200000 [16:50<71:15:59,  1.29s/it, loss=0.0982, lr=3.66e-06, step=732]Training:   0%|          | 733/200000 [16:51<67:46:49,  1.22s/it, loss=0.0982, lr=3.66e-06, step=732]Training:   0%|          | 733/200000 [16:51<67:46:49,  1.22s/it, loss=0.1053, lr=3.66e-06, step=733]Training:   0%|          | 734/200000 [16:53<70:23:16,  1.27s/it, loss=0.1053, lr=3.66e-06, step=733]Training:   0%|          | 734/200000 [16:53<70:23:16,  1.27s/it, loss=0.0767, lr=3.67e-06, step=734]Training:   0%|          | 735/200000 [16:54<67:09:53,  1.21s/it, loss=0.0767, lr=3.67e-06, step=734]Training:   0%|          | 735/200000 [16:54<67:09:53,  1.21s/it, loss=0.0799, lr=3.67e-06, step=735]Training:   0%|          | 736/200000 [16:55<69:45:37,  1.26s/it, loss=0.0799, lr=3.67e-06, step=735]Training:   0%|          | 736/200000 [16:55<69:45:37,  1.26s/it, loss=0.0449, lr=3.68e-06, step=736]Training:   0%|          | 737/200000 [16:56<71:17:56,  1.29s/it, loss=0.0449, lr=3.68e-06, step=736]Training:   0%|          | 737/200000 [16:56<71:17:56,  1.29s/it, loss=0.0811, lr=3.68e-06, step=737]Training:   0%|          | 738/200000 [16:58<74:35:39,  1.35s/it, loss=0.0811, lr=3.68e-06, step=737]Training:   0%|          | 738/200000 [16:58<74:35:39,  1.35s/it, loss=0.0518, lr=3.69e-06, step=738]Training:   0%|          | 739/200000 [16:59<76:38:27,  1.38s/it, loss=0.0518, lr=3.69e-06, step=738]Training:   0%|          | 739/200000 [16:59<76:38:27,  1.38s/it, loss=0.1150, lr=3.69e-06, step=739]Training:   0%|          | 740/200000 [17:01<71:59:48,  1.30s/it, loss=0.1150, lr=3.69e-06, step=739]Training:   0%|          | 740/200000 [17:01<71:59:48,  1.30s/it, loss=0.0459, lr=3.70e-06, step=740]Training:   0%|          | 741/200000 [17:02<68:18:45,  1.23s/it, loss=0.0459, lr=3.70e-06, step=740]Training:   0%|          | 741/200000 [17:02<68:18:45,  1.23s/it, loss=0.1082, lr=3.70e-06, step=741]Training:   0%|          | 742/200000 [17:03<71:39:02,  1.29s/it, loss=0.1082, lr=3.70e-06, step=741]Training:   0%|          | 742/200000 [17:03<71:39:02,  1.29s/it, loss=0.2203, lr=3.71e-06, step=742]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 743/200000 [17:04<68:04:33,  1.23s/it, loss=0.2203, lr=3.71e-06, step=742]Training:   0%|          | 743/200000 [17:04<68:04:33,  1.23s/it, loss=0.0885, lr=3.71e-06, step=743]Training:   0%|          | 744/200000 [17:05<69:04:18,  1.25s/it, loss=0.0885, lr=3.71e-06, step=743]Training:   0%|          | 744/200000 [17:05<69:04:18,  1.25s/it, loss=0.0601, lr=3.72e-06, step=744]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 745/200000 [17:07<66:14:11,  1.20s/it, loss=0.0601, lr=3.72e-06, step=744]Training:   0%|          | 745/200000 [17:07<66:14:11,  1.20s/it, loss=0.2121, lr=3.72e-06, step=745]Training:   0%|          | 746/200000 [17:08<64:12:51,  1.16s/it, loss=0.2121, lr=3.72e-06, step=745]Training:   0%|          | 746/200000 [17:08<64:12:51,  1.16s/it, loss=0.1873, lr=3.73e-06, step=746]Training:   0%|          | 747/200000 [17:09<68:42:31,  1.24s/it, loss=0.1873, lr=3.73e-06, step=746]Training:   0%|          | 747/200000 [17:09<68:42:31,  1.24s/it, loss=0.1015, lr=3.73e-06, step=747]Training:   0%|          | 748/200000 [17:10<72:18:45,  1.31s/it, loss=0.1015, lr=3.73e-06, step=747]Training:   0%|          | 748/200000 [17:10<72:18:45,  1.31s/it, loss=0.0478, lr=3.74e-06, step=748]Training:   0%|          | 749/200000 [17:12<73:25:53,  1.33s/it, loss=0.0478, lr=3.74e-06, step=748]Training:   0%|          | 749/200000 [17:12<73:25:53,  1.33s/it, loss=0.0604, lr=3.74e-06, step=749]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 750/200000 [17:13<69:14:35,  1.25s/it, loss=0.0604, lr=3.74e-06, step=749]Training:   0%|          | 750/200000 [17:13<69:14:35,  1.25s/it, loss=0.1167, lr=3.75e-06, step=750]Training:   0%|          | 751/200000 [17:14<66:21:31,  1.20s/it, loss=0.1167, lr=3.75e-06, step=750]Training:   0%|          | 751/200000 [17:14<66:21:31,  1.20s/it, loss=0.0862, lr=3.75e-06, step=751]Training:   0%|          | 752/200000 [17:15<68:38:18,  1.24s/it, loss=0.0862, lr=3.75e-06, step=751]Training:   0%|          | 752/200000 [17:15<68:38:18,  1.24s/it, loss=0.1130, lr=3.76e-06, step=752]Training:   0%|          | 753/200000 [17:17<70:43:17,  1.28s/it, loss=0.1130, lr=3.76e-06, step=752]Training:   0%|          | 753/200000 [17:17<70:43:17,  1.28s/it, loss=0.1347, lr=3.76e-06, step=753]Training:   0%|          | 754/200000 [17:18<67:20:42,  1.22s/it, loss=0.1347, lr=3.76e-06, step=753]Training:   0%|          | 754/200000 [17:18<67:20:42,  1.22s/it, loss=0.0746, lr=3.77e-06, step=754]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 755/200000 [17:19<68:54:21,  1.25s/it, loss=0.0746, lr=3.77e-06, step=754]Training:   0%|          | 755/200000 [17:19<68:54:21,  1.25s/it, loss=0.0867, lr=3.77e-06, step=755]Training:   0%|          | 756/200000 [17:20<66:06:27,  1.19s/it, loss=0.0867, lr=3.77e-06, step=755]Training:   0%|          | 756/200000 [17:20<66:06:27,  1.19s/it, loss=0.2378, lr=3.78e-06, step=756]Training:   0%|          | 757/200000 [17:21<68:07:00,  1.23s/it, loss=0.2378, lr=3.78e-06, step=756]Training:   0%|          | 757/200000 [17:21<68:07:00,  1.23s/it, loss=0.0857, lr=3.78e-06, step=757]Training:   0%|          | 758/200000 [17:23<65:31:36,  1.18s/it, loss=0.0857, lr=3.78e-06, step=757]Training:   0%|          | 758/200000 [17:23<65:31:36,  1.18s/it, loss=0.0542, lr=3.79e-06, step=758]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 759/200000 [17:24<69:30:08,  1.26s/it, loss=0.0542, lr=3.79e-06, step=758]Training:   0%|          | 759/200000 [17:24<69:30:08,  1.26s/it, loss=0.0845, lr=3.79e-06, step=759]Training:   0%|          | 760/200000 [17:25<71:46:40,  1.30s/it, loss=0.0845, lr=3.79e-06, step=759]Training:   0%|          | 760/200000 [17:25<71:46:40,  1.30s/it, loss=0.1232, lr=3.80e-06, step=760]Training:   0%|          | 761/200000 [17:26<68:05:30,  1.23s/it, loss=0.1232, lr=3.80e-06, step=760]Training:   0%|          | 761/200000 [17:26<68:05:30,  1.23s/it, loss=0.1157, lr=3.80e-06, step=761]Training:   0%|          | 762/200000 [17:28<65:32:35,  1.18s/it, loss=0.1157, lr=3.80e-06, step=761]Training:   0%|          | 762/200000 [17:28<65:32:35,  1.18s/it, loss=0.0888, lr=3.81e-06, step=762]Training:   0%|          | 763/200000 [17:29<68:06:13,  1.23s/it, loss=0.0888, lr=3.81e-06, step=762]Training:   0%|          | 763/200000 [17:29<68:06:13,  1.23s/it, loss=0.1253, lr=3.81e-06, step=763]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 764/200000 [17:30<71:23:40,  1.29s/it, loss=0.1253, lr=3.81e-06, step=763]Training:   0%|          | 764/200000 [17:30<71:23:40,  1.29s/it, loss=0.0727, lr=3.82e-06, step=764]Training:   0%|          | 765/200000 [17:31<67:51:57,  1.23s/it, loss=0.0727, lr=3.82e-06, step=764]Training:   0%|          | 765/200000 [17:31<67:51:57,  1.23s/it, loss=0.1984, lr=3.82e-06, step=765]Training:   0%|          | 766/200000 [17:33<70:50:46,  1.28s/it, loss=0.1984, lr=3.82e-06, step=765]Training:   0%|          | 766/200000 [17:33<70:50:46,  1.28s/it, loss=0.0619, lr=3.83e-06, step=766]Training:   0%|          | 767/200000 [17:34<67:27:51,  1.22s/it, loss=0.0619, lr=3.83e-06, step=766]Training:   0%|          | 767/200000 [17:34<67:27:51,  1.22s/it, loss=0.0914, lr=3.83e-06, step=767]Training:   0%|          | 768/200000 [17:35<70:16:54,  1.27s/it, loss=0.0914, lr=3.83e-06, step=767]Training:   0%|          | 768/200000 [17:35<70:16:54,  1.27s/it, loss=0.1254, lr=3.84e-06, step=768]Training:   0%|          | 769/200000 [17:37<71:54:54,  1.30s/it, loss=0.1254, lr=3.84e-06, step=768]Training:   0%|          | 769/200000 [17:37<71:54:54,  1.30s/it, loss=0.0709, lr=3.84e-06, step=769]Training:   0%|          | 770/200000 [17:38<75:09:22,  1.36s/it, loss=0.0709, lr=3.84e-06, step=769]Training:   0%|          | 770/200000 [17:38<75:09:22,  1.36s/it, loss=0.1007, lr=3.85e-06, step=770]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 771/200000 [17:40<77:57:48,  1.41s/it, loss=0.1007, lr=3.85e-06, step=770]Training:   0%|          | 771/200000 [17:40<77:57:48,  1.41s/it, loss=0.2480, lr=3.85e-06, step=771]Training:   0%|          | 772/200000 [17:41<72:25:55,  1.31s/it, loss=0.2480, lr=3.85e-06, step=771]Training:   0%|          | 772/200000 [17:41<72:25:55,  1.31s/it, loss=0.0904, lr=3.86e-06, step=772]Training:   0%|          | 773/200000 [17:42<68:36:33,  1.24s/it, loss=0.0904, lr=3.86e-06, step=772]Training:   0%|          | 773/200000 [17:42<68:36:33,  1.24s/it, loss=0.1485, lr=3.86e-06, step=773]Training:   0%|          | 774/200000 [17:43<71:24:37,  1.29s/it, loss=0.1485, lr=3.86e-06, step=773]Training:   0%|          | 774/200000 [17:43<71:24:37,  1.29s/it, loss=0.1567, lr=3.87e-06, step=774]Training:   0%|          | 775/200000 [17:44<67:54:22,  1.23s/it, loss=0.1567, lr=3.87e-06, step=774]Training:   0%|          | 775/200000 [17:44<67:54:22,  1.23s/it, loss=0.0592, lr=3.87e-06, step=775]Training:   0%|          | 776/200000 [17:46<69:41:05,  1.26s/it, loss=0.0592, lr=3.87e-06, step=775]Training:   0%|          | 776/200000 [17:46<69:41:05,  1.26s/it, loss=0.2528, lr=3.88e-06, step=776]Training:   0%|          | 777/200000 [17:47<66:39:53,  1.20s/it, loss=0.2528, lr=3.88e-06, step=776]Training:   0%|          | 777/200000 [17:47<66:39:53,  1.20s/it, loss=0.1120, lr=3.88e-06, step=777]Training:   0%|          | 778/200000 [17:48<64:30:36,  1.17s/it, loss=0.1120, lr=3.88e-06, step=777]Training:   0%|          | 778/200000 [17:48<64:30:36,  1.17s/it, loss=0.0859, lr=3.89e-06, step=778]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 779/200000 [17:49<69:06:10,  1.25s/it, loss=0.0859, lr=3.89e-06, step=778]Training:   0%|          | 779/200000 [17:49<69:06:10,  1.25s/it, loss=0.1165, lr=3.89e-06, step=779]Training:   0%|          | 780/200000 [17:51<72:50:36,  1.32s/it, loss=0.1165, lr=3.89e-06, step=779]Training:   0%|          | 780/200000 [17:51<72:50:36,  1.32s/it, loss=0.0703, lr=3.90e-06, step=780]Training:   0%|          | 781/200000 [17:52<73:58:00,  1.34s/it, loss=0.0703, lr=3.90e-06, step=780]Training:   0%|          | 781/200000 [17:52<73:58:00,  1.34s/it, loss=0.1941, lr=3.90e-06, step=781]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 782/200000 [17:53<69:38:08,  1.26s/it, loss=0.1941, lr=3.90e-06, step=781]Training:   0%|          | 782/200000 [17:53<69:38:08,  1.26s/it, loss=0.1184, lr=3.91e-06, step=782]Training:   0%|          | 783/200000 [17:54<66:38:27,  1.20s/it, loss=0.1184, lr=3.91e-06, step=782]Training:   0%|          | 783/200000 [17:54<66:38:27,  1.20s/it, loss=0.1195, lr=3.91e-06, step=783]Training:   0%|          | 784/200000 [17:56<69:10:34,  1.25s/it, loss=0.1195, lr=3.91e-06, step=783]Training:   0%|          | 784/200000 [17:56<69:10:34,  1.25s/it, loss=0.1035, lr=3.92e-06, step=784]Training:   0%|          | 785/200000 [17:57<71:59:10,  1.30s/it, loss=0.1035, lr=3.92e-06, step=784]Training:   0%|          | 785/200000 [17:57<71:59:10,  1.30s/it, loss=0.2193, lr=3.92e-06, step=785]Training:   0%|          | 786/200000 [17:58<68:14:33,  1.23s/it, loss=0.2193, lr=3.92e-06, step=785]Training:   0%|          | 786/200000 [17:58<68:14:33,  1.23s/it, loss=0.0973, lr=3.93e-06, step=786]Training:   0%|          | 787/200000 [17:59<71:16:00,  1.29s/it, loss=0.0973, lr=3.93e-06, step=786]Training:   0%|          | 787/200000 [17:59<71:16:00,  1.29s/it, loss=0.1033, lr=3.93e-06, step=787]Training:   0%|          | 788/200000 [18:01<67:43:16,  1.22s/it, loss=0.1033, lr=3.93e-06, step=787]Training:   0%|          | 788/200000 [18:01<67:43:16,  1.22s/it, loss=0.0804, lr=3.94e-06, step=788]Training:   0%|          | 789/200000 [18:02<70:00:42,  1.27s/it, loss=0.0804, lr=3.94e-06, step=788]Training:   0%|          | 789/200000 [18:02<70:00:42,  1.27s/it, loss=0.1181, lr=3.94e-06, step=789]Training:   0%|          | 790/200000 [18:03<71:03:07,  1.28s/it, loss=0.1181, lr=3.94e-06, step=789]Training:   0%|          | 790/200000 [18:03<71:03:07,  1.28s/it, loss=0.1354, lr=3.95e-06, step=790]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 791/200000 [18:05<74:19:16,  1.34s/it, loss=0.1354, lr=3.95e-06, step=790]Training:   0%|          | 791/200000 [18:05<74:19:16,  1.34s/it, loss=0.0558, lr=3.95e-06, step=791]Training:   0%|          | 792/200000 [18:06<76:25:44,  1.38s/it, loss=0.0558, lr=3.95e-06, step=791]Training:   0%|          | 792/200000 [18:06<76:25:44,  1.38s/it, loss=0.2060, lr=3.96e-06, step=792]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 793/200000 [18:07<71:23:17,  1.29s/it, loss=0.2060, lr=3.96e-06, step=792]Training:   0%|          | 793/200000 [18:07<71:23:17,  1.29s/it, loss=0.0981, lr=3.96e-06, step=793]Training:   0%|          | 794/200000 [18:08<67:48:43,  1.23s/it, loss=0.0981, lr=3.96e-06, step=793]Training:   0%|          | 794/200000 [18:08<67:48:43,  1.23s/it, loss=0.1113, lr=3.97e-06, step=794]Training:   0%|          | 795/200000 [18:10<71:24:02,  1.29s/it, loss=0.1113, lr=3.97e-06, step=794]Training:   0%|          | 795/200000 [18:10<71:24:02,  1.29s/it, loss=0.7220, lr=3.97e-06, step=795]Training:   0%|          | 796/200000 [18:11<67:49:40,  1.23s/it, loss=0.7220, lr=3.97e-06, step=795]Training:   0%|          | 796/200000 [18:11<67:49:40,  1.23s/it, loss=0.0799, lr=3.98e-06, step=796]Training:   0%|          | 797/200000 [18:12<68:47:19,  1.24s/it, loss=0.0799, lr=3.98e-06, step=796]Training:   0%|          | 797/200000 [18:12<68:47:19,  1.24s/it, loss=0.1124, lr=3.98e-06, step=797]Training:   0%|          | 798/200000 [18:13<66:00:38,  1.19s/it, loss=0.1124, lr=3.98e-06, step=797]Training:   0%|          | 798/200000 [18:13<66:00:38,  1.19s/it, loss=0.0706, lr=3.99e-06, step=798]Training:   0%|          | 799/200000 [18:14<64:05:10,  1.16s/it, loss=0.0706, lr=3.99e-06, step=798]Training:   0%|          | 799/200000 [18:14<64:05:10,  1.16s/it, loss=0.0887, lr=3.99e-06, step=799]Training:   0%|          | 800/200000 [18:16<69:20:05,  1.25s/it, loss=0.0887, lr=3.99e-06, step=799]Training:   0%|          | 800/200000 [18:16<69:20:05,  1.25s/it, loss=0.1058, lr=4.00e-06, step=800]23:11:31.017 [I] step=800 loss=0.1169 lr=3.76e-06 grad_norm=0.81 time=126.2s                      (701675:train_pytorch.py:582)
Training:   0%|          | 801/200000 [18:17<72:18:07,  1.31s/it, loss=0.1058, lr=4.00e-06, step=800]Training:   0%|          | 801/200000 [18:17<72:18:07,  1.31s/it, loss=0.0654, lr=4.00e-06, step=801]Training:   0%|          | 802/200000 [18:19<73:22:22,  1.33s/it, loss=0.0654, lr=4.00e-06, step=801]Training:   0%|          | 802/200000 [18:19<73:22:22,  1.33s/it, loss=0.1283, lr=4.01e-06, step=802]Training:   0%|          | 803/200000 [18:20<69:14:45,  1.25s/it, loss=0.1283, lr=4.01e-06, step=802]Training:   0%|          | 803/200000 [18:20<69:14:45,  1.25s/it, loss=0.0802, lr=4.01e-06, step=803]Training:   0%|          | 804/200000 [18:21<66:17:59,  1.20s/it, loss=0.0802, lr=4.01e-06, step=803]Training:   0%|          | 804/200000 [18:21<66:17:59,  1.20s/it, loss=0.1213, lr=4.02e-06, step=804]Training:   0%|          | 805/200000 [18:22<68:36:03,  1.24s/it, loss=0.1213, lr=4.02e-06, step=804]Training:   0%|          | 805/200000 [18:22<68:36:03,  1.24s/it, loss=0.1418, lr=4.02e-06, step=805]Training:   0%|          | 806/200000 [18:23<69:38:23,  1.26s/it, loss=0.1418, lr=4.02e-06, step=805]Training:   0%|          | 806/200000 [18:23<69:38:23,  1.26s/it, loss=0.0595, lr=4.03e-06, step=806]Training:   0%|          | 807/200000 [18:24<66:38:39,  1.20s/it, loss=0.0595, lr=4.03e-06, step=806]Training:   0%|          | 807/200000 [18:24<66:38:39,  1.20s/it, loss=0.1118, lr=4.03e-06, step=807]Training:   0%|          | 808/200000 [18:26<68:33:08,  1.24s/it, loss=0.1118, lr=4.03e-06, step=807]Training:   0%|          | 808/200000 [18:26<68:33:08,  1.24s/it, loss=0.0516, lr=4.04e-06, step=808]Training:   0%|          | 809/200000 [18:27<65:50:57,  1.19s/it, loss=0.0516, lr=4.04e-06, step=808]Training:   0%|          | 809/200000 [18:27<65:50:57,  1.19s/it, loss=0.1211, lr=4.04e-06, step=809]Training:   0%|          | 810/200000 [18:28<68:02:29,  1.23s/it, loss=0.1211, lr=4.04e-06, step=809]Training:   0%|          | 810/200000 [18:28<68:02:29,  1.23s/it, loss=0.0747, lr=4.05e-06, step=810]Training:   0%|          | 811/200000 [18:29<65:32:22,  1.18s/it, loss=0.0747, lr=4.05e-06, step=810]Training:   0%|          | 811/200000 [18:29<65:32:22,  1.18s/it, loss=0.0970, lr=4.05e-06, step=811]Training:   0%|          | 812/200000 [18:31<70:08:19,  1.27s/it, loss=0.0970, lr=4.05e-06, step=811]Training:   0%|          | 812/200000 [18:31<70:08:19,  1.27s/it, loss=0.0808, lr=4.06e-06, step=812]Training:   0%|          | 813/200000 [18:32<73:06:39,  1.32s/it, loss=0.0808, lr=4.06e-06, step=812]Training:   0%|          | 813/200000 [18:32<73:06:39,  1.32s/it, loss=0.0955, lr=4.06e-06, step=813]Training:   0%|          | 814/200000 [18:33<69:00:27,  1.25s/it, loss=0.0955, lr=4.06e-06, step=813]Training:   0%|          | 814/200000 [18:33<69:00:27,  1.25s/it, loss=0.0879, lr=4.07e-06, step=814]Training:   0%|          | 815/200000 [18:34<66:14:09,  1.20s/it, loss=0.0879, lr=4.07e-06, step=814]Training:   0%|          | 815/200000 [18:34<66:14:09,  1.20s/it, loss=0.0737, lr=4.07e-06, step=815]Training:   0%|          | 816/200000 [18:36<68:20:58,  1.24s/it, loss=0.0737, lr=4.07e-06, step=815]Training:   0%|          | 816/200000 [18:36<68:20:58,  1.24s/it, loss=0.2290, lr=4.08e-06, step=816]Training:   0%|          | 817/200000 [18:37<71:42:44,  1.30s/it, loss=0.2290, lr=4.08e-06, step=816]Training:   0%|          | 817/200000 [18:37<71:42:44,  1.30s/it, loss=0.3383, lr=4.08e-06, step=817]Training:   0%|          | 818/200000 [18:38<68:01:42,  1.23s/it, loss=0.3383, lr=4.08e-06, step=817]Training:   0%|          | 818/200000 [18:38<68:01:42,  1.23s/it, loss=0.0726, lr=4.09e-06, step=818]Training:   0%|          | 819/200000 [18:40<71:21:53,  1.29s/it, loss=0.0726, lr=4.09e-06, step=818]Training:   0%|          | 819/200000 [18:40<71:21:53,  1.29s/it, loss=0.1847, lr=4.09e-06, step=819]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 820/200000 [18:41<67:47:56,  1.23s/it, loss=0.1847, lr=4.09e-06, step=819]Training:   0%|          | 820/200000 [18:41<67:47:56,  1.23s/it, loss=0.1004, lr=4.10e-06, step=820]Training:   0%|          | 821/200000 [18:42<70:08:08,  1.27s/it, loss=0.1004, lr=4.10e-06, step=820]Training:   0%|          | 821/200000 [18:42<70:08:08,  1.27s/it, loss=0.0743, lr=4.10e-06, step=821]Training:   0%|          | 822/200000 [18:43<71:15:11,  1.29s/it, loss=0.0743, lr=4.10e-06, step=821]Training:   0%|          | 822/200000 [18:43<71:15:11,  1.29s/it, loss=0.1662, lr=4.11e-06, step=822]Training:   0%|          | 823/200000 [18:45<74:45:48,  1.35s/it, loss=0.1662, lr=4.11e-06, step=822]Training:   0%|          | 823/200000 [18:45<74:45:48,  1.35s/it, loss=0.0542, lr=4.11e-06, step=823]Training:   0%|          | 824/200000 [18:46<76:51:51,  1.39s/it, loss=0.0542, lr=4.11e-06, step=823]Training:   0%|          | 824/200000 [18:46<76:51:51,  1.39s/it, loss=0.1006, lr=4.12e-06, step=824]Training:   0%|          | 825/200000 [18:47<71:41:44,  1.30s/it, loss=0.1006, lr=4.12e-06, step=824]Training:   0%|          | 825/200000 [18:47<71:41:44,  1.30s/it, loss=0.1259, lr=4.12e-06, step=825]Training:   0%|          | 826/200000 [18:48<68:02:58,  1.23s/it, loss=0.1259, lr=4.12e-06, step=825]Training:   0%|          | 826/200000 [18:48<68:02:58,  1.23s/it, loss=0.1035, lr=4.13e-06, step=826]Training:   0%|          | 827/200000 [18:50<71:00:11,  1.28s/it, loss=0.1035, lr=4.13e-06, step=826]Training:   0%|          | 827/200000 [18:50<71:00:11,  1.28s/it, loss=0.1078, lr=4.13e-06, step=827]Training:   0%|          | 828/200000 [18:51<67:32:24,  1.22s/it, loss=0.1078, lr=4.13e-06, step=827]Training:   0%|          | 828/200000 [18:51<67:32:24,  1.22s/it, loss=0.1003, lr=4.14e-06, step=828]Training:   0%|          | 829/200000 [18:52<69:35:52,  1.26s/it, loss=0.1003, lr=4.14e-06, step=828]Training:   0%|          | 829/200000 [18:52<69:35:52,  1.26s/it, loss=0.0442, lr=4.14e-06, step=829]Training:   0%|          | 830/200000 [18:53<66:32:49,  1.20s/it, loss=0.0442, lr=4.14e-06, step=829]Training:   0%|          | 830/200000 [18:53<66:32:49,  1.20s/it, loss=0.0822, lr=4.15e-06, step=830]Training:   0%|          | 831/200000 [18:54<64:27:51,  1.17s/it, loss=0.0822, lr=4.15e-06, step=830]Training:   0%|          | 831/200000 [18:54<64:27:51,  1.17s/it, loss=0.1795, lr=4.15e-06, step=831]Training:   0%|          | 832/200000 [18:56<69:44:34,  1.26s/it, loss=0.1795, lr=4.15e-06, step=831]Training:   0%|          | 832/200000 [18:56<69:44:34,  1.26s/it, loss=0.1599, lr=4.16e-06, step=832]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 833/200000 [18:57<72:33:38,  1.31s/it, loss=0.1599, lr=4.16e-06, step=832]Training:   0%|          | 833/200000 [18:57<72:33:38,  1.31s/it, loss=0.0682, lr=4.16e-06, step=833]Training:   0%|          | 834/200000 [18:59<73:36:31,  1.33s/it, loss=0.0682, lr=4.16e-06, step=833]Training:   0%|          | 834/200000 [18:59<73:36:31,  1.33s/it, loss=0.0805, lr=4.17e-06, step=834]Training:   0%|          | 835/200000 [19:00<69:25:21,  1.25s/it, loss=0.0805, lr=4.17e-06, step=834]Training:   0%|          | 835/200000 [19:00<69:25:21,  1.25s/it, loss=0.2220, lr=4.17e-06, step=835]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 836/200000 [19:01<66:26:54,  1.20s/it, loss=0.2220, lr=4.17e-06, step=835]Training:   0%|          | 836/200000 [19:01<66:26:54,  1.20s/it, loss=0.0696, lr=4.18e-06, step=836]Training:   0%|          | 837/200000 [19:02<69:04:09,  1.25s/it, loss=0.0696, lr=4.18e-06, step=836]Training:   0%|          | 837/200000 [19:02<69:04:09,  1.25s/it, loss=0.0842, lr=4.18e-06, step=837]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 838/200000 [19:04<71:57:19,  1.30s/it, loss=0.0842, lr=4.18e-06, step=837]Training:   0%|          | 838/200000 [19:04<71:57:19,  1.30s/it, loss=0.0756, lr=4.19e-06, step=838]Training:   0%|          | 839/200000 [19:05<68:12:29,  1.23s/it, loss=0.0756, lr=4.19e-06, step=838]Training:   0%|          | 839/200000 [19:05<68:12:29,  1.23s/it, loss=0.1092, lr=4.19e-06, step=839]Training:   0%|          | 840/200000 [19:06<70:57:17,  1.28s/it, loss=0.1092, lr=4.19e-06, step=839]Training:   0%|          | 840/200000 [19:06<70:57:17,  1.28s/it, loss=0.1403, lr=4.20e-06, step=840]Training:   0%|          | 841/200000 [19:07<67:32:41,  1.22s/it, loss=0.1403, lr=4.20e-06, step=840]Training:   0%|          | 841/200000 [19:07<67:32:41,  1.22s/it, loss=0.0541, lr=4.20e-06, step=841]Training:   0%|          | 842/200000 [19:09<70:02:09,  1.27s/it, loss=0.0541, lr=4.20e-06, step=841]Training:   0%|          | 842/200000 [19:09<70:02:09,  1.27s/it, loss=0.0907, lr=4.21e-06, step=842]Training:   0%|          | 843/200000 [19:10<71:04:15,  1.28s/it, loss=0.0907, lr=4.21e-06, step=842]Training:   0%|          | 843/200000 [19:10<71:04:15,  1.28s/it, loss=0.1439, lr=4.21e-06, step=843]Training:   0%|          | 844/200000 [19:11<74:16:12,  1.34s/it, loss=0.1439, lr=4.21e-06, step=843]Training:   0%|          | 844/200000 [19:11<74:16:12,  1.34s/it, loss=0.1107, lr=4.22e-06, step=844]Training:   0%|          | 845/200000 [19:13<76:16:29,  1.38s/it, loss=0.1107, lr=4.22e-06, step=844]Training:   0%|          | 845/200000 [19:13<76:16:29,  1.38s/it, loss=0.0573, lr=4.22e-06, step=845]Training:   0%|          | 846/200000 [19:14<71:13:40,  1.29s/it, loss=0.0573, lr=4.22e-06, step=845]Training:   0%|          | 846/200000 [19:14<71:13:40,  1.29s/it, loss=0.0610, lr=4.23e-06, step=846]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 847/200000 [19:15<67:45:36,  1.22s/it, loss=0.0610, lr=4.23e-06, step=846]Training:   0%|          | 847/200000 [19:15<67:45:36,  1.22s/it, loss=0.0557, lr=4.23e-06, step=847]Training:   0%|          | 848/200000 [19:16<70:29:12,  1.27s/it, loss=0.0557, lr=4.23e-06, step=847]Training:   0%|          | 848/200000 [19:16<70:29:12,  1.27s/it, loss=0.1126, lr=4.24e-06, step=848]Training:   0%|          | 849/200000 [19:17<67:12:49,  1.22s/it, loss=0.1126, lr=4.24e-06, step=848]Training:   0%|          | 849/200000 [19:17<67:12:49,  1.22s/it, loss=0.1480, lr=4.24e-06, step=849]Training:   0%|          | 850/200000 [19:19<69:04:32,  1.25s/it, loss=0.1480, lr=4.24e-06, step=849]Training:   0%|          | 850/200000 [19:19<69:04:32,  1.25s/it, loss=0.1732, lr=4.25e-06, step=850]Training:   0%|          | 851/200000 [19:20<66:13:57,  1.20s/it, loss=0.1732, lr=4.25e-06, step=850]Training:   0%|          | 851/200000 [19:20<66:13:57,  1.20s/it, loss=0.0838, lr=4.25e-06, step=851]Training:   0%|          | 852/200000 [19:21<64:08:23,  1.16s/it, loss=0.0838, lr=4.25e-06, step=851]Training:   0%|          | 852/200000 [19:21<64:08:23,  1.16s/it, loss=0.0642, lr=4.26e-06, step=852]Training:   0%|          | 853/200000 [19:22<68:37:44,  1.24s/it, loss=0.0642, lr=4.26e-06, step=852]Training:   0%|          | 853/200000 [19:22<68:37:44,  1.24s/it, loss=0.0739, lr=4.26e-06, step=853]Training:   0%|          | 854/200000 [19:24<72:09:29,  1.30s/it, loss=0.0739, lr=4.26e-06, step=853]Training:   0%|          | 854/200000 [19:24<72:09:29,  1.30s/it, loss=0.1830, lr=4.27e-06, step=854]Training:   0%|          | 855/200000 [19:25<73:25:33,  1.33s/it, loss=0.1830, lr=4.27e-06, step=854]Training:   0%|          | 855/200000 [19:25<73:25:33,  1.33s/it, loss=0.0415, lr=4.27e-06, step=855]Training:   0%|          | 856/200000 [19:26<69:13:52,  1.25s/it, loss=0.0415, lr=4.27e-06, step=855]Training:   0%|          | 856/200000 [19:26<69:13:52,  1.25s/it, loss=0.0957, lr=4.28e-06, step=856]Training:   0%|          | 857/200000 [19:27<66:19:16,  1.20s/it, loss=0.0957, lr=4.28e-06, step=856]Training:   0%|          | 857/200000 [19:27<66:19:16,  1.20s/it, loss=0.0562, lr=4.28e-06, step=857]Training:   0%|          | 858/200000 [19:29<68:46:55,  1.24s/it, loss=0.0562, lr=4.28e-06, step=857]Training:   0%|          | 858/200000 [19:29<68:46:55,  1.24s/it, loss=0.0642, lr=4.29e-06, step=858]Training:   0%|          | 859/200000 [19:30<70:20:41,  1.27s/it, loss=0.0642, lr=4.29e-06, step=858]Training:   0%|          | 859/200000 [19:30<70:20:41,  1.27s/it, loss=0.0779, lr=4.29e-06, step=859]Training:   0%|          | 860/200000 [19:31<67:03:49,  1.21s/it, loss=0.0779, lr=4.29e-06, step=859]Training:   0%|          | 860/200000 [19:31<67:03:49,  1.21s/it, loss=0.0653, lr=4.30e-06, step=860]Training:   0%|          | 861/200000 [19:32<69:20:43,  1.25s/it, loss=0.0653, lr=4.30e-06, step=860]Training:   0%|          | 861/200000 [19:32<69:20:43,  1.25s/it, loss=0.0749, lr=4.30e-06, step=861]Training:   0%|          | 862/200000 [19:34<66:24:09,  1.20s/it, loss=0.0749, lr=4.30e-06, step=861]Training:   0%|          | 862/200000 [19:34<66:24:09,  1.20s/it, loss=0.0461, lr=4.31e-06, step=862]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 863/200000 [19:35<68:26:56,  1.24s/it, loss=0.0461, lr=4.31e-06, step=862]Training:   0%|          | 863/200000 [19:35<68:26:56,  1.24s/it, loss=0.0832, lr=4.31e-06, step=863]Training:   0%|          | 864/200000 [19:36<65:43:42,  1.19s/it, loss=0.0832, lr=4.31e-06, step=863]Training:   0%|          | 864/200000 [19:36<65:43:42,  1.19s/it, loss=0.0583, lr=4.32e-06, step=864]Training:   0%|          | 865/200000 [19:37<70:11:28,  1.27s/it, loss=0.0583, lr=4.32e-06, step=864]Training:   0%|          | 865/200000 [19:37<70:11:28,  1.27s/it, loss=0.1202, lr=4.32e-06, step=865]Training:   0%|          | 866/200000 [19:39<72:17:41,  1.31s/it, loss=0.1202, lr=4.32e-06, step=865]Training:   0%|          | 866/200000 [19:39<72:17:41,  1.31s/it, loss=0.1331, lr=4.33e-06, step=866]Training:   0%|          | 867/200000 [19:40<68:29:02,  1.24s/it, loss=0.1331, lr=4.33e-06, step=866]Training:   0%|          | 867/200000 [19:40<68:29:02,  1.24s/it, loss=0.0511, lr=4.33e-06, step=867]Training:   0%|          | 868/200000 [19:41<65:47:08,  1.19s/it, loss=0.0511, lr=4.33e-06, step=867]Training:   0%|          | 868/200000 [19:41<65:47:08,  1.19s/it, loss=0.0893, lr=4.34e-06, step=868]Training:   0%|          | 869/200000 [19:42<68:39:48,  1.24s/it, loss=0.0893, lr=4.34e-06, step=868]Training:   0%|          | 869/200000 [19:42<68:39:48,  1.24s/it, loss=0.0764, lr=4.34e-06, step=869]Training:   0%|          | 870/200000 [19:44<72:08:56,  1.30s/it, loss=0.0764, lr=4.34e-06, step=869]Training:   0%|          | 870/200000 [19:44<72:08:56,  1.30s/it, loss=0.0944, lr=4.35e-06, step=870]Training:   0%|          | 871/200000 [19:45<68:22:24,  1.24s/it, loss=0.0944, lr=4.35e-06, step=870]Training:   0%|          | 871/200000 [19:45<68:22:24,  1.24s/it, loss=0.0677, lr=4.35e-06, step=871]Training:   0%|          | 872/200000 [19:46<71:05:48,  1.29s/it, loss=0.0677, lr=4.35e-06, step=871]Training:   0%|          | 872/200000 [19:46<71:05:48,  1.29s/it, loss=0.1330, lr=4.36e-06, step=872]Training:   0%|          | 873/200000 [19:47<67:39:48,  1.22s/it, loss=0.1330, lr=4.36e-06, step=872]Training:   0%|          | 873/200000 [19:47<67:39:48,  1.22s/it, loss=0.0610, lr=4.36e-06, step=873]Training:   0%|          | 874/200000 [19:49<70:04:47,  1.27s/it, loss=0.0610, lr=4.36e-06, step=873]Training:   0%|          | 874/200000 [19:49<70:04:47,  1.27s/it, loss=0.2102, lr=4.37e-06, step=874]Training:   0%|          | 875/200000 [19:50<71:02:47,  1.28s/it, loss=0.2102, lr=4.37e-06, step=874]Training:   0%|          | 875/200000 [19:50<71:02:47,  1.28s/it, loss=0.0593, lr=4.37e-06, step=875]Training:   0%|          | 876/200000 [19:52<74:38:48,  1.35s/it, loss=0.0593, lr=4.37e-06, step=875]Training:   0%|          | 876/200000 [19:52<74:38:48,  1.35s/it, loss=0.0736, lr=4.38e-06, step=876]Training:   0%|          | 877/200000 [19:53<76:46:21,  1.39s/it, loss=0.0736, lr=4.38e-06, step=876]Training:   0%|          | 877/200000 [19:53<76:46:21,  1.39s/it, loss=0.0850, lr=4.38e-06, step=877]Training:   0%|          | 878/200000 [19:54<71:33:51,  1.29s/it, loss=0.0850, lr=4.38e-06, step=877]Training:   0%|          | 878/200000 [19:54<71:33:51,  1.29s/it, loss=0.1258, lr=4.39e-06, step=878]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 879/200000 [19:55<67:57:48,  1.23s/it, loss=0.1258, lr=4.39e-06, step=878]Training:   0%|          | 879/200000 [19:55<67:57:48,  1.23s/it, loss=0.0748, lr=4.39e-06, step=879]Training:   0%|          | 880/200000 [19:57<70:52:07,  1.28s/it, loss=0.0748, lr=4.39e-06, step=879]Training:   0%|          | 880/200000 [19:57<70:52:07,  1.28s/it, loss=0.0832, lr=4.40e-06, step=880]Training:   0%|          | 881/200000 [19:58<67:26:19,  1.22s/it, loss=0.0832, lr=4.40e-06, step=880]Training:   0%|          | 881/200000 [19:58<67:26:19,  1.22s/it, loss=0.0448, lr=4.40e-06, step=881]Training:   0%|          | 882/200000 [19:59<69:15:39,  1.25s/it, loss=0.0448, lr=4.40e-06, step=881]Training:   0%|          | 882/200000 [19:59<69:15:39,  1.25s/it, loss=0.1205, lr=4.41e-06, step=882]Training:   0%|          | 883/200000 [20:00<66:19:35,  1.20s/it, loss=0.1205, lr=4.41e-06, step=882]Training:   0%|          | 883/200000 [20:00<66:19:35,  1.20s/it, loss=0.0621, lr=4.41e-06, step=883]Training:   0%|          | 884/200000 [20:01<64:17:05,  1.16s/it, loss=0.0621, lr=4.41e-06, step=883]Training:   0%|          | 884/200000 [20:01<64:17:05,  1.16s/it, loss=0.0394, lr=4.42e-06, step=884]Training:   0%|          | 885/200000 [20:03<68:55:12,  1.25s/it, loss=0.0394, lr=4.42e-06, step=884]Training:   0%|          | 885/200000 [20:03<68:55:12,  1.25s/it, loss=0.0469, lr=4.42e-06, step=885]Training:   0%|          | 886/200000 [20:04<72:42:08,  1.31s/it, loss=0.0469, lr=4.42e-06, step=885]Training:   0%|          | 886/200000 [20:04<72:42:08,  1.31s/it, loss=0.0698, lr=4.43e-06, step=886]Training:   0%|          | 887/200000 [20:05<73:57:53,  1.34s/it, loss=0.0698, lr=4.43e-06, step=886]Training:   0%|          | 887/200000 [20:05<73:57:53,  1.34s/it, loss=0.0790, lr=4.43e-06, step=887]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 888/200000 [20:07<69:37:59,  1.26s/it, loss=0.0790, lr=4.43e-06, step=887]Training:   0%|          | 888/200000 [20:07<69:37:59,  1.26s/it, loss=0.1356, lr=4.44e-06, step=888]Training:   0%|          | 889/200000 [20:08<66:35:42,  1.20s/it, loss=0.1356, lr=4.44e-06, step=888]Training:   0%|          | 889/200000 [20:08<66:35:42,  1.20s/it, loss=0.1043, lr=4.44e-06, step=889]Training:   0%|          | 890/200000 [20:09<69:07:47,  1.25s/it, loss=0.1043, lr=4.44e-06, step=889]Training:   0%|          | 890/200000 [20:09<69:07:47,  1.25s/it, loss=0.4400, lr=4.45e-06, step=890]Training:   0%|          | 891/200000 [20:10<72:15:28,  1.31s/it, loss=0.4400, lr=4.45e-06, step=890]Training:   0%|          | 891/200000 [20:10<72:15:28,  1.31s/it, loss=0.1000, lr=4.45e-06, step=891]Training:   0%|          | 892/200000 [20:11<68:24:10,  1.24s/it, loss=0.1000, lr=4.45e-06, step=891]Training:   0%|          | 892/200000 [20:11<68:24:10,  1.24s/it, loss=0.0907, lr=4.46e-06, step=892]Training:   0%|          | 893/200000 [20:13<70:52:22,  1.28s/it, loss=0.0907, lr=4.46e-06, step=892]Training:   0%|          | 893/200000 [20:13<70:52:22,  1.28s/it, loss=0.0546, lr=4.46e-06, step=893]Training:   0%|          | 894/200000 [20:14<67:25:53,  1.22s/it, loss=0.0546, lr=4.46e-06, step=893]Training:   0%|          | 894/200000 [20:14<67:25:53,  1.22s/it, loss=0.0788, lr=4.47e-06, step=894]Training:   0%|          | 895/200000 [20:15<69:54:01,  1.26s/it, loss=0.0788, lr=4.47e-06, step=894]Training:   0%|          | 895/200000 [20:15<69:54:01,  1.26s/it, loss=0.0654, lr=4.47e-06, step=895]Training:   0%|          | 896/200000 [20:17<70:55:40,  1.28s/it, loss=0.0654, lr=4.47e-06, step=895]Training:   0%|          | 896/200000 [20:17<70:55:40,  1.28s/it, loss=0.0743, lr=4.48e-06, step=896]Training:   0%|          | 897/200000 [20:18<74:16:09,  1.34s/it, loss=0.0743, lr=4.48e-06, step=896]Training:   0%|          | 897/200000 [20:18<74:16:09,  1.34s/it, loss=0.1516, lr=4.48e-06, step=897]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 898/200000 [20:20<76:26:25,  1.38s/it, loss=0.1516, lr=4.48e-06, step=897]Training:   0%|          | 898/200000 [20:20<76:26:25,  1.38s/it, loss=0.0682, lr=4.49e-06, step=898]Training:   0%|          | 899/200000 [20:21<71:23:02,  1.29s/it, loss=0.0682, lr=4.49e-06, step=898]Training:   0%|          | 899/200000 [20:21<71:23:02,  1.29s/it, loss=0.2479, lr=4.49e-06, step=899]Training:   0%|          | 900/200000 [20:22<67:50:03,  1.23s/it, loss=0.2479, lr=4.49e-06, step=899]Training:   0%|          | 900/200000 [20:22<67:50:03,  1.23s/it, loss=0.1017, lr=4.50e-06, step=900]23:13:36.958 [I] step=900 loss=0.1031 lr=4.26e-06 grad_norm=0.80 time=125.9s                      (701675:train_pytorch.py:582)
Training:   0%|          | 901/200000 [20:23<71:10:32,  1.29s/it, loss=0.1017, lr=4.50e-06, step=900]Training:   0%|          | 901/200000 [20:23<71:10:32,  1.29s/it, loss=0.1184, lr=4.50e-06, step=901]Training:   0%|          | 902/200000 [20:24<67:40:07,  1.22s/it, loss=0.1184, lr=4.50e-06, step=901]Training:   0%|          | 902/200000 [20:24<67:40:07,  1.22s/it, loss=0.0552, lr=4.51e-06, step=902]Training:   0%|          | 903/200000 [20:26<69:05:59,  1.25s/it, loss=0.0552, lr=4.51e-06, step=902]Training:   0%|          | 903/200000 [20:26<69:05:59,  1.25s/it, loss=0.1710, lr=4.51e-06, step=903]Training:   0%|          | 904/200000 [20:27<66:13:27,  1.20s/it, loss=0.1710, lr=4.51e-06, step=903]Training:   0%|          | 904/200000 [20:27<66:13:27,  1.20s/it, loss=0.0679, lr=4.52e-06, step=904]Training:   0%|          | 905/200000 [20:28<64:15:23,  1.16s/it, loss=0.0679, lr=4.52e-06, step=904]Training:   0%|          | 905/200000 [20:28<64:15:23,  1.16s/it, loss=0.1546, lr=4.52e-06, step=905]Training:   0%|          | 906/200000 [20:29<68:42:51,  1.24s/it, loss=0.1546, lr=4.52e-06, step=905]Training:   0%|          | 906/200000 [20:29<68:42:51,  1.24s/it, loss=0.0935, lr=4.53e-06, step=906]Training:   0%|          | 907/200000 [20:31<72:10:55,  1.31s/it, loss=0.0935, lr=4.53e-06, step=906]Training:   0%|          | 907/200000 [20:31<72:10:55,  1.31s/it, loss=0.1056, lr=4.53e-06, step=907]Training:   0%|          | 908/200000 [20:32<72:45:16,  1.32s/it, loss=0.1056, lr=4.53e-06, step=907]Training:   0%|          | 908/200000 [20:32<72:45:16,  1.32s/it, loss=0.1295, lr=4.54e-06, step=908]Training:   0%|          | 909/200000 [20:33<68:48:20,  1.24s/it, loss=0.1295, lr=4.54e-06, step=908]Training:   0%|          | 909/200000 [20:33<68:48:20,  1.24s/it, loss=0.0597, lr=4.54e-06, step=909]Training:   0%|          | 910/200000 [20:34<66:03:13,  1.19s/it, loss=0.0597, lr=4.54e-06, step=909]Training:   0%|          | 910/200000 [20:34<66:03:13,  1.19s/it, loss=0.0806, lr=4.55e-06, step=910]Training:   0%|          | 911/200000 [20:35<68:23:28,  1.24s/it, loss=0.0806, lr=4.55e-06, step=910]Training:   0%|          | 911/200000 [20:35<68:23:28,  1.24s/it, loss=0.0624, lr=4.55e-06, step=911]Training:   0%|          | 912/200000 [20:37<70:08:49,  1.27s/it, loss=0.0624, lr=4.55e-06, step=911]Training:   0%|          | 912/200000 [20:37<70:08:49,  1.27s/it, loss=0.0707, lr=4.56e-06, step=912]Training:   0%|          | 913/200000 [20:38<66:54:01,  1.21s/it, loss=0.0707, lr=4.56e-06, step=912]Training:   0%|          | 913/200000 [20:38<66:54:01,  1.21s/it, loss=0.1102, lr=4.56e-06, step=913]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 914/200000 [20:39<68:34:35,  1.24s/it, loss=0.1102, lr=4.56e-06, step=913]Training:   0%|          | 914/200000 [20:39<68:34:35,  1.24s/it, loss=0.0515, lr=4.57e-06, step=914]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 915/200000 [20:40<65:52:14,  1.19s/it, loss=0.0515, lr=4.57e-06, step=914]Training:   0%|          | 915/200000 [20:40<65:52:14,  1.19s/it, loss=0.0988, lr=4.57e-06, step=915]Training:   0%|          | 916/200000 [20:42<67:59:04,  1.23s/it, loss=0.0988, lr=4.57e-06, step=915]Training:   0%|          | 916/200000 [20:42<67:59:04,  1.23s/it, loss=0.0729, lr=4.58e-06, step=916]Training:   0%|          | 917/200000 [20:43<65:27:44,  1.18s/it, loss=0.0729, lr=4.58e-06, step=916]Training:   0%|          | 917/200000 [20:43<65:27:44,  1.18s/it, loss=0.2184, lr=4.58e-06, step=917]Training:   0%|          | 918/200000 [20:44<70:02:52,  1.27s/it, loss=0.2184, lr=4.58e-06, step=917]Training:   0%|          | 918/200000 [20:44<70:02:52,  1.27s/it, loss=0.1496, lr=4.59e-06, step=918]Training:   0%|          | 919/200000 [20:46<73:02:12,  1.32s/it, loss=0.1496, lr=4.59e-06, step=918]Training:   0%|          | 919/200000 [20:46<73:02:12,  1.32s/it, loss=0.2418, lr=4.59e-06, step=919]Training:   0%|          | 920/200000 [20:47<68:59:41,  1.25s/it, loss=0.2418, lr=4.59e-06, step=919]Training:   0%|          | 920/200000 [20:47<68:59:41,  1.25s/it, loss=0.0971, lr=4.60e-06, step=920]Training:   0%|          | 921/200000 [20:48<66:10:56,  1.20s/it, loss=0.0971, lr=4.60e-06, step=920]Training:   0%|          | 921/200000 [20:48<66:10:56,  1.20s/it, loss=0.0599, lr=4.60e-06, step=921]Training:   0%|          | 922/200000 [20:49<69:02:54,  1.25s/it, loss=0.0599, lr=4.60e-06, step=921]Training:   0%|          | 922/200000 [20:49<69:02:54,  1.25s/it, loss=0.0786, lr=4.61e-06, step=922]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 923/200000 [20:50<71:54:23,  1.30s/it, loss=0.0786, lr=4.61e-06, step=922]Training:   0%|          | 923/200000 [20:50<71:54:23,  1.30s/it, loss=0.0737, lr=4.61e-06, step=923]Training:   0%|          | 924/200000 [20:52<68:10:34,  1.23s/it, loss=0.0737, lr=4.61e-06, step=923]Training:   0%|          | 924/200000 [20:52<68:10:34,  1.23s/it, loss=0.1095, lr=4.62e-06, step=924]Training:   0%|          | 925/200000 [20:53<70:54:53,  1.28s/it, loss=0.1095, lr=4.62e-06, step=924]Training:   0%|          | 925/200000 [20:53<70:54:53,  1.28s/it, loss=0.0724, lr=4.62e-06, step=925]Training:   0%|          | 926/200000 [20:54<67:32:14,  1.22s/it, loss=0.0724, lr=4.62e-06, step=925]Training:   0%|          | 926/200000 [20:54<67:32:14,  1.22s/it, loss=0.0996, lr=4.63e-06, step=926]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 927/200000 [20:55<69:56:37,  1.26s/it, loss=0.0996, lr=4.63e-06, step=926]Training:   0%|          | 927/200000 [20:55<69:56:37,  1.26s/it, loss=0.1123, lr=4.63e-06, step=927]Training:   0%|          | 928/200000 [20:57<71:00:48,  1.28s/it, loss=0.1123, lr=4.63e-06, step=927]Training:   0%|          | 928/200000 [20:57<71:00:48,  1.28s/it, loss=0.0630, lr=4.64e-06, step=928]Training:   0%|          | 929/200000 [20:58<74:39:38,  1.35s/it, loss=0.0630, lr=4.64e-06, step=928]Training:   0%|          | 929/200000 [20:58<74:39:38,  1.35s/it, loss=0.0601, lr=4.64e-06, step=929]Training:   0%|          | 930/200000 [21:00<77:31:49,  1.40s/it, loss=0.0601, lr=4.64e-06, step=929]Training:   0%|          | 930/200000 [21:00<77:31:49,  1.40s/it, loss=0.0933, lr=4.65e-06, step=930]Training:   0%|          | 931/200000 [21:01<72:09:44,  1.30s/it, loss=0.0933, lr=4.65e-06, step=930]Training:   0%|          | 931/200000 [21:01<72:09:44,  1.30s/it, loss=0.0567, lr=4.65e-06, step=931]Training:   0%|          | 932/200000 [21:02<68:20:10,  1.24s/it, loss=0.0567, lr=4.65e-06, step=931]Training:   0%|          | 932/200000 [21:02<68:20:10,  1.24s/it, loss=0.1610, lr=4.66e-06, step=932]Training:   0%|          | 933/200000 [21:03<72:04:02,  1.30s/it, loss=0.1610, lr=4.66e-06, step=932]Training:   0%|          | 933/200000 [21:03<72:04:02,  1.30s/it, loss=0.2028, lr=4.66e-06, step=933]Training:   0%|          | 934/200000 [21:04<68:15:37,  1.23s/it, loss=0.2028, lr=4.66e-06, step=933]Training:   0%|          | 934/200000 [21:04<68:15:37,  1.23s/it, loss=0.1275, lr=4.67e-06, step=934]Training:   0%|          | 935/200000 [21:06<70:25:12,  1.27s/it, loss=0.1275, lr=4.67e-06, step=934]Training:   0%|          | 935/200000 [21:06<70:25:12,  1.27s/it, loss=0.1186, lr=4.67e-06, step=935]Training:   0%|          | 936/200000 [21:07<67:06:48,  1.21s/it, loss=0.1186, lr=4.67e-06, step=935]Training:   0%|          | 936/200000 [21:07<67:06:48,  1.21s/it, loss=0.0599, lr=4.68e-06, step=936]Training:   0%|          | 937/200000 [21:08<64:48:07,  1.17s/it, loss=0.0599, lr=4.68e-06, step=936]Training:   0%|          | 937/200000 [21:08<64:48:07,  1.17s/it, loss=0.0661, lr=4.68e-06, step=937]Training:   0%|          | 938/200000 [21:09<69:15:01,  1.25s/it, loss=0.0661, lr=4.68e-06, step=937]Training:   0%|          | 938/200000 [21:09<69:15:01,  1.25s/it, loss=0.2749, lr=4.69e-06, step=938]Training:   0%|          | 939/200000 [21:11<72:54:34,  1.32s/it, loss=0.2749, lr=4.69e-06, step=938]Training:   0%|          | 939/200000 [21:11<72:54:34,  1.32s/it, loss=0.0719, lr=4.69e-06, step=939]Training:   0%|          | 940/200000 [21:12<74:06:30,  1.34s/it, loss=0.0719, lr=4.69e-06, step=939]Training:   0%|          | 940/200000 [21:12<74:06:30,  1.34s/it, loss=0.1117, lr=4.70e-06, step=940]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 941/200000 [21:13<69:42:05,  1.26s/it, loss=0.1117, lr=4.70e-06, step=940]Training:   0%|          | 941/200000 [21:13<69:42:05,  1.26s/it, loss=0.0432, lr=4.70e-06, step=941]Training:   0%|          | 942/200000 [21:14<66:38:07,  1.21s/it, loss=0.0432, lr=4.70e-06, step=941]Training:   0%|          | 942/200000 [21:14<66:38:07,  1.21s/it, loss=0.0920, lr=4.71e-06, step=942]Training:   0%|          | 943/200000 [21:16<69:05:57,  1.25s/it, loss=0.0920, lr=4.71e-06, step=942]Training:   0%|          | 943/200000 [21:16<69:05:57,  1.25s/it, loss=0.0512, lr=4.71e-06, step=943]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 944/200000 [21:17<71:22:22,  1.29s/it, loss=0.0512, lr=4.71e-06, step=943]Training:   0%|          | 944/200000 [21:17<71:22:22,  1.29s/it, loss=0.0805, lr=4.72e-06, step=944]Training:   0%|          | 945/200000 [21:18<67:47:27,  1.23s/it, loss=0.0805, lr=4.72e-06, step=944]Training:   0%|          | 945/200000 [21:18<67:47:27,  1.23s/it, loss=0.1211, lr=4.72e-06, step=945]Training:   0%|          | 946/200000 [21:20<70:48:42,  1.28s/it, loss=0.1211, lr=4.72e-06, step=945]Training:   0%|          | 946/200000 [21:20<70:48:42,  1.28s/it, loss=0.1019, lr=4.73e-06, step=946]Training:   0%|          | 947/200000 [21:21<67:23:40,  1.22s/it, loss=0.1019, lr=4.73e-06, step=946]Training:   0%|          | 947/200000 [21:21<67:23:40,  1.22s/it, loss=0.0638, lr=4.73e-06, step=947]Training:   0%|          | 948/200000 [21:22<69:13:54,  1.25s/it, loss=0.0638, lr=4.73e-06, step=947]Training:   0%|          | 948/200000 [21:22<69:13:54,  1.25s/it, loss=0.1126, lr=4.74e-06, step=948]Training:   0%|          | 949/200000 [21:23<70:55:30,  1.28s/it, loss=0.1126, lr=4.74e-06, step=948]Training:   0%|          | 949/200000 [21:23<70:55:30,  1.28s/it, loss=0.0346, lr=4.74e-06, step=949]Training:   0%|          | 950/200000 [21:25<74:18:57,  1.34s/it, loss=0.0346, lr=4.74e-06, step=949]Training:   0%|          | 950/200000 [21:25<74:18:57,  1.34s/it, loss=0.2114, lr=4.75e-06, step=950]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 951/200000 [21:26<76:12:08,  1.38s/it, loss=0.2114, lr=4.75e-06, step=950]Training:   0%|          | 951/200000 [21:26<76:12:08,  1.38s/it, loss=0.0869, lr=4.75e-06, step=951]Training:   0%|          | 952/200000 [21:27<71:10:05,  1.29s/it, loss=0.0869, lr=4.75e-06, step=951]Training:   0%|          | 952/200000 [21:27<71:10:05,  1.29s/it, loss=0.1201, lr=4.76e-06, step=952]Training:   0%|          | 953/200000 [21:28<67:42:12,  1.22s/it, loss=0.1201, lr=4.76e-06, step=952]Training:   0%|          | 953/200000 [21:28<67:42:12,  1.22s/it, loss=0.1383, lr=4.76e-06, step=953]Training:   0%|          | 954/200000 [21:30<71:15:34,  1.29s/it, loss=0.1383, lr=4.76e-06, step=953]Training:   0%|          | 954/200000 [21:30<71:15:34,  1.29s/it, loss=0.0732, lr=4.77e-06, step=954]Training:   0%|          | 955/200000 [21:31<67:45:29,  1.23s/it, loss=0.0732, lr=4.77e-06, step=954]Training:   0%|          | 955/200000 [21:31<67:45:29,  1.23s/it, loss=0.1043, lr=4.77e-06, step=955]Training:   0%|          | 956/200000 [21:32<69:02:17,  1.25s/it, loss=0.1043, lr=4.77e-06, step=955]Training:   0%|          | 956/200000 [21:32<69:02:17,  1.25s/it, loss=0.0540, lr=4.78e-06, step=956]Training:   0%|          | 957/200000 [21:33<66:11:56,  1.20s/it, loss=0.0540, lr=4.78e-06, step=956]Training:   0%|          | 957/200000 [21:33<66:11:56,  1.20s/it, loss=0.0676, lr=4.78e-06, step=957]Training:   0%|          | 958/200000 [21:34<64:09:13,  1.16s/it, loss=0.0676, lr=4.78e-06, step=957]Training:   0%|          | 958/200000 [21:34<64:09:13,  1.16s/it, loss=0.1046, lr=4.79e-06, step=958]Training:   0%|          | 959/200000 [21:36<68:31:55,  1.24s/it, loss=0.1046, lr=4.79e-06, step=958]Training:   0%|          | 959/200000 [21:36<68:31:55,  1.24s/it, loss=0.0547, lr=4.79e-06, step=959]Training:   0%|          | 960/200000 [21:37<71:28:53,  1.29s/it, loss=0.0547, lr=4.79e-06, step=959]Training:   0%|          | 960/200000 [21:37<71:28:53,  1.29s/it, loss=0.0502, lr=4.80e-06, step=960]Training:   0%|          | 961/200000 [21:39<72:37:19,  1.31s/it, loss=0.0502, lr=4.80e-06, step=960]Training:   0%|          | 961/200000 [21:39<72:37:19,  1.31s/it, loss=0.0617, lr=4.80e-06, step=961]Training:   0%|          | 962/200000 [21:40<68:39:14,  1.24s/it, loss=0.0617, lr=4.80e-06, step=961]Training:   0%|          | 962/200000 [21:40<68:39:14,  1.24s/it, loss=0.0726, lr=4.81e-06, step=962]Training:   0%|          | 963/200000 [21:41<65:54:45,  1.19s/it, loss=0.0726, lr=4.81e-06, step=962]Training:   0%|          | 963/200000 [21:41<65:54:45,  1.19s/it, loss=0.0605, lr=4.81e-06, step=963]Training:   0%|          | 964/200000 [21:42<68:39:21,  1.24s/it, loss=0.0605, lr=4.81e-06, step=963]Training:   0%|          | 964/200000 [21:42<68:39:21,  1.24s/it, loss=0.0823, lr=4.82e-06, step=964]Training:   0%|          | 965/200000 [21:43<70:05:29,  1.27s/it, loss=0.0823, lr=4.82e-06, step=964]Training:   0%|          | 965/200000 [21:43<70:05:29,  1.27s/it, loss=0.1190, lr=4.82e-06, step=965]Training:   0%|          | 966/200000 [21:45<66:52:37,  1.21s/it, loss=0.1190, lr=4.82e-06, step=965]Training:   0%|          | 966/200000 [21:45<66:52:37,  1.21s/it, loss=0.0618, lr=4.83e-06, step=966]Training:   0%|          | 967/200000 [21:46<68:50:55,  1.25s/it, loss=0.0618, lr=4.83e-06, step=966]Training:   0%|          | 967/200000 [21:46<68:50:55,  1.25s/it, loss=0.0436, lr=4.83e-06, step=967]Training:   0%|          | 968/200000 [21:47<66:01:13,  1.19s/it, loss=0.0436, lr=4.83e-06, step=967]Training:   0%|          | 968/200000 [21:47<66:01:13,  1.19s/it, loss=0.0853, lr=4.84e-06, step=968]Training:   0%|          | 969/200000 [21:48<67:55:48,  1.23s/it, loss=0.0853, lr=4.84e-06, step=968]Training:   0%|          | 969/200000 [21:48<67:55:48,  1.23s/it, loss=0.0527, lr=4.84e-06, step=969]Training:   0%|          | 970/200000 [21:49<65:23:22,  1.18s/it, loss=0.0527, lr=4.84e-06, step=969]Training:   0%|          | 970/200000 [21:49<65:23:22,  1.18s/it, loss=0.1148, lr=4.85e-06, step=970]Training:   0%|          | 971/200000 [21:51<69:25:41,  1.26s/it, loss=0.1148, lr=4.85e-06, step=970]Training:   0%|          | 971/200000 [21:51<69:25:41,  1.26s/it, loss=0.0721, lr=4.85e-06, step=971]Training:   0%|          | 972/200000 [21:52<72:35:23,  1.31s/it, loss=0.0721, lr=4.85e-06, step=971]Training:   0%|          | 972/200000 [21:52<72:35:23,  1.31s/it, loss=0.1010, lr=4.86e-06, step=972]Training:   0%|          | 973/200000 [21:53<68:40:57,  1.24s/it, loss=0.1010, lr=4.86e-06, step=972]Training:   0%|          | 973/200000 [21:53<68:40:57,  1.24s/it, loss=0.1302, lr=4.86e-06, step=973]Training:   0%|          | 974/200000 [21:54<65:56:58,  1.19s/it, loss=0.1302, lr=4.86e-06, step=973]Training:   0%|          | 974/200000 [21:54<65:56:58,  1.19s/it, loss=0.0726, lr=4.87e-06, step=974]Training:   0%|          | 975/200000 [21:56<68:25:15,  1.24s/it, loss=0.0726, lr=4.87e-06, step=974]Training:   0%|          | 975/200000 [21:56<68:25:15,  1.24s/it, loss=0.0520, lr=4.87e-06, step=975]Training:   0%|          | 976/200000 [21:57<71:08:58,  1.29s/it, loss=0.0520, lr=4.87e-06, step=975]Training:   0%|          | 976/200000 [21:57<71:08:58,  1.29s/it, loss=0.0687, lr=4.88e-06, step=976]Training:   0%|          | 977/200000 [21:58<67:40:06,  1.22s/it, loss=0.0687, lr=4.88e-06, step=976]Training:   0%|          | 977/200000 [21:58<67:40:06,  1.22s/it, loss=0.1565, lr=4.88e-06, step=977]Training:   0%|          | 978/200000 [22:00<71:02:44,  1.29s/it, loss=0.1565, lr=4.88e-06, step=977]Training:   0%|          | 978/200000 [22:00<71:02:44,  1.29s/it, loss=0.0995, lr=4.89e-06, step=978]Training:   0%|          | 979/200000 [22:01<67:35:53,  1.22s/it, loss=0.0995, lr=4.89e-06, step=978]Training:   0%|          | 979/200000 [22:01<67:35:53,  1.22s/it, loss=0.0494, lr=4.89e-06, step=979]Training:   0%|          | 980/200000 [22:02<69:55:39,  1.26s/it, loss=0.0494, lr=4.89e-06, step=979]Training:   0%|          | 980/200000 [22:02<69:55:39,  1.26s/it, loss=0.0821, lr=4.90e-06, step=980]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 981/200000 [22:03<71:01:06,  1.28s/it, loss=0.0821, lr=4.90e-06, step=980]Training:   0%|          | 981/200000 [22:03<71:01:06,  1.28s/it, loss=0.5260, lr=4.90e-06, step=981]Training:   0%|          | 982/200000 [22:05<74:40:33,  1.35s/it, loss=0.5260, lr=4.90e-06, step=981]Training:   0%|          | 982/200000 [22:05<74:40:33,  1.35s/it, loss=0.0881, lr=4.91e-06, step=982]Training:   0%|          | 983/200000 [22:06<77:41:08,  1.41s/it, loss=0.0881, lr=4.91e-06, step=982]Training:   0%|          | 983/200000 [22:06<77:41:08,  1.41s/it, loss=0.1060, lr=4.91e-06, step=983]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 984/200000 [22:07<72:11:37,  1.31s/it, loss=0.1060, lr=4.91e-06, step=983]Training:   0%|          | 984/200000 [22:07<72:11:37,  1.31s/it, loss=0.0511, lr=4.92e-06, step=984]Training:   0%|          | 985/200000 [22:09<68:22:07,  1.24s/it, loss=0.0511, lr=4.92e-06, step=984]Training:   0%|          | 985/200000 [22:09<68:22:07,  1.24s/it, loss=0.0652, lr=4.92e-06, step=985]Training:   0%|          | 986/200000 [22:10<71:57:07,  1.30s/it, loss=0.0652, lr=4.92e-06, step=985]Training:   0%|          | 986/200000 [22:10<71:57:07,  1.30s/it, loss=0.0757, lr=4.93e-06, step=986]Training:   0%|          | 987/200000 [22:11<68:13:53,  1.23s/it, loss=0.0757, lr=4.93e-06, step=986]Training:   0%|          | 987/200000 [22:11<68:13:53,  1.23s/it, loss=0.1495, lr=4.93e-06, step=987]Training:   0%|          | 988/200000 [22:12<69:49:34,  1.26s/it, loss=0.1495, lr=4.93e-06, step=987]Training:   0%|          | 988/200000 [22:12<69:49:34,  1.26s/it, loss=0.1511, lr=4.94e-06, step=988]Training:   0%|          | 989/200000 [22:14<66:43:50,  1.21s/it, loss=0.1511, lr=4.94e-06, step=988]Training:   0%|          | 989/200000 [22:14<66:43:50,  1.21s/it, loss=0.0546, lr=4.94e-06, step=989]Training:   0%|          | 990/200000 [22:15<64:33:17,  1.17s/it, loss=0.0546, lr=4.94e-06, step=989]Training:   0%|          | 990/200000 [22:15<64:33:17,  1.17s/it, loss=0.0735, lr=4.95e-06, step=990]Training:   0%|          | 991/200000 [22:16<69:02:44,  1.25s/it, loss=0.0735, lr=4.95e-06, step=990]Training:   0%|          | 991/200000 [22:16<69:02:44,  1.25s/it, loss=0.0719, lr=4.95e-06, step=991]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   0%|          | 992/200000 [22:17<72:17:19,  1.31s/it, loss=0.0719, lr=4.95e-06, step=991]Training:   0%|          | 992/200000 [22:17<72:17:19,  1.31s/it, loss=0.0527, lr=4.96e-06, step=992]Training:   0%|          | 993/200000 [22:19<73:37:36,  1.33s/it, loss=0.0527, lr=4.96e-06, step=992]Training:   0%|          | 993/200000 [22:19<73:37:36,  1.33s/it, loss=0.1523, lr=4.96e-06, step=993]Training:   0%|          | 994/200000 [22:20<69:22:35,  1.26s/it, loss=0.1523, lr=4.96e-06, step=993]Training:   0%|          | 994/200000 [22:20<69:22:35,  1.26s/it, loss=0.0715, lr=4.97e-06, step=994]Training:   0%|          | 995/200000 [22:21<66:26:52,  1.20s/it, loss=0.0715, lr=4.97e-06, step=994]Training:   0%|          | 995/200000 [22:21<66:26:52,  1.20s/it, loss=0.0770, lr=4.97e-06, step=995]Training:   0%|          | 996/200000 [22:22<68:57:34,  1.25s/it, loss=0.0770, lr=4.97e-06, step=995]Training:   0%|          | 996/200000 [22:22<68:57:34,  1.25s/it, loss=0.0862, lr=4.98e-06, step=996]Training:   0%|          | 997/200000 [22:24<72:05:19,  1.30s/it, loss=0.0862, lr=4.98e-06, step=996]Training:   0%|          | 997/200000 [22:24<72:05:19,  1.30s/it, loss=0.3099, lr=4.98e-06, step=997]Training:   0%|          | 998/200000 [22:25<68:18:20,  1.24s/it, loss=0.3099, lr=4.98e-06, step=997]Training:   0%|          | 998/200000 [22:25<68:18:20,  1.24s/it, loss=0.7559, lr=4.99e-06, step=998]Training:   0%|          | 999/200000 [22:26<70:51:30,  1.28s/it, loss=0.7559, lr=4.99e-06, step=998]Training:   0%|          | 999/200000 [22:26<70:51:30,  1.28s/it, loss=0.0574, lr=4.99e-06, step=999]Training:   0%|          | 1000/200000 [22:27<67:27:49,  1.22s/it, loss=0.0574, lr=4.99e-06, step=999]Training:   0%|          | 1000/200000 [22:27<67:27:49,  1.22s/it, loss=0.0863, lr=5.00e-06, step=1000]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
23:15:42.490 [I] step=1000 loss=0.1081 lr=4.76e-06 grad_norm=0.83 time=125.5s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1001/200000 [22:29<69:25:00,  1.26s/it, loss=0.0863, lr=5.00e-06, step=1000]Training:   1%|          | 1001/200000 [22:29<69:25:00,  1.26s/it, loss=0.1809, lr=5.00e-06, step=1001]Training:   1%|          | 1002/200000 [22:30<70:35:55,  1.28s/it, loss=0.1809, lr=5.00e-06, step=1001]Training:   1%|          | 1002/200000 [22:30<70:35:55,  1.28s/it, loss=0.2409, lr=5.01e-06, step=1002]Training:   1%|          | 1003/200000 [22:31<73:56:58,  1.34s/it, loss=0.2409, lr=5.01e-06, step=1002]Training:   1%|          | 1003/200000 [22:31<73:56:58,  1.34s/it, loss=0.0579, lr=5.01e-06, step=1003]Training:   1%|          | 1004/200000 [22:33<75:56:27,  1.37s/it, loss=0.0579, lr=5.01e-06, step=1003]Training:   1%|          | 1004/200000 [22:33<75:56:27,  1.37s/it, loss=0.0901, lr=5.02e-06, step=1004]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1005/200000 [22:34<71:01:45,  1.28s/it, loss=0.0901, lr=5.02e-06, step=1004]Training:   1%|          | 1005/200000 [22:34<71:01:45,  1.28s/it, loss=0.1152, lr=5.02e-06, step=1005]Training:   1%|          | 1006/200000 [22:35<67:32:48,  1.22s/it, loss=0.1152, lr=5.02e-06, step=1005]Training:   1%|          | 1006/200000 [22:35<67:32:48,  1.22s/it, loss=0.0607, lr=5.03e-06, step=1006]Training:   1%|          | 1007/200000 [22:37<70:48:47,  1.28s/it, loss=0.0607, lr=5.03e-06, step=1006]Training:   1%|          | 1007/200000 [22:37<70:48:47,  1.28s/it, loss=0.1977, lr=5.03e-06, step=1007]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1008/200000 [22:38<67:24:09,  1.22s/it, loss=0.1977, lr=5.03e-06, step=1007]Training:   1%|          | 1008/200000 [22:38<67:24:09,  1.22s/it, loss=0.0809, lr=5.04e-06, step=1008]Training:   1%|          | 1009/200000 [22:39<68:35:42,  1.24s/it, loss=0.0809, lr=5.04e-06, step=1008]Training:   1%|          | 1009/200000 [22:39<68:35:42,  1.24s/it, loss=0.0759, lr=5.04e-06, step=1009]Training:   1%|          | 1010/200000 [22:40<65:50:12,  1.19s/it, loss=0.0759, lr=5.04e-06, step=1009]Training:   1%|          | 1010/200000 [22:40<65:50:12,  1.19s/it, loss=0.1126, lr=5.05e-06, step=1010]Training:   1%|          | 1011/200000 [22:41<63:55:08,  1.16s/it, loss=0.1126, lr=5.05e-06, step=1010]Training:   1%|          | 1011/200000 [22:41<63:55:08,  1.16s/it, loss=0.0559, lr=5.05e-06, step=1011]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1012/200000 [22:42<68:20:23,  1.24s/it, loss=0.0559, lr=5.05e-06, step=1011]Training:   1%|          | 1012/200000 [22:42<68:20:23,  1.24s/it, loss=0.0766, lr=5.06e-06, step=1012]Training:   1%|          | 1013/200000 [22:44<72:00:24,  1.30s/it, loss=0.0766, lr=5.06e-06, step=1012]Training:   1%|          | 1013/200000 [22:44<72:00:24,  1.30s/it, loss=0.0607, lr=5.06e-06, step=1013]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1014/200000 [22:45<73:20:14,  1.33s/it, loss=0.0607, lr=5.06e-06, step=1013]Training:   1%|          | 1014/200000 [22:45<73:20:14,  1.33s/it, loss=0.1191, lr=5.07e-06, step=1014]Training:   1%|          | 1015/200000 [22:46<69:09:39,  1.25s/it, loss=0.1191, lr=5.07e-06, step=1014]Training:   1%|          | 1015/200000 [22:46<69:09:39,  1.25s/it, loss=0.1248, lr=5.07e-06, step=1015]Training:   1%|          | 1016/200000 [22:47<66:14:05,  1.20s/it, loss=0.1248, lr=5.07e-06, step=1015]Training:   1%|          | 1016/200000 [22:47<66:14:05,  1.20s/it, loss=0.1634, lr=5.08e-06, step=1016]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1017/200000 [22:49<68:42:27,  1.24s/it, loss=0.1634, lr=5.08e-06, step=1016]Training:   1%|          | 1017/200000 [22:49<68:42:27,  1.24s/it, loss=0.0897, lr=5.08e-06, step=1017]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1018/200000 [22:50<70:21:16,  1.27s/it, loss=0.0897, lr=5.08e-06, step=1017]Training:   1%|          | 1018/200000 [22:50<70:21:16,  1.27s/it, loss=0.1299, lr=5.09e-06, step=1018]Training:   1%|          | 1019/200000 [22:51<67:05:32,  1.21s/it, loss=0.1299, lr=5.09e-06, step=1018]Training:   1%|          | 1019/200000 [22:51<67:05:32,  1.21s/it, loss=0.0659, lr=5.09e-06, step=1019]Training:   1%|          | 1020/200000 [22:53<68:46:38,  1.24s/it, loss=0.0659, lr=5.09e-06, step=1019]Training:   1%|          | 1020/200000 [22:53<68:46:38,  1.24s/it, loss=0.1229, lr=5.10e-06, step=1020]Training:   1%|          | 1021/200000 [22:54<66:01:38,  1.19s/it, loss=0.1229, lr=5.10e-06, step=1020]Training:   1%|          | 1021/200000 [22:54<66:01:38,  1.19s/it, loss=0.0628, lr=5.10e-06, step=1021]Training:   1%|          | 1022/200000 [22:55<68:01:05,  1.23s/it, loss=0.0628, lr=5.10e-06, step=1021]Training:   1%|          | 1022/200000 [22:55<68:01:05,  1.23s/it, loss=0.1075, lr=5.11e-06, step=1022]Training:   1%|          | 1023/200000 [22:56<65:26:18,  1.18s/it, loss=0.1075, lr=5.11e-06, step=1022]Training:   1%|          | 1023/200000 [22:56<65:26:18,  1.18s/it, loss=0.1324, lr=5.11e-06, step=1023]Training:   1%|          | 1024/200000 [22:57<70:03:49,  1.27s/it, loss=0.1324, lr=5.11e-06, step=1023]Training:   1%|          | 1024/200000 [22:57<70:03:49,  1.27s/it, loss=0.1017, lr=5.12e-06, step=1024]Training:   1%|          | 1025/200000 [22:59<73:02:05,  1.32s/it, loss=0.1017, lr=5.12e-06, step=1024]Training:   1%|          | 1025/200000 [22:59<73:02:05,  1.32s/it, loss=0.1235, lr=5.12e-06, step=1025]Training:   1%|          | 1026/200000 [23:00<68:57:48,  1.25s/it, loss=0.1235, lr=5.12e-06, step=1025]Training:   1%|          | 1026/200000 [23:00<68:57:48,  1.25s/it, loss=0.0740, lr=5.13e-06, step=1026]Training:   1%|          | 1027/200000 [23:01<66:08:42,  1.20s/it, loss=0.0740, lr=5.13e-06, step=1026]Training:   1%|          | 1027/200000 [23:01<66:08:42,  1.20s/it, loss=0.0793, lr=5.13e-06, step=1027]Training:   1%|          | 1028/200000 [23:02<68:36:04,  1.24s/it, loss=0.0793, lr=5.13e-06, step=1027]Training:   1%|          | 1028/200000 [23:02<68:36:04,  1.24s/it, loss=0.1712, lr=5.14e-06, step=1028]Training:   1%|          | 1029/200000 [23:04<71:24:23,  1.29s/it, loss=0.1712, lr=5.14e-06, step=1028]Training:   1%|          | 1029/200000 [23:04<71:24:23,  1.29s/it, loss=0.0436, lr=5.14e-06, step=1029]Training:   1%|          | 1030/200000 [23:05<67:47:46,  1.23s/it, loss=0.0436, lr=5.14e-06, step=1029]Training:   1%|          | 1030/200000 [23:05<67:47:46,  1.23s/it, loss=0.0891, lr=5.15e-06, step=1030]Training:   1%|          | 1031/200000 [23:06<71:03:14,  1.29s/it, loss=0.0891, lr=5.15e-06, step=1030]Training:   1%|          | 1031/200000 [23:06<71:03:14,  1.29s/it, loss=0.1116, lr=5.15e-06, step=1031]Training:   1%|          | 1032/200000 [23:07<67:36:21,  1.22s/it, loss=0.1116, lr=5.15e-06, step=1031]Training:   1%|          | 1032/200000 [23:07<67:36:21,  1.22s/it, loss=0.0834, lr=5.16e-06, step=1032]Training:   1%|          | 1033/200000 [23:09<70:02:23,  1.27s/it, loss=0.0834, lr=5.16e-06, step=1032]Training:   1%|          | 1033/200000 [23:09<70:02:23,  1.27s/it, loss=0.0529, lr=5.16e-06, step=1033]Training:   1%|          | 1034/200000 [23:10<70:59:36,  1.28s/it, loss=0.0529, lr=5.16e-06, step=1033]Training:   1%|          | 1034/200000 [23:10<70:59:36,  1.28s/it, loss=0.0920, lr=5.17e-06, step=1034]Training:   1%|          | 1035/200000 [23:12<74:22:37,  1.35s/it, loss=0.0920, lr=5.17e-06, step=1034]Training:   1%|          | 1035/200000 [23:12<74:22:37,  1.35s/it, loss=0.0639, lr=5.17e-06, step=1035]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1036/200000 [23:13<77:22:07,  1.40s/it, loss=0.0639, lr=5.17e-06, step=1035]Training:   1%|          | 1036/200000 [23:13<77:22:07,  1.40s/it, loss=0.1292, lr=5.18e-06, step=1036]Training:   1%|          | 1037/200000 [23:14<72:02:36,  1.30s/it, loss=0.1292, lr=5.18e-06, step=1036]Training:   1%|          | 1037/200000 [23:14<72:02:36,  1.30s/it, loss=0.1043, lr=5.18e-06, step=1037]Training:   1%|          | 1038/200000 [23:15<68:16:30,  1.24s/it, loss=0.1043, lr=5.18e-06, step=1037]Training:   1%|          | 1038/200000 [23:15<68:16:30,  1.24s/it, loss=0.0537, lr=5.19e-06, step=1038]Training:   1%|          | 1039/200000 [23:17<72:09:46,  1.31s/it, loss=0.0537, lr=5.19e-06, step=1038]Training:   1%|          | 1039/200000 [23:17<72:09:46,  1.31s/it, loss=0.1004, lr=5.19e-06, step=1039]Training:   1%|          | 1040/200000 [23:18<68:22:05,  1.24s/it, loss=0.1004, lr=5.19e-06, step=1039]Training:   1%|          | 1040/200000 [23:18<68:22:05,  1.24s/it, loss=0.0633, lr=5.20e-06, step=1040]Training:   1%|          | 1041/200000 [23:19<69:30:43,  1.26s/it, loss=0.0633, lr=5.20e-06, step=1040]Training:   1%|          | 1041/200000 [23:19<69:30:43,  1.26s/it, loss=0.0591, lr=5.20e-06, step=1041]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1042/200000 [23:20<66:28:00,  1.20s/it, loss=0.0591, lr=5.20e-06, step=1041]Training:   1%|          | 1042/200000 [23:20<66:28:00,  1.20s/it, loss=0.1178, lr=5.21e-06, step=1042]Training:   1%|          | 1043/200000 [23:21<64:22:13,  1.16s/it, loss=0.1178, lr=5.21e-06, step=1042]Training:   1%|          | 1043/200000 [23:21<64:22:13,  1.16s/it, loss=0.0500, lr=5.21e-06, step=1043]Training:   1%|          | 1044/200000 [23:23<68:57:07,  1.25s/it, loss=0.0500, lr=5.21e-06, step=1043]Training:   1%|          | 1044/200000 [23:23<68:57:07,  1.25s/it, loss=0.0631, lr=5.22e-06, step=1044]Training:   1%|          | 1045/200000 [23:24<72:36:25,  1.31s/it, loss=0.0631, lr=5.22e-06, step=1044]Training:   1%|          | 1045/200000 [23:24<72:36:25,  1.31s/it, loss=0.0495, lr=5.22e-06, step=1045]Training:   1%|          | 1046/200000 [23:26<74:34:52,  1.35s/it, loss=0.0495, lr=5.22e-06, step=1045]Training:   1%|          | 1046/200000 [23:26<74:34:52,  1.35s/it, loss=0.1217, lr=5.23e-06, step=1046]Training:   1%|          | 1047/200000 [23:27<70:05:14,  1.27s/it, loss=0.1217, lr=5.23e-06, step=1046]Training:   1%|          | 1047/200000 [23:27<70:05:14,  1.27s/it, loss=0.1083, lr=5.23e-06, step=1047]Training:   1%|          | 1048/200000 [23:28<66:51:52,  1.21s/it, loss=0.1083, lr=5.23e-06, step=1047]Training:   1%|          | 1048/200000 [23:28<66:51:52,  1.21s/it, loss=0.1178, lr=5.24e-06, step=1048]Training:   1%|          | 1049/200000 [23:29<69:07:32,  1.25s/it, loss=0.1178, lr=5.24e-06, step=1048]Training:   1%|          | 1049/200000 [23:29<69:07:32,  1.25s/it, loss=0.1645, lr=5.24e-06, step=1049]Training:   1%|          | 1050/200000 [23:30<71:21:54,  1.29s/it, loss=0.1645, lr=5.24e-06, step=1049]Training:   1%|          | 1050/200000 [23:30<71:21:54,  1.29s/it, loss=0.0737, lr=5.25e-06, step=1050]Training:   1%|          | 1051/200000 [23:32<67:49:09,  1.23s/it, loss=0.0737, lr=5.25e-06, step=1050]Training:   1%|          | 1051/200000 [23:32<67:49:09,  1.23s/it, loss=0.0565, lr=5.25e-06, step=1051]Training:   1%|          | 1052/200000 [23:33<70:38:44,  1.28s/it, loss=0.0565, lr=5.25e-06, step=1051]Training:   1%|          | 1052/200000 [23:33<70:38:44,  1.28s/it, loss=0.0446, lr=5.26e-06, step=1052]Training:   1%|          | 1053/200000 [23:34<67:16:07,  1.22s/it, loss=0.0446, lr=5.26e-06, step=1052]Training:   1%|          | 1053/200000 [23:34<67:16:07,  1.22s/it, loss=0.0537, lr=5.26e-06, step=1053]Training:   1%|          | 1054/200000 [23:35<69:06:53,  1.25s/it, loss=0.0537, lr=5.26e-06, step=1053]Training:   1%|          | 1054/200000 [23:35<69:06:53,  1.25s/it, loss=0.0802, lr=5.27e-06, step=1054]Training:   1%|          | 1055/200000 [23:37<70:28:51,  1.28s/it, loss=0.0802, lr=5.27e-06, step=1054]Training:   1%|          | 1055/200000 [23:37<70:28:51,  1.28s/it, loss=0.0798, lr=5.27e-06, step=1055]Training:   1%|          | 1056/200000 [23:38<74:04:01,  1.34s/it, loss=0.0798, lr=5.27e-06, step=1055]Training:   1%|          | 1056/200000 [23:38<74:04:01,  1.34s/it, loss=0.0643, lr=5.28e-06, step=1056]Training:   1%|          | 1057/200000 [23:40<76:54:43,  1.39s/it, loss=0.0643, lr=5.28e-06, step=1056]Training:   1%|          | 1057/200000 [23:40<76:54:43,  1.39s/it, loss=0.0783, lr=5.28e-06, step=1057]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1058/200000 [23:41<71:40:16,  1.30s/it, loss=0.0783, lr=5.28e-06, step=1057]Training:   1%|          | 1058/200000 [23:41<71:40:16,  1.30s/it, loss=0.0869, lr=5.29e-06, step=1058]Training:   1%|          | 1059/200000 [23:42<68:02:01,  1.23s/it, loss=0.0869, lr=5.29e-06, step=1058]Training:   1%|          | 1059/200000 [23:42<68:02:01,  1.23s/it, loss=0.0909, lr=5.29e-06, step=1059]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1060/200000 [23:43<71:05:37,  1.29s/it, loss=0.0909, lr=5.29e-06, step=1059]Training:   1%|          | 1060/200000 [23:43<71:05:37,  1.29s/it, loss=0.0920, lr=5.30e-06, step=1060]Training:   1%|          | 1061/200000 [23:44<67:38:01,  1.22s/it, loss=0.0920, lr=5.30e-06, step=1060]Training:   1%|          | 1061/200000 [23:44<67:38:01,  1.22s/it, loss=0.1115, lr=5.30e-06, step=1061]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1062/200000 [23:46<69:13:03,  1.25s/it, loss=0.1115, lr=5.30e-06, step=1061]Training:   1%|          | 1062/200000 [23:46<69:13:03,  1.25s/it, loss=0.1024, lr=5.31e-06, step=1062]Training:   1%|          | 1063/200000 [23:47<66:18:50,  1.20s/it, loss=0.1024, lr=5.31e-06, step=1062]Training:   1%|          | 1063/200000 [23:47<66:18:50,  1.20s/it, loss=0.0847, lr=5.31e-06, step=1063]Training:   1%|          | 1064/200000 [23:48<64:15:28,  1.16s/it, loss=0.0847, lr=5.31e-06, step=1063]Training:   1%|          | 1064/200000 [23:48<64:15:28,  1.16s/it, loss=0.0556, lr=5.32e-06, step=1064]Training:   1%|          | 1065/200000 [23:49<68:40:19,  1.24s/it, loss=0.0556, lr=5.32e-06, step=1064]Training:   1%|          | 1065/200000 [23:49<68:40:19,  1.24s/it, loss=0.0902, lr=5.32e-06, step=1065]Training:   1%|          | 1066/200000 [23:51<72:27:45,  1.31s/it, loss=0.0902, lr=5.32e-06, step=1065]Training:   1%|          | 1066/200000 [23:51<72:27:45,  1.31s/it, loss=0.0529, lr=5.33e-06, step=1066]Training:   1%|          | 1067/200000 [23:52<74:11:25,  1.34s/it, loss=0.0529, lr=5.33e-06, step=1066]Training:   1%|          | 1067/200000 [23:52<74:11:25,  1.34s/it, loss=0.0468, lr=5.33e-06, step=1067]Training:   1%|          | 1068/200000 [23:53<69:46:55,  1.26s/it, loss=0.0468, lr=5.33e-06, step=1067]Training:   1%|          | 1068/200000 [23:53<69:46:55,  1.26s/it, loss=0.1309, lr=5.34e-06, step=1068]Training:   1%|          | 1069/200000 [23:54<66:40:59,  1.21s/it, loss=0.1309, lr=5.34e-06, step=1068]Training:   1%|          | 1069/200000 [23:54<66:40:59,  1.21s/it, loss=0.2478, lr=5.34e-06, step=1069]Training:   1%|          | 1070/200000 [23:56<68:37:20,  1.24s/it, loss=0.2478, lr=5.34e-06, step=1069]Training:   1%|          | 1070/200000 [23:56<68:37:20,  1.24s/it, loss=0.0792, lr=5.35e-06, step=1070]Training:   1%|          | 1071/200000 [23:57<69:57:04,  1.27s/it, loss=0.0792, lr=5.35e-06, step=1070]Training:   1%|          | 1071/200000 [23:57<69:57:04,  1.27s/it, loss=0.0678, lr=5.35e-06, step=1071]Training:   1%|          | 1072/200000 [23:58<66:46:23,  1.21s/it, loss=0.0678, lr=5.35e-06, step=1071]Training:   1%|          | 1072/200000 [23:58<66:46:23,  1.21s/it, loss=0.1221, lr=5.36e-06, step=1072]Training:   1%|          | 1073/200000 [23:59<68:43:50,  1.24s/it, loss=0.1221, lr=5.36e-06, step=1072]Training:   1%|          | 1073/200000 [23:59<68:43:50,  1.24s/it, loss=0.0525, lr=5.36e-06, step=1073]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1074/200000 [24:00<65:58:49,  1.19s/it, loss=0.0525, lr=5.36e-06, step=1073]Training:   1%|          | 1074/200000 [24:00<65:58:49,  1.19s/it, loss=0.0770, lr=5.37e-06, step=1074]Training:   1%|          | 1075/200000 [24:02<67:36:50,  1.22s/it, loss=0.0770, lr=5.37e-06, step=1074]Training:   1%|          | 1075/200000 [24:02<67:36:50,  1.22s/it, loss=0.0798, lr=5.37e-06, step=1075]Training:   1%|          | 1076/200000 [24:03<65:08:34,  1.18s/it, loss=0.0798, lr=5.37e-06, step=1075]Training:   1%|          | 1076/200000 [24:03<65:08:34,  1.18s/it, loss=0.0890, lr=5.38e-06, step=1076]Training:   1%|          | 1077/200000 [24:04<69:52:25,  1.26s/it, loss=0.0890, lr=5.38e-06, step=1076]Training:   1%|          | 1077/200000 [24:04<69:52:25,  1.26s/it, loss=0.0721, lr=5.38e-06, step=1077]Training:   1%|          | 1078/200000 [24:06<72:52:30,  1.32s/it, loss=0.0721, lr=5.38e-06, step=1077]Training:   1%|          | 1078/200000 [24:06<72:52:30,  1.32s/it, loss=0.0515, lr=5.39e-06, step=1078]Training:   1%|          | 1079/200000 [24:07<68:51:56,  1.25s/it, loss=0.0515, lr=5.39e-06, step=1078]Training:   1%|          | 1079/200000 [24:07<68:51:56,  1.25s/it, loss=0.0767, lr=5.39e-06, step=1079]Training:   1%|          | 1080/200000 [24:08<66:01:39,  1.19s/it, loss=0.0767, lr=5.39e-06, step=1079]Training:   1%|          | 1080/200000 [24:08<66:01:39,  1.19s/it, loss=0.0439, lr=5.40e-06, step=1080]Training:   1%|          | 1081/200000 [24:09<68:14:01,  1.23s/it, loss=0.0439, lr=5.40e-06, step=1080]Training:   1%|          | 1081/200000 [24:09<68:14:01,  1.23s/it, loss=0.2119, lr=5.40e-06, step=1081]Training:   1%|          | 1082/200000 [24:11<71:11:57,  1.29s/it, loss=0.2119, lr=5.40e-06, step=1081]Training:   1%|          | 1082/200000 [24:11<71:11:57,  1.29s/it, loss=0.0840, lr=5.41e-06, step=1082]Training:   1%|          | 1083/200000 [24:12<67:42:36,  1.23s/it, loss=0.0840, lr=5.41e-06, step=1082]Training:   1%|          | 1083/200000 [24:12<67:42:36,  1.23s/it, loss=0.0588, lr=5.41e-06, step=1083]Training:   1%|          | 1084/200000 [24:13<70:57:58,  1.28s/it, loss=0.0588, lr=5.41e-06, step=1083]Training:   1%|          | 1084/200000 [24:13<70:57:58,  1.28s/it, loss=0.0773, lr=5.42e-06, step=1084]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1085/200000 [24:14<67:33:58,  1.22s/it, loss=0.0773, lr=5.42e-06, step=1084]Training:   1%|          | 1085/200000 [24:14<67:33:58,  1.22s/it, loss=0.1107, lr=5.42e-06, step=1085]Training:   1%|          | 1086/200000 [24:16<69:53:20,  1.26s/it, loss=0.1107, lr=5.42e-06, step=1085]Training:   1%|          | 1086/200000 [24:16<69:53:20,  1.26s/it, loss=0.1525, lr=5.43e-06, step=1086]Training:   1%|          | 1087/200000 [24:17<71:06:02,  1.29s/it, loss=0.1525, lr=5.43e-06, step=1086]Training:   1%|          | 1087/200000 [24:17<71:06:02,  1.29s/it, loss=0.1099, lr=5.43e-06, step=1087]Training:   1%|          | 1088/200000 [24:18<74:40:08,  1.35s/it, loss=0.1099, lr=5.43e-06, step=1087]Training:   1%|          | 1088/200000 [24:18<74:40:08,  1.35s/it, loss=0.1773, lr=5.44e-06, step=1088]Training:   1%|          | 1089/200000 [24:20<77:35:04,  1.40s/it, loss=0.1773, lr=5.44e-06, step=1088]Training:   1%|          | 1089/200000 [24:20<77:35:04,  1.40s/it, loss=0.0987, lr=5.44e-06, step=1089]Training:   1%|          | 1090/200000 [24:21<72:09:45,  1.31s/it, loss=0.0987, lr=5.44e-06, step=1089]Training:   1%|          | 1090/200000 [24:21<72:09:45,  1.31s/it, loss=0.0657, lr=5.45e-06, step=1090]Training:   1%|          | 1091/200000 [24:22<68:22:08,  1.24s/it, loss=0.0657, lr=5.45e-06, step=1090]Training:   1%|          | 1091/200000 [24:22<68:22:08,  1.24s/it, loss=0.0773, lr=5.45e-06, step=1091]Training:   1%|          | 1092/200000 [24:23<71:14:59,  1.29s/it, loss=0.0773, lr=5.45e-06, step=1091]Training:   1%|          | 1092/200000 [24:23<71:14:59,  1.29s/it, loss=0.0760, lr=5.46e-06, step=1092]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1093/200000 [24:25<67:40:58,  1.22s/it, loss=0.0760, lr=5.46e-06, step=1092]Training:   1%|          | 1093/200000 [24:25<67:40:58,  1.22s/it, loss=0.0884, lr=5.46e-06, step=1093]Training:   1%|          | 1094/200000 [24:26<69:13:35,  1.25s/it, loss=0.0884, lr=5.46e-06, step=1093]Training:   1%|          | 1094/200000 [24:26<69:13:35,  1.25s/it, loss=0.0958, lr=5.47e-06, step=1094]Training:   1%|          | 1095/200000 [24:27<66:18:14,  1.20s/it, loss=0.0958, lr=5.47e-06, step=1094]Training:   1%|          | 1095/200000 [24:27<66:18:14,  1.20s/it, loss=0.1274, lr=5.47e-06, step=1095]Training:   1%|          | 1096/200000 [24:28<64:14:55,  1.16s/it, loss=0.1274, lr=5.47e-06, step=1095]Training:   1%|          | 1096/200000 [24:28<64:14:55,  1.16s/it, loss=0.1045, lr=5.48e-06, step=1096]Training:   1%|          | 1097/200000 [24:29<68:48:30,  1.25s/it, loss=0.1045, lr=5.48e-06, step=1096]Training:   1%|          | 1097/200000 [24:29<68:48:30,  1.25s/it, loss=0.0493, lr=5.48e-06, step=1097]Training:   1%|          | 1098/200000 [24:31<72:14:30,  1.31s/it, loss=0.0493, lr=5.48e-06, step=1097]Training:   1%|          | 1098/200000 [24:31<72:14:30,  1.31s/it, loss=0.3815, lr=5.49e-06, step=1098]Training:   1%|          | 1099/200000 [24:32<74:18:40,  1.34s/it, loss=0.3815, lr=5.49e-06, step=1098]Training:   1%|          | 1099/200000 [24:32<74:18:40,  1.34s/it, loss=0.0709, lr=5.49e-06, step=1099]Training:   1%|          | 1100/200000 [24:33<69:50:11,  1.26s/it, loss=0.0709, lr=5.49e-06, step=1099]Training:   1%|          | 1100/200000 [24:33<69:50:11,  1.26s/it, loss=0.1179, lr=5.50e-06, step=1100]23:17:48.290 [I] step=1100 loss=0.0968 lr=5.26e-06 grad_norm=0.77 time=125.8s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1101/200000 [24:34<66:44:56,  1.21s/it, loss=0.1179, lr=5.50e-06, step=1100]Training:   1%|          | 1101/200000 [24:34<66:44:56,  1.21s/it, loss=0.0728, lr=5.50e-06, step=1101]Training:   1%|          | 1102/200000 [24:36<69:18:28,  1.25s/it, loss=0.0728, lr=5.50e-06, step=1101]Training:   1%|          | 1102/200000 [24:36<69:18:28,  1.25s/it, loss=0.0784, lr=5.51e-06, step=1102]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1103/200000 [24:37<71:38:15,  1.30s/it, loss=0.0784, lr=5.51e-06, step=1102]Training:   1%|          | 1103/200000 [24:37<71:38:15,  1.30s/it, loss=0.0433, lr=5.51e-06, step=1103]Training:   1%|          | 1104/200000 [24:38<67:59:25,  1.23s/it, loss=0.0433, lr=5.51e-06, step=1103]Training:   1%|          | 1104/200000 [24:38<67:59:25,  1.23s/it, loss=0.0459, lr=5.52e-06, step=1104]Training:   1%|          | 1105/200000 [24:40<70:28:06,  1.28s/it, loss=0.0459, lr=5.52e-06, step=1104]Training:   1%|          | 1105/200000 [24:40<70:28:06,  1.28s/it, loss=0.0849, lr=5.52e-06, step=1105]Training:   1%|          | 1106/200000 [24:41<67:09:40,  1.22s/it, loss=0.0849, lr=5.52e-06, step=1105]Training:   1%|          | 1106/200000 [24:41<67:09:40,  1.22s/it, loss=0.0897, lr=5.53e-06, step=1106]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1107/200000 [24:42<69:03:37,  1.25s/it, loss=0.0897, lr=5.53e-06, step=1106]Training:   1%|          | 1107/200000 [24:42<69:03:37,  1.25s/it, loss=0.0880, lr=5.53e-06, step=1107]Training:   1%|          | 1108/200000 [24:43<70:21:25,  1.27s/it, loss=0.0880, lr=5.53e-06, step=1107]Training:   1%|          | 1108/200000 [24:43<70:21:25,  1.27s/it, loss=0.0707, lr=5.54e-06, step=1108]Training:   1%|          | 1109/200000 [24:45<73:49:05,  1.34s/it, loss=0.0707, lr=5.54e-06, step=1108]Training:   1%|          | 1109/200000 [24:45<73:49:05,  1.34s/it, loss=0.0495, lr=5.54e-06, step=1109]Training:   1%|          | 1110/200000 [24:46<76:39:15,  1.39s/it, loss=0.0495, lr=5.54e-06, step=1109]Training:   1%|          | 1110/200000 [24:46<76:39:15,  1.39s/it, loss=0.0626, lr=5.55e-06, step=1110]Training:   1%|          | 1111/200000 [24:47<71:29:45,  1.29s/it, loss=0.0626, lr=5.55e-06, step=1110]Training:   1%|          | 1111/200000 [24:47<71:29:45,  1.29s/it, loss=0.1637, lr=5.55e-06, step=1111]Training:   1%|          | 1112/200000 [24:49<67:54:19,  1.23s/it, loss=0.1637, lr=5.55e-06, step=1111]Training:   1%|          | 1112/200000 [24:49<67:54:19,  1.23s/it, loss=0.0633, lr=5.56e-06, step=1112]Training:   1%|          | 1113/200000 [24:50<71:12:32,  1.29s/it, loss=0.0633, lr=5.56e-06, step=1112]Training:   1%|          | 1113/200000 [24:50<71:12:32,  1.29s/it, loss=0.1110, lr=5.56e-06, step=1113]Training:   1%|          | 1114/200000 [24:51<67:41:00,  1.23s/it, loss=0.1110, lr=5.56e-06, step=1113]Training:   1%|          | 1114/200000 [24:51<67:41:00,  1.23s/it, loss=0.0902, lr=5.57e-06, step=1114]Training:   1%|          | 1115/200000 [24:52<69:56:49,  1.27s/it, loss=0.0902, lr=5.57e-06, step=1114]Training:   1%|          | 1115/200000 [24:52<69:56:49,  1.27s/it, loss=0.1042, lr=5.57e-06, step=1115]Training:   1%|          | 1116/200000 [24:54<66:48:59,  1.21s/it, loss=0.1042, lr=5.57e-06, step=1115]Training:   1%|          | 1116/200000 [24:54<66:48:59,  1.21s/it, loss=0.0950, lr=5.58e-06, step=1116]Training:   1%|          | 1117/200000 [24:55<64:36:53,  1.17s/it, loss=0.0950, lr=5.58e-06, step=1116]Training:   1%|          | 1117/200000 [24:55<64:36:53,  1.17s/it, loss=0.0665, lr=5.58e-06, step=1117]Training:   1%|          | 1118/200000 [24:56<68:52:20,  1.25s/it, loss=0.0665, lr=5.58e-06, step=1117]Training:   1%|          | 1118/200000 [24:56<68:52:20,  1.25s/it, loss=0.0692, lr=5.59e-06, step=1118]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1119/200000 [24:57<71:38:18,  1.30s/it, loss=0.0692, lr=5.59e-06, step=1118]Training:   1%|          | 1119/200000 [24:57<71:38:18,  1.30s/it, loss=0.0700, lr=5.59e-06, step=1119]Training:   1%|          | 1120/200000 [24:59<73:53:00,  1.34s/it, loss=0.0700, lr=5.59e-06, step=1119]Training:   1%|          | 1120/200000 [24:59<73:53:00,  1.34s/it, loss=0.0694, lr=5.60e-06, step=1120]Training:   1%|          | 1121/200000 [25:00<69:35:13,  1.26s/it, loss=0.0694, lr=5.60e-06, step=1120]Training:   1%|          | 1121/200000 [25:00<69:35:13,  1.26s/it, loss=0.0577, lr=5.60e-06, step=1121]Training:   1%|          | 1122/200000 [25:01<66:32:28,  1.20s/it, loss=0.0577, lr=5.60e-06, step=1121]Training:   1%|          | 1122/200000 [25:01<66:32:28,  1.20s/it, loss=0.0526, lr=5.61e-06, step=1122]Training:   1%|          | 1123/200000 [25:02<68:57:08,  1.25s/it, loss=0.0526, lr=5.61e-06, step=1122]Training:   1%|          | 1123/200000 [25:02<68:57:08,  1.25s/it, loss=0.0879, lr=5.61e-06, step=1123]Training:   1%|          | 1124/200000 [25:04<69:50:22,  1.26s/it, loss=0.0879, lr=5.61e-06, step=1123]Training:   1%|          | 1124/200000 [25:04<69:50:22,  1.26s/it, loss=0.0977, lr=5.62e-06, step=1124]Training:   1%|          | 1125/200000 [25:05<66:45:12,  1.21s/it, loss=0.0977, lr=5.62e-06, step=1124]Training:   1%|          | 1125/200000 [25:05<66:45:12,  1.21s/it, loss=0.1139, lr=5.62e-06, step=1125]Training:   1%|          | 1126/200000 [25:06<68:45:44,  1.24s/it, loss=0.1139, lr=5.62e-06, step=1125]Training:   1%|          | 1126/200000 [25:06<68:45:44,  1.24s/it, loss=0.1097, lr=5.63e-06, step=1126]Training:   1%|          | 1127/200000 [25:07<65:58:37,  1.19s/it, loss=0.1097, lr=5.63e-06, step=1126]Training:   1%|          | 1127/200000 [25:07<65:58:37,  1.19s/it, loss=0.0743, lr=5.63e-06, step=1127]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1128/200000 [25:08<68:19:20,  1.24s/it, loss=0.0743, lr=5.63e-06, step=1127]Training:   1%|          | 1128/200000 [25:08<68:19:20,  1.24s/it, loss=0.0462, lr=5.64e-06, step=1128]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1129/200000 [25:10<65:39:03,  1.19s/it, loss=0.0462, lr=5.64e-06, step=1128]Training:   1%|          | 1129/200000 [25:10<65:39:03,  1.19s/it, loss=0.0625, lr=5.64e-06, step=1129]Training:   1%|          | 1130/200000 [25:11<70:13:47,  1.27s/it, loss=0.0625, lr=5.64e-06, step=1129]Training:   1%|          | 1130/200000 [25:11<70:13:47,  1.27s/it, loss=0.0953, lr=5.65e-06, step=1130]Training:   1%|          | 1131/200000 [25:12<73:07:16,  1.32s/it, loss=0.0953, lr=5.65e-06, step=1130]Training:   1%|          | 1131/200000 [25:12<73:07:16,  1.32s/it, loss=0.1367, lr=5.65e-06, step=1131]Training:   1%|          | 1132/200000 [25:14<69:00:30,  1.25s/it, loss=0.1367, lr=5.65e-06, step=1131]Training:   1%|          | 1132/200000 [25:14<69:00:30,  1.25s/it, loss=0.0845, lr=5.66e-06, step=1132]Training:   1%|          | 1133/200000 [25:15<66:07:13,  1.20s/it, loss=0.0845, lr=5.66e-06, step=1132]Training:   1%|          | 1133/200000 [25:15<66:07:13,  1.20s/it, loss=0.2227, lr=5.66e-06, step=1133]Training:   1%|          | 1134/200000 [25:16<68:30:55,  1.24s/it, loss=0.2227, lr=5.66e-06, step=1133]Training:   1%|          | 1134/200000 [25:16<68:30:55,  1.24s/it, loss=0.0603, lr=5.67e-06, step=1134]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1135/200000 [25:17<71:14:48,  1.29s/it, loss=0.0603, lr=5.67e-06, step=1134]Training:   1%|          | 1135/200000 [25:17<71:14:48,  1.29s/it, loss=0.0959, lr=5.67e-06, step=1135]Training:   1%|          | 1136/200000 [25:18<68:19:34,  1.24s/it, loss=0.0959, lr=5.67e-06, step=1135]Training:   1%|          | 1136/200000 [25:18<68:19:34,  1.24s/it, loss=0.4725, lr=5.68e-06, step=1136]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1137/200000 [25:20<71:41:18,  1.30s/it, loss=0.4725, lr=5.68e-06, step=1136]Training:   1%|          | 1137/200000 [25:20<71:41:18,  1.30s/it, loss=0.0881, lr=5.68e-06, step=1137]Training:   1%|          | 1138/200000 [25:21<68:00:32,  1.23s/it, loss=0.0881, lr=5.68e-06, step=1137]Training:   1%|          | 1138/200000 [25:21<68:00:32,  1.23s/it, loss=0.4619, lr=5.69e-06, step=1138]Training:   1%|          | 1139/200000 [25:22<70:17:44,  1.27s/it, loss=0.4619, lr=5.69e-06, step=1138]Training:   1%|          | 1139/200000 [25:22<70:17:44,  1.27s/it, loss=0.1340, lr=5.69e-06, step=1139]Training:   1%|          | 1140/200000 [25:24<71:11:01,  1.29s/it, loss=0.1340, lr=5.69e-06, step=1139]Training:   1%|          | 1140/200000 [25:24<71:11:01,  1.29s/it, loss=0.0797, lr=5.70e-06, step=1140]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1141/200000 [25:25<74:43:47,  1.35s/it, loss=0.0797, lr=5.70e-06, step=1140]Training:   1%|          | 1141/200000 [25:25<74:43:47,  1.35s/it, loss=0.0465, lr=5.70e-06, step=1141]Training:   1%|          | 1142/200000 [25:27<77:31:59,  1.40s/it, loss=0.0465, lr=5.70e-06, step=1141]Training:   1%|          | 1142/200000 [25:27<77:31:59,  1.40s/it, loss=0.0376, lr=5.71e-06, step=1142]Training:   1%|          | 1143/200000 [25:28<72:07:46,  1.31s/it, loss=0.0376, lr=5.71e-06, step=1142]Training:   1%|          | 1143/200000 [25:28<72:07:46,  1.31s/it, loss=0.0846, lr=5.71e-06, step=1143]Training:   1%|          | 1144/200000 [25:29<68:19:41,  1.24s/it, loss=0.0846, lr=5.71e-06, step=1143]Training:   1%|          | 1144/200000 [25:29<68:19:41,  1.24s/it, loss=0.1478, lr=5.72e-06, step=1144]Training:   1%|          | 1145/200000 [25:30<72:01:17,  1.30s/it, loss=0.1478, lr=5.72e-06, step=1144]Training:   1%|          | 1145/200000 [25:30<72:01:17,  1.30s/it, loss=0.0939, lr=5.72e-06, step=1145]Training:   1%|          | 1146/200000 [25:31<68:12:10,  1.23s/it, loss=0.0939, lr=5.72e-06, step=1145]Training:   1%|          | 1146/200000 [25:31<68:12:10,  1.23s/it, loss=0.0885, lr=5.73e-06, step=1146]Training:   1%|          | 1147/200000 [25:33<69:47:07,  1.26s/it, loss=0.0885, lr=5.73e-06, step=1146]Training:   1%|          | 1147/200000 [25:33<69:47:07,  1.26s/it, loss=0.1055, lr=5.73e-06, step=1147]Training:   1%|          | 1148/200000 [25:34<66:38:53,  1.21s/it, loss=0.1055, lr=5.73e-06, step=1147]Training:   1%|          | 1148/200000 [25:34<66:38:53,  1.21s/it, loss=0.0432, lr=5.74e-06, step=1148]Training:   1%|          | 1149/200000 [25:35<64:29:29,  1.17s/it, loss=0.0432, lr=5.74e-06, step=1148]Training:   1%|          | 1149/200000 [25:35<64:29:29,  1.17s/it, loss=0.0613, lr=5.74e-06, step=1149]Training:   1%|          | 1150/200000 [25:36<68:59:24,  1.25s/it, loss=0.0613, lr=5.74e-06, step=1149]Training:   1%|          | 1150/200000 [25:36<68:59:24,  1.25s/it, loss=0.1471, lr=5.75e-06, step=1150]Training:   1%|          | 1151/200000 [25:38<72:37:46,  1.31s/it, loss=0.1471, lr=5.75e-06, step=1150]Training:   1%|          | 1151/200000 [25:38<72:37:46,  1.31s/it, loss=0.0899, lr=5.75e-06, step=1151]Training:   1%|          | 1152/200000 [25:39<74:01:01,  1.34s/it, loss=0.0899, lr=5.75e-06, step=1151]Training:   1%|          | 1152/200000 [25:39<74:01:01,  1.34s/it, loss=0.1473, lr=5.76e-06, step=1152]Training:   1%|          | 1153/200000 [25:40<69:39:20,  1.26s/it, loss=0.1473, lr=5.76e-06, step=1152]Training:   1%|          | 1153/200000 [25:40<69:39:20,  1.26s/it, loss=0.1404, lr=5.76e-06, step=1153]Training:   1%|          | 1154/200000 [25:41<66:35:58,  1.21s/it, loss=0.1404, lr=5.76e-06, step=1153]Training:   1%|          | 1154/200000 [25:41<66:35:58,  1.21s/it, loss=0.6149, lr=5.77e-06, step=1154]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1155/200000 [25:43<68:49:05,  1.25s/it, loss=0.6149, lr=5.77e-06, step=1154]Training:   1%|          | 1155/200000 [25:43<68:49:05,  1.25s/it, loss=0.0629, lr=5.77e-06, step=1155]Training:   1%|          | 1156/200000 [25:44<71:46:31,  1.30s/it, loss=0.0629, lr=5.77e-06, step=1155]Training:   1%|          | 1156/200000 [25:44<71:46:31,  1.30s/it, loss=0.0486, lr=5.78e-06, step=1156]Training:   1%|          | 1157/200000 [25:45<68:04:35,  1.23s/it, loss=0.0486, lr=5.78e-06, step=1156]Training:   1%|          | 1157/200000 [25:45<68:04:35,  1.23s/it, loss=0.1617, lr=5.78e-06, step=1157]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1158/200000 [25:47<70:30:02,  1.28s/it, loss=0.1617, lr=5.78e-06, step=1157]Training:   1%|          | 1158/200000 [25:47<70:30:02,  1.28s/it, loss=0.0448, lr=5.79e-06, step=1158]Training:   1%|          | 1159/200000 [25:48<67:11:33,  1.22s/it, loss=0.0448, lr=5.79e-06, step=1158]Training:   1%|          | 1159/200000 [25:48<67:11:33,  1.22s/it, loss=0.1436, lr=5.79e-06, step=1159]Training:   1%|          | 1160/200000 [25:49<68:44:40,  1.24s/it, loss=0.1436, lr=5.79e-06, step=1159]Training:   1%|          | 1160/200000 [25:49<68:44:40,  1.24s/it, loss=0.0653, lr=5.80e-06, step=1160]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1161/200000 [25:50<70:09:18,  1.27s/it, loss=0.0653, lr=5.80e-06, step=1160]Training:   1%|          | 1161/200000 [25:50<70:09:18,  1.27s/it, loss=0.0592, lr=5.80e-06, step=1161]Training:   1%|          | 1162/200000 [25:52<73:37:18,  1.33s/it, loss=0.0592, lr=5.80e-06, step=1161]Training:   1%|          | 1162/200000 [25:52<73:37:18,  1.33s/it, loss=0.1281, lr=5.81e-06, step=1162]Training:   1%|          | 1163/200000 [25:53<76:29:12,  1.38s/it, loss=0.1281, lr=5.81e-06, step=1162]Training:   1%|          | 1163/200000 [25:53<76:29:12,  1.38s/it, loss=0.0768, lr=5.81e-06, step=1163]Training:   1%|          | 1164/200000 [25:54<71:22:59,  1.29s/it, loss=0.0768, lr=5.81e-06, step=1163]Training:   1%|          | 1164/200000 [25:54<71:22:59,  1.29s/it, loss=0.0607, lr=5.82e-06, step=1164]Training:   1%|          | 1165/200000 [25:55<67:50:23,  1.23s/it, loss=0.0607, lr=5.82e-06, step=1164]Training:   1%|          | 1165/200000 [25:55<67:50:23,  1.23s/it, loss=0.0651, lr=5.82e-06, step=1165]Training:   1%|          | 1166/200000 [25:57<71:16:23,  1.29s/it, loss=0.0651, lr=5.82e-06, step=1165]Training:   1%|          | 1166/200000 [25:57<71:16:23,  1.29s/it, loss=0.0868, lr=5.83e-06, step=1166]Training:   1%|          | 1167/200000 [25:58<67:45:44,  1.23s/it, loss=0.0868, lr=5.83e-06, step=1166]Training:   1%|          | 1167/200000 [25:58<67:45:44,  1.23s/it, loss=0.0912, lr=5.83e-06, step=1167]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1168/200000 [25:59<68:56:02,  1.25s/it, loss=0.0912, lr=5.83e-06, step=1167]Training:   1%|          | 1168/200000 [25:59<68:56:02,  1.25s/it, loss=0.0590, lr=5.84e-06, step=1168]Training:   1%|          | 1169/200000 [26:00<66:06:13,  1.20s/it, loss=0.0590, lr=5.84e-06, step=1168]Training:   1%|          | 1169/200000 [26:00<66:06:13,  1.20s/it, loss=0.0640, lr=5.84e-06, step=1169]Training:   1%|          | 1170/200000 [26:01<64:05:48,  1.16s/it, loss=0.0640, lr=5.84e-06, step=1169]Training:   1%|          | 1170/200000 [26:01<64:05:48,  1.16s/it, loss=0.0522, lr=5.85e-06, step=1170]Training:   1%|          | 1171/200000 [26:03<68:28:24,  1.24s/it, loss=0.0522, lr=5.85e-06, step=1170]Training:   1%|          | 1171/200000 [26:03<68:28:24,  1.24s/it, loss=0.0641, lr=5.85e-06, step=1171]Training:   1%|          | 1172/200000 [26:04<71:44:15,  1.30s/it, loss=0.0641, lr=5.85e-06, step=1171]Training:   1%|          | 1172/200000 [26:04<71:44:15,  1.30s/it, loss=0.0445, lr=5.86e-06, step=1172]Training:   1%|          | 1173/200000 [26:06<72:30:36,  1.31s/it, loss=0.0445, lr=5.86e-06, step=1172]Training:   1%|          | 1173/200000 [26:06<72:30:36,  1.31s/it, loss=0.1094, lr=5.86e-06, step=1173]Training:   1%|          | 1174/200000 [26:07<68:34:30,  1.24s/it, loss=0.1094, lr=5.86e-06, step=1173]Training:   1%|          | 1174/200000 [26:07<68:34:30,  1.24s/it, loss=0.0669, lr=5.87e-06, step=1174]Training:   1%|          | 1175/200000 [26:08<65:48:53,  1.19s/it, loss=0.0669, lr=5.87e-06, step=1174]Training:   1%|          | 1175/200000 [26:08<65:48:53,  1.19s/it, loss=0.1248, lr=5.87e-06, step=1175]Training:   1%|          | 1176/200000 [26:09<67:21:05,  1.22s/it, loss=0.1248, lr=5.87e-06, step=1175]Training:   1%|          | 1176/200000 [26:09<67:21:05,  1.22s/it, loss=0.0578, lr=5.88e-06, step=1176]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1177/200000 [26:10<69:02:20,  1.25s/it, loss=0.0578, lr=5.88e-06, step=1176]Training:   1%|          | 1177/200000 [26:10<69:02:20,  1.25s/it, loss=0.1425, lr=5.88e-06, step=1177]Training:   1%|          | 1178/200000 [26:11<66:10:39,  1.20s/it, loss=0.1425, lr=5.88e-06, step=1177]Training:   1%|          | 1178/200000 [26:11<66:10:39,  1.20s/it, loss=0.1232, lr=5.89e-06, step=1178]Training:   1%|          | 1179/200000 [26:13<68:14:35,  1.24s/it, loss=0.1232, lr=5.89e-06, step=1178]Training:   1%|          | 1179/200000 [26:13<68:14:35,  1.24s/it, loss=0.0677, lr=5.89e-06, step=1179]Training:   1%|          | 1180/200000 [26:14<65:33:28,  1.19s/it, loss=0.0677, lr=5.89e-06, step=1179]Training:   1%|          | 1180/200000 [26:14<65:33:28,  1.19s/it, loss=0.0431, lr=5.90e-06, step=1180]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1181/200000 [26:15<67:17:58,  1.22s/it, loss=0.0431, lr=5.90e-06, step=1180]Training:   1%|          | 1181/200000 [26:15<67:17:58,  1.22s/it, loss=0.0607, lr=5.90e-06, step=1181]Training:   1%|          | 1182/200000 [26:16<64:57:25,  1.18s/it, loss=0.0607, lr=5.90e-06, step=1181]Training:   1%|          | 1182/200000 [26:16<64:57:25,  1.18s/it, loss=0.1264, lr=5.91e-06, step=1182]Training:   1%|          | 1183/200000 [26:18<69:43:42,  1.26s/it, loss=0.1264, lr=5.91e-06, step=1182]Training:   1%|          | 1183/200000 [26:18<69:43:42,  1.26s/it, loss=0.0660, lr=5.91e-06, step=1183]Training:   1%|          | 1184/200000 [26:19<72:49:14,  1.32s/it, loss=0.0660, lr=5.91e-06, step=1183]Training:   1%|          | 1184/200000 [26:19<72:49:14,  1.32s/it, loss=0.0749, lr=5.92e-06, step=1184]Training:   1%|          | 1185/200000 [26:20<68:48:01,  1.25s/it, loss=0.0749, lr=5.92e-06, step=1184]Training:   1%|          | 1185/200000 [26:20<68:48:01,  1.25s/it, loss=0.1570, lr=5.92e-06, step=1185]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1186/200000 [26:21<65:59:53,  1.20s/it, loss=0.1570, lr=5.92e-06, step=1185]Training:   1%|          | 1186/200000 [26:21<65:59:53,  1.20s/it, loss=0.0507, lr=5.93e-06, step=1186]Training:   1%|          | 1187/200000 [26:23<68:14:12,  1.24s/it, loss=0.0507, lr=5.93e-06, step=1186]Training:   1%|          | 1187/200000 [26:23<68:14:12,  1.24s/it, loss=0.1366, lr=5.93e-06, step=1187]Training:   1%|          | 1188/200000 [26:24<71:01:25,  1.29s/it, loss=0.1366, lr=5.93e-06, step=1187]Training:   1%|          | 1188/200000 [26:24<71:01:25,  1.29s/it, loss=0.0510, lr=5.94e-06, step=1188]Training:   1%|          | 1189/200000 [26:25<67:30:46,  1.22s/it, loss=0.0510, lr=5.94e-06, step=1188]Training:   1%|          | 1189/200000 [26:25<67:30:46,  1.22s/it, loss=0.0415, lr=5.94e-06, step=1189]Training:   1%|          | 1190/200000 [26:27<70:53:42,  1.28s/it, loss=0.0415, lr=5.94e-06, step=1189]Training:   1%|          | 1190/200000 [26:27<70:53:42,  1.28s/it, loss=0.0597, lr=5.95e-06, step=1190]Training:   1%|          | 1191/200000 [26:28<67:28:28,  1.22s/it, loss=0.0597, lr=5.95e-06, step=1190]Training:   1%|          | 1191/200000 [26:28<67:28:28,  1.22s/it, loss=0.1297, lr=5.95e-06, step=1191]Training:   1%|          | 1192/200000 [26:29<70:24:12,  1.27s/it, loss=0.1297, lr=5.95e-06, step=1191]Training:   1%|          | 1192/200000 [26:29<70:24:12,  1.27s/it, loss=0.0950, lr=5.96e-06, step=1192]Training:   1%|          | 1193/200000 [26:30<71:26:20,  1.29s/it, loss=0.0950, lr=5.96e-06, step=1192]Training:   1%|          | 1193/200000 [26:30<71:26:20,  1.29s/it, loss=0.0788, lr=5.96e-06, step=1193]Training:   1%|          | 1194/200000 [26:32<74:52:09,  1.36s/it, loss=0.0788, lr=5.96e-06, step=1193]Training:   1%|          | 1194/200000 [26:32<74:52:09,  1.36s/it, loss=0.0826, lr=5.97e-06, step=1194]Training:   1%|          | 1195/200000 [26:33<77:44:16,  1.41s/it, loss=0.0826, lr=5.97e-06, step=1194]Training:   1%|          | 1195/200000 [26:33<77:44:16,  1.41s/it, loss=0.0784, lr=5.97e-06, step=1195]Training:   1%|          | 1196/200000 [26:34<72:13:32,  1.31s/it, loss=0.0784, lr=5.97e-06, step=1195]Training:   1%|          | 1196/200000 [26:34<72:13:32,  1.31s/it, loss=0.0464, lr=5.98e-06, step=1196]Training:   1%|          | 1197/200000 [26:35<68:23:55,  1.24s/it, loss=0.0464, lr=5.98e-06, step=1196]Training:   1%|          | 1197/200000 [26:35<68:23:55,  1.24s/it, loss=0.0587, lr=5.98e-06, step=1197]Training:   1%|          | 1198/200000 [26:37<71:49:33,  1.30s/it, loss=0.0587, lr=5.98e-06, step=1197]Training:   1%|          | 1198/200000 [26:37<71:49:33,  1.30s/it, loss=0.1013, lr=5.99e-06, step=1198]Training:   1%|          | 1199/200000 [26:38<68:06:09,  1.23s/it, loss=0.1013, lr=5.99e-06, step=1198]Training:   1%|          | 1199/200000 [26:38<68:06:09,  1.23s/it, loss=0.0578, lr=5.99e-06, step=1199]Training:   1%|          | 1200/200000 [26:39<69:47:34,  1.26s/it, loss=0.0578, lr=5.99e-06, step=1199]Training:   1%|          | 1200/200000 [26:39<69:47:34,  1.26s/it, loss=0.0583, lr=6.00e-06, step=1200]23:19:54.239 [I] step=1200 loss=0.0977 lr=5.76e-06 grad_norm=0.78 time=125.9s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1201/200000 [26:40<66:42:32,  1.21s/it, loss=0.0583, lr=6.00e-06, step=1200]Training:   1%|          | 1201/200000 [26:40<66:42:32,  1.21s/it, loss=0.0417, lr=6.00e-06, step=1201]Training:   1%|          | 1202/200000 [26:42<64:30:07,  1.17s/it, loss=0.0417, lr=6.00e-06, step=1201]Training:   1%|          | 1202/200000 [26:42<64:30:07,  1.17s/it, loss=0.1255, lr=6.01e-06, step=1202]Training:   1%|          | 1203/200000 [26:43<68:58:56,  1.25s/it, loss=0.1255, lr=6.01e-06, step=1202]Training:   1%|          | 1203/200000 [26:43<68:58:56,  1.25s/it, loss=0.0554, lr=6.01e-06, step=1203]Training:   1%|          | 1204/200000 [26:44<72:38:29,  1.32s/it, loss=0.0554, lr=6.01e-06, step=1203]Training:   1%|          | 1204/200000 [26:44<72:38:29,  1.32s/it, loss=0.0684, lr=6.02e-06, step=1204]Training:   1%|          | 1205/200000 [26:46<73:56:08,  1.34s/it, loss=0.0684, lr=6.02e-06, step=1204]Training:   1%|          | 1205/200000 [26:46<73:56:08,  1.34s/it, loss=0.1265, lr=6.02e-06, step=1205]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1206/200000 [26:47<69:34:33,  1.26s/it, loss=0.1265, lr=6.02e-06, step=1205]Training:   1%|          | 1206/200000 [26:47<69:34:33,  1.26s/it, loss=0.0731, lr=6.03e-06, step=1206]Training:   1%|          | 1207/200000 [26:48<66:32:25,  1.21s/it, loss=0.0731, lr=6.03e-06, step=1206]Training:   1%|          | 1207/200000 [26:48<66:32:25,  1.21s/it, loss=0.1047, lr=6.03e-06, step=1207]Training:   1%|          | 1208/200000 [26:49<68:58:35,  1.25s/it, loss=0.1047, lr=6.03e-06, step=1207]Training:   1%|          | 1208/200000 [26:49<68:58:35,  1.25s/it, loss=0.1234, lr=6.04e-06, step=1208]Training:   1%|          | 1209/200000 [26:51<70:49:23,  1.28s/it, loss=0.1234, lr=6.04e-06, step=1208]Training:   1%|          | 1209/200000 [26:51<70:49:23,  1.28s/it, loss=0.1019, lr=6.04e-06, step=1209]Training:   1%|          | 1210/200000 [26:52<67:24:34,  1.22s/it, loss=0.1019, lr=6.04e-06, step=1209]Training:   1%|          | 1210/200000 [26:52<67:24:34,  1.22s/it, loss=0.1269, lr=6.05e-06, step=1210]Training:   1%|          | 1211/200000 [26:53<70:18:22,  1.27s/it, loss=0.1269, lr=6.05e-06, step=1210]Training:   1%|          | 1211/200000 [26:53<70:18:22,  1.27s/it, loss=0.0435, lr=6.05e-06, step=1211]Training:   1%|          | 1212/200000 [26:54<67:02:58,  1.21s/it, loss=0.0435, lr=6.05e-06, step=1211]Training:   1%|          | 1212/200000 [26:54<67:02:58,  1.21s/it, loss=0.2207, lr=6.06e-06, step=1212]Training:   1%|          | 1213/200000 [26:56<68:53:38,  1.25s/it, loss=0.2207, lr=6.06e-06, step=1212]Training:   1%|          | 1213/200000 [26:56<68:53:38,  1.25s/it, loss=0.0827, lr=6.06e-06, step=1213]Training:   1%|          | 1214/200000 [26:57<70:10:19,  1.27s/it, loss=0.0827, lr=6.06e-06, step=1213]Training:   1%|          | 1214/200000 [26:57<70:10:19,  1.27s/it, loss=0.0719, lr=6.07e-06, step=1214]Training:   1%|          | 1215/200000 [26:58<73:46:58,  1.34s/it, loss=0.0719, lr=6.07e-06, step=1214]Training:   1%|          | 1215/200000 [26:58<73:46:58,  1.34s/it, loss=0.1340, lr=6.07e-06, step=1215]Training:   1%|          | 1216/200000 [27:00<76:27:48,  1.38s/it, loss=0.1340, lr=6.07e-06, step=1215]Training:   1%|          | 1216/200000 [27:00<76:27:48,  1.38s/it, loss=0.0886, lr=6.08e-06, step=1216]Training:   1%|          | 1217/200000 [27:01<71:22:32,  1.29s/it, loss=0.0886, lr=6.08e-06, step=1216]Training:   1%|          | 1217/200000 [27:01<71:22:32,  1.29s/it, loss=0.2811, lr=6.08e-06, step=1217]Training:   1%|          | 1218/200000 [27:02<67:48:05,  1.23s/it, loss=0.2811, lr=6.08e-06, step=1217]Training:   1%|          | 1218/200000 [27:02<67:48:05,  1.23s/it, loss=0.0774, lr=6.09e-06, step=1218]Training:   1%|          | 1219/200000 [27:03<70:45:48,  1.28s/it, loss=0.0774, lr=6.09e-06, step=1218]Training:   1%|          | 1219/200000 [27:03<70:45:48,  1.28s/it, loss=0.1511, lr=6.09e-06, step=1219]Training:   1%|          | 1220/200000 [27:04<67:21:20,  1.22s/it, loss=0.1511, lr=6.09e-06, step=1219]Training:   1%|          | 1220/200000 [27:04<67:21:20,  1.22s/it, loss=0.0823, lr=6.10e-06, step=1220]Training:   1%|          | 1221/200000 [27:06<68:46:46,  1.25s/it, loss=0.0823, lr=6.10e-06, step=1220]Training:   1%|          | 1221/200000 [27:06<68:46:46,  1.25s/it, loss=0.0726, lr=6.10e-06, step=1221]Training:   1%|          | 1222/200000 [27:07<65:57:41,  1.19s/it, loss=0.0726, lr=6.10e-06, step=1221]Training:   1%|          | 1222/200000 [27:07<65:57:41,  1.19s/it, loss=0.1003, lr=6.11e-06, step=1222]Training:   1%|          | 1223/200000 [27:08<64:00:59,  1.16s/it, loss=0.1003, lr=6.11e-06, step=1222]Training:   1%|          | 1223/200000 [27:08<64:00:59,  1.16s/it, loss=0.3645, lr=6.11e-06, step=1223]Training:   1%|          | 1224/200000 [27:09<68:27:18,  1.24s/it, loss=0.3645, lr=6.11e-06, step=1223]Training:   1%|          | 1224/200000 [27:09<68:27:18,  1.24s/it, loss=0.0864, lr=6.12e-06, step=1224]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1225/200000 [27:11<71:11:32,  1.29s/it, loss=0.0864, lr=6.12e-06, step=1224]Training:   1%|          | 1225/200000 [27:11<71:11:32,  1.29s/it, loss=0.1060, lr=6.12e-06, step=1225]Training:   1%|          | 1226/200000 [27:12<72:42:36,  1.32s/it, loss=0.1060, lr=6.12e-06, step=1225]Training:   1%|          | 1226/200000 [27:12<72:42:36,  1.32s/it, loss=0.1636, lr=6.13e-06, step=1226]Training:   1%|          | 1227/200000 [27:13<68:42:41,  1.24s/it, loss=0.1636, lr=6.13e-06, step=1226]Training:   1%|          | 1227/200000 [27:13<68:42:41,  1.24s/it, loss=0.1654, lr=6.13e-06, step=1227]Training:   1%|          | 1228/200000 [27:14<65:52:54,  1.19s/it, loss=0.1654, lr=6.13e-06, step=1227]Training:   1%|          | 1228/200000 [27:14<65:52:54,  1.19s/it, loss=0.0594, lr=6.14e-06, step=1228]Training:   1%|          | 1229/200000 [27:16<67:41:12,  1.23s/it, loss=0.0594, lr=6.14e-06, step=1228]Training:   1%|          | 1229/200000 [27:16<67:41:12,  1.23s/it, loss=0.0697, lr=6.14e-06, step=1229]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1230/200000 [27:17<68:57:57,  1.25s/it, loss=0.0697, lr=6.14e-06, step=1229]Training:   1%|          | 1230/200000 [27:17<68:57:57,  1.25s/it, loss=0.0827, lr=6.15e-06, step=1230]Training:   1%|          | 1231/200000 [27:18<66:06:20,  1.20s/it, loss=0.0827, lr=6.15e-06, step=1230]Training:   1%|          | 1231/200000 [27:18<66:06:20,  1.20s/it, loss=0.0823, lr=6.15e-06, step=1231]Training:   1%|          | 1232/200000 [27:19<68:13:50,  1.24s/it, loss=0.0823, lr=6.15e-06, step=1231]Training:   1%|          | 1232/200000 [27:19<68:13:50,  1.24s/it, loss=0.0511, lr=6.16e-06, step=1232]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1233/200000 [27:20<65:35:54,  1.19s/it, loss=0.0511, lr=6.16e-06, step=1232]Training:   1%|          | 1233/200000 [27:20<65:35:54,  1.19s/it, loss=0.0487, lr=6.16e-06, step=1233]Training:   1%|          | 1234/200000 [27:22<67:06:48,  1.22s/it, loss=0.0487, lr=6.16e-06, step=1233]Training:   1%|          | 1234/200000 [27:22<67:06:48,  1.22s/it, loss=0.0896, lr=6.17e-06, step=1234]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1235/200000 [27:23<64:47:40,  1.17s/it, loss=0.0896, lr=6.17e-06, step=1234]Training:   1%|          | 1235/200000 [27:23<64:47:40,  1.17s/it, loss=0.1301, lr=6.17e-06, step=1235]Training:   1%|          | 1236/200000 [27:24<69:31:10,  1.26s/it, loss=0.1301, lr=6.17e-06, step=1235]Training:   1%|          | 1236/200000 [27:24<69:31:10,  1.26s/it, loss=0.1136, lr=6.18e-06, step=1236]Training:   1%|          | 1237/200000 [27:26<72:38:16,  1.32s/it, loss=0.1136, lr=6.18e-06, step=1236]Training:   1%|          | 1237/200000 [27:26<72:38:16,  1.32s/it, loss=0.1733, lr=6.18e-06, step=1237]Training:   1%|          | 1238/200000 [27:27<68:42:33,  1.24s/it, loss=0.1733, lr=6.18e-06, step=1237]Training:   1%|          | 1238/200000 [27:27<68:42:33,  1.24s/it, loss=0.0886, lr=6.19e-06, step=1238]Training:   1%|          | 1239/200000 [27:28<65:56:46,  1.19s/it, loss=0.0886, lr=6.19e-06, step=1238]Training:   1%|          | 1239/200000 [27:28<65:56:46,  1.19s/it, loss=0.0474, lr=6.19e-06, step=1239]Training:   1%|          | 1240/200000 [27:29<68:23:10,  1.24s/it, loss=0.0474, lr=6.19e-06, step=1239]Training:   1%|          | 1240/200000 [27:29<68:23:10,  1.24s/it, loss=0.0690, lr=6.20e-06, step=1240]Training:   1%|          | 1241/200000 [27:31<71:36:23,  1.30s/it, loss=0.0690, lr=6.20e-06, step=1240]Training:   1%|          | 1241/200000 [27:31<71:36:23,  1.30s/it, loss=0.1018, lr=6.20e-06, step=1241]Training:   1%|          | 1242/200000 [27:32<67:56:21,  1.23s/it, loss=0.1018, lr=6.20e-06, step=1241]Training:   1%|          | 1242/200000 [27:32<67:56:21,  1.23s/it, loss=0.0810, lr=6.21e-06, step=1242]Training:   1%|          | 1243/200000 [27:33<71:18:35,  1.29s/it, loss=0.0810, lr=6.21e-06, step=1242]Training:   1%|          | 1243/200000 [27:33<71:18:35,  1.29s/it, loss=0.0473, lr=6.21e-06, step=1243]Training:   1%|          | 1244/200000 [27:34<67:44:36,  1.23s/it, loss=0.0473, lr=6.21e-06, step=1243]Training:   1%|          | 1244/200000 [27:34<67:44:36,  1.23s/it, loss=0.0566, lr=6.22e-06, step=1244]Training:   1%|          | 1245/200000 [27:36<70:01:36,  1.27s/it, loss=0.0566, lr=6.22e-06, step=1244]Training:   1%|          | 1245/200000 [27:36<70:01:36,  1.27s/it, loss=0.1369, lr=6.22e-06, step=1245]Training:   1%|          | 1246/200000 [27:37<70:59:01,  1.29s/it, loss=0.1369, lr=6.22e-06, step=1245]Training:   1%|          | 1246/200000 [27:37<70:59:01,  1.29s/it, loss=0.0557, lr=6.23e-06, step=1246]Training:   1%|          | 1247/200000 [27:38<74:35:08,  1.35s/it, loss=0.0557, lr=6.23e-06, step=1246]Training:   1%|          | 1247/200000 [27:38<74:35:08,  1.35s/it, loss=0.0655, lr=6.23e-06, step=1247]Training:   1%|          | 1248/200000 [27:40<77:30:15,  1.40s/it, loss=0.0655, lr=6.23e-06, step=1247]Training:   1%|          | 1248/200000 [27:40<77:30:15,  1.40s/it, loss=0.0563, lr=6.24e-06, step=1248]Training:   1%|          | 1249/200000 [27:41<72:05:22,  1.31s/it, loss=0.0563, lr=6.24e-06, step=1248]Training:   1%|          | 1249/200000 [27:41<72:05:22,  1.31s/it, loss=0.0654, lr=6.24e-06, step=1249]Training:   1%|          | 1250/200000 [27:42<68:15:16,  1.24s/it, loss=0.0654, lr=6.24e-06, step=1249]Training:   1%|          | 1250/200000 [27:42<68:15:16,  1.24s/it, loss=0.0960, lr=6.25e-06, step=1250]Training:   1%|          | 1251/200000 [27:43<71:15:56,  1.29s/it, loss=0.0960, lr=6.25e-06, step=1250]Training:   1%|          | 1251/200000 [27:43<71:15:56,  1.29s/it, loss=0.0662, lr=6.25e-06, step=1251]Training:   1%|          | 1252/200000 [27:45<67:42:06,  1.23s/it, loss=0.0662, lr=6.25e-06, step=1251]Training:   1%|          | 1252/200000 [27:45<67:42:06,  1.23s/it, loss=0.0703, lr=6.26e-06, step=1252]Training:   1%|          | 1253/200000 [27:46<69:15:14,  1.25s/it, loss=0.0703, lr=6.26e-06, step=1252]Training:   1%|          | 1253/200000 [27:46<69:15:14,  1.25s/it, loss=0.0610, lr=6.26e-06, step=1253]Training:   1%|          | 1254/200000 [27:47<66:18:01,  1.20s/it, loss=0.0610, lr=6.26e-06, step=1253]Training:   1%|          | 1254/200000 [27:47<66:18:01,  1.20s/it, loss=0.0701, lr=6.27e-06, step=1254]Training:   1%|          | 1255/200000 [27:48<64:14:51,  1.16s/it, loss=0.0701, lr=6.27e-06, step=1254]Training:   1%|          | 1255/200000 [27:48<64:14:51,  1.16s/it, loss=0.0649, lr=6.27e-06, step=1255]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1256/200000 [27:49<68:50:55,  1.25s/it, loss=0.0649, lr=6.27e-06, step=1255]Training:   1%|          | 1256/200000 [27:49<68:50:55,  1.25s/it, loss=0.1327, lr=6.28e-06, step=1256]Training:   1%|          | 1257/200000 [27:51<72:35:26,  1.31s/it, loss=0.1327, lr=6.28e-06, step=1256]Training:   1%|          | 1257/200000 [27:51<72:35:26,  1.31s/it, loss=0.1068, lr=6.28e-06, step=1257]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1258/200000 [27:52<74:34:43,  1.35s/it, loss=0.1068, lr=6.28e-06, step=1257]Training:   1%|          | 1258/200000 [27:52<74:34:43,  1.35s/it, loss=0.9096, lr=6.29e-06, step=1258]Training:   1%|          | 1259/200000 [27:53<70:02:45,  1.27s/it, loss=0.9096, lr=6.29e-06, step=1258]Training:   1%|          | 1259/200000 [27:53<70:02:45,  1.27s/it, loss=0.0653, lr=6.29e-06, step=1259]Training:   1%|          | 1260/200000 [27:55<66:50:49,  1.21s/it, loss=0.0653, lr=6.29e-06, step=1259]Training:   1%|          | 1260/200000 [27:55<66:50:49,  1.21s/it, loss=0.1051, lr=6.30e-06, step=1260]Training:   1%|          | 1261/200000 [27:56<69:13:34,  1.25s/it, loss=0.1051, lr=6.30e-06, step=1260]Training:   1%|          | 1261/200000 [27:56<69:13:34,  1.25s/it, loss=0.0857, lr=6.30e-06, step=1261]Training:   1%|          | 1262/200000 [27:57<71:04:53,  1.29s/it, loss=0.0857, lr=6.30e-06, step=1261]Training:   1%|          | 1262/200000 [27:57<71:04:53,  1.29s/it, loss=0.0612, lr=6.31e-06, step=1262]Training:   1%|          | 1263/200000 [27:58<67:36:56,  1.22s/it, loss=0.0612, lr=6.31e-06, step=1262]Training:   1%|          | 1263/200000 [27:58<67:36:56,  1.22s/it, loss=0.0660, lr=6.31e-06, step=1263]Training:   1%|          | 1264/200000 [28:00<70:20:42,  1.27s/it, loss=0.0660, lr=6.31e-06, step=1263]Training:   1%|          | 1264/200000 [28:00<70:20:42,  1.27s/it, loss=0.0590, lr=6.32e-06, step=1264]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1265/200000 [28:01<67:04:21,  1.21s/it, loss=0.0590, lr=6.32e-06, step=1264]Training:   1%|          | 1265/200000 [28:01<67:04:21,  1.21s/it, loss=0.1669, lr=6.32e-06, step=1265]Training:   1%|          | 1266/200000 [28:02<68:53:08,  1.25s/it, loss=0.1669, lr=6.32e-06, step=1265]Training:   1%|          | 1266/200000 [28:02<68:53:08,  1.25s/it, loss=0.0919, lr=6.33e-06, step=1266]Training:   1%|          | 1267/200000 [28:03<70:12:43,  1.27s/it, loss=0.0919, lr=6.33e-06, step=1266]Training:   1%|          | 1267/200000 [28:03<70:12:43,  1.27s/it, loss=0.0708, lr=6.33e-06, step=1267]Training:   1%|          | 1268/200000 [28:05<73:39:12,  1.33s/it, loss=0.0708, lr=6.33e-06, step=1267]Training:   1%|          | 1268/200000 [28:05<73:39:12,  1.33s/it, loss=0.1393, lr=6.34e-06, step=1268]Training:   1%|          | 1269/200000 [28:06<76:32:02,  1.39s/it, loss=0.1393, lr=6.34e-06, step=1268]Training:   1%|          | 1269/200000 [28:06<76:32:02,  1.39s/it, loss=0.0899, lr=6.34e-06, step=1269]Training:   1%|          | 1270/200000 [28:08<71:22:08,  1.29s/it, loss=0.0899, lr=6.34e-06, step=1269]Training:   1%|          | 1270/200000 [28:08<71:22:08,  1.29s/it, loss=0.0664, lr=6.35e-06, step=1270]Training:   1%|          | 1271/200000 [28:09<67:43:52,  1.23s/it, loss=0.0664, lr=6.35e-06, step=1270]Training:   1%|          | 1271/200000 [28:09<67:43:52,  1.23s/it, loss=0.0519, lr=6.35e-06, step=1271]Training:   1%|          | 1272/200000 [28:10<71:12:01,  1.29s/it, loss=0.0519, lr=6.35e-06, step=1271]Training:   1%|          | 1272/200000 [28:10<71:12:01,  1.29s/it, loss=0.0881, lr=6.36e-06, step=1272]Training:   1%|          | 1273/200000 [28:11<67:39:11,  1.23s/it, loss=0.0881, lr=6.36e-06, step=1272]Training:   1%|          | 1273/200000 [28:11<67:39:11,  1.23s/it, loss=0.0671, lr=6.36e-06, step=1273]Training:   1%|          | 1274/200000 [28:12<69:54:19,  1.27s/it, loss=0.0671, lr=6.36e-06, step=1273]Training:   1%|          | 1274/200000 [28:12<69:54:19,  1.27s/it, loss=0.0585, lr=6.37e-06, step=1274]Training:   1%|          | 1275/200000 [28:14<66:47:43,  1.21s/it, loss=0.0585, lr=6.37e-06, step=1274]Training:   1%|          | 1275/200000 [28:14<66:47:43,  1.21s/it, loss=0.0582, lr=6.37e-06, step=1275]Training:   1%|          | 1276/200000 [28:15<64:35:54,  1.17s/it, loss=0.0582, lr=6.37e-06, step=1275]Training:   1%|          | 1276/200000 [28:15<64:35:54,  1.17s/it, loss=0.0467, lr=6.38e-06, step=1276]Training:   1%|          | 1277/200000 [28:16<68:52:34,  1.25s/it, loss=0.0467, lr=6.38e-06, step=1276]Training:   1%|          | 1277/200000 [28:16<68:52:34,  1.25s/it, loss=0.0439, lr=6.38e-06, step=1277]Training:   1%|          | 1278/200000 [28:17<71:38:24,  1.30s/it, loss=0.0439, lr=6.38e-06, step=1277]Training:   1%|          | 1278/200000 [28:17<71:38:24,  1.30s/it, loss=0.0886, lr=6.39e-06, step=1278]Training:   1%|          | 1279/200000 [28:19<73:24:38,  1.33s/it, loss=0.0886, lr=6.39e-06, step=1278]Training:   1%|          | 1279/200000 [28:19<73:24:38,  1.33s/it, loss=0.0436, lr=6.39e-06, step=1279]Training:   1%|          | 1280/200000 [28:20<69:12:16,  1.25s/it, loss=0.0436, lr=6.39e-06, step=1279]Training:   1%|          | 1280/200000 [28:20<69:12:16,  1.25s/it, loss=0.1485, lr=6.40e-06, step=1280]Training:   1%|          | 1281/200000 [28:21<66:16:57,  1.20s/it, loss=0.1485, lr=6.40e-06, step=1280]Training:   1%|          | 1281/200000 [28:21<66:16:57,  1.20s/it, loss=0.0370, lr=6.40e-06, step=1281]Training:   1%|          | 1282/200000 [28:22<68:50:01,  1.25s/it, loss=0.0370, lr=6.40e-06, step=1281]Training:   1%|          | 1282/200000 [28:22<68:50:01,  1.25s/it, loss=0.0778, lr=6.41e-06, step=1282]Training:   1%|          | 1283/200000 [28:24<70:01:54,  1.27s/it, loss=0.0778, lr=6.41e-06, step=1282]Training:   1%|          | 1283/200000 [28:24<70:01:54,  1.27s/it, loss=0.0611, lr=6.41e-06, step=1283]Training:   1%|          | 1284/200000 [28:25<66:49:44,  1.21s/it, loss=0.0611, lr=6.41e-06, step=1283]Training:   1%|          | 1284/200000 [28:25<66:49:44,  1.21s/it, loss=0.1503, lr=6.42e-06, step=1284]Training:   1%|          | 1285/200000 [28:26<68:57:25,  1.25s/it, loss=0.1503, lr=6.42e-06, step=1284]Training:   1%|          | 1285/200000 [28:26<68:57:25,  1.25s/it, loss=0.0657, lr=6.42e-06, step=1285]Training:   1%|          | 1286/200000 [28:27<66:03:54,  1.20s/it, loss=0.0657, lr=6.42e-06, step=1285]Training:   1%|          | 1286/200000 [28:27<66:03:54,  1.20s/it, loss=0.1196, lr=6.43e-06, step=1286]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1287/200000 [28:28<68:04:33,  1.23s/it, loss=0.1196, lr=6.43e-06, step=1286]Training:   1%|          | 1287/200000 [28:28<68:04:33,  1.23s/it, loss=0.0730, lr=6.43e-06, step=1287]Training:   1%|          | 1288/200000 [28:30<65:27:15,  1.19s/it, loss=0.0730, lr=6.43e-06, step=1287]Training:   1%|          | 1288/200000 [28:30<65:27:15,  1.19s/it, loss=0.0872, lr=6.44e-06, step=1288]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1289/200000 [28:31<70:05:19,  1.27s/it, loss=0.0872, lr=6.44e-06, step=1288]Training:   1%|          | 1289/200000 [28:31<70:05:19,  1.27s/it, loss=0.0717, lr=6.44e-06, step=1289]Training:   1%|          | 1290/200000 [28:32<72:56:42,  1.32s/it, loss=0.0717, lr=6.44e-06, step=1289]Training:   1%|          | 1290/200000 [28:32<72:56:42,  1.32s/it, loss=0.0507, lr=6.45e-06, step=1290]Training:   1%|          | 1291/200000 [28:34<68:53:41,  1.25s/it, loss=0.0507, lr=6.45e-06, step=1290]Training:   1%|          | 1291/200000 [28:34<68:53:41,  1.25s/it, loss=0.1373, lr=6.45e-06, step=1291]Training:   1%|          | 1292/200000 [28:35<66:03:09,  1.20s/it, loss=0.1373, lr=6.45e-06, step=1291]Training:   1%|          | 1292/200000 [28:35<66:03:09,  1.20s/it, loss=0.0819, lr=6.46e-06, step=1292]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1293/200000 [28:36<69:34:02,  1.26s/it, loss=0.0819, lr=6.46e-06, step=1292]Training:   1%|          | 1293/200000 [28:36<69:34:02,  1.26s/it, loss=0.0560, lr=6.46e-06, step=1293]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1294/200000 [28:37<71:50:20,  1.30s/it, loss=0.0560, lr=6.46e-06, step=1293]Training:   1%|          | 1294/200000 [28:37<71:50:20,  1.30s/it, loss=0.1318, lr=6.47e-06, step=1294]Training:   1%|          | 1295/200000 [28:39<68:04:43,  1.23s/it, loss=0.1318, lr=6.47e-06, step=1294]Training:   1%|          | 1295/200000 [28:39<68:04:43,  1.23s/it, loss=0.0671, lr=6.47e-06, step=1295]Training:   1%|          | 1296/200000 [28:40<70:52:57,  1.28s/it, loss=0.0671, lr=6.47e-06, step=1295]Training:   1%|          | 1296/200000 [28:40<70:52:57,  1.28s/it, loss=0.0505, lr=6.48e-06, step=1296]Training:   1%|          | 1297/200000 [28:41<67:26:48,  1.22s/it, loss=0.0505, lr=6.48e-06, step=1296]Training:   1%|          | 1297/200000 [28:41<67:26:48,  1.22s/it, loss=0.0535, lr=6.48e-06, step=1297]Training:   1%|          | 1298/200000 [28:42<69:55:14,  1.27s/it, loss=0.0535, lr=6.48e-06, step=1297]Training:   1%|          | 1298/200000 [28:42<69:55:14,  1.27s/it, loss=0.0525, lr=6.49e-06, step=1298]Training:   1%|          | 1299/200000 [28:44<70:57:20,  1.29s/it, loss=0.0525, lr=6.49e-06, step=1298]Training:   1%|          | 1299/200000 [28:44<70:57:20,  1.29s/it, loss=0.0601, lr=6.49e-06, step=1299]Training:   1%|          | 1300/200000 [28:45<74:34:17,  1.35s/it, loss=0.0601, lr=6.49e-06, step=1299]Training:   1%|          | 1300/200000 [28:45<74:34:17,  1.35s/it, loss=0.0771, lr=6.50e-06, step=1300]23:22:00.529 [I] step=1300 loss=0.0996 lr=6.26e-06 grad_norm=0.82 time=126.3s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1301/200000 [28:47<77:33:01,  1.41s/it, loss=0.0771, lr=6.50e-06, step=1300]Training:   1%|          | 1301/200000 [28:47<77:33:01,  1.41s/it, loss=0.0920, lr=6.50e-06, step=1301]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1302/200000 [28:48<72:05:39,  1.31s/it, loss=0.0920, lr=6.50e-06, step=1301]Training:   1%|          | 1302/200000 [28:48<72:05:39,  1.31s/it, loss=0.1085, lr=6.51e-06, step=1302]Training:   1%|          | 1303/200000 [28:49<68:15:59,  1.24s/it, loss=0.1085, lr=6.51e-06, step=1302]Training:   1%|          | 1303/200000 [28:49<68:15:59,  1.24s/it, loss=0.0520, lr=6.51e-06, step=1303]Training:   1%|          | 1304/200000 [28:50<72:16:27,  1.31s/it, loss=0.0520, lr=6.51e-06, step=1303]Training:   1%|          | 1304/200000 [28:50<72:16:27,  1.31s/it, loss=0.0509, lr=6.52e-06, step=1304]Training:   1%|          | 1305/200000 [28:51<68:24:16,  1.24s/it, loss=0.0509, lr=6.52e-06, step=1304]Training:   1%|          | 1305/200000 [28:51<68:24:16,  1.24s/it, loss=0.0553, lr=6.52e-06, step=1305]Training:   1%|          | 1306/200000 [28:53<70:13:04,  1.27s/it, loss=0.0553, lr=6.52e-06, step=1305]Training:   1%|          | 1306/200000 [28:53<70:13:04,  1.27s/it, loss=0.0829, lr=6.53e-06, step=1306]Training:   1%|          | 1307/200000 [28:54<66:59:18,  1.21s/it, loss=0.0829, lr=6.53e-06, step=1306]Training:   1%|          | 1307/200000 [28:54<66:59:18,  1.21s/it, loss=0.0665, lr=6.53e-06, step=1307]Training:   1%|          | 1308/200000 [28:55<64:41:55,  1.17s/it, loss=0.0665, lr=6.53e-06, step=1307]Training:   1%|          | 1308/200000 [28:55<64:41:55,  1.17s/it, loss=0.7751, lr=6.54e-06, step=1308]Training:   1%|          | 1309/200000 [28:56<69:46:13,  1.26s/it, loss=0.7751, lr=6.54e-06, step=1308]Training:   1%|          | 1309/200000 [28:56<69:46:13,  1.26s/it, loss=0.0577, lr=6.54e-06, step=1309]Training:   1%|          | 1310/200000 [28:58<73:00:55,  1.32s/it, loss=0.0577, lr=6.54e-06, step=1309]Training:   1%|          | 1310/200000 [28:58<73:00:55,  1.32s/it, loss=0.0630, lr=6.55e-06, step=1310]Training:   1%|          | 1311/200000 [28:59<74:58:27,  1.36s/it, loss=0.0630, lr=6.55e-06, step=1310]Training:   1%|          | 1311/200000 [28:59<74:58:27,  1.36s/it, loss=0.1726, lr=6.55e-06, step=1311]Training:   1%|          | 1312/200000 [29:00<70:17:41,  1.27s/it, loss=0.1726, lr=6.55e-06, step=1311]Training:   1%|          | 1312/200000 [29:00<70:17:41,  1.27s/it, loss=0.0650, lr=6.56e-06, step=1312]Training:   1%|          | 1313/200000 [29:01<67:03:38,  1.22s/it, loss=0.0650, lr=6.56e-06, step=1312]Training:   1%|          | 1313/200000 [29:01<67:03:38,  1.22s/it, loss=0.0642, lr=6.56e-06, step=1313]Training:   1%|          | 1314/200000 [29:03<69:18:49,  1.26s/it, loss=0.0642, lr=6.56e-06, step=1313]Training:   1%|          | 1314/200000 [29:03<69:18:49,  1.26s/it, loss=0.0758, lr=6.57e-06, step=1314]Training:   1%|          | 1315/200000 [29:04<71:48:48,  1.30s/it, loss=0.0758, lr=6.57e-06, step=1314]Training:   1%|          | 1315/200000 [29:04<71:48:48,  1.30s/it, loss=0.0586, lr=6.57e-06, step=1315]Training:   1%|          | 1316/200000 [29:05<68:04:52,  1.23s/it, loss=0.0586, lr=6.57e-06, step=1315]Training:   1%|          | 1316/200000 [29:05<68:04:52,  1.23s/it, loss=0.1436, lr=6.58e-06, step=1316]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1317/200000 [29:07<70:41:58,  1.28s/it, loss=0.1436, lr=6.58e-06, step=1316]Training:   1%|          | 1317/200000 [29:07<70:41:58,  1.28s/it, loss=0.0546, lr=6.58e-06, step=1317]Training:   1%|          | 1318/200000 [29:08<67:16:43,  1.22s/it, loss=0.0546, lr=6.58e-06, step=1317]Training:   1%|          | 1318/200000 [29:08<67:16:43,  1.22s/it, loss=0.1127, lr=6.59e-06, step=1318]Training:   1%|          | 1319/200000 [29:09<68:51:08,  1.25s/it, loss=0.1127, lr=6.59e-06, step=1318]Training:   1%|          | 1319/200000 [29:09<68:51:08,  1.25s/it, loss=0.0616, lr=6.59e-06, step=1319]Training:   1%|          | 1320/200000 [29:10<70:09:53,  1.27s/it, loss=0.0616, lr=6.59e-06, step=1319]Training:   1%|          | 1320/200000 [29:10<70:09:53,  1.27s/it, loss=0.0654, lr=6.60e-06, step=1320]Training:   1%|          | 1321/200000 [29:12<73:34:00,  1.33s/it, loss=0.0654, lr=6.60e-06, step=1320]Training:   1%|          | 1321/200000 [29:12<73:34:00,  1.33s/it, loss=0.0750, lr=6.60e-06, step=1321]Training:   1%|          | 1322/200000 [29:13<76:26:50,  1.39s/it, loss=0.0750, lr=6.60e-06, step=1321]Training:   1%|          | 1322/200000 [29:13<76:26:50,  1.39s/it, loss=0.0517, lr=6.61e-06, step=1322]Training:   1%|          | 1323/200000 [29:14<71:21:46,  1.29s/it, loss=0.0517, lr=6.61e-06, step=1322]Training:   1%|          | 1323/200000 [29:14<71:21:46,  1.29s/it, loss=0.0345, lr=6.61e-06, step=1323]Training:   1%|          | 1324/200000 [29:16<67:48:10,  1.23s/it, loss=0.0345, lr=6.61e-06, step=1323]Training:   1%|          | 1324/200000 [29:16<67:48:10,  1.23s/it, loss=0.0985, lr=6.62e-06, step=1324]Training:   1%|          | 1325/200000 [29:17<71:43:40,  1.30s/it, loss=0.0985, lr=6.62e-06, step=1324]Training:   1%|          | 1325/200000 [29:17<71:43:40,  1.30s/it, loss=0.0817, lr=6.62e-06, step=1325]Training:   1%|          | 1326/200000 [29:18<68:00:10,  1.23s/it, loss=0.0817, lr=6.62e-06, step=1325]Training:   1%|          | 1326/200000 [29:18<68:00:10,  1.23s/it, loss=0.0727, lr=6.63e-06, step=1326]Training:   1%|          | 1327/200000 [29:19<69:31:06,  1.26s/it, loss=0.0727, lr=6.63e-06, step=1326]Training:   1%|          | 1327/200000 [29:19<69:31:06,  1.26s/it, loss=0.0465, lr=6.63e-06, step=1327]Training:   1%|          | 1328/200000 [29:20<66:27:55,  1.20s/it, loss=0.0465, lr=6.63e-06, step=1327]Training:   1%|          | 1328/200000 [29:20<66:27:55,  1.20s/it, loss=0.0435, lr=6.64e-06, step=1328]Training:   1%|          | 1329/200000 [29:22<64:20:20,  1.17s/it, loss=0.0435, lr=6.64e-06, step=1328]Training:   1%|          | 1329/200000 [29:22<64:20:20,  1.17s/it, loss=0.0666, lr=6.64e-06, step=1329]Training:   1%|          | 1330/200000 [29:23<69:27:58,  1.26s/it, loss=0.0666, lr=6.64e-06, step=1329]Training:   1%|          | 1330/200000 [29:23<69:27:58,  1.26s/it, loss=0.0992, lr=6.65e-06, step=1330]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1331/200000 [29:24<72:53:01,  1.32s/it, loss=0.0992, lr=6.65e-06, step=1330]Training:   1%|          | 1331/200000 [29:24<72:53:01,  1.32s/it, loss=0.0818, lr=6.65e-06, step=1331]Training:   1%|          | 1332/200000 [29:26<73:38:10,  1.33s/it, loss=0.0818, lr=6.65e-06, step=1331]Training:   1%|          | 1332/200000 [29:26<73:38:10,  1.33s/it, loss=0.0411, lr=6.66e-06, step=1332]Training:   1%|          | 1333/200000 [29:27<69:24:23,  1.26s/it, loss=0.0411, lr=6.66e-06, step=1332]Training:   1%|          | 1333/200000 [29:27<69:24:23,  1.26s/it, loss=0.1769, lr=6.66e-06, step=1333]Training:   1%|          | 1334/200000 [29:28<66:23:27,  1.20s/it, loss=0.1769, lr=6.66e-06, step=1333]Training:   1%|          | 1334/200000 [29:28<66:23:27,  1.20s/it, loss=0.0664, lr=6.67e-06, step=1334]Training:   1%|          | 1335/200000 [29:29<68:52:41,  1.25s/it, loss=0.0664, lr=6.67e-06, step=1334]Training:   1%|          | 1335/200000 [29:29<68:52:41,  1.25s/it, loss=0.0924, lr=6.67e-06, step=1335]Training:   1%|          | 1336/200000 [29:31<69:51:58,  1.27s/it, loss=0.0924, lr=6.67e-06, step=1335]Training:   1%|          | 1336/200000 [29:31<69:51:58,  1.27s/it, loss=0.0645, lr=6.68e-06, step=1336]Training:   1%|          | 1337/200000 [29:32<66:43:38,  1.21s/it, loss=0.0645, lr=6.68e-06, step=1336]Training:   1%|          | 1337/200000 [29:32<66:43:38,  1.21s/it, loss=0.0499, lr=6.68e-06, step=1337]Training:   1%|          | 1338/200000 [29:33<68:59:25,  1.25s/it, loss=0.0499, lr=6.68e-06, step=1337]Training:   1%|          | 1338/200000 [29:33<68:59:25,  1.25s/it, loss=0.0977, lr=6.69e-06, step=1338]Training:   1%|          | 1339/200000 [29:34<66:07:57,  1.20s/it, loss=0.0977, lr=6.69e-06, step=1338]Training:   1%|          | 1339/200000 [29:34<66:07:57,  1.20s/it, loss=0.0855, lr=6.69e-06, step=1339]Training:   1%|          | 1340/200000 [29:35<67:45:44,  1.23s/it, loss=0.0855, lr=6.69e-06, step=1339]Training:   1%|          | 1340/200000 [29:35<67:45:44,  1.23s/it, loss=0.0369, lr=6.70e-06, step=1340]Training:   1%|          | 1341/200000 [29:37<65:14:43,  1.18s/it, loss=0.0369, lr=6.70e-06, step=1340]Training:   1%|          | 1341/200000 [29:37<65:14:43,  1.18s/it, loss=0.0293, lr=6.70e-06, step=1341]Training:   1%|          | 1342/200000 [29:38<69:53:44,  1.27s/it, loss=0.0293, lr=6.70e-06, step=1341]Training:   1%|          | 1342/200000 [29:38<69:53:44,  1.27s/it, loss=0.1125, lr=6.71e-06, step=1342]Training:   1%|          | 1343/200000 [29:39<72:54:47,  1.32s/it, loss=0.1125, lr=6.71e-06, step=1342]Training:   1%|          | 1343/200000 [29:39<72:54:47,  1.32s/it, loss=0.0635, lr=6.71e-06, step=1343]Training:   1%|          | 1344/200000 [29:41<68:51:20,  1.25s/it, loss=0.0635, lr=6.71e-06, step=1343]Training:   1%|          | 1344/200000 [29:41<68:51:20,  1.25s/it, loss=0.0597, lr=6.72e-06, step=1344]Training:   1%|          | 1345/200000 [29:42<66:01:06,  1.20s/it, loss=0.0597, lr=6.72e-06, step=1344]Training:   1%|          | 1345/200000 [29:42<66:01:06,  1.20s/it, loss=0.1400, lr=6.72e-06, step=1345]Training:   1%|          | 1346/200000 [29:43<68:40:37,  1.24s/it, loss=0.1400, lr=6.72e-06, step=1345]Training:   1%|          | 1346/200000 [29:43<68:40:37,  1.24s/it, loss=0.0822, lr=6.73e-06, step=1346]Training:   1%|          | 1347/200000 [29:44<71:22:34,  1.29s/it, loss=0.0822, lr=6.73e-06, step=1346]Training:   1%|          | 1347/200000 [29:44<71:22:34,  1.29s/it, loss=0.0469, lr=6.73e-06, step=1347]Training:   1%|          | 1348/200000 [29:45<67:45:10,  1.23s/it, loss=0.0469, lr=6.73e-06, step=1347]Training:   1%|          | 1348/200000 [29:45<67:45:10,  1.23s/it, loss=0.0887, lr=6.74e-06, step=1348]Training:   1%|          | 1349/200000 [29:47<70:36:52,  1.28s/it, loss=0.0887, lr=6.74e-06, step=1348]Training:   1%|          | 1349/200000 [29:47<70:36:52,  1.28s/it, loss=0.0373, lr=6.74e-06, step=1349]Training:   1%|          | 1350/200000 [29:48<67:14:49,  1.22s/it, loss=0.0373, lr=6.74e-06, step=1349]Training:   1%|          | 1350/200000 [29:48<67:14:49,  1.22s/it, loss=0.0977, lr=6.75e-06, step=1350]Training:   1%|          | 1351/200000 [29:49<69:47:52,  1.26s/it, loss=0.0977, lr=6.75e-06, step=1350]Training:   1%|          | 1351/200000 [29:49<69:47:52,  1.26s/it, loss=0.0456, lr=6.75e-06, step=1351]Training:   1%|          | 1352/200000 [29:51<70:48:28,  1.28s/it, loss=0.0456, lr=6.75e-06, step=1351]Training:   1%|          | 1352/200000 [29:51<70:48:28,  1.28s/it, loss=0.3651, lr=6.76e-06, step=1352]Training:   1%|          | 1353/200000 [29:52<74:24:16,  1.35s/it, loss=0.3651, lr=6.76e-06, step=1352]Training:   1%|          | 1353/200000 [29:52<74:24:16,  1.35s/it, loss=0.1763, lr=6.76e-06, step=1353]Training:   1%|          | 1354/200000 [29:54<77:22:42,  1.40s/it, loss=0.1763, lr=6.76e-06, step=1353]Training:   1%|          | 1354/200000 [29:54<77:22:42,  1.40s/it, loss=0.0465, lr=6.77e-06, step=1354]Training:   1%|          | 1355/200000 [29:55<71:59:52,  1.30s/it, loss=0.0465, lr=6.77e-06, step=1354]Training:   1%|          | 1355/200000 [29:55<71:59:52,  1.30s/it, loss=0.1861, lr=6.77e-06, step=1355]Training:   1%|          | 1356/200000 [29:56<68:11:29,  1.24s/it, loss=0.1861, lr=6.77e-06, step=1355]Training:   1%|          | 1356/200000 [29:56<68:11:29,  1.24s/it, loss=0.0773, lr=6.78e-06, step=1356]Training:   1%|          | 1357/200000 [29:57<72:14:51,  1.31s/it, loss=0.0773, lr=6.78e-06, step=1356]Training:   1%|          | 1357/200000 [29:57<72:14:51,  1.31s/it, loss=0.0384, lr=6.78e-06, step=1357]Training:   1%|          | 1358/200000 [29:58<68:23:37,  1.24s/it, loss=0.0384, lr=6.78e-06, step=1357]Training:   1%|          | 1358/200000 [29:58<68:23:37,  1.24s/it, loss=0.0746, lr=6.79e-06, step=1358]Training:   1%|          | 1359/200000 [30:00<69:58:40,  1.27s/it, loss=0.0746, lr=6.79e-06, step=1358]Training:   1%|          | 1359/200000 [30:00<69:58:40,  1.27s/it, loss=0.0763, lr=6.79e-06, step=1359]Training:   1%|          | 1360/200000 [30:01<66:45:26,  1.21s/it, loss=0.0763, lr=6.79e-06, step=1359]Training:   1%|          | 1360/200000 [30:01<66:45:26,  1.21s/it, loss=0.0539, lr=6.80e-06, step=1360]Training:   1%|          | 1361/200000 [30:02<64:33:34,  1.17s/it, loss=0.0539, lr=6.80e-06, step=1360]Training:   1%|          | 1361/200000 [30:02<64:33:34,  1.17s/it, loss=0.1247, lr=6.80e-06, step=1361]Training:   1%|          | 1362/200000 [30:03<69:09:13,  1.25s/it, loss=0.1247, lr=6.80e-06, step=1361]Training:   1%|          | 1362/200000 [30:03<69:09:13,  1.25s/it, loss=0.0911, lr=6.81e-06, step=1362]Training:   1%|          | 1363/200000 [30:05<72:46:02,  1.32s/it, loss=0.0911, lr=6.81e-06, step=1362]Training:   1%|          | 1363/200000 [30:05<72:46:02,  1.32s/it, loss=0.0785, lr=6.81e-06, step=1363]Training:   1%|          | 1364/200000 [30:06<74:48:05,  1.36s/it, loss=0.0785, lr=6.81e-06, step=1363]Training:   1%|          | 1364/200000 [30:06<74:48:05,  1.36s/it, loss=0.0861, lr=6.82e-06, step=1364]Training:   1%|          | 1365/200000 [30:07<70:12:31,  1.27s/it, loss=0.0861, lr=6.82e-06, step=1364]Training:   1%|          | 1365/200000 [30:07<70:12:31,  1.27s/it, loss=0.0570, lr=6.82e-06, step=1365]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1366/200000 [30:08<66:56:33,  1.21s/it, loss=0.0570, lr=6.82e-06, step=1365]Training:   1%|          | 1366/200000 [30:08<66:56:33,  1.21s/it, loss=0.0494, lr=6.83e-06, step=1366]Training:   1%|          | 1367/200000 [30:10<69:22:20,  1.26s/it, loss=0.0494, lr=6.83e-06, step=1366]Training:   1%|          | 1367/200000 [30:10<69:22:20,  1.26s/it, loss=0.0523, lr=6.83e-06, step=1367]Training:   1%|          | 1368/200000 [30:11<72:01:43,  1.31s/it, loss=0.0523, lr=6.83e-06, step=1367]Training:   1%|          | 1368/200000 [30:11<72:01:43,  1.31s/it, loss=0.0709, lr=6.84e-06, step=1368]Training:   1%|          | 1369/200000 [30:12<68:13:27,  1.24s/it, loss=0.0709, lr=6.84e-06, step=1368]Training:   1%|          | 1369/200000 [30:12<68:13:27,  1.24s/it, loss=0.0475, lr=6.84e-06, step=1369]Training:   1%|          | 1370/200000 [30:14<71:08:41,  1.29s/it, loss=0.0475, lr=6.84e-06, step=1369]Training:   1%|          | 1370/200000 [30:14<71:08:41,  1.29s/it, loss=0.0714, lr=6.85e-06, step=1370]Training:   1%|          | 1371/200000 [30:15<67:35:32,  1.23s/it, loss=0.0714, lr=6.85e-06, step=1370]Training:   1%|          | 1371/200000 [30:15<67:35:32,  1.23s/it, loss=0.0430, lr=6.85e-06, step=1371]Training:   1%|          | 1372/200000 [30:16<68:53:23,  1.25s/it, loss=0.0430, lr=6.85e-06, step=1371]Training:   1%|          | 1372/200000 [30:16<68:53:23,  1.25s/it, loss=0.1346, lr=6.86e-06, step=1372]Training:   1%|          | 1373/200000 [30:17<70:10:24,  1.27s/it, loss=0.1346, lr=6.86e-06, step=1372]Training:   1%|          | 1373/200000 [30:17<70:10:24,  1.27s/it, loss=0.0459, lr=6.86e-06, step=1373]Training:   1%|          | 1374/200000 [30:19<73:42:01,  1.34s/it, loss=0.0459, lr=6.86e-06, step=1373]Training:   1%|          | 1374/200000 [30:19<73:42:01,  1.34s/it, loss=0.0519, lr=6.87e-06, step=1374]Training:   1%|          | 1375/200000 [30:20<76:33:27,  1.39s/it, loss=0.0519, lr=6.87e-06, step=1374]Training:   1%|          | 1375/200000 [30:20<76:33:27,  1.39s/it, loss=0.1239, lr=6.87e-06, step=1375]Training:   1%|          | 1376/200000 [30:21<71:23:01,  1.29s/it, loss=0.1239, lr=6.87e-06, step=1375]Training:   1%|          | 1376/200000 [30:21<71:23:01,  1.29s/it, loss=0.0473, lr=6.88e-06, step=1376]Training:   1%|          | 1377/200000 [30:22<67:47:13,  1.23s/it, loss=0.0473, lr=6.88e-06, step=1376]Training:   1%|          | 1377/200000 [30:22<67:47:13,  1.23s/it, loss=0.0456, lr=6.88e-06, step=1377]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1378/200000 [30:24<71:44:05,  1.30s/it, loss=0.0456, lr=6.88e-06, step=1377]Training:   1%|          | 1378/200000 [30:24<71:44:05,  1.30s/it, loss=0.0578, lr=6.89e-06, step=1378]Training:   1%|          | 1379/200000 [30:25<68:02:42,  1.23s/it, loss=0.0578, lr=6.89e-06, step=1378]Training:   1%|          | 1379/200000 [30:25<68:02:42,  1.23s/it, loss=0.0706, lr=6.89e-06, step=1379]Training:   1%|          | 1380/200000 [30:26<69:03:20,  1.25s/it, loss=0.0706, lr=6.89e-06, step=1379]Training:   1%|          | 1380/200000 [30:26<69:03:20,  1.25s/it, loss=0.1816, lr=6.90e-06, step=1380]Training:   1%|          | 1381/200000 [30:27<66:07:54,  1.20s/it, loss=0.1816, lr=6.90e-06, step=1380]Training:   1%|          | 1381/200000 [30:27<66:07:54,  1.20s/it, loss=0.0842, lr=6.90e-06, step=1381]Training:   1%|          | 1382/200000 [30:28<64:08:15,  1.16s/it, loss=0.0842, lr=6.90e-06, step=1381]Training:   1%|          | 1382/200000 [30:28<64:08:15,  1.16s/it, loss=0.0788, lr=6.91e-06, step=1382]Training:   1%|          | 1383/200000 [30:30<69:14:02,  1.25s/it, loss=0.0788, lr=6.91e-06, step=1382]Training:   1%|          | 1383/200000 [30:30<69:14:02,  1.25s/it, loss=0.1437, lr=6.91e-06, step=1383]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1384/200000 [30:31<72:40:44,  1.32s/it, loss=0.1437, lr=6.91e-06, step=1383]Training:   1%|          | 1384/200000 [30:31<72:40:44,  1.32s/it, loss=0.0844, lr=6.92e-06, step=1384]Training:   1%|          | 1385/200000 [30:33<74:24:29,  1.35s/it, loss=0.0844, lr=6.92e-06, step=1384]Training:   1%|          | 1385/200000 [30:33<74:24:29,  1.35s/it, loss=0.0503, lr=6.92e-06, step=1385]Training:   1%|          | 1386/200000 [30:34<69:53:55,  1.27s/it, loss=0.0503, lr=6.92e-06, step=1385]Training:   1%|          | 1386/200000 [30:34<69:53:55,  1.27s/it, loss=0.0776, lr=6.93e-06, step=1386]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1387/200000 [30:35<66:44:50,  1.21s/it, loss=0.0776, lr=6.93e-06, step=1386]Training:   1%|          | 1387/200000 [30:35<66:44:50,  1.21s/it, loss=0.0593, lr=6.93e-06, step=1387]Training:   1%|          | 1388/200000 [30:36<68:44:03,  1.25s/it, loss=0.0593, lr=6.93e-06, step=1387]Training:   1%|          | 1388/200000 [30:36<68:44:03,  1.25s/it, loss=0.0597, lr=6.94e-06, step=1388]Training:   1%|          | 1389/200000 [30:38<69:55:29,  1.27s/it, loss=0.0597, lr=6.94e-06, step=1388]Training:   1%|          | 1389/200000 [30:38<69:55:29,  1.27s/it, loss=0.0980, lr=6.94e-06, step=1389]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1390/200000 [30:39<66:45:24,  1.21s/it, loss=0.0980, lr=6.94e-06, step=1389]Training:   1%|          | 1390/200000 [30:39<66:45:24,  1.21s/it, loss=0.0341, lr=6.95e-06, step=1390]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1391/200000 [30:40<69:07:48,  1.25s/it, loss=0.0341, lr=6.95e-06, step=1390]Training:   1%|          | 1391/200000 [30:40<69:07:48,  1.25s/it, loss=0.0565, lr=6.95e-06, step=1391]Training:   1%|          | 1392/200000 [30:41<66:13:07,  1.20s/it, loss=0.0565, lr=6.95e-06, step=1391]Training:   1%|          | 1392/200000 [30:41<66:13:07,  1.20s/it, loss=0.0801, lr=6.96e-06, step=1392]Training:   1%|          | 1393/200000 [30:42<68:07:33,  1.23s/it, loss=0.0801, lr=6.96e-06, step=1392]Training:   1%|          | 1393/200000 [30:42<68:07:33,  1.23s/it, loss=0.0658, lr=6.96e-06, step=1393]Training:   1%|          | 1394/200000 [30:44<65:29:02,  1.19s/it, loss=0.0658, lr=6.96e-06, step=1393]Training:   1%|          | 1394/200000 [30:44<65:29:02,  1.19s/it, loss=0.0566, lr=6.97e-06, step=1394]Training:   1%|          | 1395/200000 [30:45<70:02:46,  1.27s/it, loss=0.0566, lr=6.97e-06, step=1394]Training:   1%|          | 1395/200000 [30:45<70:02:46,  1.27s/it, loss=0.1320, lr=6.97e-06, step=1395]Training:   1%|          | 1396/200000 [30:46<73:00:48,  1.32s/it, loss=0.1320, lr=6.97e-06, step=1395]Training:   1%|          | 1396/200000 [30:46<73:00:48,  1.32s/it, loss=0.0671, lr=6.98e-06, step=1396]Training:   1%|          | 1397/200000 [30:48<68:58:13,  1.25s/it, loss=0.0671, lr=6.98e-06, step=1396]Training:   1%|          | 1397/200000 [30:48<68:58:13,  1.25s/it, loss=0.1748, lr=6.98e-06, step=1397]Training:   1%|          | 1398/200000 [30:49<66:01:55,  1.20s/it, loss=0.1748, lr=6.98e-06, step=1397]Training:   1%|          | 1398/200000 [30:49<66:01:55,  1.20s/it, loss=0.0619, lr=6.99e-06, step=1398]Training:   1%|          | 1399/200000 [30:50<69:29:13,  1.26s/it, loss=0.0619, lr=6.99e-06, step=1398]Training:   1%|          | 1399/200000 [30:50<69:29:13,  1.26s/it, loss=0.0675, lr=6.99e-06, step=1399]Training:   1%|          | 1400/200000 [30:51<71:49:00,  1.30s/it, loss=0.0675, lr=6.99e-06, step=1399]Training:   1%|          | 1400/200000 [30:51<71:49:00,  1.30s/it, loss=0.1219, lr=7.00e-06, step=1400]23:24:06.289 [I] step=1400 loss=0.0879 lr=6.76e-06 grad_norm=0.84 time=125.8s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1401/200000 [30:52<68:06:22,  1.23s/it, loss=0.1219, lr=7.00e-06, step=1400]Training:   1%|          | 1401/200000 [30:52<68:06:22,  1.23s/it, loss=0.0536, lr=7.00e-06, step=1401]Training:   1%|          | 1402/200000 [30:54<70:56:14,  1.29s/it, loss=0.0536, lr=7.00e-06, step=1401]Training:   1%|          | 1402/200000 [30:54<70:56:14,  1.29s/it, loss=0.1566, lr=7.01e-06, step=1402]Training:   1%|          | 1403/200000 [30:55<67:31:40,  1.22s/it, loss=0.1566, lr=7.01e-06, step=1402]Training:   1%|          | 1403/200000 [30:55<67:31:40,  1.22s/it, loss=0.0919, lr=7.01e-06, step=1403]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1404/200000 [30:56<70:43:24,  1.28s/it, loss=0.0919, lr=7.01e-06, step=1403]Training:   1%|          | 1404/200000 [30:56<70:43:24,  1.28s/it, loss=0.0289, lr=7.02e-06, step=1404]Training:   1%|          | 1405/200000 [30:58<71:33:54,  1.30s/it, loss=0.0289, lr=7.02e-06, step=1404]Training:   1%|          | 1405/200000 [30:58<71:33:54,  1.30s/it, loss=0.0559, lr=7.02e-06, step=1405]Training:   1%|          | 1406/200000 [30:59<75:00:10,  1.36s/it, loss=0.0559, lr=7.02e-06, step=1405]Training:   1%|          | 1406/200000 [30:59<75:00:10,  1.36s/it, loss=0.0460, lr=7.03e-06, step=1406]Training:   1%|          | 1407/200000 [31:01<77:23:10,  1.40s/it, loss=0.0460, lr=7.03e-06, step=1406]Training:   1%|          | 1407/200000 [31:01<77:23:10,  1.40s/it, loss=0.0392, lr=7.03e-06, step=1407]Training:   1%|          | 1408/200000 [31:02<71:59:04,  1.30s/it, loss=0.0392, lr=7.03e-06, step=1407]Training:   1%|          | 1408/200000 [31:02<71:59:04,  1.30s/it, loss=0.0489, lr=7.04e-06, step=1408]Training:   1%|          | 1409/200000 [31:03<68:11:57,  1.24s/it, loss=0.0489, lr=7.04e-06, step=1408]Training:   1%|          | 1409/200000 [31:03<68:11:57,  1.24s/it, loss=0.0666, lr=7.04e-06, step=1409]Training:   1%|          | 1410/200000 [31:04<72:17:33,  1.31s/it, loss=0.0666, lr=7.04e-06, step=1409]Training:   1%|          | 1410/200000 [31:04<72:17:33,  1.31s/it, loss=0.0382, lr=7.05e-06, step=1410]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1411/200000 [31:05<68:26:17,  1.24s/it, loss=0.0382, lr=7.05e-06, step=1410]Training:   1%|          | 1411/200000 [31:05<68:26:17,  1.24s/it, loss=0.0657, lr=7.05e-06, step=1411]Training:   1%|          | 1412/200000 [31:07<70:07:32,  1.27s/it, loss=0.0657, lr=7.05e-06, step=1411]Training:   1%|          | 1412/200000 [31:07<70:07:32,  1.27s/it, loss=0.0384, lr=7.06e-06, step=1412]Training:   1%|          | 1413/200000 [31:08<66:55:19,  1.21s/it, loss=0.0384, lr=7.06e-06, step=1412]Training:   1%|          | 1413/200000 [31:08<66:55:19,  1.21s/it, loss=0.1730, lr=7.06e-06, step=1413]Training:   1%|          | 1414/200000 [31:09<64:36:27,  1.17s/it, loss=0.1730, lr=7.06e-06, step=1413]Training:   1%|          | 1414/200000 [31:09<64:36:27,  1.17s/it, loss=0.0495, lr=7.07e-06, step=1414]Training:   1%|          | 1415/200000 [31:10<69:50:07,  1.27s/it, loss=0.0495, lr=7.07e-06, step=1414]Training:   1%|          | 1415/200000 [31:10<69:50:07,  1.27s/it, loss=0.0415, lr=7.07e-06, step=1415]Training:   1%|          | 1416/200000 [31:12<73:14:29,  1.33s/it, loss=0.0415, lr=7.07e-06, step=1415]Training:   1%|          | 1416/200000 [31:12<73:14:29,  1.33s/it, loss=0.0765, lr=7.08e-06, step=1416]Training:   1%|          | 1417/200000 [31:13<74:49:31,  1.36s/it, loss=0.0765, lr=7.08e-06, step=1416]Training:   1%|          | 1417/200000 [31:13<74:49:31,  1.36s/it, loss=0.1077, lr=7.08e-06, step=1417]Training:   1%|          | 1418/200000 [31:14<70:11:25,  1.27s/it, loss=0.1077, lr=7.08e-06, step=1417]Training:   1%|          | 1418/200000 [31:14<70:11:25,  1.27s/it, loss=0.0538, lr=7.09e-06, step=1418]Training:   1%|          | 1419/200000 [31:15<66:56:24,  1.21s/it, loss=0.0538, lr=7.09e-06, step=1418]Training:   1%|          | 1419/200000 [31:15<66:56:24,  1.21s/it, loss=0.1380, lr=7.09e-06, step=1419]Training:   1%|          | 1420/200000 [31:17<69:21:22,  1.26s/it, loss=0.1380, lr=7.09e-06, step=1419]Training:   1%|          | 1420/200000 [31:17<69:21:22,  1.26s/it, loss=0.0470, lr=7.10e-06, step=1420]Training:   1%|          | 1421/200000 [31:18<71:37:33,  1.30s/it, loss=0.0470, lr=7.10e-06, step=1420]Training:   1%|          | 1421/200000 [31:18<71:37:33,  1.30s/it, loss=0.0654, lr=7.10e-06, step=1421]Training:   1%|          | 1422/200000 [31:19<67:57:11,  1.23s/it, loss=0.0654, lr=7.10e-06, step=1421]Training:   1%|          | 1422/200000 [31:19<67:57:11,  1.23s/it, loss=0.0404, lr=7.11e-06, step=1422]Training:   1%|          | 1423/200000 [31:21<70:39:36,  1.28s/it, loss=0.0404, lr=7.11e-06, step=1422]Training:   1%|          | 1423/200000 [31:21<70:39:36,  1.28s/it, loss=0.0956, lr=7.11e-06, step=1423]Training:   1%|          | 1424/200000 [31:22<67:12:52,  1.22s/it, loss=0.0956, lr=7.11e-06, step=1423]Training:   1%|          | 1424/200000 [31:22<67:12:52,  1.22s/it, loss=0.1465, lr=7.12e-06, step=1424]Training:   1%|          | 1425/200000 [31:23<68:58:57,  1.25s/it, loss=0.1465, lr=7.12e-06, step=1424]Training:   1%|          | 1425/200000 [31:23<68:58:57,  1.25s/it, loss=0.1004, lr=7.12e-06, step=1425]Training:   1%|          | 1426/200000 [31:24<70:16:44,  1.27s/it, loss=0.1004, lr=7.12e-06, step=1425]Training:   1%|          | 1426/200000 [31:24<70:16:44,  1.27s/it, loss=0.2713, lr=7.13e-06, step=1426]Training:   1%|          | 1427/200000 [31:26<73:41:51,  1.34s/it, loss=0.2713, lr=7.13e-06, step=1426]Training:   1%|          | 1427/200000 [31:26<73:41:51,  1.34s/it, loss=0.0447, lr=7.13e-06, step=1427]Training:   1%|          | 1428/200000 [31:27<76:30:09,  1.39s/it, loss=0.0447, lr=7.13e-06, step=1427]Training:   1%|          | 1428/200000 [31:27<76:30:09,  1.39s/it, loss=0.0599, lr=7.14e-06, step=1428]Training:   1%|          | 1429/200000 [31:28<71:22:25,  1.29s/it, loss=0.0599, lr=7.14e-06, step=1428]Training:   1%|          | 1429/200000 [31:28<71:22:25,  1.29s/it, loss=0.1549, lr=7.14e-06, step=1429]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1430/200000 [31:30<67:44:38,  1.23s/it, loss=0.1549, lr=7.14e-06, step=1429]Training:   1%|          | 1430/200000 [31:30<67:44:38,  1.23s/it, loss=0.0381, lr=7.15e-06, step=1430]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1431/200000 [31:31<70:56:39,  1.29s/it, loss=0.0381, lr=7.15e-06, step=1430]Training:   1%|          | 1431/200000 [31:31<70:56:39,  1.29s/it, loss=0.2456, lr=7.15e-06, step=1431]Training:   1%|          | 1432/200000 [31:32<67:29:38,  1.22s/it, loss=0.2456, lr=7.15e-06, step=1431]Training:   1%|          | 1432/200000 [31:32<67:29:38,  1.22s/it, loss=0.0596, lr=7.16e-06, step=1432]Training:   1%|          | 1433/200000 [31:33<68:50:25,  1.25s/it, loss=0.0596, lr=7.16e-06, step=1432]Training:   1%|          | 1433/200000 [31:33<68:50:25,  1.25s/it, loss=0.1558, lr=7.16e-06, step=1433]Training:   1%|          | 1434/200000 [31:34<66:00:39,  1.20s/it, loss=0.1558, lr=7.16e-06, step=1433]Training:   1%|          | 1434/200000 [31:34<66:00:39,  1.20s/it, loss=0.0884, lr=7.17e-06, step=1434]Training:   1%|          | 1435/200000 [31:36<64:02:37,  1.16s/it, loss=0.0884, lr=7.17e-06, step=1434]Training:   1%|          | 1435/200000 [31:36<64:02:37,  1.16s/it, loss=0.0643, lr=7.17e-06, step=1435]Training:   1%|          | 1436/200000 [31:37<68:51:43,  1.25s/it, loss=0.0643, lr=7.17e-06, step=1435]Training:   1%|          | 1436/200000 [31:37<68:51:43,  1.25s/it, loss=0.0636, lr=7.18e-06, step=1436]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1437/200000 [31:38<71:36:22,  1.30s/it, loss=0.0636, lr=7.18e-06, step=1436]Training:   1%|          | 1437/200000 [31:38<71:36:22,  1.30s/it, loss=0.0456, lr=7.18e-06, step=1437]Training:   1%|          | 1438/200000 [31:40<73:52:45,  1.34s/it, loss=0.0456, lr=7.18e-06, step=1437]Training:   1%|          | 1438/200000 [31:40<73:52:45,  1.34s/it, loss=0.0938, lr=7.19e-06, step=1438]Training:   1%|          | 1439/200000 [31:41<69:30:56,  1.26s/it, loss=0.0938, lr=7.19e-06, step=1438]Training:   1%|          | 1439/200000 [31:41<69:30:56,  1.26s/it, loss=0.0447, lr=7.19e-06, step=1439]Training:   1%|          | 1440/200000 [31:42<66:28:24,  1.21s/it, loss=0.0447, lr=7.19e-06, step=1439]Training:   1%|          | 1440/200000 [31:42<66:28:24,  1.21s/it, loss=0.0547, lr=7.20e-06, step=1440]Training:   1%|          | 1441/200000 [31:43<68:36:26,  1.24s/it, loss=0.0547, lr=7.20e-06, step=1440]Training:   1%|          | 1441/200000 [31:43<68:36:26,  1.24s/it, loss=0.0755, lr=7.20e-06, step=1441]Training:   1%|          | 1442/200000 [31:45<70:01:53,  1.27s/it, loss=0.0755, lr=7.20e-06, step=1441]Training:   1%|          | 1442/200000 [31:45<70:01:53,  1.27s/it, loss=0.0619, lr=7.21e-06, step=1442]Training:   1%|          | 1443/200000 [31:46<66:54:26,  1.21s/it, loss=0.0619, lr=7.21e-06, step=1442]Training:   1%|          | 1443/200000 [31:46<66:54:26,  1.21s/it, loss=0.0835, lr=7.21e-06, step=1443]Training:   1%|          | 1444/200000 [31:47<68:51:36,  1.25s/it, loss=0.0835, lr=7.21e-06, step=1443]Training:   1%|          | 1444/200000 [31:47<68:51:36,  1.25s/it, loss=0.0588, lr=7.22e-06, step=1444]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1445/200000 [31:48<66:01:25,  1.20s/it, loss=0.0588, lr=7.22e-06, step=1444]Training:   1%|          | 1445/200000 [31:48<66:01:25,  1.20s/it, loss=0.0765, lr=7.22e-06, step=1445]Training:   1%|          | 1446/200000 [31:49<67:33:03,  1.22s/it, loss=0.0765, lr=7.22e-06, step=1445]Training:   1%|          | 1446/200000 [31:49<67:33:03,  1.22s/it, loss=0.0337, lr=7.23e-06, step=1446]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1447/200000 [31:50<65:06:17,  1.18s/it, loss=0.0337, lr=7.23e-06, step=1446]Training:   1%|          | 1447/200000 [31:50<65:06:17,  1.18s/it, loss=0.0512, lr=7.23e-06, step=1447]Training:   1%|          | 1448/200000 [31:52<69:44:13,  1.26s/it, loss=0.0512, lr=7.23e-06, step=1447]Training:   1%|          | 1448/200000 [31:52<69:44:13,  1.26s/it, loss=0.0511, lr=7.24e-06, step=1448]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1449/200000 [31:53<72:50:22,  1.32s/it, loss=0.0511, lr=7.24e-06, step=1448]Training:   1%|          | 1449/200000 [31:53<72:50:22,  1.32s/it, loss=0.0861, lr=7.24e-06, step=1449]Training:   1%|          | 1450/200000 [31:54<68:47:09,  1.25s/it, loss=0.0861, lr=7.24e-06, step=1449]Training:   1%|          | 1450/200000 [31:54<68:47:09,  1.25s/it, loss=0.0276, lr=7.25e-06, step=1450]Training:   1%|          | 1451/200000 [31:56<65:58:50,  1.20s/it, loss=0.0276, lr=7.25e-06, step=1450]Training:   1%|          | 1451/200000 [31:56<65:58:50,  1.20s/it, loss=0.1120, lr=7.25e-06, step=1451]Training:   1%|          | 1452/200000 [31:57<69:27:18,  1.26s/it, loss=0.1120, lr=7.25e-06, step=1451]Training:   1%|          | 1452/200000 [31:57<69:27:18,  1.26s/it, loss=0.0931, lr=7.26e-06, step=1452]Training:   1%|          | 1453/200000 [31:58<71:55:08,  1.30s/it, loss=0.0931, lr=7.26e-06, step=1452]Training:   1%|          | 1453/200000 [31:58<71:55:08,  1.30s/it, loss=0.1336, lr=7.26e-06, step=1453]Training:   1%|          | 1454/200000 [31:59<68:08:47,  1.24s/it, loss=0.1336, lr=7.26e-06, step=1453]Training:   1%|          | 1454/200000 [31:59<68:08:47,  1.24s/it, loss=0.1233, lr=7.27e-06, step=1454]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1455/200000 [32:01<70:47:27,  1.28s/it, loss=0.1233, lr=7.27e-06, step=1454]Training:   1%|          | 1455/200000 [32:01<70:47:27,  1.28s/it, loss=0.0701, lr=7.27e-06, step=1455]Training:   1%|          | 1456/200000 [32:02<67:21:09,  1.22s/it, loss=0.0701, lr=7.27e-06, step=1455]Training:   1%|          | 1456/200000 [32:02<67:21:09,  1.22s/it, loss=0.0478, lr=7.28e-06, step=1456]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1457/200000 [32:03<70:33:07,  1.28s/it, loss=0.0478, lr=7.28e-06, step=1456]Training:   1%|          | 1457/200000 [32:03<70:33:07,  1.28s/it, loss=0.0576, lr=7.28e-06, step=1457]Training:   1%|          | 1458/200000 [32:05<71:25:36,  1.30s/it, loss=0.0576, lr=7.28e-06, step=1457]Training:   1%|          | 1458/200000 [32:05<71:25:36,  1.30s/it, loss=0.0984, lr=7.29e-06, step=1458]Training:   1%|          | 1459/200000 [32:06<74:50:24,  1.36s/it, loss=0.0984, lr=7.29e-06, step=1458]Training:   1%|          | 1459/200000 [32:06<74:50:24,  1.36s/it, loss=0.1112, lr=7.29e-06, step=1459]Training:   1%|          | 1460/200000 [32:08<77:40:20,  1.41s/it, loss=0.1112, lr=7.29e-06, step=1459]Training:   1%|          | 1460/200000 [32:08<77:40:20,  1.41s/it, loss=0.0473, lr=7.30e-06, step=1460]Training:   1%|          | 1461/200000 [32:09<72:10:26,  1.31s/it, loss=0.0473, lr=7.30e-06, step=1460]Training:   1%|          | 1461/200000 [32:09<72:10:26,  1.31s/it, loss=0.1523, lr=7.30e-06, step=1461]Training:   1%|          | 1462/200000 [32:10<68:19:07,  1.24s/it, loss=0.1523, lr=7.30e-06, step=1461]Training:   1%|          | 1462/200000 [32:10<68:19:07,  1.24s/it, loss=0.0921, lr=7.31e-06, step=1462]Training:   1%|          | 1463/200000 [32:11<72:22:26,  1.31s/it, loss=0.0921, lr=7.31e-06, step=1462]Training:   1%|          | 1463/200000 [32:11<72:22:26,  1.31s/it, loss=0.0666, lr=7.31e-06, step=1463]Training:   1%|          | 1464/200000 [32:12<68:29:33,  1.24s/it, loss=0.0666, lr=7.31e-06, step=1463]Training:   1%|          | 1464/200000 [32:12<68:29:33,  1.24s/it, loss=0.0639, lr=7.32e-06, step=1464]Training:   1%|          | 1465/200000 [32:14<70:01:53,  1.27s/it, loss=0.0639, lr=7.32e-06, step=1464]Training:   1%|          | 1465/200000 [32:14<70:01:53,  1.27s/it, loss=0.4235, lr=7.32e-06, step=1465]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1466/200000 [32:15<66:47:37,  1.21s/it, loss=0.4235, lr=7.32e-06, step=1465]Training:   1%|          | 1466/200000 [32:15<66:47:37,  1.21s/it, loss=0.0327, lr=7.33e-06, step=1466]Training:   1%|          | 1467/200000 [32:16<64:34:57,  1.17s/it, loss=0.0327, lr=7.33e-06, step=1466]Training:   1%|          | 1467/200000 [32:16<64:34:57,  1.17s/it, loss=0.0424, lr=7.33e-06, step=1467]Training:   1%|          | 1468/200000 [32:17<69:03:11,  1.25s/it, loss=0.0424, lr=7.33e-06, step=1467]Training:   1%|          | 1468/200000 [32:17<69:03:11,  1.25s/it, loss=0.0438, lr=7.34e-06, step=1468]Training:   1%|          | 1469/200000 [32:19<72:30:23,  1.31s/it, loss=0.0438, lr=7.34e-06, step=1468]Training:   1%|          | 1469/200000 [32:19<72:30:23,  1.31s/it, loss=0.0770, lr=7.34e-06, step=1469]Training:   1%|          | 1470/200000 [32:20<74:30:30,  1.35s/it, loss=0.0770, lr=7.34e-06, step=1469]Training:   1%|          | 1470/200000 [32:20<74:30:30,  1.35s/it, loss=0.0506, lr=7.35e-06, step=1470]Training:   1%|          | 1471/200000 [32:21<70:00:37,  1.27s/it, loss=0.0506, lr=7.35e-06, step=1470]Training:   1%|          | 1471/200000 [32:21<70:00:37,  1.27s/it, loss=0.0617, lr=7.35e-06, step=1471]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1472/200000 [32:22<66:49:42,  1.21s/it, loss=0.0617, lr=7.35e-06, step=1471]Training:   1%|          | 1472/200000 [32:22<66:49:42,  1.21s/it, loss=0.0858, lr=7.36e-06, step=1472]Training:   1%|          | 1473/200000 [32:24<68:55:17,  1.25s/it, loss=0.0858, lr=7.36e-06, step=1472]Training:   1%|          | 1473/200000 [32:24<68:55:17,  1.25s/it, loss=0.1791, lr=7.36e-06, step=1473]Training:   1%|          | 1474/200000 [32:25<71:13:07,  1.29s/it, loss=0.1791, lr=7.36e-06, step=1473]Training:   1%|          | 1474/200000 [32:25<71:13:07,  1.29s/it, loss=0.1836, lr=7.37e-06, step=1474]Training:   1%|          | 1475/200000 [32:26<67:39:31,  1.23s/it, loss=0.1836, lr=7.37e-06, step=1474]Training:   1%|          | 1475/200000 [32:26<67:39:31,  1.23s/it, loss=0.0573, lr=7.37e-06, step=1475]Training:   1%|          | 1476/200000 [32:28<70:40:57,  1.28s/it, loss=0.0573, lr=7.37e-06, step=1475]Training:   1%|          | 1476/200000 [32:28<70:40:57,  1.28s/it, loss=0.0698, lr=7.38e-06, step=1476]Training:   1%|          | 1477/200000 [32:29<67:15:45,  1.22s/it, loss=0.0698, lr=7.38e-06, step=1476]Training:   1%|          | 1477/200000 [32:29<67:15:45,  1.22s/it, loss=0.0599, lr=7.38e-06, step=1477]Training:   1%|          | 1478/200000 [32:30<68:42:41,  1.25s/it, loss=0.0599, lr=7.38e-06, step=1477]Training:   1%|          | 1478/200000 [32:30<68:42:41,  1.25s/it, loss=0.1077, lr=7.39e-06, step=1478]Training:   1%|          | 1479/200000 [32:31<70:05:04,  1.27s/it, loss=0.1077, lr=7.39e-06, step=1478]Training:   1%|          | 1479/200000 [32:31<70:05:04,  1.27s/it, loss=0.1006, lr=7.39e-06, step=1479]Training:   1%|          | 1480/200000 [32:33<73:26:43,  1.33s/it, loss=0.1006, lr=7.39e-06, step=1479]Training:   1%|          | 1480/200000 [32:33<73:26:43,  1.33s/it, loss=0.1681, lr=7.40e-06, step=1480]Training:   1%|          | 1481/200000 [32:34<76:23:16,  1.39s/it, loss=0.1681, lr=7.40e-06, step=1480]Training:   1%|          | 1481/200000 [32:34<76:23:16,  1.39s/it, loss=0.0446, lr=7.40e-06, step=1481]Training:   1%|          | 1482/200000 [32:35<71:18:35,  1.29s/it, loss=0.0446, lr=7.40e-06, step=1481]Training:   1%|          | 1482/200000 [32:35<71:18:35,  1.29s/it, loss=0.1877, lr=7.41e-06, step=1482]Training:   1%|          | 1483/200000 [32:36<67:44:57,  1.23s/it, loss=0.1877, lr=7.41e-06, step=1482]Training:   1%|          | 1483/200000 [32:36<67:44:57,  1.23s/it, loss=0.1022, lr=7.41e-06, step=1483]Training:   1%|          | 1484/200000 [32:38<71:36:34,  1.30s/it, loss=0.1022, lr=7.41e-06, step=1483]Training:   1%|          | 1484/200000 [32:38<71:36:34,  1.30s/it, loss=0.0403, lr=7.42e-06, step=1484]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1485/200000 [32:39<67:59:43,  1.23s/it, loss=0.0403, lr=7.42e-06, step=1484]Training:   1%|          | 1485/200000 [32:39<67:59:43,  1.23s/it, loss=0.0495, lr=7.42e-06, step=1485]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1486/200000 [32:40<69:42:54,  1.26s/it, loss=0.0495, lr=7.42e-06, step=1485]Training:   1%|          | 1486/200000 [32:40<69:42:54,  1.26s/it, loss=0.0704, lr=7.43e-06, step=1486]Training:   1%|          | 1487/200000 [32:41<66:40:49,  1.21s/it, loss=0.0704, lr=7.43e-06, step=1486]Training:   1%|          | 1487/200000 [32:41<66:40:49,  1.21s/it, loss=0.0995, lr=7.43e-06, step=1487]Training:   1%|          | 1488/200000 [32:42<64:28:56,  1.17s/it, loss=0.0995, lr=7.43e-06, step=1487]Training:   1%|          | 1488/200000 [32:42<64:28:56,  1.17s/it, loss=0.0619, lr=7.44e-06, step=1488]Training:   1%|          | 1489/200000 [32:44<69:28:47,  1.26s/it, loss=0.0619, lr=7.44e-06, step=1488]Training:   1%|          | 1489/200000 [32:44<69:28:47,  1.26s/it, loss=0.2141, lr=7.44e-06, step=1489]Training:   1%|          | 1490/200000 [32:45<72:46:20,  1.32s/it, loss=0.2141, lr=7.44e-06, step=1489]Training:   1%|          | 1490/200000 [32:45<72:46:20,  1.32s/it, loss=0.1035, lr=7.45e-06, step=1490]Training:   1%|          | 1491/200000 [32:47<74:24:07,  1.35s/it, loss=0.1035, lr=7.45e-06, step=1490]Training:   1%|          | 1491/200000 [32:47<74:24:07,  1.35s/it, loss=0.0439, lr=7.45e-06, step=1491]Training:   1%|          | 1492/200000 [32:48<69:53:26,  1.27s/it, loss=0.0439, lr=7.45e-06, step=1491]Training:   1%|          | 1492/200000 [32:48<69:53:26,  1.27s/it, loss=0.2645, lr=7.46e-06, step=1492]Training:   1%|          | 1493/200000 [32:49<66:43:41,  1.21s/it, loss=0.2645, lr=7.46e-06, step=1492]Training:   1%|          | 1493/200000 [32:49<66:43:41,  1.21s/it, loss=0.0942, lr=7.46e-06, step=1493]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1494/200000 [32:50<68:44:59,  1.25s/it, loss=0.0942, lr=7.46e-06, step=1493]Training:   1%|          | 1494/200000 [32:50<68:44:59,  1.25s/it, loss=0.0403, lr=7.47e-06, step=1494]Training:   1%|          | 1495/200000 [32:52<69:51:59,  1.27s/it, loss=0.0403, lr=7.47e-06, step=1494]Training:   1%|          | 1495/200000 [32:52<69:51:59,  1.27s/it, loss=0.0765, lr=7.47e-06, step=1495]Training:   1%|          | 1496/200000 [32:53<66:44:25,  1.21s/it, loss=0.0765, lr=7.47e-06, step=1495]Training:   1%|          | 1496/200000 [32:53<66:44:25,  1.21s/it, loss=0.0518, lr=7.48e-06, step=1496]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1497/200000 [32:54<69:04:06,  1.25s/it, loss=0.0518, lr=7.48e-06, step=1496]Training:   1%|          | 1497/200000 [32:54<69:04:06,  1.25s/it, loss=0.0807, lr=7.48e-06, step=1497]Training:   1%|          | 1498/200000 [32:55<66:09:16,  1.20s/it, loss=0.0807, lr=7.48e-06, step=1497]Training:   1%|          | 1498/200000 [32:55<66:09:16,  1.20s/it, loss=0.0748, lr=7.49e-06, step=1498]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1499/200000 [32:56<67:29:26,  1.22s/it, loss=0.0748, lr=7.49e-06, step=1498]Training:   1%|          | 1499/200000 [32:56<67:29:26,  1.22s/it, loss=0.0909, lr=7.49e-06, step=1499]Training:   1%|          | 1500/200000 [32:58<65:03:53,  1.18s/it, loss=0.0909, lr=7.49e-06, step=1499]Training:   1%|          | 1500/200000 [32:58<65:03:53,  1.18s/it, loss=0.0738, lr=7.50e-06, step=1500]23:26:12.768 [I] step=1500 loss=0.0884 lr=7.26e-06 grad_norm=0.94 time=126.5s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1501/200000 [32:59<69:37:01,  1.26s/it, loss=0.0738, lr=7.50e-06, step=1500]Training:   1%|          | 1501/200000 [32:59<69:37:01,  1.26s/it, loss=0.0642, lr=7.50e-06, step=1501]Training:   1%|          | 1502/200000 [33:00<72:40:45,  1.32s/it, loss=0.0642, lr=7.50e-06, step=1501]Training:   1%|          | 1502/200000 [33:00<72:40:45,  1.32s/it, loss=0.0925, lr=7.51e-06, step=1502]Training:   1%|          | 1503/200000 [33:01<68:41:39,  1.25s/it, loss=0.0925, lr=7.51e-06, step=1502]Training:   1%|          | 1503/200000 [33:01<68:41:39,  1.25s/it, loss=0.0606, lr=7.51e-06, step=1503]Training:   1%|          | 1504/200000 [33:03<65:50:37,  1.19s/it, loss=0.0606, lr=7.51e-06, step=1503]Training:   1%|          | 1504/200000 [33:03<65:50:37,  1.19s/it, loss=0.1508, lr=7.52e-06, step=1504]Training:   1%|          | 1505/200000 [33:04<69:16:12,  1.26s/it, loss=0.1508, lr=7.52e-06, step=1504]Training:   1%|          | 1505/200000 [33:04<69:16:12,  1.26s/it, loss=0.1021, lr=7.52e-06, step=1505]Training:   1%|          | 1506/200000 [33:05<71:40:02,  1.30s/it, loss=0.1021, lr=7.52e-06, step=1505]Training:   1%|          | 1506/200000 [33:05<71:40:02,  1.30s/it, loss=0.0669, lr=7.53e-06, step=1506]Training:   1%|          | 1507/200000 [33:06<67:59:16,  1.23s/it, loss=0.0669, lr=7.53e-06, step=1506]Training:   1%|          | 1507/200000 [33:06<67:59:16,  1.23s/it, loss=0.1049, lr=7.53e-06, step=1507]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1508/200000 [33:08<70:38:08,  1.28s/it, loss=0.1049, lr=7.53e-06, step=1507]Training:   1%|          | 1508/200000 [33:08<70:38:08,  1.28s/it, loss=0.0816, lr=7.54e-06, step=1508]Training:   1%|          | 1509/200000 [33:09<67:15:14,  1.22s/it, loss=0.0816, lr=7.54e-06, step=1508]Training:   1%|          | 1509/200000 [33:09<67:15:14,  1.22s/it, loss=0.0734, lr=7.54e-06, step=1509]Training:   1%|          | 1510/200000 [33:10<70:21:23,  1.28s/it, loss=0.0734, lr=7.54e-06, step=1509]Training:   1%|          | 1510/200000 [33:10<70:21:23,  1.28s/it, loss=0.0607, lr=7.55e-06, step=1510]Training:   1%|          | 1511/200000 [33:12<71:11:46,  1.29s/it, loss=0.0607, lr=7.55e-06, step=1510]Training:   1%|          | 1511/200000 [33:12<71:11:46,  1.29s/it, loss=0.1017, lr=7.55e-06, step=1511]Training:   1%|          | 1512/200000 [33:13<74:39:28,  1.35s/it, loss=0.1017, lr=7.55e-06, step=1511]Training:   1%|          | 1512/200000 [33:13<74:39:28,  1.35s/it, loss=0.0597, lr=7.56e-06, step=1512]Training:   1%|          | 1513/200000 [33:15<77:32:40,  1.41s/it, loss=0.0597, lr=7.56e-06, step=1512]Training:   1%|          | 1513/200000 [33:15<77:32:40,  1.41s/it, loss=0.0428, lr=7.56e-06, step=1513]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1514/200000 [33:16<72:05:06,  1.31s/it, loss=0.0428, lr=7.56e-06, step=1513]Training:   1%|          | 1514/200000 [33:16<72:05:06,  1.31s/it, loss=0.1283, lr=7.57e-06, step=1514]Training:   1%|          | 1515/200000 [33:17<68:18:59,  1.24s/it, loss=0.1283, lr=7.57e-06, step=1514]Training:   1%|          | 1515/200000 [33:17<68:18:59,  1.24s/it, loss=0.0518, lr=7.57e-06, step=1515]Training:   1%|          | 1516/200000 [33:18<72:22:49,  1.31s/it, loss=0.0518, lr=7.57e-06, step=1515]Training:   1%|          | 1516/200000 [33:18<72:22:49,  1.31s/it, loss=0.0694, lr=7.58e-06, step=1516]Training:   1%|          | 1517/200000 [33:19<68:30:05,  1.24s/it, loss=0.0694, lr=7.58e-06, step=1516]Training:   1%|          | 1517/200000 [33:19<68:30:05,  1.24s/it, loss=0.0319, lr=7.58e-06, step=1517]Training:   1%|          | 1518/200000 [33:21<69:46:35,  1.27s/it, loss=0.0319, lr=7.58e-06, step=1517]Training:   1%|          | 1518/200000 [33:21<69:46:35,  1.27s/it, loss=0.0931, lr=7.59e-06, step=1518]Training:   1%|          | 1519/200000 [33:22<66:39:22,  1.21s/it, loss=0.0931, lr=7.59e-06, step=1518]Training:   1%|          | 1519/200000 [33:22<66:39:22,  1.21s/it, loss=0.0808, lr=7.59e-06, step=1519]Training:   1%|          | 1520/200000 [33:23<64:28:49,  1.17s/it, loss=0.0808, lr=7.59e-06, step=1519]Training:   1%|          | 1520/200000 [33:23<64:28:49,  1.17s/it, loss=0.0658, lr=7.60e-06, step=1520]Training:   1%|          | 1521/200000 [33:24<69:41:20,  1.26s/it, loss=0.0658, lr=7.60e-06, step=1520]Training:   1%|          | 1521/200000 [33:24<69:41:20,  1.26s/it, loss=0.1186, lr=7.60e-06, step=1521]Training:   1%|          | 1522/200000 [33:26<73:04:09,  1.33s/it, loss=0.1186, lr=7.60e-06, step=1521]Training:   1%|          | 1522/200000 [33:26<73:04:09,  1.33s/it, loss=0.0720, lr=7.61e-06, step=1522]Training:   1%|          | 1523/200000 [33:27<74:47:10,  1.36s/it, loss=0.0720, lr=7.61e-06, step=1522]Training:   1%|          | 1523/200000 [33:27<74:47:10,  1.36s/it, loss=0.0754, lr=7.61e-06, step=1523]Training:   1%|          | 1524/200000 [33:28<70:07:14,  1.27s/it, loss=0.0754, lr=7.61e-06, step=1523]Training:   1%|          | 1524/200000 [33:28<70:07:14,  1.27s/it, loss=0.1623, lr=7.62e-06, step=1524]Training:   1%|          | 1525/200000 [33:29<66:53:12,  1.21s/it, loss=0.1623, lr=7.62e-06, step=1524]Training:   1%|          | 1525/200000 [33:29<66:53:12,  1.21s/it, loss=0.0758, lr=7.62e-06, step=1525]Training:   1%|          | 1526/200000 [33:31<69:16:08,  1.26s/it, loss=0.0758, lr=7.62e-06, step=1525]Training:   1%|          | 1526/200000 [33:31<69:16:08,  1.26s/it, loss=0.0390, lr=7.63e-06, step=1526]Training:   1%|          | 1527/200000 [33:32<71:29:01,  1.30s/it, loss=0.0390, lr=7.63e-06, step=1526]Training:   1%|          | 1527/200000 [33:32<71:29:01,  1.30s/it, loss=0.0375, lr=7.63e-06, step=1527]Training:   1%|          | 1528/200000 [33:33<67:50:09,  1.23s/it, loss=0.0375, lr=7.63e-06, step=1527]Training:   1%|          | 1528/200000 [33:33<67:50:09,  1.23s/it, loss=0.0418, lr=7.64e-06, step=1528]Training:   1%|          | 1529/200000 [33:35<70:39:48,  1.28s/it, loss=0.0418, lr=7.64e-06, step=1528]Training:   1%|          | 1529/200000 [33:35<70:39:48,  1.28s/it, loss=0.1934, lr=7.64e-06, step=1529]Training:   1%|          | 1530/200000 [33:36<67:14:38,  1.22s/it, loss=0.1934, lr=7.64e-06, step=1529]Training:   1%|          | 1530/200000 [33:36<67:14:38,  1.22s/it, loss=0.0497, lr=7.65e-06, step=1530]Training:   1%|          | 1531/200000 [33:37<68:38:31,  1.25s/it, loss=0.0497, lr=7.65e-06, step=1530]Training:   1%|          | 1531/200000 [33:37<68:38:31,  1.25s/it, loss=0.0827, lr=7.65e-06, step=1531]Training:   1%|          | 1532/200000 [33:38<70:37:55,  1.28s/it, loss=0.0827, lr=7.65e-06, step=1531]Training:   1%|          | 1532/200000 [33:38<70:37:55,  1.28s/it, loss=0.0531, lr=7.66e-06, step=1532]Training:   1%|          | 1533/200000 [33:40<73:51:51,  1.34s/it, loss=0.0531, lr=7.66e-06, step=1532]Training:   1%|          | 1533/200000 [33:40<73:51:51,  1.34s/it, loss=0.0718, lr=7.66e-06, step=1533]Training:   1%|          | 1534/200000 [33:41<76:35:59,  1.39s/it, loss=0.0718, lr=7.66e-06, step=1533]Training:   1%|          | 1534/200000 [33:41<76:35:59,  1.39s/it, loss=0.0367, lr=7.67e-06, step=1534]Training:   1%|          | 1535/200000 [33:42<71:27:53,  1.30s/it, loss=0.0367, lr=7.67e-06, step=1534]Training:   1%|          | 1535/200000 [33:42<71:27:53,  1.30s/it, loss=0.0465, lr=7.67e-06, step=1535]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1536/200000 [33:44<67:49:41,  1.23s/it, loss=0.0465, lr=7.67e-06, step=1535]Training:   1%|          | 1536/200000 [33:44<67:49:41,  1.23s/it, loss=0.0734, lr=7.68e-06, step=1536]Training:   1%|          | 1537/200000 [33:45<71:44:41,  1.30s/it, loss=0.0734, lr=7.68e-06, step=1536]Training:   1%|          | 1537/200000 [33:45<71:44:41,  1.30s/it, loss=0.1028, lr=7.68e-06, step=1537]Training:   1%|          | 1538/200000 [33:46<68:01:49,  1.23s/it, loss=0.1028, lr=7.68e-06, step=1537]Training:   1%|          | 1538/200000 [33:46<68:01:49,  1.23s/it, loss=0.0449, lr=7.69e-06, step=1538]Training:   1%|          | 1539/200000 [33:47<69:13:42,  1.26s/it, loss=0.0449, lr=7.69e-06, step=1538]Training:   1%|          | 1539/200000 [33:47<69:13:42,  1.26s/it, loss=0.0800, lr=7.69e-06, step=1539]Training:   1%|          | 1540/200000 [33:48<66:14:32,  1.20s/it, loss=0.0800, lr=7.69e-06, step=1539]Training:   1%|          | 1540/200000 [33:48<66:14:32,  1.20s/it, loss=0.1177, lr=7.70e-06, step=1540]Training:   1%|          | 1541/200000 [33:50<64:11:07,  1.16s/it, loss=0.1177, lr=7.70e-06, step=1540]Training:   1%|          | 1541/200000 [33:50<64:11:07,  1.16s/it, loss=0.0406, lr=7.70e-06, step=1541]Training:   1%|          | 1542/200000 [33:51<69:16:49,  1.26s/it, loss=0.0406, lr=7.70e-06, step=1541]Training:   1%|          | 1542/200000 [33:51<69:16:49,  1.26s/it, loss=0.0647, lr=7.71e-06, step=1542]Training:   1%|          | 1543/200000 [33:52<72:41:28,  1.32s/it, loss=0.0647, lr=7.71e-06, step=1542]Training:   1%|          | 1543/200000 [33:52<72:41:28,  1.32s/it, loss=0.1194, lr=7.71e-06, step=1543]Training:   1%|          | 1544/200000 [33:54<73:54:08,  1.34s/it, loss=0.1194, lr=7.71e-06, step=1543]Training:   1%|          | 1544/200000 [33:54<73:54:08,  1.34s/it, loss=0.0656, lr=7.72e-06, step=1544]Training:   1%|          | 1545/200000 [33:55<69:30:59,  1.26s/it, loss=0.0656, lr=7.72e-06, step=1544]Training:   1%|          | 1545/200000 [33:55<69:30:59,  1.26s/it, loss=0.0846, lr=7.72e-06, step=1545]Training:   1%|          | 1546/200000 [33:56<66:26:33,  1.21s/it, loss=0.0846, lr=7.72e-06, step=1545]Training:   1%|          | 1546/200000 [33:56<66:26:33,  1.21s/it, loss=0.0635, lr=7.73e-06, step=1546]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1547/200000 [33:57<68:46:40,  1.25s/it, loss=0.0635, lr=7.73e-06, step=1546]Training:   1%|          | 1547/200000 [33:57<68:46:40,  1.25s/it, loss=0.0487, lr=7.73e-06, step=1547]Training:   1%|          | 1548/200000 [33:59<70:06:47,  1.27s/it, loss=0.0487, lr=7.73e-06, step=1547]Training:   1%|          | 1548/200000 [33:59<70:06:47,  1.27s/it, loss=0.0425, lr=7.74e-06, step=1548]Training:   1%|          | 1549/200000 [34:00<66:52:03,  1.21s/it, loss=0.0425, lr=7.74e-06, step=1548]Training:   1%|          | 1549/200000 [34:00<66:52:03,  1.21s/it, loss=0.0520, lr=7.74e-06, step=1549]Training:   1%|          | 1550/200000 [34:01<68:56:27,  1.25s/it, loss=0.0520, lr=7.74e-06, step=1549]Training:   1%|          | 1550/200000 [34:01<68:56:27,  1.25s/it, loss=0.0714, lr=7.75e-06, step=1550]Training:   1%|          | 1551/200000 [34:02<66:05:01,  1.20s/it, loss=0.0714, lr=7.75e-06, step=1550]Training:   1%|          | 1551/200000 [34:02<66:05:01,  1.20s/it, loss=0.0773, lr=7.75e-06, step=1551]Training:   1%|          | 1552/200000 [34:03<68:08:13,  1.24s/it, loss=0.0773, lr=7.75e-06, step=1551]Training:   1%|          | 1552/200000 [34:03<68:08:13,  1.24s/it, loss=0.0564, lr=7.76e-06, step=1552]Training:   1%|          | 1553/200000 [34:05<65:30:24,  1.19s/it, loss=0.0564, lr=7.76e-06, step=1552]Training:   1%|          | 1553/200000 [34:05<65:30:24,  1.19s/it, loss=0.0750, lr=7.76e-06, step=1553]Training:   1%|          | 1554/200000 [34:06<69:24:38,  1.26s/it, loss=0.0750, lr=7.76e-06, step=1553]Training:   1%|          | 1554/200000 [34:06<69:24:38,  1.26s/it, loss=0.0383, lr=7.77e-06, step=1554]Training:   1%|          | 1555/200000 [34:07<72:34:44,  1.32s/it, loss=0.0383, lr=7.77e-06, step=1554]Training:   1%|          | 1555/200000 [34:07<72:34:44,  1.32s/it, loss=0.0437, lr=7.77e-06, step=1555]Training:   1%|          | 1556/200000 [34:09<68:33:45,  1.24s/it, loss=0.0437, lr=7.77e-06, step=1555]Training:   1%|          | 1556/200000 [34:09<68:33:45,  1.24s/it, loss=0.0615, lr=7.78e-06, step=1556]Training:   1%|          | 1557/200000 [34:10<65:47:40,  1.19s/it, loss=0.0615, lr=7.78e-06, step=1556]Training:   1%|          | 1557/200000 [34:10<65:47:40,  1.19s/it, loss=0.0918, lr=7.78e-06, step=1557]Training:   1%|          | 1558/200000 [34:11<68:28:26,  1.24s/it, loss=0.0918, lr=7.78e-06, step=1557]Training:   1%|          | 1558/200000 [34:11<68:28:26,  1.24s/it, loss=0.0397, lr=7.79e-06, step=1558]Training:   1%|          | 1559/200000 [34:12<71:12:37,  1.29s/it, loss=0.0397, lr=7.79e-06, step=1558]Training:   1%|          | 1559/200000 [34:12<71:12:37,  1.29s/it, loss=0.0686, lr=7.79e-06, step=1559]Training:   1%|          | 1560/200000 [34:13<67:37:38,  1.23s/it, loss=0.0686, lr=7.79e-06, step=1559]Training:   1%|          | 1560/200000 [34:13<67:37:38,  1.23s/it, loss=0.0459, lr=7.80e-06, step=1560]Training:   1%|          | 1561/200000 [34:15<70:49:06,  1.28s/it, loss=0.0459, lr=7.80e-06, step=1560]Training:   1%|          | 1561/200000 [34:15<70:49:06,  1.28s/it, loss=0.0465, lr=7.80e-06, step=1561]Training:   1%|          | 1562/200000 [34:16<67:21:01,  1.22s/it, loss=0.0465, lr=7.80e-06, step=1561]Training:   1%|          | 1562/200000 [34:16<67:21:01,  1.22s/it, loss=0.0725, lr=7.81e-06, step=1562]Training:   1%|          | 1563/200000 [34:17<69:45:47,  1.27s/it, loss=0.0725, lr=7.81e-06, step=1562]Training:   1%|          | 1563/200000 [34:17<69:45:47,  1.27s/it, loss=0.0697, lr=7.81e-06, step=1563]Training:   1%|          | 1564/200000 [34:19<71:21:55,  1.29s/it, loss=0.0697, lr=7.81e-06, step=1563]Training:   1%|          | 1564/200000 [34:19<71:21:55,  1.29s/it, loss=0.0248, lr=7.82e-06, step=1564]Training:   1%|          | 1565/200000 [34:20<74:40:08,  1.35s/it, loss=0.0248, lr=7.82e-06, step=1564]Training:   1%|          | 1565/200000 [34:20<74:40:08,  1.35s/it, loss=0.0623, lr=7.82e-06, step=1565]Training:   1%|          | 1566/200000 [34:22<77:33:10,  1.41s/it, loss=0.0623, lr=7.82e-06, step=1565]Training:   1%|          | 1566/200000 [34:22<77:33:10,  1.41s/it, loss=0.1416, lr=7.83e-06, step=1566]Training:   1%|          | 1567/200000 [34:23<72:07:52,  1.31s/it, loss=0.1416, lr=7.83e-06, step=1566]Training:   1%|          | 1567/200000 [34:23<72:07:52,  1.31s/it, loss=0.0854, lr=7.83e-06, step=1567]Training:   1%|          | 1568/200000 [34:24<68:15:13,  1.24s/it, loss=0.0854, lr=7.83e-06, step=1567]Training:   1%|          | 1568/200000 [34:24<68:15:13,  1.24s/it, loss=0.1718, lr=7.84e-06, step=1568]Training:   1%|          | 1569/200000 [34:25<72:19:06,  1.31s/it, loss=0.1718, lr=7.84e-06, step=1568]Training:   1%|          | 1569/200000 [34:25<72:19:06,  1.31s/it, loss=0.0420, lr=7.84e-06, step=1569]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1570/200000 [34:26<68:24:17,  1.24s/it, loss=0.0420, lr=7.84e-06, step=1569]Training:   1%|          | 1570/200000 [34:26<68:24:17,  1.24s/it, loss=0.0451, lr=7.85e-06, step=1570]Training:   1%|          | 1571/200000 [34:28<70:20:49,  1.28s/it, loss=0.0451, lr=7.85e-06, step=1570]Training:   1%|          | 1571/200000 [34:28<70:20:49,  1.28s/it, loss=0.0575, lr=7.85e-06, step=1571]Training:   1%|          | 1572/200000 [34:29<67:01:10,  1.22s/it, loss=0.0575, lr=7.85e-06, step=1571]Training:   1%|          | 1572/200000 [34:29<67:01:10,  1.22s/it, loss=0.0900, lr=7.86e-06, step=1572]Training:   1%|          | 1573/200000 [34:30<64:43:09,  1.17s/it, loss=0.0900, lr=7.86e-06, step=1572]Training:   1%|          | 1573/200000 [34:30<64:43:09,  1.17s/it, loss=0.0552, lr=7.86e-06, step=1573]Training:   1%|          | 1574/200000 [34:31<69:40:36,  1.26s/it, loss=0.0552, lr=7.86e-06, step=1573]Training:   1%|          | 1574/200000 [34:31<69:40:36,  1.26s/it, loss=0.1730, lr=7.87e-06, step=1574]Training:   1%|          | 1575/200000 [34:33<73:05:24,  1.33s/it, loss=0.1730, lr=7.87e-06, step=1574]Training:   1%|          | 1575/200000 [34:33<73:05:24,  1.33s/it, loss=0.0799, lr=7.87e-06, step=1575]Training:   1%|          | 1576/200000 [34:34<74:31:17,  1.35s/it, loss=0.0799, lr=7.87e-06, step=1575]Training:   1%|          | 1576/200000 [34:34<74:31:17,  1.35s/it, loss=0.0569, lr=7.88e-06, step=1576]Training:   1%|          | 1577/200000 [34:35<69:59:04,  1.27s/it, loss=0.0569, lr=7.88e-06, step=1576]Training:   1%|          | 1577/200000 [34:35<69:59:04,  1.27s/it, loss=0.0496, lr=7.88e-06, step=1577]Training:   1%|          | 1578/200000 [34:36<66:48:40,  1.21s/it, loss=0.0496, lr=7.88e-06, step=1577]Training:   1%|          | 1578/200000 [34:36<66:48:40,  1.21s/it, loss=0.1058, lr=7.89e-06, step=1578]Training:   1%|          | 1579/200000 [34:38<68:59:36,  1.25s/it, loss=0.1058, lr=7.89e-06, step=1578]Training:   1%|          | 1579/200000 [34:38<68:59:36,  1.25s/it, loss=0.1410, lr=7.89e-06, step=1579]Training:   1%|          | 1580/200000 [34:39<71:29:44,  1.30s/it, loss=0.1410, lr=7.89e-06, step=1579]Training:   1%|          | 1580/200000 [34:39<71:29:44,  1.30s/it, loss=0.0422, lr=7.90e-06, step=1580]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1581/200000 [34:40<67:50:51,  1.23s/it, loss=0.0422, lr=7.90e-06, step=1580]Training:   1%|          | 1581/200000 [34:40<67:50:51,  1.23s/it, loss=0.0669, lr=7.90e-06, step=1581]Training:   1%|          | 1582/200000 [34:42<70:42:23,  1.28s/it, loss=0.0669, lr=7.90e-06, step=1581]Training:   1%|          | 1582/200000 [34:42<70:42:23,  1.28s/it, loss=0.1022, lr=7.91e-06, step=1582]Training:   1%|          | 1583/200000 [34:43<67:19:55,  1.22s/it, loss=0.1022, lr=7.91e-06, step=1582]Training:   1%|          | 1583/200000 [34:43<67:19:55,  1.22s/it, loss=0.1367, lr=7.91e-06, step=1583]Training:   1%|          | 1584/200000 [34:44<68:46:54,  1.25s/it, loss=0.1367, lr=7.91e-06, step=1583]Training:   1%|          | 1584/200000 [34:44<68:46:54,  1.25s/it, loss=0.0711, lr=7.92e-06, step=1584]Training:   1%|          | 1585/200000 [34:45<70:33:39,  1.28s/it, loss=0.0711, lr=7.92e-06, step=1584]Training:   1%|          | 1585/200000 [34:45<70:33:39,  1.28s/it, loss=0.0911, lr=7.92e-06, step=1585]Training:   1%|          | 1586/200000 [34:47<73:56:34,  1.34s/it, loss=0.0911, lr=7.92e-06, step=1585]Training:   1%|          | 1586/200000 [34:47<73:56:34,  1.34s/it, loss=0.0528, lr=7.93e-06, step=1586]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1587/200000 [34:48<76:38:59,  1.39s/it, loss=0.0528, lr=7.93e-06, step=1586]Training:   1%|          | 1587/200000 [34:48<76:38:59,  1.39s/it, loss=0.0519, lr=7.93e-06, step=1587]Training:   1%|          | 1588/200000 [34:49<71:26:33,  1.30s/it, loss=0.0519, lr=7.93e-06, step=1587]Training:   1%|          | 1588/200000 [34:49<71:26:33,  1.30s/it, loss=0.0512, lr=7.94e-06, step=1588]Training:   1%|          | 1589/200000 [34:51<67:50:33,  1.23s/it, loss=0.0512, lr=7.94e-06, step=1588]Training:   1%|          | 1589/200000 [34:51<67:50:33,  1.23s/it, loss=0.0398, lr=7.94e-06, step=1589]Training:   1%|          | 1590/200000 [34:52<71:42:56,  1.30s/it, loss=0.0398, lr=7.94e-06, step=1589]Training:   1%|          | 1590/200000 [34:52<71:42:56,  1.30s/it, loss=0.0557, lr=7.95e-06, step=1590]Training:   1%|          | 1591/200000 [34:53<68:02:46,  1.23s/it, loss=0.0557, lr=7.95e-06, step=1590]Training:   1%|          | 1591/200000 [34:53<68:02:46,  1.23s/it, loss=0.0585, lr=7.95e-06, step=1591]Training:   1%|          | 1592/200000 [34:54<69:03:55,  1.25s/it, loss=0.0585, lr=7.95e-06, step=1591]Training:   1%|          | 1592/200000 [34:54<69:03:55,  1.25s/it, loss=0.0679, lr=7.96e-06, step=1592]Training:   1%|          | 1593/200000 [34:55<66:07:57,  1.20s/it, loss=0.0679, lr=7.96e-06, step=1592]Training:   1%|          | 1593/200000 [34:55<66:07:57,  1.20s/it, loss=0.0764, lr=7.96e-06, step=1593]Training:   1%|          | 1594/200000 [34:57<64:04:30,  1.16s/it, loss=0.0764, lr=7.96e-06, step=1593]Training:   1%|          | 1594/200000 [34:57<64:04:30,  1.16s/it, loss=0.0465, lr=7.97e-06, step=1594]Training:   1%|          | 1595/200000 [34:58<69:17:38,  1.26s/it, loss=0.0465, lr=7.97e-06, step=1594]Training:   1%|          | 1595/200000 [34:58<69:17:38,  1.26s/it, loss=0.1210, lr=7.97e-06, step=1595]Training:   1%|          | 1596/200000 [34:59<72:02:46,  1.31s/it, loss=0.1210, lr=7.97e-06, step=1595]Training:   1%|          | 1596/200000 [34:59<72:02:46,  1.31s/it, loss=0.0853, lr=7.98e-06, step=1596]Training:   1%|          | 1597/200000 [35:01<73:28:11,  1.33s/it, loss=0.0853, lr=7.98e-06, step=1596]Training:   1%|          | 1597/200000 [35:01<73:28:11,  1.33s/it, loss=0.0647, lr=7.98e-06, step=1597]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1598/200000 [35:02<69:12:36,  1.26s/it, loss=0.0647, lr=7.98e-06, step=1597]Training:   1%|          | 1598/200000 [35:02<69:12:36,  1.26s/it, loss=0.0509, lr=7.99e-06, step=1598]Training:   1%|          | 1599/200000 [35:03<66:14:38,  1.20s/it, loss=0.0509, lr=7.99e-06, step=1598]Training:   1%|          | 1599/200000 [35:03<66:14:38,  1.20s/it, loss=0.0948, lr=7.99e-06, step=1599]Training:   1%|          | 1600/200000 [35:04<68:41:17,  1.25s/it, loss=0.0948, lr=7.99e-06, step=1599]Training:   1%|          | 1600/200000 [35:04<68:41:17,  1.25s/it, loss=0.0745, lr=8.00e-06, step=1600]23:28:19.443 [I] step=1600 loss=0.0749 lr=7.76e-06 grad_norm=0.90 time=126.7s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1601/200000 [35:06<70:02:08,  1.27s/it, loss=0.0745, lr=8.00e-06, step=1600]Training:   1%|          | 1601/200000 [35:06<70:02:08,  1.27s/it, loss=0.0683, lr=8.00e-06, step=1601]Training:   1%|          | 1602/200000 [35:07<66:46:28,  1.21s/it, loss=0.0683, lr=8.00e-06, step=1601]Training:   1%|          | 1602/200000 [35:07<66:46:28,  1.21s/it, loss=0.1602, lr=8.01e-06, step=1602]Training:   1%|          | 1603/200000 [35:08<68:35:37,  1.24s/it, loss=0.1602, lr=8.01e-06, step=1602]Training:   1%|          | 1603/200000 [35:08<68:35:37,  1.24s/it, loss=0.0362, lr=8.01e-06, step=1603]Training:   1%|          | 1604/200000 [35:09<65:46:32,  1.19s/it, loss=0.0362, lr=8.01e-06, step=1603]Training:   1%|          | 1604/200000 [35:09<65:46:32,  1.19s/it, loss=0.0401, lr=8.02e-06, step=1604]Training:   1%|          | 1605/200000 [35:10<67:19:59,  1.22s/it, loss=0.0401, lr=8.02e-06, step=1604]Training:   1%|          | 1605/200000 [35:10<67:19:59,  1.22s/it, loss=0.0950, lr=8.02e-06, step=1605]Training:   1%|          | 1606/200000 [35:11<64:53:50,  1.18s/it, loss=0.0950, lr=8.02e-06, step=1605]Training:   1%|          | 1606/200000 [35:11<64:53:50,  1.18s/it, loss=0.1008, lr=8.03e-06, step=1606]Training:   1%|          | 1607/200000 [35:13<69:40:39,  1.26s/it, loss=0.1008, lr=8.03e-06, step=1606]Training:   1%|          | 1607/200000 [35:13<69:40:39,  1.26s/it, loss=0.0823, lr=8.03e-06, step=1607]Training:   1%|          | 1608/200000 [35:14<72:40:15,  1.32s/it, loss=0.0823, lr=8.03e-06, step=1607]Training:   1%|          | 1608/200000 [35:14<72:40:15,  1.32s/it, loss=0.0518, lr=8.04e-06, step=1608]Training:   1%|          | 1609/200000 [35:15<68:38:17,  1.25s/it, loss=0.0518, lr=8.04e-06, step=1608]Training:   1%|          | 1609/200000 [35:15<68:38:17,  1.25s/it, loss=0.0685, lr=8.04e-06, step=1609]Training:   1%|          | 1610/200000 [35:17<65:48:45,  1.19s/it, loss=0.0685, lr=8.04e-06, step=1609]Training:   1%|          | 1610/200000 [35:17<65:48:45,  1.19s/it, loss=0.0682, lr=8.05e-06, step=1610]Training:   1%|          | 1611/200000 [35:18<69:18:17,  1.26s/it, loss=0.0682, lr=8.05e-06, step=1610]Training:   1%|          | 1611/200000 [35:18<69:18:17,  1.26s/it, loss=0.0504, lr=8.05e-06, step=1611]Training:   1%|          | 1612/200000 [35:19<71:55:45,  1.31s/it, loss=0.0504, lr=8.05e-06, step=1611]Training:   1%|          | 1612/200000 [35:19<71:55:45,  1.31s/it, loss=0.0490, lr=8.06e-06, step=1612]Training:   1%|          | 1613/200000 [35:20<68:07:45,  1.24s/it, loss=0.0490, lr=8.06e-06, step=1612]Training:   1%|          | 1613/200000 [35:20<68:07:45,  1.24s/it, loss=0.1824, lr=8.06e-06, step=1613]Training:   1%|          | 1614/200000 [35:22<70:45:48,  1.28s/it, loss=0.1824, lr=8.06e-06, step=1613]Training:   1%|          | 1614/200000 [35:22<70:45:48,  1.28s/it, loss=0.0440, lr=8.07e-06, step=1614]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1615/200000 [35:23<67:21:53,  1.22s/it, loss=0.0440, lr=8.07e-06, step=1614]Training:   1%|          | 1615/200000 [35:23<67:21:53,  1.22s/it, loss=0.0355, lr=8.07e-06, step=1615]Training:   1%|          | 1616/200000 [35:24<70:13:38,  1.27s/it, loss=0.0355, lr=8.07e-06, step=1615]Training:   1%|          | 1616/200000 [35:24<70:13:38,  1.27s/it, loss=0.0944, lr=8.08e-06, step=1616]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1617/200000 [35:26<71:51:50,  1.30s/it, loss=0.0944, lr=8.08e-06, step=1616]Training:   1%|          | 1617/200000 [35:26<71:51:50,  1.30s/it, loss=0.0873, lr=8.08e-06, step=1617]Training:   1%|          | 1618/200000 [35:27<75:08:43,  1.36s/it, loss=0.0873, lr=8.08e-06, step=1617]Training:   1%|          | 1618/200000 [35:27<75:08:43,  1.36s/it, loss=0.0871, lr=8.09e-06, step=1618]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1619/200000 [35:29<77:21:54,  1.40s/it, loss=0.0871, lr=8.09e-06, step=1618]Training:   1%|          | 1619/200000 [35:29<77:21:54,  1.40s/it, loss=0.0443, lr=8.09e-06, step=1619]Training:   1%|          | 1620/200000 [35:30<72:02:24,  1.31s/it, loss=0.0443, lr=8.09e-06, step=1619]Training:   1%|          | 1620/200000 [35:30<72:02:24,  1.31s/it, loss=0.0672, lr=8.10e-06, step=1620]Training:   1%|          | 1621/200000 [35:31<68:19:56,  1.24s/it, loss=0.0672, lr=8.10e-06, step=1620]Training:   1%|          | 1621/200000 [35:31<68:19:56,  1.24s/it, loss=0.1351, lr=8.10e-06, step=1621]Training:   1%|          | 1622/200000 [35:32<72:29:52,  1.32s/it, loss=0.1351, lr=8.10e-06, step=1621]Training:   1%|          | 1622/200000 [35:32<72:29:52,  1.32s/it, loss=0.1584, lr=8.11e-06, step=1622]Training:   1%|          | 1623/200000 [35:33<68:39:35,  1.25s/it, loss=0.1584, lr=8.11e-06, step=1622]Training:   1%|          | 1623/200000 [35:33<68:39:35,  1.25s/it, loss=0.0441, lr=8.11e-06, step=1623]Training:   1%|          | 1624/200000 [35:35<69:59:45,  1.27s/it, loss=0.0441, lr=8.11e-06, step=1623]Training:   1%|          | 1624/200000 [35:35<69:59:45,  1.27s/it, loss=0.0440, lr=8.12e-06, step=1624]Training:   1%|          | 1625/200000 [35:36<66:55:45,  1.21s/it, loss=0.0440, lr=8.12e-06, step=1624]Training:   1%|          | 1625/200000 [35:36<66:55:45,  1.21s/it, loss=0.0609, lr=8.12e-06, step=1625]Training:   1%|          | 1626/200000 [35:37<64:44:52,  1.18s/it, loss=0.0609, lr=8.12e-06, step=1625]Training:   1%|          | 1626/200000 [35:37<64:44:52,  1.18s/it, loss=0.0653, lr=8.13e-06, step=1626]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1627/200000 [35:38<69:49:13,  1.27s/it, loss=0.0653, lr=8.13e-06, step=1626]Training:   1%|          | 1627/200000 [35:38<69:49:13,  1.27s/it, loss=0.0769, lr=8.13e-06, step=1627]Training:   1%|          | 1628/200000 [35:40<73:19:15,  1.33s/it, loss=0.0769, lr=8.13e-06, step=1627]Training:   1%|          | 1628/200000 [35:40<73:19:15,  1.33s/it, loss=0.0738, lr=8.14e-06, step=1628]Training:   1%|          | 1629/200000 [35:41<74:40:41,  1.36s/it, loss=0.0738, lr=8.14e-06, step=1628]Training:   1%|          | 1629/200000 [35:41<74:40:41,  1.36s/it, loss=0.0635, lr=8.14e-06, step=1629]Training:   1%|          | 1630/200000 [35:42<70:11:44,  1.27s/it, loss=0.0635, lr=8.14e-06, step=1629]Training:   1%|          | 1630/200000 [35:42<70:11:44,  1.27s/it, loss=0.2090, lr=8.15e-06, step=1630]Training:   1%|          | 1631/200000 [35:43<67:00:26,  1.22s/it, loss=0.2090, lr=8.15e-06, step=1630]Training:   1%|          | 1631/200000 [35:43<67:00:26,  1.22s/it, loss=0.0738, lr=8.15e-06, step=1631]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1632/200000 [35:45<69:14:59,  1.26s/it, loss=0.0738, lr=8.15e-06, step=1631]Training:   1%|          | 1632/200000 [35:45<69:14:59,  1.26s/it, loss=0.0488, lr=8.16e-06, step=1632]Training:   1%|          | 1633/200000 [35:46<71:53:19,  1.30s/it, loss=0.0488, lr=8.16e-06, step=1632]Training:   1%|          | 1633/200000 [35:46<71:53:19,  1.30s/it, loss=0.4144, lr=8.16e-06, step=1633]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1634/200000 [35:47<68:17:07,  1.24s/it, loss=0.4144, lr=8.16e-06, step=1633]Training:   1%|          | 1634/200000 [35:47<68:17:07,  1.24s/it, loss=0.0661, lr=8.17e-06, step=1634]Training:   1%|          | 1635/200000 [35:49<70:59:24,  1.29s/it, loss=0.0661, lr=8.17e-06, step=1634]Training:   1%|          | 1635/200000 [35:49<70:59:24,  1.29s/it, loss=0.0411, lr=8.17e-06, step=1635]Training:   1%|          | 1636/200000 [35:50<67:36:22,  1.23s/it, loss=0.0411, lr=8.17e-06, step=1635]Training:   1%|          | 1636/200000 [35:50<67:36:22,  1.23s/it, loss=0.0522, lr=8.18e-06, step=1636]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1637/200000 [35:51<69:22:22,  1.26s/it, loss=0.0522, lr=8.18e-06, step=1636]Training:   1%|          | 1637/200000 [35:51<69:22:22,  1.26s/it, loss=0.0684, lr=8.18e-06, step=1637]Training:   1%|          | 1638/200000 [35:52<70:49:55,  1.29s/it, loss=0.0684, lr=8.18e-06, step=1637]Training:   1%|          | 1638/200000 [35:52<70:49:55,  1.29s/it, loss=0.0642, lr=8.19e-06, step=1638]Training:   1%|          | 1639/200000 [35:54<74:13:37,  1.35s/it, loss=0.0642, lr=8.19e-06, step=1638]Training:   1%|          | 1639/200000 [35:54<74:13:37,  1.35s/it, loss=0.0587, lr=8.19e-06, step=1639]Training:   1%|          | 1640/200000 [35:55<76:58:31,  1.40s/it, loss=0.0587, lr=8.19e-06, step=1639]Training:   1%|          | 1640/200000 [35:55<76:58:31,  1.40s/it, loss=0.1100, lr=8.20e-06, step=1640]Training:   1%|          | 1641/200000 [35:57<71:45:18,  1.30s/it, loss=0.1100, lr=8.20e-06, step=1640]Training:   1%|          | 1641/200000 [35:57<71:45:18,  1.30s/it, loss=0.0724, lr=8.20e-06, step=1641]Training:   1%|          | 1642/200000 [35:58<68:10:27,  1.24s/it, loss=0.0724, lr=8.20e-06, step=1641]Training:   1%|          | 1642/200000 [35:58<68:10:27,  1.24s/it, loss=0.0435, lr=8.21e-06, step=1642]Training:   1%|          | 1643/200000 [35:59<71:12:54,  1.29s/it, loss=0.0435, lr=8.21e-06, step=1642]Training:   1%|          | 1643/200000 [35:59<71:12:54,  1.29s/it, loss=0.0385, lr=8.21e-06, step=1643]Training:   1%|          | 1644/200000 [36:00<67:44:16,  1.23s/it, loss=0.0385, lr=8.21e-06, step=1643]Training:   1%|          | 1644/200000 [36:00<67:44:16,  1.23s/it, loss=0.0399, lr=8.22e-06, step=1644]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1645/200000 [36:01<69:09:53,  1.26s/it, loss=0.0399, lr=8.22e-06, step=1644]Training:   1%|          | 1645/200000 [36:01<69:09:53,  1.26s/it, loss=0.0564, lr=8.22e-06, step=1645]Training:   1%|          | 1646/200000 [36:03<66:20:08,  1.20s/it, loss=0.0564, lr=8.22e-06, step=1645]Training:   1%|          | 1646/200000 [36:03<66:20:08,  1.20s/it, loss=0.0605, lr=8.23e-06, step=1646]Training:   1%|          | 1647/200000 [36:04<64:22:39,  1.17s/it, loss=0.0605, lr=8.23e-06, step=1646]Training:   1%|          | 1647/200000 [36:04<64:22:39,  1.17s/it, loss=0.0749, lr=8.23e-06, step=1647]Training:   1%|          | 1648/200000 [36:05<69:11:42,  1.26s/it, loss=0.0749, lr=8.23e-06, step=1647]Training:   1%|          | 1648/200000 [36:05<69:11:42,  1.26s/it, loss=0.0515, lr=8.24e-06, step=1648]Training:   1%|          | 1649/200000 [36:06<71:43:28,  1.30s/it, loss=0.0515, lr=8.24e-06, step=1648]Training:   1%|          | 1649/200000 [36:06<71:43:28,  1.30s/it, loss=0.0554, lr=8.24e-06, step=1649]Training:   1%|          | 1650/200000 [36:08<73:28:12,  1.33s/it, loss=0.0554, lr=8.24e-06, step=1649]Training:   1%|          | 1650/200000 [36:08<73:28:12,  1.33s/it, loss=0.0625, lr=8.25e-06, step=1650]Training:   1%|          | 1651/200000 [36:09<69:21:54,  1.26s/it, loss=0.0625, lr=8.25e-06, step=1650]Training:   1%|          | 1651/200000 [36:09<69:21:54,  1.26s/it, loss=0.0352, lr=8.25e-06, step=1651]Training:   1%|          | 1652/200000 [36:10<66:27:57,  1.21s/it, loss=0.0352, lr=8.25e-06, step=1651]Training:   1%|          | 1652/200000 [36:10<66:27:57,  1.21s/it, loss=0.0569, lr=8.26e-06, step=1652]Training:   1%|          | 1653/200000 [36:11<68:52:46,  1.25s/it, loss=0.0569, lr=8.26e-06, step=1652]Training:   1%|          | 1653/200000 [36:11<68:52:46,  1.25s/it, loss=0.0730, lr=8.26e-06, step=1653]Training:   1%|          | 1654/200000 [36:13<70:06:59,  1.27s/it, loss=0.0730, lr=8.26e-06, step=1653]Training:   1%|          | 1654/200000 [36:13<70:06:59,  1.27s/it, loss=0.0656, lr=8.27e-06, step=1654]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1655/200000 [36:14<66:58:12,  1.22s/it, loss=0.0656, lr=8.27e-06, step=1654]Training:   1%|          | 1655/200000 [36:14<66:58:12,  1.22s/it, loss=0.0499, lr=8.27e-06, step=1655]Training:   1%|          | 1656/200000 [36:15<68:35:52,  1.25s/it, loss=0.0499, lr=8.27e-06, step=1655]Training:   1%|          | 1656/200000 [36:15<68:35:52,  1.25s/it, loss=0.0962, lr=8.28e-06, step=1656]Training:   1%|          | 1657/200000 [36:16<65:57:00,  1.20s/it, loss=0.0962, lr=8.28e-06, step=1656]Training:   1%|          | 1657/200000 [36:16<65:57:00,  1.20s/it, loss=0.0837, lr=8.28e-06, step=1657]Training:   1%|          | 1658/200000 [36:18<68:01:35,  1.23s/it, loss=0.0837, lr=8.28e-06, step=1657]Training:   1%|          | 1658/200000 [36:18<68:01:35,  1.23s/it, loss=0.0416, lr=8.29e-06, step=1658]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1659/200000 [36:19<65:34:23,  1.19s/it, loss=0.0416, lr=8.29e-06, step=1658]Training:   1%|          | 1659/200000 [36:19<65:34:23,  1.19s/it, loss=0.1493, lr=8.29e-06, step=1659]Training:   1%|          | 1660/200000 [36:20<69:47:49,  1.27s/it, loss=0.1493, lr=8.29e-06, step=1659]Training:   1%|          | 1660/200000 [36:20<69:47:49,  1.27s/it, loss=0.0412, lr=8.30e-06, step=1660]Training:   1%|          | 1661/200000 [36:22<72:49:18,  1.32s/it, loss=0.0412, lr=8.30e-06, step=1660]Training:   1%|          | 1661/200000 [36:22<72:49:18,  1.32s/it, loss=0.0444, lr=8.30e-06, step=1661]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1662/200000 [36:23<68:54:05,  1.25s/it, loss=0.0444, lr=8.30e-06, step=1661]Training:   1%|          | 1662/200000 [36:23<68:54:05,  1.25s/it, loss=0.0666, lr=8.31e-06, step=1662]Training:   1%|          | 1663/200000 [36:24<66:05:19,  1.20s/it, loss=0.0666, lr=8.31e-06, step=1662]Training:   1%|          | 1663/200000 [36:24<66:05:19,  1.20s/it, loss=0.2070, lr=8.31e-06, step=1663]Training:   1%|          | 1664/200000 [36:25<69:27:10,  1.26s/it, loss=0.2070, lr=8.31e-06, step=1663]Training:   1%|          | 1664/200000 [36:25<69:27:10,  1.26s/it, loss=0.0573, lr=8.32e-06, step=1664]Training:   1%|          | 1665/200000 [36:27<71:50:21,  1.30s/it, loss=0.0573, lr=8.32e-06, step=1664]Training:   1%|          | 1665/200000 [36:27<71:50:21,  1.30s/it, loss=0.2181, lr=8.32e-06, step=1665]Training:   1%|          | 1666/200000 [36:28<68:11:04,  1.24s/it, loss=0.2181, lr=8.32e-06, step=1665]Training:   1%|          | 1666/200000 [36:28<68:11:04,  1.24s/it, loss=0.0465, lr=8.33e-06, step=1666]Training:   1%|          | 1667/200000 [36:29<71:37:51,  1.30s/it, loss=0.0465, lr=8.33e-06, step=1666]Training:   1%|          | 1667/200000 [36:29<71:37:51,  1.30s/it, loss=0.0501, lr=8.33e-06, step=1667]Training:   1%|          | 1668/200000 [36:30<68:06:22,  1.24s/it, loss=0.0501, lr=8.33e-06, step=1667]Training:   1%|          | 1668/200000 [36:30<68:06:22,  1.24s/it, loss=0.0347, lr=8.34e-06, step=1668]Training:   1%|          | 1669/200000 [36:32<70:27:37,  1.28s/it, loss=0.0347, lr=8.34e-06, step=1668]Training:   1%|          | 1669/200000 [36:32<70:27:37,  1.28s/it, loss=0.0422, lr=8.34e-06, step=1669]Training:   1%|          | 1670/200000 [36:33<71:13:40,  1.29s/it, loss=0.0422, lr=8.34e-06, step=1669]Training:   1%|          | 1670/200000 [36:33<71:13:40,  1.29s/it, loss=0.0699, lr=8.35e-06, step=1670]Training:   1%|          | 1671/200000 [36:34<74:47:42,  1.36s/it, loss=0.0699, lr=8.35e-06, step=1670]Training:   1%|          | 1671/200000 [36:34<74:47:42,  1.36s/it, loss=0.0540, lr=8.35e-06, step=1671]Training:   1%|          | 1672/200000 [36:36<77:41:23,  1.41s/it, loss=0.0540, lr=8.35e-06, step=1671]Training:   1%|          | 1672/200000 [36:36<77:41:23,  1.41s/it, loss=0.0351, lr=8.36e-06, step=1672]Training:   1%|          | 1673/200000 [36:37<72:16:21,  1.31s/it, loss=0.0351, lr=8.36e-06, step=1672]Training:   1%|          | 1673/200000 [36:37<72:16:21,  1.31s/it, loss=0.0727, lr=8.36e-06, step=1673]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1674/200000 [36:38<68:34:07,  1.24s/it, loss=0.0727, lr=8.36e-06, step=1673]Training:   1%|          | 1674/200000 [36:38<68:34:07,  1.24s/it, loss=0.0456, lr=8.37e-06, step=1674]Training:   1%|          | 1675/200000 [36:40<72:36:58,  1.32s/it, loss=0.0456, lr=8.37e-06, step=1674]Training:   1%|          | 1675/200000 [36:40<72:36:58,  1.32s/it, loss=0.1017, lr=8.37e-06, step=1675]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1676/200000 [36:41<68:42:40,  1.25s/it, loss=0.1017, lr=8.37e-06, step=1675]Training:   1%|          | 1676/200000 [36:41<68:42:40,  1.25s/it, loss=0.0585, lr=8.38e-06, step=1676]Training:   1%|          | 1677/200000 [36:42<69:55:43,  1.27s/it, loss=0.0585, lr=8.38e-06, step=1676]Training:   1%|          | 1677/200000 [36:42<69:55:43,  1.27s/it, loss=0.0746, lr=8.38e-06, step=1677]Training:   1%|          | 1678/200000 [36:43<66:49:16,  1.21s/it, loss=0.0746, lr=8.38e-06, step=1677]Training:   1%|          | 1678/200000 [36:43<66:49:16,  1.21s/it, loss=0.0695, lr=8.39e-06, step=1678]Training:   1%|          | 1679/200000 [36:44<64:41:34,  1.17s/it, loss=0.0695, lr=8.39e-06, step=1678]Training:   1%|          | 1679/200000 [36:44<64:41:34,  1.17s/it, loss=0.0961, lr=8.39e-06, step=1679]Training:   1%|          | 1680/200000 [36:46<69:27:12,  1.26s/it, loss=0.0961, lr=8.39e-06, step=1679]Training:   1%|          | 1680/200000 [36:46<69:27:12,  1.26s/it, loss=0.0833, lr=8.40e-06, step=1680]Training:   1%|          | 1681/200000 [36:47<72:43:45,  1.32s/it, loss=0.0833, lr=8.40e-06, step=1680]Training:   1%|          | 1681/200000 [36:47<72:43:45,  1.32s/it, loss=0.0866, lr=8.40e-06, step=1681]Training:   1%|          | 1682/200000 [36:48<74:36:24,  1.35s/it, loss=0.0866, lr=8.40e-06, step=1681]Training:   1%|          | 1682/200000 [36:48<74:36:24,  1.35s/it, loss=0.1426, lr=8.41e-06, step=1682]Training:   1%|          | 1683/200000 [36:50<70:13:20,  1.27s/it, loss=0.1426, lr=8.41e-06, step=1682]Training:   1%|          | 1683/200000 [36:50<70:13:20,  1.27s/it, loss=0.0719, lr=8.41e-06, step=1683]Training:   1%|          | 1684/200000 [36:51<67:06:54,  1.22s/it, loss=0.0719, lr=8.41e-06, step=1683]Training:   1%|          | 1684/200000 [36:51<67:06:54,  1.22s/it, loss=0.0691, lr=8.42e-06, step=1684]Training:   1%|          | 1685/200000 [36:52<69:20:26,  1.26s/it, loss=0.0691, lr=8.42e-06, step=1684]Training:   1%|          | 1685/200000 [36:52<69:20:26,  1.26s/it, loss=0.0703, lr=8.42e-06, step=1685]Training:   1%|          | 1686/200000 [36:53<71:30:47,  1.30s/it, loss=0.0703, lr=8.42e-06, step=1685]Training:   1%|          | 1686/200000 [36:53<71:30:47,  1.30s/it, loss=0.0560, lr=8.43e-06, step=1686]Training:   1%|          | 1687/200000 [36:54<67:55:09,  1.23s/it, loss=0.0560, lr=8.43e-06, step=1686]Training:   1%|          | 1687/200000 [36:54<67:55:09,  1.23s/it, loss=0.0568, lr=8.43e-06, step=1687]Training:   1%|          | 1688/200000 [36:56<70:26:10,  1.28s/it, loss=0.0568, lr=8.43e-06, step=1687]Training:   1%|          | 1688/200000 [36:56<70:26:10,  1.28s/it, loss=0.0881, lr=8.44e-06, step=1688]Training:   1%|          | 1689/200000 [36:57<67:08:27,  1.22s/it, loss=0.0881, lr=8.44e-06, step=1688]Training:   1%|          | 1689/200000 [36:57<67:08:27,  1.22s/it, loss=0.0455, lr=8.44e-06, step=1689]Training:   1%|          | 1690/200000 [36:58<69:08:20,  1.26s/it, loss=0.0455, lr=8.44e-06, step=1689]Training:   1%|          | 1690/200000 [36:58<69:08:20,  1.26s/it, loss=0.0368, lr=8.45e-06, step=1690]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1691/200000 [37:00<71:06:56,  1.29s/it, loss=0.0368, lr=8.45e-06, step=1690]Training:   1%|          | 1691/200000 [37:00<71:06:56,  1.29s/it, loss=0.0499, lr=8.45e-06, step=1691]Training:   1%|          | 1692/200000 [37:01<74:19:41,  1.35s/it, loss=0.0499, lr=8.45e-06, step=1691]Training:   1%|          | 1692/200000 [37:01<74:19:41,  1.35s/it, loss=0.1374, lr=8.46e-06, step=1692]Training:   1%|          | 1693/200000 [37:03<77:08:29,  1.40s/it, loss=0.1374, lr=8.46e-06, step=1692]Training:   1%|          | 1693/200000 [37:03<77:08:29,  1.40s/it, loss=0.0356, lr=8.46e-06, step=1693]Training:   1%|          | 1694/200000 [37:04<71:56:27,  1.31s/it, loss=0.0356, lr=8.46e-06, step=1693]Training:   1%|          | 1694/200000 [37:04<71:56:27,  1.31s/it, loss=0.0586, lr=8.47e-06, step=1694]Training:   1%|          | 1695/200000 [37:05<68:15:54,  1.24s/it, loss=0.0586, lr=8.47e-06, step=1694]Training:   1%|          | 1695/200000 [37:05<68:15:54,  1.24s/it, loss=0.0423, lr=8.47e-06, step=1695]Training:   1%|          | 1696/200000 [37:06<72:09:18,  1.31s/it, loss=0.0423, lr=8.47e-06, step=1695]Training:   1%|          | 1696/200000 [37:06<72:09:18,  1.31s/it, loss=0.0906, lr=8.48e-06, step=1696]Training:   1%|          | 1697/200000 [37:07<68:24:29,  1.24s/it, loss=0.0906, lr=8.48e-06, step=1696]Training:   1%|          | 1697/200000 [37:07<68:24:29,  1.24s/it, loss=0.0294, lr=8.48e-06, step=1697]Training:   1%|          | 1698/200000 [37:09<70:22:58,  1.28s/it, loss=0.0294, lr=8.48e-06, step=1697]Training:   1%|          | 1698/200000 [37:09<70:22:58,  1.28s/it, loss=0.0916, lr=8.49e-06, step=1698]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1699/200000 [37:10<67:09:25,  1.22s/it, loss=0.0916, lr=8.49e-06, step=1698]Training:   1%|          | 1699/200000 [37:10<67:09:25,  1.22s/it, loss=0.0351, lr=8.49e-06, step=1699]Training:   1%|          | 1700/200000 [37:11<64:53:29,  1.18s/it, loss=0.0351, lr=8.49e-06, step=1699]Training:   1%|          | 1700/200000 [37:11<64:53:29,  1.18s/it, loss=0.0506, lr=8.50e-06, step=1700]23:30:26.155 [I] step=1700 loss=0.0763 lr=8.26e-06 grad_norm=0.99 time=126.7s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1701/200000 [37:12<69:30:42,  1.26s/it, loss=0.0506, lr=8.50e-06, step=1700]Training:   1%|          | 1701/200000 [37:12<69:30:42,  1.26s/it, loss=0.1403, lr=8.50e-06, step=1701]Training:   1%|          | 1702/200000 [37:14<72:07:51,  1.31s/it, loss=0.1403, lr=8.50e-06, step=1701]Training:   1%|          | 1702/200000 [37:14<72:07:51,  1.31s/it, loss=0.1025, lr=8.51e-06, step=1702]Training:   1%|          | 1703/200000 [37:15<73:31:04,  1.33s/it, loss=0.1025, lr=8.51e-06, step=1702]Training:   1%|          | 1703/200000 [37:15<73:31:04,  1.33s/it, loss=0.0571, lr=8.51e-06, step=1703]Training:   1%|          | 1704/200000 [37:16<69:20:03,  1.26s/it, loss=0.0571, lr=8.51e-06, step=1703]Training:   1%|          | 1704/200000 [37:16<69:20:03,  1.26s/it, loss=0.0540, lr=8.52e-06, step=1704]Training:   1%|          | 1705/200000 [37:17<66:25:25,  1.21s/it, loss=0.0540, lr=8.52e-06, step=1704]Training:   1%|          | 1705/200000 [37:17<66:25:25,  1.21s/it, loss=0.0732, lr=8.52e-06, step=1705]Training:   1%|          | 1706/200000 [37:19<68:54:20,  1.25s/it, loss=0.0732, lr=8.52e-06, step=1705]Training:   1%|          | 1706/200000 [37:19<68:54:20,  1.25s/it, loss=0.0683, lr=8.53e-06, step=1706]Training:   1%|          | 1707/200000 [37:20<69:50:47,  1.27s/it, loss=0.0683, lr=8.53e-06, step=1706]Training:   1%|          | 1707/200000 [37:20<69:50:47,  1.27s/it, loss=0.0890, lr=8.53e-06, step=1707]Training:   1%|          | 1708/200000 [37:21<66:49:25,  1.21s/it, loss=0.0890, lr=8.53e-06, step=1707]Training:   1%|          | 1708/200000 [37:21<66:49:25,  1.21s/it, loss=0.0993, lr=8.54e-06, step=1708]Training:   1%|          | 1709/200000 [37:22<68:52:10,  1.25s/it, loss=0.0993, lr=8.54e-06, step=1708]Training:   1%|          | 1709/200000 [37:22<68:52:10,  1.25s/it, loss=0.0489, lr=8.54e-06, step=1709]Training:   1%|          | 1710/200000 [37:23<66:07:54,  1.20s/it, loss=0.0489, lr=8.54e-06, step=1709]Training:   1%|          | 1710/200000 [37:23<66:07:54,  1.20s/it, loss=0.0410, lr=8.55e-06, step=1710]Training:   1%|          | 1711/200000 [37:25<67:40:31,  1.23s/it, loss=0.0410, lr=8.55e-06, step=1710]Training:   1%|          | 1711/200000 [37:25<67:40:31,  1.23s/it, loss=0.0450, lr=8.55e-06, step=1711]Training:   1%|          | 1712/200000 [37:26<65:16:10,  1.18s/it, loss=0.0450, lr=8.55e-06, step=1711]Training:   1%|          | 1712/200000 [37:26<65:16:10,  1.18s/it, loss=0.1079, lr=8.56e-06, step=1712]Training:   1%|          | 1713/200000 [37:27<70:18:40,  1.28s/it, loss=0.1079, lr=8.56e-06, step=1712]Training:   1%|          | 1713/200000 [37:27<70:18:40,  1.28s/it, loss=0.1010, lr=8.56e-06, step=1713]Training:   1%|          | 1714/200000 [37:29<72:37:20,  1.32s/it, loss=0.1010, lr=8.56e-06, step=1713]Training:   1%|          | 1714/200000 [37:29<72:37:20,  1.32s/it, loss=0.1574, lr=8.57e-06, step=1714]Training:   1%|          | 1715/200000 [37:30<68:44:22,  1.25s/it, loss=0.1574, lr=8.57e-06, step=1714]Training:   1%|          | 1715/200000 [37:30<68:44:22,  1.25s/it, loss=0.0449, lr=8.57e-06, step=1715]Training:   1%|          | 1716/200000 [37:31<65:59:50,  1.20s/it, loss=0.0449, lr=8.57e-06, step=1715]Training:   1%|          | 1716/200000 [37:31<65:59:50,  1.20s/it, loss=0.0517, lr=8.58e-06, step=1716]Training:   1%|          | 1717/200000 [37:32<68:42:29,  1.25s/it, loss=0.0517, lr=8.58e-06, step=1716]Training:   1%|          | 1717/200000 [37:32<68:42:29,  1.25s/it, loss=0.0640, lr=8.58e-06, step=1717]Training:   1%|          | 1718/200000 [37:34<71:38:54,  1.30s/it, loss=0.0640, lr=8.58e-06, step=1717]Training:   1%|          | 1718/200000 [37:34<71:38:54,  1.30s/it, loss=0.0450, lr=8.59e-06, step=1718]Training:   1%|          | 1719/200000 [37:35<68:04:22,  1.24s/it, loss=0.0450, lr=8.59e-06, step=1718]Training:   1%|          | 1719/200000 [37:35<68:04:22,  1.24s/it, loss=0.0558, lr=8.59e-06, step=1719]Training:   1%|          | 1720/200000 [37:36<71:24:21,  1.30s/it, loss=0.0558, lr=8.59e-06, step=1719]Training:   1%|          | 1720/200000 [37:36<71:24:21,  1.30s/it, loss=0.0496, lr=8.60e-06, step=1720]Training:   1%|          | 1721/200000 [37:37<67:53:22,  1.23s/it, loss=0.0496, lr=8.60e-06, step=1720]Training:   1%|          | 1721/200000 [37:37<67:53:22,  1.23s/it, loss=0.0796, lr=8.60e-06, step=1721]Training:   1%|          | 1722/200000 [37:39<70:56:57,  1.29s/it, loss=0.0796, lr=8.60e-06, step=1721]Training:   1%|          | 1722/200000 [37:39<70:56:57,  1.29s/it, loss=0.1015, lr=8.61e-06, step=1722]Training:   1%|          | 1723/200000 [37:40<72:40:47,  1.32s/it, loss=0.1015, lr=8.61e-06, step=1722]Training:   1%|          | 1723/200000 [37:40<72:40:47,  1.32s/it, loss=0.0892, lr=8.61e-06, step=1723]Training:   1%|          | 1724/200000 [37:42<75:58:49,  1.38s/it, loss=0.0892, lr=8.61e-06, step=1723]Training:   1%|          | 1724/200000 [37:42<75:58:49,  1.38s/it, loss=0.1077, lr=8.62e-06, step=1724]Training:   1%|          | 1725/200000 [37:43<78:36:26,  1.43s/it, loss=0.1077, lr=8.62e-06, step=1724]Training:   1%|          | 1725/200000 [37:43<78:36:26,  1.43s/it, loss=0.0494, lr=8.62e-06, step=1725]Training:   1%|          | 1726/200000 [37:44<72:56:28,  1.32s/it, loss=0.0494, lr=8.62e-06, step=1725]Training:   1%|          | 1726/200000 [37:44<72:56:28,  1.32s/it, loss=0.0501, lr=8.63e-06, step=1726]Training:   1%|          | 1727/200000 [37:45<68:56:41,  1.25s/it, loss=0.0501, lr=8.63e-06, step=1726]Training:   1%|          | 1727/200000 [37:45<68:56:41,  1.25s/it, loss=0.0553, lr=8.63e-06, step=1727]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1728/200000 [37:47<72:27:01,  1.32s/it, loss=0.0553, lr=8.63e-06, step=1727]Training:   1%|          | 1728/200000 [37:47<72:27:01,  1.32s/it, loss=0.1344, lr=8.64e-06, step=1728]Training:   1%|          | 1729/200000 [37:48<68:34:57,  1.25s/it, loss=0.1344, lr=8.64e-06, step=1728]Training:   1%|          | 1729/200000 [37:48<68:34:57,  1.25s/it, loss=0.0530, lr=8.64e-06, step=1729]Training:   1%|          | 1730/200000 [37:49<69:34:53,  1.26s/it, loss=0.0530, lr=8.64e-06, step=1729]Training:   1%|          | 1730/200000 [37:49<69:34:53,  1.26s/it, loss=0.0542, lr=8.65e-06, step=1730]Training:   1%|          | 1731/200000 [37:50<66:34:54,  1.21s/it, loss=0.0542, lr=8.65e-06, step=1730]Training:   1%|          | 1731/200000 [37:50<66:34:54,  1.21s/it, loss=0.0319, lr=8.65e-06, step=1731]Training:   1%|          | 1732/200000 [37:51<64:28:38,  1.17s/it, loss=0.0319, lr=8.65e-06, step=1731]Training:   1%|          | 1732/200000 [37:51<64:28:38,  1.17s/it, loss=0.0398, lr=8.66e-06, step=1732]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1733/200000 [37:53<69:46:33,  1.27s/it, loss=0.0398, lr=8.66e-06, step=1732]Training:   1%|          | 1733/200000 [37:53<69:46:33,  1.27s/it, loss=0.0428, lr=8.66e-06, step=1733]Training:   1%|          | 1734/200000 [37:54<72:57:53,  1.32s/it, loss=0.0428, lr=8.66e-06, step=1733]Training:   1%|          | 1734/200000 [37:54<72:57:53,  1.32s/it, loss=0.2546, lr=8.67e-06, step=1734]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1735/200000 [37:56<74:04:33,  1.35s/it, loss=0.2546, lr=8.67e-06, step=1734]Training:   1%|          | 1735/200000 [37:56<74:04:33,  1.35s/it, loss=0.0560, lr=8.67e-06, step=1735]Training:   1%|          | 1736/200000 [37:57<69:43:23,  1.27s/it, loss=0.0560, lr=8.67e-06, step=1735]Training:   1%|          | 1736/200000 [37:57<69:43:23,  1.27s/it, loss=0.0629, lr=8.68e-06, step=1736]Training:   1%|          | 1737/200000 [37:58<66:39:44,  1.21s/it, loss=0.0629, lr=8.68e-06, step=1736]Training:   1%|          | 1737/200000 [37:58<66:39:44,  1.21s/it, loss=0.0442, lr=8.68e-06, step=1737]Training:   1%|          | 1738/200000 [37:59<69:03:19,  1.25s/it, loss=0.0442, lr=8.68e-06, step=1737]Training:   1%|          | 1738/200000 [37:59<69:03:19,  1.25s/it, loss=0.0562, lr=8.69e-06, step=1738]Training:   1%|          | 1739/200000 [38:01<71:04:12,  1.29s/it, loss=0.0562, lr=8.69e-06, step=1738]Training:   1%|          | 1739/200000 [38:01<71:04:12,  1.29s/it, loss=0.0432, lr=8.69e-06, step=1739]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1740/200000 [38:02<67:37:43,  1.23s/it, loss=0.0432, lr=8.69e-06, step=1739]Training:   1%|          | 1740/200000 [38:02<67:37:43,  1.23s/it, loss=0.0953, lr=8.70e-06, step=1740]Training:   1%|          | 1741/200000 [38:03<70:23:16,  1.28s/it, loss=0.0953, lr=8.70e-06, step=1740]Training:   1%|          | 1741/200000 [38:03<70:23:16,  1.28s/it, loss=0.0824, lr=8.70e-06, step=1741]WARNING:root:Token length (53) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1742/200000 [38:04<67:11:26,  1.22s/it, loss=0.0824, lr=8.70e-06, step=1741]Training:   1%|          | 1742/200000 [38:04<67:11:26,  1.22s/it, loss=0.0452, lr=8.71e-06, step=1742]Training:   1%|          | 1743/200000 [38:06<69:11:12,  1.26s/it, loss=0.0452, lr=8.71e-06, step=1742]Training:   1%|          | 1743/200000 [38:06<69:11:12,  1.26s/it, loss=0.0575, lr=8.71e-06, step=1743]Training:   1%|          | 1744/200000 [38:07<71:06:59,  1.29s/it, loss=0.0575, lr=8.71e-06, step=1743]Training:   1%|          | 1744/200000 [38:07<71:06:59,  1.29s/it, loss=0.0300, lr=8.72e-06, step=1744]Training:   1%|          | 1745/200000 [38:08<74:20:20,  1.35s/it, loss=0.0300, lr=8.72e-06, step=1744]Training:   1%|          | 1745/200000 [38:08<74:20:20,  1.35s/it, loss=0.0607, lr=8.72e-06, step=1745]Training:   1%|          | 1746/200000 [38:10<77:05:04,  1.40s/it, loss=0.0607, lr=8.72e-06, step=1745]Training:   1%|          | 1746/200000 [38:10<77:05:04,  1.40s/it, loss=0.0541, lr=8.73e-06, step=1746]Training:   1%|          | 1747/200000 [38:11<71:52:47,  1.31s/it, loss=0.0541, lr=8.73e-06, step=1746]Training:   1%|          | 1747/200000 [38:11<71:52:47,  1.31s/it, loss=0.1019, lr=8.73e-06, step=1747]Training:   1%|          | 1748/200000 [38:12<68:14:02,  1.24s/it, loss=0.1019, lr=8.73e-06, step=1747]Training:   1%|          | 1748/200000 [38:12<68:14:02,  1.24s/it, loss=0.0729, lr=8.74e-06, step=1748]Training:   1%|          | 1749/200000 [38:13<71:20:19,  1.30s/it, loss=0.0729, lr=8.74e-06, step=1748]Training:   1%|          | 1749/200000 [38:14<71:20:19,  1.30s/it, loss=0.0426, lr=8.74e-06, step=1749]Training:   1%|          | 1750/200000 [38:15<67:50:15,  1.23s/it, loss=0.0426, lr=8.74e-06, step=1749]Training:   1%|          | 1750/200000 [38:15<67:50:15,  1.23s/it, loss=0.0691, lr=8.75e-06, step=1750]Training:   1%|          | 1751/200000 [38:16<69:05:27,  1.25s/it, loss=0.0691, lr=8.75e-06, step=1750]Training:   1%|          | 1751/200000 [38:16<69:05:27,  1.25s/it, loss=0.1084, lr=8.75e-06, step=1751]Training:   1%|          | 1752/200000 [38:17<66:16:06,  1.20s/it, loss=0.1084, lr=8.75e-06, step=1751]Training:   1%|          | 1752/200000 [38:17<66:16:06,  1.20s/it, loss=0.0862, lr=8.76e-06, step=1752]Training:   1%|          | 1753/200000 [38:18<64:18:30,  1.17s/it, loss=0.0862, lr=8.76e-06, step=1752]Training:   1%|          | 1753/200000 [38:18<64:18:30,  1.17s/it, loss=0.0353, lr=8.76e-06, step=1753]Training:   1%|          | 1754/200000 [38:20<68:52:44,  1.25s/it, loss=0.0353, lr=8.76e-06, step=1753]Training:   1%|          | 1754/200000 [38:20<68:52:44,  1.25s/it, loss=0.0544, lr=8.77e-06, step=1754]Training:   1%|          | 1755/200000 [38:21<72:17:17,  1.31s/it, loss=0.0544, lr=8.77e-06, step=1754]Training:   1%|          | 1755/200000 [38:21<72:17:17,  1.31s/it, loss=0.0452, lr=8.77e-06, step=1755]Training:   1%|          | 1756/200000 [38:22<74:13:55,  1.35s/it, loss=0.0452, lr=8.77e-06, step=1755]Training:   1%|          | 1756/200000 [38:22<74:13:55,  1.35s/it, loss=0.0323, lr=8.78e-06, step=1756]Training:   1%|          | 1757/200000 [38:23<69:50:51,  1.27s/it, loss=0.0323, lr=8.78e-06, step=1756]Training:   1%|          | 1757/200000 [38:23<69:50:51,  1.27s/it, loss=0.0664, lr=8.78e-06, step=1757]Training:   1%|          | 1758/200000 [38:25<66:46:17,  1.21s/it, loss=0.0664, lr=8.78e-06, step=1757]Training:   1%|          | 1758/200000 [38:25<66:46:17,  1.21s/it, loss=0.0762, lr=8.79e-06, step=1758]Training:   1%|          | 1759/200000 [38:26<68:22:50,  1.24s/it, loss=0.0762, lr=8.79e-06, step=1758]Training:   1%|          | 1759/200000 [38:26<68:22:50,  1.24s/it, loss=0.0529, lr=8.79e-06, step=1759]Training:   1%|          | 1760/200000 [38:27<69:48:19,  1.27s/it, loss=0.0529, lr=8.79e-06, step=1759]Training:   1%|          | 1760/200000 [38:27<69:48:19,  1.27s/it, loss=0.0316, lr=8.80e-06, step=1760]Training:   1%|          | 1761/200000 [38:28<66:47:14,  1.21s/it, loss=0.0316, lr=8.80e-06, step=1760]Training:   1%|          | 1761/200000 [38:28<66:47:14,  1.21s/it, loss=0.1027, lr=8.80e-06, step=1761]Training:   1%|          | 1762/200000 [38:30<68:33:29,  1.25s/it, loss=0.1027, lr=8.80e-06, step=1761]Training:   1%|          | 1762/200000 [38:30<68:33:29,  1.25s/it, loss=0.0407, lr=8.81e-06, step=1762]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1763/200000 [38:31<65:53:47,  1.20s/it, loss=0.0407, lr=8.81e-06, step=1762]Training:   1%|          | 1763/200000 [38:31<65:53:47,  1.20s/it, loss=0.0557, lr=8.81e-06, step=1763]Training:   1%|          | 1764/200000 [38:32<68:00:31,  1.24s/it, loss=0.0557, lr=8.81e-06, step=1763]Training:   1%|          | 1764/200000 [38:32<68:00:31,  1.24s/it, loss=0.0837, lr=8.82e-06, step=1764]Training:   1%|          | 1765/200000 [38:33<65:28:53,  1.19s/it, loss=0.0837, lr=8.82e-06, step=1764]Training:   1%|          | 1765/200000 [38:33<65:28:53,  1.19s/it, loss=0.0600, lr=8.82e-06, step=1765]Training:   1%|          | 1766/200000 [38:35<69:52:35,  1.27s/it, loss=0.0600, lr=8.82e-06, step=1765]Training:   1%|          | 1766/200000 [38:35<69:52:35,  1.27s/it, loss=0.1069, lr=8.83e-06, step=1766]Training:   1%|          | 1767/200000 [38:36<72:02:25,  1.31s/it, loss=0.1069, lr=8.83e-06, step=1766]Training:   1%|          | 1767/200000 [38:36<72:02:25,  1.31s/it, loss=0.1514, lr=8.83e-06, step=1767]Training:   1%|          | 1768/200000 [38:37<68:18:04,  1.24s/it, loss=0.1514, lr=8.83e-06, step=1767]Training:   1%|          | 1768/200000 [38:37<68:18:04,  1.24s/it, loss=0.0669, lr=8.84e-06, step=1768]Training:   1%|          | 1769/200000 [38:38<65:39:56,  1.19s/it, loss=0.0669, lr=8.84e-06, step=1768]Training:   1%|          | 1769/200000 [38:38<65:39:56,  1.19s/it, loss=0.0972, lr=8.84e-06, step=1769]Training:   1%|          | 1770/200000 [38:39<68:34:40,  1.25s/it, loss=0.0972, lr=8.84e-06, step=1769]Training:   1%|          | 1770/200000 [38:39<68:34:40,  1.25s/it, loss=0.0788, lr=8.85e-06, step=1770]Training:   1%|          | 1771/200000 [38:41<71:16:04,  1.29s/it, loss=0.0788, lr=8.85e-06, step=1770]Training:   1%|          | 1771/200000 [38:41<71:16:04,  1.29s/it, loss=0.0559, lr=8.85e-06, step=1771]Training:   1%|          | 1772/200000 [38:42<67:47:45,  1.23s/it, loss=0.0559, lr=8.85e-06, step=1771]Training:   1%|          | 1772/200000 [38:42<67:47:45,  1.23s/it, loss=0.0455, lr=8.86e-06, step=1772]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1773/200000 [38:43<70:51:45,  1.29s/it, loss=0.0455, lr=8.86e-06, step=1772]Training:   1%|          | 1773/200000 [38:43<70:51:45,  1.29s/it, loss=0.1206, lr=8.86e-06, step=1773]Training:   1%|          | 1774/200000 [38:44<67:28:52,  1.23s/it, loss=0.1206, lr=8.86e-06, step=1773]Training:   1%|          | 1774/200000 [38:44<67:28:52,  1.23s/it, loss=0.0425, lr=8.87e-06, step=1774]Training:   1%|          | 1775/200000 [38:46<70:42:52,  1.28s/it, loss=0.0425, lr=8.87e-06, step=1774]Training:   1%|          | 1775/200000 [38:46<70:42:52,  1.28s/it, loss=0.0463, lr=8.87e-06, step=1775]Training:   1%|          | 1776/200000 [38:47<71:51:52,  1.31s/it, loss=0.0463, lr=8.87e-06, step=1775]Training:   1%|          | 1776/200000 [38:47<71:51:52,  1.31s/it, loss=0.0633, lr=8.88e-06, step=1776]Training:   1%|          | 1777/200000 [38:49<75:15:50,  1.37s/it, loss=0.0633, lr=8.88e-06, step=1776]Training:   1%|          | 1777/200000 [38:49<75:15:50,  1.37s/it, loss=0.0313, lr=8.88e-06, step=1777]Training:   1%|          | 1778/200000 [38:50<77:59:47,  1.42s/it, loss=0.0313, lr=8.88e-06, step=1777]Training:   1%|          | 1778/200000 [38:50<77:59:47,  1.42s/it, loss=0.0942, lr=8.89e-06, step=1778]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1779/200000 [38:51<72:32:37,  1.32s/it, loss=0.0942, lr=8.89e-06, step=1778]Training:   1%|          | 1779/200000 [38:51<72:32:37,  1.32s/it, loss=0.0308, lr=8.89e-06, step=1779]Training:   1%|          | 1780/200000 [38:52<68:42:44,  1.25s/it, loss=0.0308, lr=8.89e-06, step=1779]Training:   1%|          | 1780/200000 [38:52<68:42:44,  1.25s/it, loss=0.0350, lr=8.90e-06, step=1780]Training:   1%|          | 1781/200000 [38:54<72:43:38,  1.32s/it, loss=0.0350, lr=8.90e-06, step=1780]Training:   1%|          | 1781/200000 [38:54<72:43:38,  1.32s/it, loss=0.0479, lr=8.90e-06, step=1781]Training:   1%|          | 1782/200000 [38:55<68:48:03,  1.25s/it, loss=0.0479, lr=8.90e-06, step=1781]Training:   1%|          | 1782/200000 [38:55<68:48:03,  1.25s/it, loss=0.0378, lr=8.91e-06, step=1782]Training:   1%|          | 1783/200000 [38:56<69:56:00,  1.27s/it, loss=0.0378, lr=8.91e-06, step=1782]Training:   1%|          | 1783/200000 [38:56<69:56:00,  1.27s/it, loss=0.0668, lr=8.91e-06, step=1783]Training:   1%|          | 1784/200000 [38:57<66:51:01,  1.21s/it, loss=0.0668, lr=8.91e-06, step=1783]Training:   1%|          | 1784/200000 [38:57<66:51:01,  1.21s/it, loss=0.0482, lr=8.92e-06, step=1784]Training:   1%|          | 1785/200000 [38:59<64:39:47,  1.17s/it, loss=0.0482, lr=8.92e-06, step=1784]Training:   1%|          | 1785/200000 [38:59<64:39:47,  1.17s/it, loss=0.0298, lr=8.92e-06, step=1785]Training:   1%|          | 1786/200000 [39:00<69:49:49,  1.27s/it, loss=0.0298, lr=8.92e-06, step=1785]Training:   1%|          | 1786/200000 [39:00<69:49:49,  1.27s/it, loss=0.0768, lr=8.93e-06, step=1786]Training:   1%|          | 1787/200000 [39:01<73:13:46,  1.33s/it, loss=0.0768, lr=8.93e-06, step=1786]Training:   1%|          | 1787/200000 [39:01<73:13:46,  1.33s/it, loss=0.1326, lr=8.93e-06, step=1787]Training:   1%|          | 1788/200000 [39:03<75:13:25,  1.37s/it, loss=0.1326, lr=8.93e-06, step=1787]Training:   1%|          | 1788/200000 [39:03<75:13:25,  1.37s/it, loss=0.0328, lr=8.94e-06, step=1788]Training:   1%|          | 1789/200000 [39:04<70:35:28,  1.28s/it, loss=0.0328, lr=8.94e-06, step=1788]Training:   1%|          | 1789/200000 [39:04<70:35:28,  1.28s/it, loss=0.0504, lr=8.94e-06, step=1789]Training:   1%|          | 1790/200000 [39:05<67:17:15,  1.22s/it, loss=0.0504, lr=8.94e-06, step=1789]Training:   1%|          | 1790/200000 [39:05<67:17:15,  1.22s/it, loss=0.0527, lr=8.95e-06, step=1790]Training:   1%|          | 1791/200000 [39:06<69:33:00,  1.26s/it, loss=0.0527, lr=8.95e-06, step=1790]Training:   1%|          | 1791/200000 [39:06<69:33:00,  1.26s/it, loss=0.0599, lr=8.95e-06, step=1791]Training:   1%|          | 1792/200000 [39:08<71:39:19,  1.30s/it, loss=0.0599, lr=8.95e-06, step=1791]Training:   1%|          | 1792/200000 [39:08<71:39:19,  1.30s/it, loss=0.0734, lr=8.96e-06, step=1792]Training:   1%|          | 1793/200000 [39:09<68:01:47,  1.24s/it, loss=0.0734, lr=8.96e-06, step=1792]Training:   1%|          | 1793/200000 [39:09<68:01:47,  1.24s/it, loss=0.0797, lr=8.96e-06, step=1793]Training:   1%|          | 1794/200000 [39:10<70:53:06,  1.29s/it, loss=0.0797, lr=8.96e-06, step=1793]Training:   1%|          | 1794/200000 [39:10<70:53:06,  1.29s/it, loss=0.0369, lr=8.97e-06, step=1794]Training:   1%|          | 1795/200000 [39:11<67:30:36,  1.23s/it, loss=0.0369, lr=8.97e-06, step=1794]Training:   1%|          | 1795/200000 [39:11<67:30:36,  1.23s/it, loss=0.0836, lr=8.97e-06, step=1795]Training:   1%|          | 1796/200000 [39:13<69:26:36,  1.26s/it, loss=0.0836, lr=8.97e-06, step=1795]Training:   1%|          | 1796/200000 [39:13<69:26:36,  1.26s/it, loss=0.0561, lr=8.98e-06, step=1796]Training:   1%|          | 1797/200000 [39:14<70:53:24,  1.29s/it, loss=0.0561, lr=8.98e-06, step=1796]Training:   1%|          | 1797/200000 [39:14<70:53:24,  1.29s/it, loss=0.1166, lr=8.98e-06, step=1797]Training:   1%|          | 1798/200000 [39:16<74:17:47,  1.35s/it, loss=0.1166, lr=8.98e-06, step=1797]Training:   1%|          | 1798/200000 [39:16<74:17:47,  1.35s/it, loss=0.3299, lr=8.99e-06, step=1798]Training:   1%|          | 1799/200000 [39:17<77:04:08,  1.40s/it, loss=0.3299, lr=8.99e-06, step=1798]Training:   1%|          | 1799/200000 [39:17<77:04:08,  1.40s/it, loss=0.0529, lr=8.99e-06, step=1799]Training:   1%|          | 1800/200000 [39:18<71:51:02,  1.31s/it, loss=0.0529, lr=8.99e-06, step=1799]Training:   1%|          | 1800/200000 [39:18<71:51:02,  1.31s/it, loss=0.0356, lr=9.00e-06, step=1800]23:32:33.099 [I] step=1800 loss=0.0709 lr=8.76e-06 grad_norm=0.99 time=126.9s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1801/200000 [39:19<68:10:46,  1.24s/it, loss=0.0356, lr=9.00e-06, step=1800]Training:   1%|          | 1801/200000 [39:19<68:10:46,  1.24s/it, loss=0.1214, lr=9.00e-06, step=1801]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1802/200000 [39:21<71:55:05,  1.31s/it, loss=0.1214, lr=9.00e-06, step=1801]Training:   1%|          | 1802/200000 [39:21<71:55:05,  1.31s/it, loss=0.0595, lr=9.01e-06, step=1802]Training:   1%|          | 1803/200000 [39:22<68:12:39,  1.24s/it, loss=0.0595, lr=9.01e-06, step=1802]Training:   1%|          | 1803/200000 [39:22<68:12:39,  1.24s/it, loss=0.1057, lr=9.01e-06, step=1803]Training:   1%|          | 1804/200000 [39:23<69:21:28,  1.26s/it, loss=0.1057, lr=9.01e-06, step=1803]Training:   1%|          | 1804/200000 [39:23<69:21:28,  1.26s/it, loss=0.0258, lr=9.02e-06, step=1804]Training:   1%|          | 1805/200000 [39:24<66:26:03,  1.21s/it, loss=0.0258, lr=9.02e-06, step=1804]Training:   1%|          | 1805/200000 [39:24<66:26:03,  1.21s/it, loss=0.0459, lr=9.02e-06, step=1805]Training:   1%|          | 1806/200000 [39:25<64:25:02,  1.17s/it, loss=0.0459, lr=9.02e-06, step=1805]Training:   1%|          | 1806/200000 [39:25<64:25:02,  1.17s/it, loss=0.0746, lr=9.03e-06, step=1806]Training:   1%|          | 1807/200000 [39:27<69:33:51,  1.26s/it, loss=0.0746, lr=9.03e-06, step=1806]Training:   1%|          | 1807/200000 [39:27<69:33:51,  1.26s/it, loss=0.1329, lr=9.03e-06, step=1807]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1808/200000 [39:28<72:59:26,  1.33s/it, loss=0.1329, lr=9.03e-06, step=1807]Training:   1%|          | 1808/200000 [39:28<72:59:26,  1.33s/it, loss=0.0340, lr=9.04e-06, step=1808]Training:   1%|          | 1809/200000 [39:30<73:29:49,  1.34s/it, loss=0.0340, lr=9.04e-06, step=1808]Training:   1%|          | 1809/200000 [39:30<73:29:49,  1.34s/it, loss=0.0501, lr=9.04e-06, step=1809]Training:   1%|          | 1810/200000 [39:31<69:18:19,  1.26s/it, loss=0.0501, lr=9.04e-06, step=1809]Training:   1%|          | 1810/200000 [39:31<69:18:19,  1.26s/it, loss=0.5852, lr=9.05e-06, step=1810]Training:   1%|          | 1811/200000 [39:32<66:25:00,  1.21s/it, loss=0.5852, lr=9.05e-06, step=1810]Training:   1%|          | 1811/200000 [39:32<66:25:00,  1.21s/it, loss=0.0295, lr=9.05e-06, step=1811]Training:   1%|          | 1812/200000 [39:33<68:41:46,  1.25s/it, loss=0.0295, lr=9.05e-06, step=1811]Training:   1%|          | 1812/200000 [39:33<68:41:46,  1.25s/it, loss=0.0748, lr=9.06e-06, step=1812]Training:   1%|          | 1813/200000 [39:34<70:07:01,  1.27s/it, loss=0.0748, lr=9.06e-06, step=1812]Training:   1%|          | 1813/200000 [39:34<70:07:01,  1.27s/it, loss=0.0653, lr=9.06e-06, step=1813]Training:   1%|          | 1814/200000 [39:36<66:57:43,  1.22s/it, loss=0.0653, lr=9.06e-06, step=1813]Training:   1%|          | 1814/200000 [39:36<66:57:43,  1.22s/it, loss=0.0774, lr=9.07e-06, step=1814]Training:   1%|          | 1815/200000 [39:37<68:59:01,  1.25s/it, loss=0.0774, lr=9.07e-06, step=1814]Training:   1%|          | 1815/200000 [39:37<68:59:01,  1.25s/it, loss=0.0501, lr=9.07e-06, step=1815]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1816/200000 [39:38<66:08:31,  1.20s/it, loss=0.0501, lr=9.07e-06, step=1815]Training:   1%|          | 1816/200000 [39:38<66:08:31,  1.20s/it, loss=0.0259, lr=9.08e-06, step=1816]Training:   1%|          | 1817/200000 [39:39<69:19:23,  1.26s/it, loss=0.0259, lr=9.08e-06, step=1816]Training:   1%|          | 1817/200000 [39:39<69:19:23,  1.26s/it, loss=0.0613, lr=9.08e-06, step=1817]Training:   1%|          | 1818/200000 [39:40<66:24:59,  1.21s/it, loss=0.0613, lr=9.08e-06, step=1817]Training:   1%|          | 1818/200000 [39:40<66:24:59,  1.21s/it, loss=0.0765, lr=9.09e-06, step=1818]Training:   1%|          | 1819/200000 [39:42<70:33:35,  1.28s/it, loss=0.0765, lr=9.09e-06, step=1818]Training:   1%|          | 1819/200000 [39:42<70:33:35,  1.28s/it, loss=0.0353, lr=9.09e-06, step=1819]Training:   1%|          | 1820/200000 [39:43<73:28:20,  1.33s/it, loss=0.0353, lr=9.09e-06, step=1819]Training:   1%|          | 1820/200000 [39:43<73:28:20,  1.33s/it, loss=0.0891, lr=9.10e-06, step=1820]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1821/200000 [39:44<69:20:52,  1.26s/it, loss=0.0891, lr=9.10e-06, step=1820]Training:   1%|          | 1821/200000 [39:44<69:20:52,  1.26s/it, loss=0.0670, lr=9.10e-06, step=1821]Training:   1%|          | 1822/200000 [39:46<66:27:50,  1.21s/it, loss=0.0670, lr=9.10e-06, step=1821]Training:   1%|          | 1822/200000 [39:46<66:27:50,  1.21s/it, loss=0.0679, lr=9.11e-06, step=1822]Training:   1%|          | 1823/200000 [39:47<69:52:07,  1.27s/it, loss=0.0679, lr=9.11e-06, step=1822]Training:   1%|          | 1823/200000 [39:47<69:52:07,  1.27s/it, loss=0.0272, lr=9.11e-06, step=1823]Training:   1%|          | 1824/200000 [39:48<72:12:32,  1.31s/it, loss=0.0272, lr=9.11e-06, step=1823]Training:   1%|          | 1824/200000 [39:48<72:12:32,  1.31s/it, loss=0.0460, lr=9.12e-06, step=1824]Training:   1%|          | 1825/200000 [39:49<68:25:36,  1.24s/it, loss=0.0460, lr=9.12e-06, step=1824]Training:   1%|          | 1825/200000 [39:49<68:25:36,  1.24s/it, loss=0.1305, lr=9.12e-06, step=1825]Training:   1%|          | 1826/200000 [39:51<71:06:02,  1.29s/it, loss=0.1305, lr=9.12e-06, step=1825]Training:   1%|          | 1826/200000 [39:51<71:06:02,  1.29s/it, loss=0.0477, lr=9.13e-06, step=1826]Training:   1%|          | 1827/200000 [39:52<67:38:19,  1.23s/it, loss=0.0477, lr=9.13e-06, step=1826]Training:   1%|          | 1827/200000 [39:52<67:38:19,  1.23s/it, loss=0.0601, lr=9.13e-06, step=1827]Training:   1%|          | 1828/200000 [39:53<69:51:14,  1.27s/it, loss=0.0601, lr=9.13e-06, step=1827]Training:   1%|          | 1828/200000 [39:53<69:51:14,  1.27s/it, loss=0.0416, lr=9.14e-06, step=1828]Training:   1%|          | 1829/200000 [39:55<70:56:46,  1.29s/it, loss=0.0416, lr=9.14e-06, step=1828]Training:   1%|          | 1829/200000 [39:55<70:56:46,  1.29s/it, loss=0.0700, lr=9.14e-06, step=1829]Training:   1%|          | 1830/200000 [39:56<74:30:22,  1.35s/it, loss=0.0700, lr=9.14e-06, step=1829]Training:   1%|          | 1830/200000 [39:56<74:30:22,  1.35s/it, loss=0.0749, lr=9.15e-06, step=1830]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1831/200000 [39:58<77:28:06,  1.41s/it, loss=0.0749, lr=9.15e-06, step=1830]Training:   1%|          | 1831/200000 [39:58<77:28:06,  1.41s/it, loss=0.0408, lr=9.15e-06, step=1831]Training:   1%|          | 1832/200000 [39:59<72:05:48,  1.31s/it, loss=0.0408, lr=9.15e-06, step=1831]Training:   1%|          | 1832/200000 [39:59<72:05:48,  1.31s/it, loss=0.0432, lr=9.16e-06, step=1832]Training:   1%|          | 1833/200000 [40:00<68:21:36,  1.24s/it, loss=0.0432, lr=9.16e-06, step=1832]Training:   1%|          | 1833/200000 [40:00<68:21:36,  1.24s/it, loss=0.1082, lr=9.16e-06, step=1833]Training:   1%|          | 1834/200000 [40:01<72:24:37,  1.32s/it, loss=0.1082, lr=9.16e-06, step=1833]Training:   1%|          | 1834/200000 [40:01<72:24:37,  1.32s/it, loss=0.0355, lr=9.17e-06, step=1834]Training:   1%|          | 1835/200000 [40:02<68:34:21,  1.25s/it, loss=0.0355, lr=9.17e-06, step=1834]Training:   1%|          | 1835/200000 [40:02<68:34:21,  1.25s/it, loss=0.0561, lr=9.17e-06, step=1835]Training:   1%|          | 1836/200000 [40:04<70:13:07,  1.28s/it, loss=0.0561, lr=9.17e-06, step=1835]Training:   1%|          | 1836/200000 [40:04<70:13:07,  1.28s/it, loss=0.0679, lr=9.18e-06, step=1836]Training:   1%|          | 1837/200000 [40:05<67:02:28,  1.22s/it, loss=0.0679, lr=9.18e-06, step=1836]Training:   1%|          | 1837/200000 [40:05<67:02:28,  1.22s/it, loss=0.0474, lr=9.18e-06, step=1837]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1838/200000 [40:06<64:50:27,  1.18s/it, loss=0.0474, lr=9.18e-06, step=1837]Training:   1%|          | 1838/200000 [40:06<64:50:27,  1.18s/it, loss=0.0620, lr=9.19e-06, step=1838]Training:   1%|          | 1839/200000 [40:07<69:12:10,  1.26s/it, loss=0.0620, lr=9.19e-06, step=1838]Training:   1%|          | 1839/200000 [40:07<69:12:10,  1.26s/it, loss=0.0832, lr=9.19e-06, step=1839]Training:   1%|          | 1840/200000 [40:09<72:52:30,  1.32s/it, loss=0.0832, lr=9.19e-06, step=1839]Training:   1%|          | 1840/200000 [40:09<72:52:30,  1.32s/it, loss=0.1018, lr=9.20e-06, step=1840]Training:   1%|          | 1841/200000 [40:10<74:19:28,  1.35s/it, loss=0.1018, lr=9.20e-06, step=1840]Training:   1%|          | 1841/200000 [40:10<74:19:28,  1.35s/it, loss=0.0835, lr=9.20e-06, step=1841]Training:   1%|          | 1842/200000 [40:11<69:55:36,  1.27s/it, loss=0.0835, lr=9.20e-06, step=1841]Training:   1%|          | 1842/200000 [40:11<69:55:36,  1.27s/it, loss=0.0850, lr=9.21e-06, step=1842]Training:   1%|          | 1843/200000 [40:12<66:48:36,  1.21s/it, loss=0.0850, lr=9.21e-06, step=1842]Training:   1%|          | 1843/200000 [40:12<66:48:36,  1.21s/it, loss=0.0416, lr=9.21e-06, step=1843]Training:   1%|          | 1844/200000 [40:14<69:16:18,  1.26s/it, loss=0.0416, lr=9.21e-06, step=1843]Training:   1%|          | 1844/200000 [40:14<69:16:18,  1.26s/it, loss=0.0497, lr=9.22e-06, step=1844]Training:   1%|          | 1845/200000 [40:15<71:35:58,  1.30s/it, loss=0.0497, lr=9.22e-06, step=1844]Training:   1%|          | 1845/200000 [40:15<71:35:58,  1.30s/it, loss=0.0421, lr=9.22e-06, step=1845]Training:   1%|          | 1846/200000 [40:16<68:01:42,  1.24s/it, loss=0.0421, lr=9.22e-06, step=1845]Training:   1%|          | 1846/200000 [40:16<68:01:42,  1.24s/it, loss=0.1021, lr=9.23e-06, step=1846]Training:   1%|          | 1847/200000 [40:18<70:47:16,  1.29s/it, loss=0.1021, lr=9.23e-06, step=1846]Training:   1%|          | 1847/200000 [40:18<70:47:16,  1.29s/it, loss=0.0354, lr=9.23e-06, step=1847]Training:   1%|          | 1848/200000 [40:19<67:25:42,  1.23s/it, loss=0.0354, lr=9.23e-06, step=1847]Training:   1%|          | 1848/200000 [40:19<67:25:42,  1.23s/it, loss=0.0954, lr=9.24e-06, step=1848]Training:   1%|          | 1849/200000 [40:20<70:13:06,  1.28s/it, loss=0.0954, lr=9.24e-06, step=1848]Training:   1%|          | 1849/200000 [40:20<70:13:06,  1.28s/it, loss=0.1174, lr=9.24e-06, step=1849]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1850/200000 [40:21<71:21:49,  1.30s/it, loss=0.1174, lr=9.24e-06, step=1849]Training:   1%|          | 1850/200000 [40:21<71:21:49,  1.30s/it, loss=0.0402, lr=9.25e-06, step=1850]Training:   1%|          | 1851/200000 [40:23<74:27:45,  1.35s/it, loss=0.0402, lr=9.25e-06, step=1850]Training:   1%|          | 1851/200000 [40:23<74:27:45,  1.35s/it, loss=0.0479, lr=9.25e-06, step=1851]Training:   1%|          | 1852/200000 [40:24<77:09:03,  1.40s/it, loss=0.0479, lr=9.25e-06, step=1851]Training:   1%|          | 1852/200000 [40:24<77:09:03,  1.40s/it, loss=0.1002, lr=9.26e-06, step=1852]Training:   1%|          | 1853/200000 [40:26<71:52:20,  1.31s/it, loss=0.1002, lr=9.26e-06, step=1852]Training:   1%|          | 1853/200000 [40:26<71:52:20,  1.31s/it, loss=0.0512, lr=9.26e-06, step=1853]Training:   1%|          | 1854/200000 [40:27<68:12:23,  1.24s/it, loss=0.0512, lr=9.26e-06, step=1853]Training:   1%|          | 1854/200000 [40:27<68:12:23,  1.24s/it, loss=0.0425, lr=9.27e-06, step=1854]Training:   1%|          | 1855/200000 [40:28<72:01:42,  1.31s/it, loss=0.0425, lr=9.27e-06, step=1854]Training:   1%|          | 1855/200000 [40:28<72:01:42,  1.31s/it, loss=0.0516, lr=9.27e-06, step=1855]Training:   1%|          | 1856/200000 [40:29<68:18:16,  1.24s/it, loss=0.0516, lr=9.27e-06, step=1855]Training:   1%|          | 1856/200000 [40:29<68:18:16,  1.24s/it, loss=0.0636, lr=9.28e-06, step=1856]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1857/200000 [40:30<69:14:45,  1.26s/it, loss=0.0636, lr=9.28e-06, step=1856]Training:   1%|          | 1857/200000 [40:30<69:14:45,  1.26s/it, loss=0.0457, lr=9.28e-06, step=1857]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1858/200000 [40:32<66:22:23,  1.21s/it, loss=0.0457, lr=9.28e-06, step=1857]Training:   1%|          | 1858/200000 [40:32<66:22:23,  1.21s/it, loss=0.0403, lr=9.29e-06, step=1858]Training:   1%|          | 1859/200000 [40:33<64:23:16,  1.17s/it, loss=0.0403, lr=9.29e-06, step=1858]Training:   1%|          | 1859/200000 [40:33<64:23:16,  1.17s/it, loss=0.0492, lr=9.29e-06, step=1859]Training:   1%|          | 1860/200000 [40:34<69:17:33,  1.26s/it, loss=0.0492, lr=9.29e-06, step=1859]Training:   1%|          | 1860/200000 [40:34<69:17:33,  1.26s/it, loss=0.0247, lr=9.30e-06, step=1860]Training:   1%|          | 1861/200000 [40:36<72:38:55,  1.32s/it, loss=0.0247, lr=9.30e-06, step=1860]Training:   1%|          | 1861/200000 [40:36<72:38:55,  1.32s/it, loss=0.1255, lr=9.30e-06, step=1861]Training:   1%|          | 1862/200000 [40:37<74:07:50,  1.35s/it, loss=0.1255, lr=9.30e-06, step=1861]Training:   1%|          | 1862/200000 [40:37<74:07:50,  1.35s/it, loss=0.0566, lr=9.31e-06, step=1862]Training:   1%|          | 1863/200000 [40:38<69:49:30,  1.27s/it, loss=0.0566, lr=9.31e-06, step=1862]Training:   1%|          | 1863/200000 [40:38<69:49:30,  1.27s/it, loss=0.0505, lr=9.31e-06, step=1863]Training:   1%|          | 1864/200000 [40:39<66:46:18,  1.21s/it, loss=0.0505, lr=9.31e-06, step=1863]Training:   1%|          | 1864/200000 [40:39<66:46:18,  1.21s/it, loss=0.0390, lr=9.32e-06, step=1864]Training:   1%|          | 1865/200000 [40:41<69:01:44,  1.25s/it, loss=0.0390, lr=9.32e-06, step=1864]Training:   1%|          | 1865/200000 [40:41<69:01:44,  1.25s/it, loss=0.1377, lr=9.32e-06, step=1865]Training:   1%|          | 1866/200000 [40:42<70:06:27,  1.27s/it, loss=0.1377, lr=9.32e-06, step=1865]Training:   1%|          | 1866/200000 [40:42<70:06:27,  1.27s/it, loss=0.0687, lr=9.33e-06, step=1866]Training:   1%|          | 1867/200000 [40:43<66:56:04,  1.22s/it, loss=0.0687, lr=9.33e-06, step=1866]Training:   1%|          | 1867/200000 [40:43<66:56:04,  1.22s/it, loss=0.0438, lr=9.33e-06, step=1867]Training:   1%|          | 1868/200000 [40:44<69:11:46,  1.26s/it, loss=0.0438, lr=9.33e-06, step=1867]Training:   1%|          | 1868/200000 [40:44<69:11:46,  1.26s/it, loss=0.1326, lr=9.34e-06, step=1868]Training:   1%|          | 1869/200000 [40:45<66:16:43,  1.20s/it, loss=0.1326, lr=9.34e-06, step=1868]Training:   1%|          | 1869/200000 [40:45<66:16:43,  1.20s/it, loss=0.0530, lr=9.34e-06, step=1869]Training:   1%|          | 1870/200000 [40:47<69:28:43,  1.26s/it, loss=0.0530, lr=9.34e-06, step=1869]Training:   1%|          | 1870/200000 [40:47<69:28:43,  1.26s/it, loss=0.0942, lr=9.35e-06, step=1870]Training:   1%|          | 1871/200000 [40:48<66:28:20,  1.21s/it, loss=0.0942, lr=9.35e-06, step=1870]Training:   1%|          | 1871/200000 [40:48<66:28:20,  1.21s/it, loss=0.0440, lr=9.35e-06, step=1871]Training:   1%|          | 1872/200000 [40:49<71:13:39,  1.29s/it, loss=0.0440, lr=9.35e-06, step=1871]Training:   1%|          | 1872/200000 [40:49<71:13:39,  1.29s/it, loss=0.0507, lr=9.36e-06, step=1872]Training:   1%|          | 1873/200000 [40:51<72:48:32,  1.32s/it, loss=0.0507, lr=9.36e-06, step=1872]Training:   1%|          | 1873/200000 [40:51<72:48:32,  1.32s/it, loss=0.0702, lr=9.36e-06, step=1873]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1874/200000 [40:52<68:49:02,  1.25s/it, loss=0.0702, lr=9.36e-06, step=1873]Training:   1%|          | 1874/200000 [40:52<68:49:02,  1.25s/it, loss=0.0386, lr=9.37e-06, step=1874]Training:   1%|          | 1875/200000 [40:53<66:00:58,  1.20s/it, loss=0.0386, lr=9.37e-06, step=1874]Training:   1%|          | 1875/200000 [40:53<66:00:58,  1.20s/it, loss=0.0703, lr=9.37e-06, step=1875]Training:   1%|          | 1876/200000 [40:54<69:31:06,  1.26s/it, loss=0.0703, lr=9.37e-06, step=1875]Training:   1%|          | 1876/200000 [40:54<69:31:06,  1.26s/it, loss=0.0446, lr=9.38e-06, step=1876]Training:   1%|          | 1877/200000 [40:56<72:07:18,  1.31s/it, loss=0.0446, lr=9.38e-06, step=1876]Training:   1%|          | 1877/200000 [40:56<72:07:18,  1.31s/it, loss=0.0472, lr=9.38e-06, step=1877]Training:   1%|          | 1878/200000 [40:57<68:20:32,  1.24s/it, loss=0.0472, lr=9.38e-06, step=1877]Training:   1%|          | 1878/200000 [40:57<68:20:32,  1.24s/it, loss=0.0520, lr=9.39e-06, step=1878]Training:   1%|          | 1879/200000 [40:58<71:48:45,  1.30s/it, loss=0.0520, lr=9.39e-06, step=1878]Training:   1%|          | 1879/200000 [40:58<71:48:45,  1.30s/it, loss=0.0584, lr=9.39e-06, step=1879]Training:   1%|          | 1880/200000 [40:59<68:06:59,  1.24s/it, loss=0.0584, lr=9.39e-06, step=1879]Training:   1%|          | 1880/200000 [40:59<68:06:59,  1.24s/it, loss=0.0425, lr=9.40e-06, step=1880]Training:   1%|          | 1881/200000 [41:01<70:08:45,  1.27s/it, loss=0.0425, lr=9.40e-06, step=1880]Training:   1%|          | 1881/200000 [41:01<70:08:45,  1.27s/it, loss=0.0510, lr=9.40e-06, step=1881]Training:   1%|          | 1882/200000 [41:02<71:23:37,  1.30s/it, loss=0.0510, lr=9.40e-06, step=1881]Training:   1%|          | 1882/200000 [41:02<71:23:37,  1.30s/it, loss=0.0932, lr=9.41e-06, step=1882]Training:   1%|          | 1883/200000 [41:04<74:54:16,  1.36s/it, loss=0.0932, lr=9.41e-06, step=1882]Training:   1%|          | 1883/200000 [41:04<74:54:16,  1.36s/it, loss=0.0491, lr=9.41e-06, step=1883]Training:   1%|          | 1884/200000 [41:05<76:48:59,  1.40s/it, loss=0.0491, lr=9.41e-06, step=1883]Training:   1%|          | 1884/200000 [41:05<76:48:59,  1.40s/it, loss=0.0243, lr=9.42e-06, step=1884]Training:   1%|          | 1885/200000 [41:06<71:36:43,  1.30s/it, loss=0.0243, lr=9.42e-06, step=1884]Training:   1%|          | 1885/200000 [41:06<71:36:43,  1.30s/it, loss=0.0511, lr=9.42e-06, step=1885]Training:   1%|          | 1886/200000 [41:07<68:00:29,  1.24s/it, loss=0.0511, lr=9.42e-06, step=1885]Training:   1%|          | 1886/200000 [41:07<68:00:29,  1.24s/it, loss=0.0487, lr=9.43e-06, step=1886]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1887/200000 [41:09<72:12:30,  1.31s/it, loss=0.0487, lr=9.43e-06, step=1886]Training:   1%|          | 1887/200000 [41:09<72:12:30,  1.31s/it, loss=0.0388, lr=9.43e-06, step=1887]Training:   1%|          | 1888/200000 [41:10<68:23:04,  1.24s/it, loss=0.0388, lr=9.43e-06, step=1887]Training:   1%|          | 1888/200000 [41:10<68:23:04,  1.24s/it, loss=0.1656, lr=9.44e-06, step=1888]Training:   1%|          | 1889/200000 [41:11<69:25:14,  1.26s/it, loss=0.1656, lr=9.44e-06, step=1888]Training:   1%|          | 1889/200000 [41:11<69:25:14,  1.26s/it, loss=0.0480, lr=9.44e-06, step=1889]Training:   1%|          | 1890/200000 [41:12<66:26:03,  1.21s/it, loss=0.0480, lr=9.44e-06, step=1889]Training:   1%|          | 1890/200000 [41:12<66:26:03,  1.21s/it, loss=0.0667, lr=9.45e-06, step=1890]Training:   1%|          | 1891/200000 [41:13<64:22:12,  1.17s/it, loss=0.0667, lr=9.45e-06, step=1890]Training:   1%|          | 1891/200000 [41:13<64:22:12,  1.17s/it, loss=0.0379, lr=9.45e-06, step=1891]Training:   1%|          | 1892/200000 [41:15<68:52:51,  1.25s/it, loss=0.0379, lr=9.45e-06, step=1891]Training:   1%|          | 1892/200000 [41:15<68:52:51,  1.25s/it, loss=0.0890, lr=9.46e-06, step=1892]Training:   1%|          | 1893/200000 [41:16<72:33:53,  1.32s/it, loss=0.0890, lr=9.46e-06, step=1892]Training:   1%|          | 1893/200000 [41:16<72:33:53,  1.32s/it, loss=0.0291, lr=9.46e-06, step=1893]Training:   1%|          | 1894/200000 [41:18<74:39:21,  1.36s/it, loss=0.0291, lr=9.46e-06, step=1893]Training:   1%|          | 1894/200000 [41:18<74:39:21,  1.36s/it, loss=0.0490, lr=9.47e-06, step=1894]Training:   1%|          | 1895/200000 [41:19<70:06:30,  1.27s/it, loss=0.0490, lr=9.47e-06, step=1894]Training:   1%|          | 1895/200000 [41:19<70:06:30,  1.27s/it, loss=0.0361, lr=9.47e-06, step=1895]Training:   1%|          | 1896/200000 [41:20<66:57:01,  1.22s/it, loss=0.0361, lr=9.47e-06, step=1895]Training:   1%|          | 1896/200000 [41:20<66:57:01,  1.22s/it, loss=0.0797, lr=9.48e-06, step=1896]Training:   1%|          | 1897/200000 [41:21<69:22:48,  1.26s/it, loss=0.0797, lr=9.48e-06, step=1896]Training:   1%|          | 1897/200000 [41:21<69:22:48,  1.26s/it, loss=0.0599, lr=9.48e-06, step=1897]Training:   1%|          | 1898/200000 [41:23<71:18:53,  1.30s/it, loss=0.0599, lr=9.48e-06, step=1897]Training:   1%|          | 1898/200000 [41:23<71:18:53,  1.30s/it, loss=0.1159, lr=9.49e-06, step=1898]Training:   1%|          | 1899/200000 [41:24<67:47:27,  1.23s/it, loss=0.1159, lr=9.49e-06, step=1898]Training:   1%|          | 1899/200000 [41:24<67:47:27,  1.23s/it, loss=0.0622, lr=9.49e-06, step=1899]Training:   1%|          | 1900/200000 [41:25<70:33:05,  1.28s/it, loss=0.0622, lr=9.49e-06, step=1899]Training:   1%|          | 1900/200000 [41:25<70:33:05,  1.28s/it, loss=0.0484, lr=9.50e-06, step=1900]23:34:39.877 [I] step=1900 loss=0.0683 lr=9.26e-06 grad_norm=1.03 time=126.8s                     (701675:train_pytorch.py:582)
Training:   1%|          | 1901/200000 [41:26<67:14:04,  1.22s/it, loss=0.0484, lr=9.50e-06, step=1900]Training:   1%|          | 1901/200000 [41:26<67:14:04,  1.22s/it, loss=0.0775, lr=9.50e-06, step=1901]Training:   1%|          | 1902/200000 [41:27<70:11:42,  1.28s/it, loss=0.0775, lr=9.50e-06, step=1901]Training:   1%|          | 1902/200000 [41:27<70:11:42,  1.28s/it, loss=0.0383, lr=9.51e-06, step=1902]Training:   1%|          | 1903/200000 [41:29<71:17:31,  1.30s/it, loss=0.0383, lr=9.51e-06, step=1902]Training:   1%|          | 1903/200000 [41:29<71:17:31,  1.30s/it, loss=0.0544, lr=9.51e-06, step=1903]Training:   1%|          | 1904/200000 [41:30<74:26:15,  1.35s/it, loss=0.0544, lr=9.51e-06, step=1903]Training:   1%|          | 1904/200000 [41:30<74:26:15,  1.35s/it, loss=0.0389, lr=9.52e-06, step=1904]Training:   1%|          | 1905/200000 [41:32<76:25:30,  1.39s/it, loss=0.0389, lr=9.52e-06, step=1904]Training:   1%|          | 1905/200000 [41:32<76:25:30,  1.39s/it, loss=0.0709, lr=9.52e-06, step=1905]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1906/200000 [41:33<71:24:58,  1.30s/it, loss=0.0709, lr=9.52e-06, step=1905]Training:   1%|          | 1906/200000 [41:33<71:24:58,  1.30s/it, loss=0.0281, lr=9.53e-06, step=1906]Training:   1%|          | 1907/200000 [41:34<67:53:48,  1.23s/it, loss=0.0281, lr=9.53e-06, step=1906]Training:   1%|          | 1907/200000 [41:34<67:53:48,  1.23s/it, loss=0.0301, lr=9.53e-06, step=1907]Training:   1%|          | 1908/200000 [41:35<71:47:25,  1.30s/it, loss=0.0301, lr=9.53e-06, step=1907]Training:   1%|          | 1908/200000 [41:35<71:47:25,  1.30s/it, loss=0.0723, lr=9.54e-06, step=1908]Training:   1%|          | 1909/200000 [41:36<68:07:36,  1.24s/it, loss=0.0723, lr=9.54e-06, step=1908]Training:   1%|          | 1909/200000 [41:36<68:07:36,  1.24s/it, loss=0.0447, lr=9.54e-06, step=1909]Training:   1%|          | 1910/200000 [41:38<69:43:50,  1.27s/it, loss=0.0447, lr=9.54e-06, step=1909]Training:   1%|          | 1910/200000 [41:38<69:43:50,  1.27s/it, loss=0.0301, lr=9.55e-06, step=1910]Training:   1%|          | 1911/200000 [41:39<66:39:13,  1.21s/it, loss=0.0301, lr=9.55e-06, step=1910]Training:   1%|          | 1911/200000 [41:39<66:39:13,  1.21s/it, loss=0.0328, lr=9.55e-06, step=1911]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1912/200000 [41:40<64:32:00,  1.17s/it, loss=0.0328, lr=9.55e-06, step=1911]Training:   1%|          | 1912/200000 [41:40<64:32:00,  1.17s/it, loss=0.0384, lr=9.56e-06, step=1912]Training:   1%|          | 1913/200000 [41:41<68:44:30,  1.25s/it, loss=0.0384, lr=9.56e-06, step=1912]Training:   1%|          | 1913/200000 [41:41<68:44:30,  1.25s/it, loss=0.0547, lr=9.56e-06, step=1913]Training:   1%|          | 1914/200000 [41:43<71:47:18,  1.30s/it, loss=0.0547, lr=9.56e-06, step=1913]Training:   1%|          | 1914/200000 [41:43<71:47:18,  1.30s/it, loss=0.0423, lr=9.57e-06, step=1914]Training:   1%|          | 1915/200000 [41:44<73:05:55,  1.33s/it, loss=0.0423, lr=9.57e-06, step=1914]Training:   1%|          | 1915/200000 [41:44<73:05:55,  1.33s/it, loss=0.0368, lr=9.57e-06, step=1915]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1916/200000 [41:45<69:03:55,  1.26s/it, loss=0.0368, lr=9.57e-06, step=1915]Training:   1%|          | 1916/200000 [41:45<69:03:55,  1.26s/it, loss=0.0574, lr=9.58e-06, step=1916]Training:   1%|          | 1917/200000 [41:46<66:15:44,  1.20s/it, loss=0.0574, lr=9.58e-06, step=1916]Training:   1%|          | 1917/200000 [41:46<66:15:44,  1.20s/it, loss=0.0650, lr=9.58e-06, step=1917]Training:   1%|          | 1918/200000 [41:48<68:37:42,  1.25s/it, loss=0.0650, lr=9.58e-06, step=1917]Training:   1%|          | 1918/200000 [41:48<68:37:42,  1.25s/it, loss=0.0380, lr=9.59e-06, step=1918]Training:   1%|          | 1919/200000 [41:49<69:57:05,  1.27s/it, loss=0.0380, lr=9.59e-06, step=1918]Training:   1%|          | 1919/200000 [41:49<69:57:05,  1.27s/it, loss=0.0754, lr=9.59e-06, step=1919]Training:   1%|          | 1920/200000 [41:50<66:49:53,  1.21s/it, loss=0.0754, lr=9.59e-06, step=1919]Training:   1%|          | 1920/200000 [41:50<66:49:53,  1.21s/it, loss=0.0722, lr=9.60e-06, step=1920]Training:   1%|          | 1921/200000 [41:52<68:49:21,  1.25s/it, loss=0.0722, lr=9.60e-06, step=1920]Training:   1%|          | 1921/200000 [41:52<68:49:21,  1.25s/it, loss=0.0462, lr=9.60e-06, step=1921]Training:   1%|          | 1922/200000 [41:53<66:02:36,  1.20s/it, loss=0.0462, lr=9.60e-06, step=1921]Training:   1%|          | 1922/200000 [41:53<66:02:36,  1.20s/it, loss=0.0400, lr=9.61e-06, step=1922]Training:   1%|          | 1923/200000 [41:54<69:10:10,  1.26s/it, loss=0.0400, lr=9.61e-06, step=1922]Training:   1%|          | 1923/200000 [41:54<69:10:10,  1.26s/it, loss=0.1468, lr=9.61e-06, step=1923]Training:   1%|          | 1924/200000 [41:55<66:15:56,  1.20s/it, loss=0.1468, lr=9.61e-06, step=1923]Training:   1%|          | 1924/200000 [41:55<66:15:56,  1.20s/it, loss=0.0471, lr=9.62e-06, step=1924]Training:   1%|          | 1925/200000 [41:57<70:54:24,  1.29s/it, loss=0.0471, lr=9.62e-06, step=1924]Training:   1%|          | 1925/200000 [41:57<70:54:24,  1.29s/it, loss=0.0759, lr=9.62e-06, step=1925]Training:   1%|          | 1926/200000 [41:58<72:45:45,  1.32s/it, loss=0.0759, lr=9.62e-06, step=1925]Training:   1%|          | 1926/200000 [41:58<72:45:45,  1.32s/it, loss=0.0813, lr=9.63e-06, step=1926]Training:   1%|          | 1927/200000 [41:59<69:13:25,  1.26s/it, loss=0.0813, lr=9.63e-06, step=1926]Training:   1%|          | 1927/200000 [41:59<69:13:25,  1.26s/it, loss=0.0325, lr=9.63e-06, step=1927]Training:   1%|          | 1928/200000 [42:00<66:20:25,  1.21s/it, loss=0.0325, lr=9.63e-06, step=1927]Training:   1%|          | 1928/200000 [42:00<66:20:25,  1.21s/it, loss=0.0435, lr=9.64e-06, step=1928]Training:   1%|          | 1929/200000 [42:02<69:33:09,  1.26s/it, loss=0.0435, lr=9.64e-06, step=1928]Training:   1%|          | 1929/200000 [42:02<69:33:09,  1.26s/it, loss=0.1513, lr=9.64e-06, step=1929]Training:   1%|          | 1930/200000 [42:03<71:51:55,  1.31s/it, loss=0.1513, lr=9.64e-06, step=1929]Training:   1%|          | 1930/200000 [42:03<71:51:55,  1.31s/it, loss=0.4500, lr=9.65e-06, step=1930]Training:   1%|          | 1931/200000 [42:04<68:03:14,  1.24s/it, loss=0.4500, lr=9.65e-06, step=1930]Training:   1%|          | 1931/200000 [42:04<68:03:14,  1.24s/it, loss=0.0451, lr=9.65e-06, step=1931]Training:   1%|          | 1932/200000 [42:05<71:25:16,  1.30s/it, loss=0.0451, lr=9.65e-06, step=1931]Training:   1%|          | 1932/200000 [42:05<71:25:16,  1.30s/it, loss=0.0562, lr=9.66e-06, step=1932]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1933/200000 [42:07<67:48:27,  1.23s/it, loss=0.0562, lr=9.66e-06, step=1932]Training:   1%|          | 1933/200000 [42:07<67:48:27,  1.23s/it, loss=0.0646, lr=9.66e-06, step=1933]Training:   1%|          | 1934/200000 [42:08<70:35:02,  1.28s/it, loss=0.0646, lr=9.66e-06, step=1933]Training:   1%|          | 1934/200000 [42:08<70:35:02,  1.28s/it, loss=0.0486, lr=9.67e-06, step=1934]Training:   1%|          | 1935/200000 [42:09<71:28:02,  1.30s/it, loss=0.0486, lr=9.67e-06, step=1934]Training:   1%|          | 1935/200000 [42:09<71:28:02,  1.30s/it, loss=0.1399, lr=9.67e-06, step=1935]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1936/200000 [42:11<74:52:09,  1.36s/it, loss=0.1399, lr=9.67e-06, step=1935]Training:   1%|          | 1936/200000 [42:11<74:52:09,  1.36s/it, loss=0.0324, lr=9.68e-06, step=1936]Training:   1%|          | 1937/200000 [42:12<77:11:05,  1.40s/it, loss=0.0324, lr=9.68e-06, step=1936]Training:   1%|          | 1937/200000 [42:12<77:11:05,  1.40s/it, loss=0.0735, lr=9.68e-06, step=1937]Training:   1%|          | 1938/200000 [42:13<71:49:19,  1.31s/it, loss=0.0735, lr=9.68e-06, step=1937]Training:   1%|          | 1938/200000 [42:13<71:49:19,  1.31s/it, loss=0.0652, lr=9.69e-06, step=1938]Training:   1%|          | 1939/200000 [42:14<68:02:25,  1.24s/it, loss=0.0652, lr=9.69e-06, step=1938]Training:   1%|          | 1939/200000 [42:14<68:02:25,  1.24s/it, loss=0.0465, lr=9.69e-06, step=1939]Training:   1%|          | 1940/200000 [42:16<72:11:03,  1.31s/it, loss=0.0465, lr=9.69e-06, step=1939]Training:   1%|          | 1940/200000 [42:16<72:11:03,  1.31s/it, loss=0.0687, lr=9.70e-06, step=1940]Training:   1%|          | 1941/200000 [42:17<68:19:15,  1.24s/it, loss=0.0687, lr=9.70e-06, step=1940]Training:   1%|          | 1941/200000 [42:17<68:19:15,  1.24s/it, loss=0.0590, lr=9.70e-06, step=1941]Training:   1%|          | 1942/200000 [42:18<69:37:47,  1.27s/it, loss=0.0590, lr=9.70e-06, step=1941]Training:   1%|          | 1942/200000 [42:18<69:37:47,  1.27s/it, loss=0.0542, lr=9.71e-06, step=1942]Training:   1%|          | 1943/200000 [42:19<66:31:11,  1.21s/it, loss=0.0542, lr=9.71e-06, step=1942]Training:   1%|          | 1943/200000 [42:19<66:31:11,  1.21s/it, loss=0.0366, lr=9.71e-06, step=1943]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1944/200000 [42:20<64:22:10,  1.17s/it, loss=0.0366, lr=9.71e-06, step=1943]Training:   1%|          | 1944/200000 [42:20<64:22:10,  1.17s/it, loss=0.0493, lr=9.72e-06, step=1944]Training:   1%|          | 1945/200000 [42:22<68:49:35,  1.25s/it, loss=0.0493, lr=9.72e-06, step=1944]Training:   1%|          | 1945/200000 [42:22<68:49:35,  1.25s/it, loss=0.0421, lr=9.72e-06, step=1945]Training:   1%|          | 1946/200000 [42:23<72:29:43,  1.32s/it, loss=0.0421, lr=9.72e-06, step=1945]Training:   1%|          | 1946/200000 [42:23<72:29:43,  1.32s/it, loss=0.0437, lr=9.73e-06, step=1946]Training:   1%|          | 1947/200000 [42:25<74:12:11,  1.35s/it, loss=0.0437, lr=9.73e-06, step=1946]Training:   1%|          | 1947/200000 [42:25<74:12:11,  1.35s/it, loss=0.0835, lr=9.73e-06, step=1947]Training:   1%|          | 1948/200000 [42:26<69:45:30,  1.27s/it, loss=0.0835, lr=9.73e-06, step=1947]Training:   1%|          | 1948/200000 [42:26<69:45:30,  1.27s/it, loss=0.0634, lr=9.74e-06, step=1948]Training:   1%|          | 1949/200000 [42:27<66:38:01,  1.21s/it, loss=0.0634, lr=9.74e-06, step=1948]Training:   1%|          | 1949/200000 [42:27<66:38:01,  1.21s/it, loss=0.0518, lr=9.74e-06, step=1949]Training:   1%|          | 1950/200000 [42:28<69:02:38,  1.26s/it, loss=0.0518, lr=9.74e-06, step=1949]Training:   1%|          | 1950/200000 [42:28<69:02:38,  1.26s/it, loss=0.1045, lr=9.75e-06, step=1950]Training:   1%|          | 1951/200000 [42:30<71:18:57,  1.30s/it, loss=0.1045, lr=9.75e-06, step=1950]Training:   1%|          | 1951/200000 [42:30<71:18:57,  1.30s/it, loss=0.0719, lr=9.75e-06, step=1951]Training:   1%|          | 1952/200000 [42:31<67:43:33,  1.23s/it, loss=0.0719, lr=9.75e-06, step=1951]Training:   1%|          | 1952/200000 [42:31<67:43:33,  1.23s/it, loss=0.0325, lr=9.76e-06, step=1952]Training:   1%|          | 1953/200000 [42:32<70:23:19,  1.28s/it, loss=0.0325, lr=9.76e-06, step=1952]Training:   1%|          | 1953/200000 [42:32<70:23:19,  1.28s/it, loss=0.0824, lr=9.76e-06, step=1953]Training:   1%|          | 1954/200000 [42:33<67:06:27,  1.22s/it, loss=0.0824, lr=9.76e-06, step=1953]Training:   1%|          | 1954/200000 [42:33<67:06:27,  1.22s/it, loss=0.0467, lr=9.77e-06, step=1954]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1955/200000 [42:35<69:54:48,  1.27s/it, loss=0.0467, lr=9.77e-06, step=1954]Training:   1%|          | 1955/200000 [42:35<69:54:48,  1.27s/it, loss=0.0344, lr=9.77e-06, step=1955]Training:   1%|          | 1956/200000 [42:36<71:05:02,  1.29s/it, loss=0.0344, lr=9.77e-06, step=1955]Training:   1%|          | 1956/200000 [42:36<71:05:02,  1.29s/it, loss=0.2726, lr=9.78e-06, step=1956]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1957/200000 [42:37<74:04:24,  1.35s/it, loss=0.2726, lr=9.78e-06, step=1956]Training:   1%|          | 1957/200000 [42:37<74:04:24,  1.35s/it, loss=0.0918, lr=9.78e-06, step=1957]Training:   1%|          | 1958/200000 [42:39<75:49:28,  1.38s/it, loss=0.0918, lr=9.78e-06, step=1957]Training:   1%|          | 1958/200000 [42:39<75:49:28,  1.38s/it, loss=0.0427, lr=9.79e-06, step=1958]Training:   1%|          | 1959/200000 [42:40<70:50:53,  1.29s/it, loss=0.0427, lr=9.79e-06, step=1958]Training:   1%|          | 1959/200000 [42:40<70:50:53,  1.29s/it, loss=0.0560, lr=9.79e-06, step=1959]Training:   1%|          | 1960/200000 [42:41<67:23:13,  1.22s/it, loss=0.0560, lr=9.79e-06, step=1959]Training:   1%|          | 1960/200000 [42:41<67:23:13,  1.22s/it, loss=0.0328, lr=9.80e-06, step=1960]Training:   1%|          | 1961/200000 [42:43<71:24:20,  1.30s/it, loss=0.0328, lr=9.80e-06, step=1960]Training:   1%|          | 1961/200000 [42:43<71:24:20,  1.30s/it, loss=0.0394, lr=9.80e-06, step=1961]Training:   1%|          | 1962/200000 [42:44<67:46:00,  1.23s/it, loss=0.0394, lr=9.80e-06, step=1961]Training:   1%|          | 1962/200000 [42:44<67:46:00,  1.23s/it, loss=0.0513, lr=9.81e-06, step=1962]Training:   1%|          | 1963/200000 [42:45<68:55:11,  1.25s/it, loss=0.0513, lr=9.81e-06, step=1962]Training:   1%|          | 1963/200000 [42:45<68:55:11,  1.25s/it, loss=0.0446, lr=9.81e-06, step=1963]Training:   1%|          | 1964/200000 [42:46<66:01:04,  1.20s/it, loss=0.0446, lr=9.81e-06, step=1963]Training:   1%|          | 1964/200000 [42:46<66:01:04,  1.20s/it, loss=0.0789, lr=9.82e-06, step=1964]Training:   1%|          | 1965/200000 [42:47<63:59:32,  1.16s/it, loss=0.0789, lr=9.82e-06, step=1964]Training:   1%|          | 1965/200000 [42:47<63:59:32,  1.16s/it, loss=0.0245, lr=9.82e-06, step=1965]Training:   1%|          | 1966/200000 [42:49<68:26:50,  1.24s/it, loss=0.0245, lr=9.82e-06, step=1965]Training:   1%|          | 1966/200000 [42:49<68:26:50,  1.24s/it, loss=0.0680, lr=9.83e-06, step=1966]Training:   1%|          | 1967/200000 [42:50<71:59:26,  1.31s/it, loss=0.0680, lr=9.83e-06, step=1966]Training:   1%|          | 1967/200000 [42:50<71:59:26,  1.31s/it, loss=0.0433, lr=9.83e-06, step=1967]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1968/200000 [42:51<73:09:13,  1.33s/it, loss=0.0433, lr=9.83e-06, step=1967]Training:   1%|          | 1968/200000 [42:51<73:09:13,  1.33s/it, loss=0.0593, lr=9.84e-06, step=1968]Training:   1%|          | 1969/200000 [42:52<68:59:53,  1.25s/it, loss=0.0593, lr=9.84e-06, step=1968]Training:   1%|          | 1969/200000 [42:52<68:59:53,  1.25s/it, loss=0.0503, lr=9.84e-06, step=1969]Training:   1%|          | 1970/200000 [42:54<66:05:21,  1.20s/it, loss=0.0503, lr=9.84e-06, step=1969]Training:   1%|          | 1970/200000 [42:54<66:05:21,  1.20s/it, loss=0.0732, lr=9.85e-06, step=1970]Training:   1%|          | 1971/200000 [42:55<68:36:52,  1.25s/it, loss=0.0732, lr=9.85e-06, step=1970]Training:   1%|          | 1971/200000 [42:55<68:36:52,  1.25s/it, loss=0.0394, lr=9.85e-06, step=1971]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1972/200000 [42:56<70:01:16,  1.27s/it, loss=0.0394, lr=9.85e-06, step=1971]Training:   1%|          | 1972/200000 [42:56<70:01:16,  1.27s/it, loss=0.0576, lr=9.86e-06, step=1972]Training:   1%|          | 1973/200000 [42:57<66:48:59,  1.21s/it, loss=0.0576, lr=9.86e-06, step=1972]Training:   1%|          | 1973/200000 [42:57<66:48:59,  1.21s/it, loss=0.0388, lr=9.86e-06, step=1973]Training:   1%|          | 1974/200000 [42:59<68:39:31,  1.25s/it, loss=0.0388, lr=9.86e-06, step=1973]Training:   1%|          | 1974/200000 [42:59<68:39:31,  1.25s/it, loss=0.0640, lr=9.87e-06, step=1974]Training:   1%|          | 1975/200000 [43:00<65:48:56,  1.20s/it, loss=0.0640, lr=9.87e-06, step=1974]Training:   1%|          | 1975/200000 [43:00<65:48:56,  1.20s/it, loss=0.0504, lr=9.87e-06, step=1975]Training:   1%|          | 1976/200000 [43:01<68:07:41,  1.24s/it, loss=0.0504, lr=9.87e-06, step=1975]Training:   1%|          | 1976/200000 [43:01<68:07:41,  1.24s/it, loss=0.0637, lr=9.88e-06, step=1976]Training:   1%|          | 1977/200000 [43:02<65:25:43,  1.19s/it, loss=0.0637, lr=9.88e-06, step=1976]Training:   1%|          | 1977/200000 [43:02<65:25:43,  1.19s/it, loss=0.0484, lr=9.88e-06, step=1977]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1978/200000 [43:04<70:12:05,  1.28s/it, loss=0.0484, lr=9.88e-06, step=1977]Training:   1%|          | 1978/200000 [43:04<70:12:05,  1.28s/it, loss=0.0701, lr=9.89e-06, step=1978]Training:   1%|          | 1979/200000 [43:05<73:01:05,  1.33s/it, loss=0.0701, lr=9.89e-06, step=1978]Training:   1%|          | 1979/200000 [43:05<73:01:05,  1.33s/it, loss=0.0416, lr=9.89e-06, step=1979]Training:   1%|          | 1980/200000 [43:06<68:55:33,  1.25s/it, loss=0.0416, lr=9.89e-06, step=1979]Training:   1%|          | 1980/200000 [43:06<68:55:33,  1.25s/it, loss=0.0566, lr=9.90e-06, step=1980]Training:   1%|          | 1981/200000 [43:07<65:59:42,  1.20s/it, loss=0.0566, lr=9.90e-06, step=1980]Training:   1%|          | 1981/200000 [43:07<65:59:42,  1.20s/it, loss=0.0339, lr=9.90e-06, step=1981]Training:   1%|          | 1982/200000 [43:09<69:20:48,  1.26s/it, loss=0.0339, lr=9.90e-06, step=1981]Training:   1%|          | 1982/200000 [43:09<69:20:48,  1.26s/it, loss=0.0439, lr=9.91e-06, step=1982]Training:   1%|          | 1983/200000 [43:10<72:10:40,  1.31s/it, loss=0.0439, lr=9.91e-06, step=1982]Training:   1%|          | 1983/200000 [43:10<72:10:40,  1.31s/it, loss=0.0506, lr=9.91e-06, step=1983]Training:   1%|          | 1984/200000 [43:11<68:18:33,  1.24s/it, loss=0.0506, lr=9.91e-06, step=1983]Training:   1%|          | 1984/200000 [43:11<68:18:33,  1.24s/it, loss=0.0733, lr=9.92e-06, step=1984]Training:   1%|          | 1985/200000 [43:13<71:31:53,  1.30s/it, loss=0.0733, lr=9.92e-06, step=1984]Training:   1%|          | 1985/200000 [43:13<71:31:53,  1.30s/it, loss=0.0444, lr=9.92e-06, step=1985]Training:   1%|          | 1986/200000 [43:14<67:51:54,  1.23s/it, loss=0.0444, lr=9.92e-06, step=1985]Training:   1%|          | 1986/200000 [43:14<67:51:54,  1.23s/it, loss=0.0916, lr=9.93e-06, step=1986]Training:   1%|          | 1987/200000 [43:15<69:56:11,  1.27s/it, loss=0.0916, lr=9.93e-06, step=1986]Training:   1%|          | 1987/200000 [43:15<69:56:11,  1.27s/it, loss=0.0432, lr=9.93e-06, step=1987]Training:   1%|          | 1988/200000 [43:16<71:33:22,  1.30s/it, loss=0.0432, lr=9.93e-06, step=1987]Training:   1%|          | 1988/200000 [43:16<71:33:22,  1.30s/it, loss=0.0829, lr=9.94e-06, step=1988]Training:   1%|          | 1989/200000 [43:18<75:01:06,  1.36s/it, loss=0.0829, lr=9.94e-06, step=1988]Training:   1%|          | 1989/200000 [43:18<75:01:06,  1.36s/it, loss=0.0383, lr=9.94e-06, step=1989]Training:   1%|          | 1990/200000 [43:19<77:44:54,  1.41s/it, loss=0.0383, lr=9.94e-06, step=1989]Training:   1%|          | 1990/200000 [43:19<77:44:54,  1.41s/it, loss=0.0506, lr=9.95e-06, step=1990]Training:   1%|          | 1991/200000 [43:20<72:12:18,  1.31s/it, loss=0.0506, lr=9.95e-06, step=1990]Training:   1%|          | 1991/200000 [43:20<72:12:18,  1.31s/it, loss=0.0279, lr=9.95e-06, step=1991]Training:   1%|          | 1992/200000 [43:22<68:19:12,  1.24s/it, loss=0.0279, lr=9.95e-06, step=1991]Training:   1%|          | 1992/200000 [43:22<68:19:12,  1.24s/it, loss=0.0392, lr=9.96e-06, step=1992]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 1993/200000 [43:23<72:20:59,  1.32s/it, loss=0.0392, lr=9.96e-06, step=1992]Training:   1%|          | 1993/200000 [43:23<72:20:59,  1.32s/it, loss=0.0711, lr=9.96e-06, step=1993]Training:   1%|          | 1994/200000 [43:24<68:24:32,  1.24s/it, loss=0.0711, lr=9.96e-06, step=1993]Training:   1%|          | 1994/200000 [43:24<68:24:32,  1.24s/it, loss=0.0375, lr=9.97e-06, step=1994]Training:   1%|          | 1995/200000 [43:25<69:30:40,  1.26s/it, loss=0.0375, lr=9.97e-06, step=1994]Training:   1%|          | 1995/200000 [43:25<69:30:40,  1.26s/it, loss=0.0631, lr=9.97e-06, step=1995]Training:   1%|          | 1996/200000 [43:26<66:27:51,  1.21s/it, loss=0.0631, lr=9.97e-06, step=1995]Training:   1%|          | 1996/200000 [43:26<66:27:51,  1.21s/it, loss=0.0890, lr=9.98e-06, step=1996]Training:   1%|          | 1997/200000 [43:28<64:18:48,  1.17s/it, loss=0.0890, lr=9.98e-06, step=1996]Training:   1%|          | 1997/200000 [43:28<64:18:48,  1.17s/it, loss=0.0400, lr=9.98e-06, step=1997]Training:   1%|          | 1998/200000 [43:29<69:21:33,  1.26s/it, loss=0.0400, lr=9.98e-06, step=1997]Training:   1%|          | 1998/200000 [43:29<69:21:33,  1.26s/it, loss=0.0428, lr=9.99e-06, step=1998]Training:   1%|          | 1999/200000 [43:30<72:19:52,  1.32s/it, loss=0.0428, lr=9.99e-06, step=1998]Training:   1%|          | 1999/200000 [43:30<72:19:52,  1.32s/it, loss=0.3829, lr=9.99e-06, step=1999]Training:   1%|          | 2000/200000 [43:32<73:58:44,  1.35s/it, loss=0.3829, lr=9.99e-06, step=1999]Training:   1%|          | 2000/200000 [43:32<73:58:44,  1.35s/it, loss=0.2249, lr=1.00e-05, step=2000]23:36:46.773 [I] step=2000 loss=0.0677 lr=9.76e-06 grad_norm=0.99 time=126.9s                     (701675:train_pytorch.py:582)
Training:   1%|          | 2001/200000 [43:33<69:34:19,  1.26s/it, loss=0.2249, lr=1.00e-05, step=2000]Training:   1%|          | 2001/200000 [43:33<69:34:19,  1.26s/it, loss=0.1043, lr=1.00e-05, step=2001]Training:   1%|          | 2002/200000 [43:34<66:29:54,  1.21s/it, loss=0.1043, lr=1.00e-05, step=2001]Training:   1%|          | 2002/200000 [43:34<66:29:54,  1.21s/it, loss=0.0420, lr=1.00e-05, step=2002]Training:   1%|          | 2003/200000 [43:35<68:55:40,  1.25s/it, loss=0.0420, lr=1.00e-05, step=2002]Training:   1%|          | 2003/200000 [43:35<68:55:40,  1.25s/it, loss=0.0949, lr=1.00e-05, step=2003]Training:   1%|          | 2004/200000 [43:37<71:05:56,  1.29s/it, loss=0.0949, lr=1.00e-05, step=2003]Training:   1%|          | 2004/200000 [43:37<71:05:56,  1.29s/it, loss=0.0737, lr=1.00e-05, step=2004]Training:   1%|          | 2005/200000 [43:38<67:32:04,  1.23s/it, loss=0.0737, lr=1.00e-05, step=2004]Training:   1%|          | 2005/200000 [43:38<67:32:04,  1.23s/it, loss=0.0500, lr=1.00e-05, step=2005]Training:   1%|          | 2006/200000 [43:39<70:08:22,  1.28s/it, loss=0.0500, lr=1.00e-05, step=2005]Training:   1%|          | 2006/200000 [43:39<70:08:22,  1.28s/it, loss=0.0405, lr=1.00e-05, step=2006]Training:   1%|          | 2007/200000 [43:40<66:51:03,  1.22s/it, loss=0.0405, lr=1.00e-05, step=2006]Training:   1%|          | 2007/200000 [43:40<66:51:03,  1.22s/it, loss=0.0438, lr=1.00e-05, step=2007]Training:   1%|          | 2008/200000 [43:42<69:34:16,  1.26s/it, loss=0.0438, lr=1.00e-05, step=2007]Training:   1%|          | 2008/200000 [43:42<69:34:16,  1.26s/it, loss=0.0468, lr=1.00e-05, step=2008]Training:   1%|          | 2009/200000 [43:43<71:15:00,  1.30s/it, loss=0.0468, lr=1.00e-05, step=2008]Training:   1%|          | 2009/200000 [43:43<71:15:00,  1.30s/it, loss=0.0327, lr=1.00e-05, step=2009]Training:   1%|          | 2010/200000 [43:45<74:18:16,  1.35s/it, loss=0.0327, lr=1.00e-05, step=2009]Training:   1%|          | 2010/200000 [43:45<74:18:16,  1.35s/it, loss=0.0455, lr=1.00e-05, step=2010]Training:   1%|          | 2011/200000 [43:46<76:56:28,  1.40s/it, loss=0.0455, lr=1.00e-05, step=2010]Training:   1%|          | 2011/200000 [43:46<76:56:28,  1.40s/it, loss=0.1361, lr=1.01e-05, step=2011]Training:   1%|          | 2012/200000 [43:47<71:39:38,  1.30s/it, loss=0.1361, lr=1.01e-05, step=2011]Training:   1%|          | 2012/200000 [43:47<71:39:38,  1.30s/it, loss=0.0573, lr=1.01e-05, step=2012]Training:   1%|          | 2013/200000 [43:48<67:57:33,  1.24s/it, loss=0.0573, lr=1.01e-05, step=2012]Training:   1%|          | 2013/200000 [43:48<67:57:33,  1.24s/it, loss=0.1003, lr=1.01e-05, step=2013]Training:   1%|          | 2014/200000 [43:50<71:20:11,  1.30s/it, loss=0.1003, lr=1.01e-05, step=2013]Training:   1%|          | 2014/200000 [43:50<71:20:11,  1.30s/it, loss=0.0609, lr=1.01e-05, step=2014]Training:   1%|          | 2015/200000 [43:51<67:40:49,  1.23s/it, loss=0.0609, lr=1.01e-05, step=2014]Training:   1%|          | 2015/200000 [43:51<67:40:49,  1.23s/it, loss=0.0830, lr=1.01e-05, step=2015]Training:   1%|          | 2016/200000 [43:52<68:59:38,  1.25s/it, loss=0.0830, lr=1.01e-05, step=2015]Training:   1%|          | 2016/200000 [43:52<68:59:38,  1.25s/it, loss=0.0527, lr=1.01e-05, step=2016]Training:   1%|          | 2017/200000 [43:53<66:02:55,  1.20s/it, loss=0.0527, lr=1.01e-05, step=2016]Training:   1%|          | 2017/200000 [43:53<66:02:55,  1.20s/it, loss=0.0399, lr=1.01e-05, step=2017]Training:   1%|          | 2018/200000 [43:54<64:02:22,  1.16s/it, loss=0.0399, lr=1.01e-05, step=2017]Training:   1%|          | 2018/200000 [43:54<64:02:22,  1.16s/it, loss=0.1016, lr=1.01e-05, step=2018]Training:   1%|          | 2019/200000 [43:56<68:16:14,  1.24s/it, loss=0.1016, lr=1.01e-05, step=2018]Training:   1%|          | 2019/200000 [43:56<68:16:14,  1.24s/it, loss=0.0289, lr=1.01e-05, step=2019]Training:   1%|          | 2020/200000 [43:57<71:54:45,  1.31s/it, loss=0.0289, lr=1.01e-05, step=2019]Training:   1%|          | 2020/200000 [43:57<71:54:45,  1.31s/it, loss=0.0502, lr=1.01e-05, step=2020]Training:   1%|          | 2021/200000 [43:58<73:17:07,  1.33s/it, loss=0.0502, lr=1.01e-05, step=2020]Training:   1%|          | 2021/200000 [43:58<73:17:07,  1.33s/it, loss=0.0349, lr=1.01e-05, step=2021]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2022/200000 [44:00<69:05:34,  1.26s/it, loss=0.0349, lr=1.01e-05, step=2021]Training:   1%|          | 2022/200000 [44:00<69:05:34,  1.26s/it, loss=0.0942, lr=1.01e-05, step=2022]Training:   1%|          | 2023/200000 [44:01<66:04:25,  1.20s/it, loss=0.0942, lr=1.01e-05, step=2022]Training:   1%|          | 2023/200000 [44:01<66:04:25,  1.20s/it, loss=0.0410, lr=1.01e-05, step=2023]Training:   1%|          | 2024/200000 [44:02<68:04:26,  1.24s/it, loss=0.0410, lr=1.01e-05, step=2023]Training:   1%|          | 2024/200000 [44:02<68:04:26,  1.24s/it, loss=0.0364, lr=1.01e-05, step=2024]Training:   1%|          | 2025/200000 [44:03<69:45:57,  1.27s/it, loss=0.0364, lr=1.01e-05, step=2024]Training:   1%|          | 2025/200000 [44:03<69:45:57,  1.27s/it, loss=0.0412, lr=1.01e-05, step=2025]Training:   1%|          | 2026/200000 [44:04<66:37:38,  1.21s/it, loss=0.0412, lr=1.01e-05, step=2025]Training:   1%|          | 2026/200000 [44:04<66:37:38,  1.21s/it, loss=0.0567, lr=1.01e-05, step=2026]Training:   1%|          | 2027/200000 [44:06<68:42:07,  1.25s/it, loss=0.0567, lr=1.01e-05, step=2026]Training:   1%|          | 2027/200000 [44:06<68:42:07,  1.25s/it, loss=0.0673, lr=1.01e-05, step=2027]Training:   1%|          | 2028/200000 [44:07<65:53:02,  1.20s/it, loss=0.0673, lr=1.01e-05, step=2027]Training:   1%|          | 2028/200000 [44:07<65:53:02,  1.20s/it, loss=0.0365, lr=1.01e-05, step=2028]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2029/200000 [44:08<68:48:14,  1.25s/it, loss=0.0365, lr=1.01e-05, step=2028]Training:   1%|          | 2029/200000 [44:08<68:48:14,  1.25s/it, loss=0.0620, lr=1.01e-05, step=2029]Training:   1%|          | 2030/200000 [44:09<65:53:01,  1.20s/it, loss=0.0620, lr=1.01e-05, step=2029]Training:   1%|          | 2030/200000 [44:09<65:53:01,  1.20s/it, loss=0.0460, lr=1.01e-05, step=2030]Training:   1%|          | 2031/200000 [44:11<69:32:07,  1.26s/it, loss=0.0460, lr=1.01e-05, step=2030]Training:   1%|          | 2031/200000 [44:11<69:32:07,  1.26s/it, loss=0.0456, lr=1.02e-05, step=2031]Training:   1%|          | 2032/200000 [44:12<72:28:59,  1.32s/it, loss=0.0456, lr=1.02e-05, step=2031]Training:   1%|          | 2032/200000 [44:12<72:28:59,  1.32s/it, loss=0.0499, lr=1.02e-05, step=2032]Training:   1%|          | 2033/200000 [44:13<68:33:37,  1.25s/it, loss=0.0499, lr=1.02e-05, step=2032]Training:   1%|          | 2033/200000 [44:13<68:33:37,  1.25s/it, loss=0.0593, lr=1.02e-05, step=2033]Training:   1%|          | 2034/200000 [44:14<65:46:53,  1.20s/it, loss=0.0593, lr=1.02e-05, step=2033]Training:   1%|          | 2034/200000 [44:14<65:46:53,  1.20s/it, loss=0.0461, lr=1.02e-05, step=2034]Training:   1%|          | 2035/200000 [44:16<69:12:42,  1.26s/it, loss=0.0461, lr=1.02e-05, step=2034]Training:   1%|          | 2035/200000 [44:16<69:12:42,  1.26s/it, loss=0.1197, lr=1.02e-05, step=2035]Training:   1%|          | 2036/200000 [44:17<72:18:57,  1.32s/it, loss=0.1197, lr=1.02e-05, step=2035]Training:   1%|          | 2036/200000 [44:17<72:18:57,  1.32s/it, loss=0.0471, lr=1.02e-05, step=2036]Training:   1%|          | 2037/200000 [44:18<68:21:57,  1.24s/it, loss=0.0471, lr=1.02e-05, step=2036]Training:   1%|          | 2037/200000 [44:18<68:21:57,  1.24s/it, loss=0.0767, lr=1.02e-05, step=2037]Training:   1%|          | 2038/200000 [44:20<71:01:00,  1.29s/it, loss=0.0767, lr=1.02e-05, step=2037]Training:   1%|          | 2038/200000 [44:20<71:01:00,  1.29s/it, loss=0.1353, lr=1.02e-05, step=2038]Training:   1%|          | 2039/200000 [44:21<67:27:34,  1.23s/it, loss=0.1353, lr=1.02e-05, step=2038]Training:   1%|          | 2039/200000 [44:21<67:27:34,  1.23s/it, loss=0.0484, lr=1.02e-05, step=2039]Training:   1%|          | 2040/200000 [44:22<70:23:03,  1.28s/it, loss=0.0484, lr=1.02e-05, step=2039]Training:   1%|          | 2040/200000 [44:22<70:23:03,  1.28s/it, loss=0.0585, lr=1.02e-05, step=2040]Training:   1%|          | 2041/200000 [44:23<72:00:40,  1.31s/it, loss=0.0585, lr=1.02e-05, step=2040]Training:   1%|          | 2041/200000 [44:23<72:00:40,  1.31s/it, loss=0.0716, lr=1.02e-05, step=2041]Training:   1%|          | 2042/200000 [44:25<75:14:00,  1.37s/it, loss=0.0716, lr=1.02e-05, step=2041]Training:   1%|          | 2042/200000 [44:25<75:14:00,  1.37s/it, loss=0.0536, lr=1.02e-05, step=2042]Training:   1%|          | 2043/200000 [44:26<77:08:25,  1.40s/it, loss=0.0536, lr=1.02e-05, step=2042]Training:   1%|          | 2043/200000 [44:26<77:08:25,  1.40s/it, loss=0.0340, lr=1.02e-05, step=2043]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2044/200000 [44:28<71:47:54,  1.31s/it, loss=0.0340, lr=1.02e-05, step=2043]Training:   1%|          | 2044/200000 [44:28<71:47:54,  1.31s/it, loss=0.0407, lr=1.02e-05, step=2044]Training:   1%|          | 2045/200000 [44:29<68:01:03,  1.24s/it, loss=0.0407, lr=1.02e-05, step=2044]Training:   1%|          | 2045/200000 [44:29<68:01:03,  1.24s/it, loss=0.0690, lr=1.02e-05, step=2045]Training:   1%|          | 2046/200000 [44:30<71:19:21,  1.30s/it, loss=0.0690, lr=1.02e-05, step=2045]Training:   1%|          | 2046/200000 [44:30<71:19:21,  1.30s/it, loss=0.0250, lr=1.02e-05, step=2046]Training:   1%|          | 2047/200000 [44:31<67:37:33,  1.23s/it, loss=0.0250, lr=1.02e-05, step=2046]Training:   1%|          | 2047/200000 [44:31<67:37:33,  1.23s/it, loss=0.0636, lr=1.02e-05, step=2047]Training:   1%|          | 2048/200000 [44:32<68:44:23,  1.25s/it, loss=0.0636, lr=1.02e-05, step=2047]Training:   1%|          | 2048/200000 [44:32<68:44:23,  1.25s/it, loss=0.0957, lr=1.02e-05, step=2048]Training:   1%|          | 2049/200000 [44:33<65:48:40,  1.20s/it, loss=0.0957, lr=1.02e-05, step=2048]Training:   1%|          | 2049/200000 [44:33<65:48:40,  1.20s/it, loss=0.0586, lr=1.02e-05, step=2049]Training:   1%|          | 2050/200000 [44:35<63:49:49,  1.16s/it, loss=0.0586, lr=1.02e-05, step=2049]Training:   1%|          | 2050/200000 [44:35<63:49:49,  1.16s/it, loss=0.0504, lr=1.02e-05, step=2050]Training:   1%|          | 2051/200000 [44:36<68:25:36,  1.24s/it, loss=0.0504, lr=1.02e-05, step=2050]Training:   1%|          | 2051/200000 [44:36<68:25:36,  1.24s/it, loss=0.0383, lr=1.03e-05, step=2051]Training:   1%|          | 2052/200000 [44:37<72:03:07,  1.31s/it, loss=0.0383, lr=1.03e-05, step=2051]Training:   1%|          | 2052/200000 [44:37<72:03:07,  1.31s/it, loss=0.0449, lr=1.03e-05, step=2052]Training:   1%|          | 2053/200000 [44:39<73:38:30,  1.34s/it, loss=0.0449, lr=1.03e-05, step=2052]Training:   1%|          | 2053/200000 [44:39<73:38:30,  1.34s/it, loss=0.0328, lr=1.03e-05, step=2053]Training:   1%|          | 2054/200000 [44:40<69:17:44,  1.26s/it, loss=0.0328, lr=1.03e-05, step=2053]Training:   1%|          | 2054/200000 [44:40<69:17:44,  1.26s/it, loss=0.0511, lr=1.03e-05, step=2054]Training:   1%|          | 2055/200000 [44:41<66:14:31,  1.20s/it, loss=0.0511, lr=1.03e-05, step=2054]Training:   1%|          | 2055/200000 [44:41<66:14:31,  1.20s/it, loss=0.0904, lr=1.03e-05, step=2055]Training:   1%|          | 2056/200000 [44:42<68:42:39,  1.25s/it, loss=0.0904, lr=1.03e-05, step=2055]Training:   1%|          | 2056/200000 [44:42<68:42:39,  1.25s/it, loss=0.0589, lr=1.03e-05, step=2056]Training:   1%|          | 2057/200000 [44:44<70:51:49,  1.29s/it, loss=0.0589, lr=1.03e-05, step=2056]Training:   1%|          | 2057/200000 [44:44<70:51:49,  1.29s/it, loss=0.1337, lr=1.03e-05, step=2057]Training:   1%|          | 2058/200000 [44:45<67:19:00,  1.22s/it, loss=0.1337, lr=1.03e-05, step=2057]Training:   1%|          | 2058/200000 [44:45<67:19:00,  1.22s/it, loss=0.0739, lr=1.03e-05, step=2058]Training:   1%|          | 2059/200000 [44:46<70:04:56,  1.27s/it, loss=0.0739, lr=1.03e-05, step=2058]Training:   1%|          | 2059/200000 [44:46<70:04:56,  1.27s/it, loss=0.0485, lr=1.03e-05, step=2059]Training:   1%|          | 2060/200000 [44:47<66:53:32,  1.22s/it, loss=0.0485, lr=1.03e-05, step=2059]Training:   1%|          | 2060/200000 [44:47<66:53:32,  1.22s/it, loss=0.0395, lr=1.03e-05, step=2060]Training:   1%|          | 2061/200000 [44:49<69:44:36,  1.27s/it, loss=0.0395, lr=1.03e-05, step=2060]Training:   1%|          | 2061/200000 [44:49<69:44:36,  1.27s/it, loss=0.0562, lr=1.03e-05, step=2061]Training:   1%|          | 2062/200000 [44:50<71:20:09,  1.30s/it, loss=0.0562, lr=1.03e-05, step=2061]Training:   1%|          | 2062/200000 [44:50<71:20:09,  1.30s/it, loss=0.0503, lr=1.03e-05, step=2062]Training:   1%|          | 2063/200000 [44:52<74:30:32,  1.36s/it, loss=0.0503, lr=1.03e-05, step=2062]Training:   1%|          | 2063/200000 [44:52<74:30:32,  1.36s/it, loss=0.0544, lr=1.03e-05, step=2063]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2064/200000 [44:53<76:13:02,  1.39s/it, loss=0.0544, lr=1.03e-05, step=2063]Training:   1%|          | 2064/200000 [44:53<76:13:02,  1.39s/it, loss=0.0411, lr=1.03e-05, step=2064]Training:   1%|          | 2065/200000 [44:54<71:04:25,  1.29s/it, loss=0.0411, lr=1.03e-05, step=2064]Training:   1%|          | 2065/200000 [44:54<71:04:25,  1.29s/it, loss=0.0441, lr=1.03e-05, step=2065]Training:   1%|          | 2066/200000 [44:55<67:30:28,  1.23s/it, loss=0.0441, lr=1.03e-05, step=2065]Training:   1%|          | 2066/200000 [44:55<67:30:28,  1.23s/it, loss=0.1310, lr=1.03e-05, step=2066]Training:   1%|          | 2067/200000 [44:57<71:30:30,  1.30s/it, loss=0.1310, lr=1.03e-05, step=2066]Training:   1%|          | 2067/200000 [44:57<71:30:30,  1.30s/it, loss=0.1301, lr=1.03e-05, step=2067]Training:   1%|          | 2068/200000 [44:58<67:50:46,  1.23s/it, loss=0.1301, lr=1.03e-05, step=2067]Training:   1%|          | 2068/200000 [44:58<67:50:46,  1.23s/it, loss=0.0780, lr=1.03e-05, step=2068]Training:   1%|          | 2069/200000 [44:59<69:18:06,  1.26s/it, loss=0.0780, lr=1.03e-05, step=2068]Training:   1%|          | 2069/200000 [44:59<69:18:06,  1.26s/it, loss=0.0270, lr=1.03e-05, step=2069]Training:   1%|          | 2070/200000 [45:00<66:16:18,  1.21s/it, loss=0.0270, lr=1.03e-05, step=2069]Training:   1%|          | 2070/200000 [45:00<66:16:18,  1.21s/it, loss=0.0752, lr=1.03e-05, step=2070]Training:   1%|          | 2071/200000 [45:01<64:10:01,  1.17s/it, loss=0.0752, lr=1.03e-05, step=2070]Training:   1%|          | 2071/200000 [45:01<64:10:01,  1.17s/it, loss=0.0893, lr=1.04e-05, step=2071]Training:   1%|          | 2072/200000 [45:03<68:31:36,  1.25s/it, loss=0.0893, lr=1.04e-05, step=2071]Training:   1%|          | 2072/200000 [45:03<68:31:36,  1.25s/it, loss=0.0598, lr=1.04e-05, step=2072]Training:   1%|          | 2073/200000 [45:04<70:52:05,  1.29s/it, loss=0.0598, lr=1.04e-05, step=2072]Training:   1%|          | 2073/200000 [45:04<70:52:05,  1.29s/it, loss=0.0526, lr=1.04e-05, step=2073]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2074/200000 [45:05<72:41:11,  1.32s/it, loss=0.0526, lr=1.04e-05, step=2073]Training:   1%|          | 2074/200000 [45:05<72:41:11,  1.32s/it, loss=0.0450, lr=1.04e-05, step=2074]Training:   1%|          | 2075/200000 [45:06<68:40:56,  1.25s/it, loss=0.0450, lr=1.04e-05, step=2074]Training:   1%|          | 2075/200000 [45:06<68:40:56,  1.25s/it, loss=0.0322, lr=1.04e-05, step=2075]Training:   1%|          | 2076/200000 [45:08<65:51:09,  1.20s/it, loss=0.0322, lr=1.04e-05, step=2075]Training:   1%|          | 2076/200000 [45:08<65:51:09,  1.20s/it, loss=0.0321, lr=1.04e-05, step=2076]Training:   1%|          | 2077/200000 [45:09<68:06:09,  1.24s/it, loss=0.0321, lr=1.04e-05, step=2076]Training:   1%|          | 2077/200000 [45:09<68:06:09,  1.24s/it, loss=0.0604, lr=1.04e-05, step=2077]Training:   1%|          | 2078/200000 [45:10<69:52:15,  1.27s/it, loss=0.0604, lr=1.04e-05, step=2077]Training:   1%|          | 2078/200000 [45:10<69:52:15,  1.27s/it, loss=0.0467, lr=1.04e-05, step=2078]Training:   1%|          | 2079/200000 [45:11<66:40:02,  1.21s/it, loss=0.0467, lr=1.04e-05, step=2078]Training:   1%|          | 2079/200000 [45:11<66:40:02,  1.21s/it, loss=0.1258, lr=1.04e-05, step=2079]Training:   1%|          | 2080/200000 [45:13<69:15:48,  1.26s/it, loss=0.1258, lr=1.04e-05, step=2079]Training:   1%|          | 2080/200000 [45:13<69:15:48,  1.26s/it, loss=0.0697, lr=1.04e-05, step=2080]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2081/200000 [45:14<66:16:41,  1.21s/it, loss=0.0697, lr=1.04e-05, step=2080]Training:   1%|          | 2081/200000 [45:14<66:16:41,  1.21s/it, loss=0.0555, lr=1.04e-05, step=2081]Training:   1%|          | 2082/200000 [45:15<68:53:42,  1.25s/it, loss=0.0555, lr=1.04e-05, step=2081]Training:   1%|          | 2082/200000 [45:15<68:53:42,  1.25s/it, loss=0.0338, lr=1.04e-05, step=2082]Training:   1%|          | 2083/200000 [45:16<66:01:34,  1.20s/it, loss=0.0338, lr=1.04e-05, step=2082]Training:   1%|          | 2083/200000 [45:16<66:01:34,  1.20s/it, loss=0.0453, lr=1.04e-05, step=2083]Training:   1%|          | 2084/200000 [45:18<70:12:17,  1.28s/it, loss=0.0453, lr=1.04e-05, step=2083]Training:   1%|          | 2084/200000 [45:18<70:12:17,  1.28s/it, loss=0.0433, lr=1.04e-05, step=2084]Training:   1%|          | 2085/200000 [45:19<72:10:42,  1.31s/it, loss=0.0433, lr=1.04e-05, step=2084]Training:   1%|          | 2085/200000 [45:19<72:10:42,  1.31s/it, loss=0.0404, lr=1.04e-05, step=2085]Training:   1%|          | 2086/200000 [45:20<68:17:41,  1.24s/it, loss=0.0404, lr=1.04e-05, step=2085]Training:   1%|          | 2086/200000 [45:20<68:17:41,  1.24s/it, loss=0.0520, lr=1.04e-05, step=2086]Training:   1%|          | 2087/200000 [45:21<65:34:57,  1.19s/it, loss=0.0520, lr=1.04e-05, step=2086]Training:   1%|          | 2087/200000 [45:21<65:34:57,  1.19s/it, loss=0.0449, lr=1.04e-05, step=2087]Training:   1%|          | 2088/200000 [45:23<69:07:13,  1.26s/it, loss=0.0449, lr=1.04e-05, step=2087]Training:   1%|          | 2088/200000 [45:23<69:07:13,  1.26s/it, loss=0.0337, lr=1.04e-05, step=2088]Training:   1%|          | 2089/200000 [45:24<71:22:07,  1.30s/it, loss=0.0337, lr=1.04e-05, step=2088]Training:   1%|          | 2089/200000 [45:24<71:22:07,  1.30s/it, loss=0.0839, lr=1.04e-05, step=2089]Training:   1%|          | 2090/200000 [45:25<67:45:47,  1.23s/it, loss=0.0839, lr=1.04e-05, step=2089]Training:   1%|          | 2090/200000 [45:25<67:45:47,  1.23s/it, loss=0.0527, lr=1.04e-05, step=2090]Training:   1%|          | 2091/200000 [45:26<70:50:20,  1.29s/it, loss=0.0527, lr=1.04e-05, step=2090]Training:   1%|          | 2091/200000 [45:26<70:50:20,  1.29s/it, loss=0.0322, lr=1.05e-05, step=2091]Training:   1%|          | 2092/200000 [45:28<67:23:33,  1.23s/it, loss=0.0322, lr=1.05e-05, step=2091]Training:   1%|          | 2092/200000 [45:28<67:23:33,  1.23s/it, loss=0.0300, lr=1.05e-05, step=2092]Training:   1%|          | 2093/200000 [45:29<69:27:00,  1.26s/it, loss=0.0300, lr=1.05e-05, step=2092]Training:   1%|          | 2093/200000 [45:29<69:27:00,  1.26s/it, loss=0.0524, lr=1.05e-05, step=2093]Training:   1%|          | 2094/200000 [45:30<71:08:06,  1.29s/it, loss=0.0524, lr=1.05e-05, step=2093]Training:   1%|          | 2094/200000 [45:30<71:08:06,  1.29s/it, loss=0.0391, lr=1.05e-05, step=2094]Training:   1%|          | 2095/200000 [45:32<75:26:14,  1.37s/it, loss=0.0391, lr=1.05e-05, step=2094]Training:   1%|          | 2095/200000 [45:32<75:26:14,  1.37s/it, loss=0.0490, lr=1.05e-05, step=2095]Training:   1%|          | 2096/200000 [45:33<77:59:44,  1.42s/it, loss=0.0490, lr=1.05e-05, step=2095]Training:   1%|          | 2096/200000 [45:33<77:59:44,  1.42s/it, loss=0.0562, lr=1.05e-05, step=2096]Training:   1%|          | 2097/200000 [45:34<72:24:23,  1.32s/it, loss=0.0562, lr=1.05e-05, step=2096]Training:   1%|          | 2097/200000 [45:34<72:24:23,  1.32s/it, loss=0.0267, lr=1.05e-05, step=2097]Training:   1%|          | 2098/200000 [45:36<68:30:10,  1.25s/it, loss=0.0267, lr=1.05e-05, step=2097]Training:   1%|          | 2098/200000 [45:36<68:30:10,  1.25s/it, loss=0.0430, lr=1.05e-05, step=2098]Training:   1%|          | 2099/200000 [45:37<72:29:03,  1.32s/it, loss=0.0430, lr=1.05e-05, step=2098]Training:   1%|          | 2099/200000 [45:37<72:29:03,  1.32s/it, loss=0.0314, lr=1.05e-05, step=2099]Training:   1%|          | 2100/200000 [45:38<68:33:18,  1.25s/it, loss=0.0314, lr=1.05e-05, step=2099]Training:   1%|          | 2100/200000 [45:38<68:33:18,  1.25s/it, loss=0.0560, lr=1.05e-05, step=2100]23:38:53.235 [I] step=2100 loss=0.0580 lr=1.03e-05 grad_norm=1.02 time=126.5s                     (701675:train_pytorch.py:582)
Training:   1%|          | 2101/200000 [45:39<69:46:28,  1.27s/it, loss=0.0560, lr=1.05e-05, step=2100]Training:   1%|          | 2101/200000 [45:39<69:46:28,  1.27s/it, loss=0.0444, lr=1.05e-05, step=2101]Training:   1%|          | 2102/200000 [45:41<66:36:09,  1.21s/it, loss=0.0444, lr=1.05e-05, step=2101]Training:   1%|          | 2102/200000 [45:41<66:36:09,  1.21s/it, loss=0.0703, lr=1.05e-05, step=2102]Training:   1%|          | 2103/200000 [45:42<64:24:52,  1.17s/it, loss=0.0703, lr=1.05e-05, step=2102]Training:   1%|          | 2103/200000 [45:42<64:24:52,  1.17s/it, loss=0.1230, lr=1.05e-05, step=2103]Training:   1%|          | 2104/200000 [45:43<69:35:17,  1.27s/it, loss=0.1230, lr=1.05e-05, step=2103]Training:   1%|          | 2104/200000 [45:43<69:35:17,  1.27s/it, loss=0.0759, lr=1.05e-05, step=2104]Training:   1%|          | 2105/200000 [45:45<72:54:37,  1.33s/it, loss=0.0759, lr=1.05e-05, step=2104]Training:   1%|          | 2105/200000 [45:45<72:54:37,  1.33s/it, loss=0.0785, lr=1.05e-05, step=2105]Training:   1%|          | 2106/200000 [45:46<73:46:10,  1.34s/it, loss=0.0785, lr=1.05e-05, step=2105]Training:   1%|          | 2106/200000 [45:46<73:46:10,  1.34s/it, loss=0.0415, lr=1.05e-05, step=2106]Training:   1%|          | 2107/200000 [45:47<69:25:59,  1.26s/it, loss=0.0415, lr=1.05e-05, step=2106]Training:   1%|          | 2107/200000 [45:47<69:25:59,  1.26s/it, loss=0.0390, lr=1.05e-05, step=2107]Training:   1%|          | 2108/200000 [45:48<66:24:28,  1.21s/it, loss=0.0390, lr=1.05e-05, step=2107]Training:   1%|          | 2108/200000 [45:48<66:24:28,  1.21s/it, loss=0.0704, lr=1.05e-05, step=2108]Training:   1%|          | 2109/200000 [45:49<68:46:09,  1.25s/it, loss=0.0704, lr=1.05e-05, step=2108]Training:   1%|          | 2109/200000 [45:49<68:46:09,  1.25s/it, loss=0.0440, lr=1.05e-05, step=2109]Training:   1%|          | 2110/200000 [45:51<70:34:44,  1.28s/it, loss=0.0440, lr=1.05e-05, step=2109]Training:   1%|          | 2110/200000 [45:51<70:34:44,  1.28s/it, loss=0.0469, lr=1.05e-05, step=2110]Training:   1%|          | 2111/200000 [45:52<67:12:40,  1.22s/it, loss=0.0469, lr=1.05e-05, step=2110]Training:   1%|          | 2111/200000 [45:52<67:12:40,  1.22s/it, loss=0.0829, lr=1.06e-05, step=2111]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2112/200000 [45:53<70:24:38,  1.28s/it, loss=0.0829, lr=1.06e-05, step=2111]Training:   1%|          | 2112/200000 [45:53<70:24:38,  1.28s/it, loss=0.0812, lr=1.06e-05, step=2112]Training:   1%|          | 2113/200000 [45:54<67:04:22,  1.22s/it, loss=0.0812, lr=1.06e-05, step=2112]Training:   1%|          | 2113/200000 [45:54<67:04:22,  1.22s/it, loss=0.0644, lr=1.06e-05, step=2113]Training:   1%|          | 2114/200000 [45:56<69:55:28,  1.27s/it, loss=0.0644, lr=1.06e-05, step=2113]Training:   1%|          | 2114/200000 [45:56<69:55:28,  1.27s/it, loss=0.0489, lr=1.06e-05, step=2114]Training:   1%|          | 2115/200000 [45:57<71:32:32,  1.30s/it, loss=0.0489, lr=1.06e-05, step=2114]Training:   1%|          | 2115/200000 [45:57<71:32:32,  1.30s/it, loss=0.0316, lr=1.06e-05, step=2115]Training:   1%|          | 2116/200000 [45:59<75:25:01,  1.37s/it, loss=0.0316, lr=1.06e-05, step=2115]Training:   1%|          | 2116/200000 [45:59<75:25:01,  1.37s/it, loss=0.1645, lr=1.06e-05, step=2116]Training:   1%|          | 2117/200000 [46:00<77:46:08,  1.41s/it, loss=0.1645, lr=1.06e-05, step=2116]Training:   1%|          | 2117/200000 [46:00<77:46:08,  1.41s/it, loss=0.0511, lr=1.06e-05, step=2117]Training:   1%|          | 2118/200000 [46:01<72:15:20,  1.31s/it, loss=0.0511, lr=1.06e-05, step=2117]Training:   1%|          | 2118/200000 [46:01<72:15:20,  1.31s/it, loss=0.0506, lr=1.06e-05, step=2118]Training:   1%|          | 2119/200000 [46:02<68:21:46,  1.24s/it, loss=0.0506, lr=1.06e-05, step=2118]Training:   1%|          | 2119/200000 [46:02<68:21:46,  1.24s/it, loss=0.3394, lr=1.06e-05, step=2119]Training:   1%|          | 2120/200000 [46:04<71:56:41,  1.31s/it, loss=0.3394, lr=1.06e-05, step=2119]Training:   1%|          | 2120/200000 [46:04<71:56:41,  1.31s/it, loss=0.0848, lr=1.06e-05, step=2120]Training:   1%|          | 2121/200000 [46:05<68:08:08,  1.24s/it, loss=0.0848, lr=1.06e-05, step=2120]Training:   1%|          | 2121/200000 [46:05<68:08:08,  1.24s/it, loss=0.0250, lr=1.06e-05, step=2121]Training:   1%|          | 2122/200000 [46:06<68:59:07,  1.26s/it, loss=0.0250, lr=1.06e-05, step=2121]Training:   1%|          | 2122/200000 [46:06<68:59:07,  1.26s/it, loss=0.0299, lr=1.06e-05, step=2122]Training:   1%|          | 2123/200000 [46:07<66:04:53,  1.20s/it, loss=0.0299, lr=1.06e-05, step=2122]Training:   1%|          | 2123/200000 [46:07<66:04:53,  1.20s/it, loss=0.0916, lr=1.06e-05, step=2123]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2124/200000 [46:08<64:02:27,  1.17s/it, loss=0.0916, lr=1.06e-05, step=2123]Training:   1%|          | 2124/200000 [46:08<64:02:27,  1.17s/it, loss=0.0360, lr=1.06e-05, step=2124]Training:   1%|          | 2125/200000 [46:10<68:15:58,  1.24s/it, loss=0.0360, lr=1.06e-05, step=2124]Training:   1%|          | 2125/200000 [46:10<68:15:58,  1.24s/it, loss=0.0483, lr=1.06e-05, step=2125]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2126/200000 [46:11<71:21:06,  1.30s/it, loss=0.0483, lr=1.06e-05, step=2125]Training:   1%|          | 2126/200000 [46:11<71:21:06,  1.30s/it, loss=0.0482, lr=1.06e-05, step=2126]Training:   1%|          | 2127/200000 [46:13<72:32:27,  1.32s/it, loss=0.0482, lr=1.06e-05, step=2126]Training:   1%|          | 2127/200000 [46:13<72:32:27,  1.32s/it, loss=0.0759, lr=1.06e-05, step=2127]Training:   1%|          | 2128/200000 [46:14<68:32:30,  1.25s/it, loss=0.0759, lr=1.06e-05, step=2127]Training:   1%|          | 2128/200000 [46:14<68:32:30,  1.25s/it, loss=0.0932, lr=1.06e-05, step=2128]Training:   1%|          | 2129/200000 [46:15<65:42:08,  1.20s/it, loss=0.0932, lr=1.06e-05, step=2128]Training:   1%|          | 2129/200000 [46:15<65:42:08,  1.20s/it, loss=0.0652, lr=1.06e-05, step=2129]Training:   1%|          | 2130/200000 [46:16<67:54:59,  1.24s/it, loss=0.0652, lr=1.06e-05, step=2129]Training:   1%|          | 2130/200000 [46:16<67:54:59,  1.24s/it, loss=0.0429, lr=1.06e-05, step=2130]Training:   1%|          | 2131/200000 [46:17<69:18:03,  1.26s/it, loss=0.0429, lr=1.06e-05, step=2130]Training:   1%|          | 2131/200000 [46:17<69:18:03,  1.26s/it, loss=0.0373, lr=1.07e-05, step=2131]Training:   1%|          | 2132/200000 [46:18<66:18:41,  1.21s/it, loss=0.0373, lr=1.07e-05, step=2131]Training:   1%|          | 2132/200000 [46:18<66:18:41,  1.21s/it, loss=0.0343, lr=1.07e-05, step=2132]Training:   1%|          | 2133/200000 [46:20<68:56:39,  1.25s/it, loss=0.0343, lr=1.07e-05, step=2132]Training:   1%|          | 2133/200000 [46:20<68:56:39,  1.25s/it, loss=0.0479, lr=1.07e-05, step=2133]Training:   1%|          | 2134/200000 [46:21<66:00:44,  1.20s/it, loss=0.0479, lr=1.07e-05, step=2133]Training:   1%|          | 2134/200000 [46:21<66:00:44,  1.20s/it, loss=0.0635, lr=1.07e-05, step=2134]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2135/200000 [46:22<68:59:11,  1.26s/it, loss=0.0635, lr=1.07e-05, step=2134]Training:   1%|          | 2135/200000 [46:22<68:59:11,  1.26s/it, loss=0.0706, lr=1.07e-05, step=2135]Training:   1%|          | 2136/200000 [46:23<66:06:24,  1.20s/it, loss=0.0706, lr=1.07e-05, step=2135]Training:   1%|          | 2136/200000 [46:23<66:06:24,  1.20s/it, loss=0.0666, lr=1.07e-05, step=2136]Training:   1%|          | 2137/200000 [46:25<70:19:06,  1.28s/it, loss=0.0666, lr=1.07e-05, step=2136]Training:   1%|          | 2137/200000 [46:25<70:19:06,  1.28s/it, loss=0.0637, lr=1.07e-05, step=2137]Training:   1%|          | 2138/200000 [46:26<73:09:12,  1.33s/it, loss=0.0637, lr=1.07e-05, step=2137]Training:   1%|          | 2138/200000 [46:26<73:09:12,  1.33s/it, loss=0.0610, lr=1.07e-05, step=2138]Training:   1%|          | 2139/200000 [46:27<68:58:55,  1.26s/it, loss=0.0610, lr=1.07e-05, step=2138]Training:   1%|          | 2139/200000 [46:27<68:58:55,  1.26s/it, loss=0.0373, lr=1.07e-05, step=2139]Training:   1%|          | 2140/200000 [46:28<66:06:09,  1.20s/it, loss=0.0373, lr=1.07e-05, step=2139]Training:   1%|          | 2140/200000 [46:28<66:06:09,  1.20s/it, loss=0.1518, lr=1.07e-05, step=2140]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2141/200000 [46:30<68:59:40,  1.26s/it, loss=0.1518, lr=1.07e-05, step=2140]Training:   1%|          | 2141/200000 [46:30<68:59:40,  1.26s/it, loss=0.0779, lr=1.07e-05, step=2141]Training:   1%|          | 2142/200000 [46:31<71:18:37,  1.30s/it, loss=0.0779, lr=1.07e-05, step=2141]Training:   1%|          | 2142/200000 [46:31<71:18:37,  1.30s/it, loss=0.0405, lr=1.07e-05, step=2142]Training:   1%|          | 2143/200000 [46:32<67:43:29,  1.23s/it, loss=0.0405, lr=1.07e-05, step=2142]Training:   1%|          | 2143/200000 [46:32<67:43:29,  1.23s/it, loss=0.0283, lr=1.07e-05, step=2143]Training:   1%|          | 2144/200000 [46:34<71:26:11,  1.30s/it, loss=0.0283, lr=1.07e-05, step=2143]Training:   1%|          | 2144/200000 [46:34<71:26:11,  1.30s/it, loss=0.0572, lr=1.07e-05, step=2144]Training:   1%|          | 2145/200000 [46:35<67:47:32,  1.23s/it, loss=0.0572, lr=1.07e-05, step=2144]Training:   1%|          | 2145/200000 [46:35<67:47:32,  1.23s/it, loss=0.0438, lr=1.07e-05, step=2145]Training:   1%|          | 2146/200000 [46:36<70:39:30,  1.29s/it, loss=0.0438, lr=1.07e-05, step=2145]Training:   1%|          | 2146/200000 [46:36<70:39:30,  1.29s/it, loss=0.0229, lr=1.07e-05, step=2146]Training:   1%|          | 2147/200000 [46:38<73:28:44,  1.34s/it, loss=0.0229, lr=1.07e-05, step=2146]Training:   1%|          | 2147/200000 [46:38<73:28:44,  1.34s/it, loss=0.0756, lr=1.07e-05, step=2147]Training:   1%|          | 2148/200000 [46:39<76:11:08,  1.39s/it, loss=0.0756, lr=1.07e-05, step=2147]Training:   1%|          | 2148/200000 [46:39<76:11:08,  1.39s/it, loss=0.0611, lr=1.07e-05, step=2148]Training:   1%|          | 2149/200000 [46:41<78:18:55,  1.42s/it, loss=0.0611, lr=1.07e-05, step=2148]Training:   1%|          | 2149/200000 [46:41<78:18:55,  1.42s/it, loss=0.0536, lr=1.07e-05, step=2149]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2150/200000 [46:42<72:34:54,  1.32s/it, loss=0.0536, lr=1.07e-05, step=2149]Training:   1%|          | 2150/200000 [46:42<72:34:54,  1.32s/it, loss=0.0718, lr=1.07e-05, step=2150]Training:   1%|          | 2151/200000 [46:43<68:38:26,  1.25s/it, loss=0.0718, lr=1.07e-05, step=2150]Training:   1%|          | 2151/200000 [46:43<68:38:26,  1.25s/it, loss=0.0467, lr=1.08e-05, step=2151]Training:   1%|          | 2152/200000 [46:44<72:31:26,  1.32s/it, loss=0.0467, lr=1.08e-05, step=2151]Training:   1%|          | 2152/200000 [46:44<72:31:26,  1.32s/it, loss=0.0522, lr=1.08e-05, step=2152]Training:   1%|          | 2153/200000 [46:45<68:30:34,  1.25s/it, loss=0.0522, lr=1.08e-05, step=2152]Training:   1%|          | 2153/200000 [46:45<68:30:34,  1.25s/it, loss=0.0627, lr=1.08e-05, step=2153]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2154/200000 [46:47<69:47:00,  1.27s/it, loss=0.0627, lr=1.08e-05, step=2153]Training:   1%|          | 2154/200000 [46:47<69:47:00,  1.27s/it, loss=0.1109, lr=1.08e-05, step=2154]Training:   1%|          | 2155/200000 [46:48<66:41:55,  1.21s/it, loss=0.1109, lr=1.08e-05, step=2154]Training:   1%|          | 2155/200000 [46:48<66:41:55,  1.21s/it, loss=0.0656, lr=1.08e-05, step=2155]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2156/200000 [46:49<64:27:57,  1.17s/it, loss=0.0656, lr=1.08e-05, step=2155]Training:   1%|          | 2156/200000 [46:49<64:27:57,  1.17s/it, loss=0.1054, lr=1.08e-05, step=2156]Training:   1%|          | 2157/200000 [46:50<68:55:24,  1.25s/it, loss=0.1054, lr=1.08e-05, step=2156]Training:   1%|          | 2157/200000 [46:50<68:55:24,  1.25s/it, loss=0.0528, lr=1.08e-05, step=2157]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2158/200000 [46:52<71:46:37,  1.31s/it, loss=0.0528, lr=1.08e-05, step=2157]Training:   1%|          | 2158/200000 [46:52<71:46:37,  1.31s/it, loss=0.0394, lr=1.08e-05, step=2158]Training:   1%|          | 2159/200000 [46:53<73:06:13,  1.33s/it, loss=0.0394, lr=1.08e-05, step=2158]Training:   1%|          | 2159/200000 [46:53<73:06:13,  1.33s/it, loss=0.0335, lr=1.08e-05, step=2159]Training:   1%|          | 2160/200000 [46:54<68:56:04,  1.25s/it, loss=0.0335, lr=1.08e-05, step=2159]Training:   1%|          | 2160/200000 [46:54<68:56:04,  1.25s/it, loss=0.0369, lr=1.08e-05, step=2160]Training:   1%|          | 2161/200000 [46:55<66:02:29,  1.20s/it, loss=0.0369, lr=1.08e-05, step=2160]Training:   1%|          | 2161/200000 [46:55<66:02:29,  1.20s/it, loss=0.0370, lr=1.08e-05, step=2161]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2162/200000 [46:57<68:17:01,  1.24s/it, loss=0.0370, lr=1.08e-05, step=2161]Training:   1%|          | 2162/200000 [46:57<68:17:01,  1.24s/it, loss=0.0374, lr=1.08e-05, step=2162]Training:   1%|          | 2163/200000 [46:58<70:50:23,  1.29s/it, loss=0.0374, lr=1.08e-05, step=2162]Training:   1%|          | 2163/200000 [46:58<70:50:23,  1.29s/it, loss=0.0552, lr=1.08e-05, step=2163]Training:   1%|          | 2164/200000 [46:59<67:21:50,  1.23s/it, loss=0.0552, lr=1.08e-05, step=2163]Training:   1%|          | 2164/200000 [46:59<67:21:50,  1.23s/it, loss=0.0401, lr=1.08e-05, step=2164]Training:   1%|          | 2165/200000 [47:01<71:20:03,  1.30s/it, loss=0.0401, lr=1.08e-05, step=2164]Training:   1%|          | 2165/200000 [47:01<71:20:03,  1.30s/it, loss=0.0557, lr=1.08e-05, step=2165]Training:   1%|          | 2166/200000 [47:02<67:42:46,  1.23s/it, loss=0.0557, lr=1.08e-05, step=2165]Training:   1%|          | 2166/200000 [47:02<67:42:46,  1.23s/it, loss=0.0614, lr=1.08e-05, step=2166]Training:   1%|          | 2167/200000 [47:03<70:28:20,  1.28s/it, loss=0.0614, lr=1.08e-05, step=2166]Training:   1%|          | 2167/200000 [47:03<70:28:20,  1.28s/it, loss=0.0456, lr=1.08e-05, step=2167]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2168/200000 [47:04<73:01:56,  1.33s/it, loss=0.0456, lr=1.08e-05, step=2167]Training:   1%|          | 2168/200000 [47:04<73:01:56,  1.33s/it, loss=0.0376, lr=1.08e-05, step=2168]Training:   1%|          | 2169/200000 [47:06<75:37:56,  1.38s/it, loss=0.0376, lr=1.08e-05, step=2168]Training:   1%|          | 2169/200000 [47:06<75:37:56,  1.38s/it, loss=0.0554, lr=1.08e-05, step=2169]Training:   1%|          | 2170/200000 [47:07<77:51:13,  1.42s/it, loss=0.0554, lr=1.08e-05, step=2169]Training:   1%|          | 2170/200000 [47:07<77:51:13,  1.42s/it, loss=0.0733, lr=1.08e-05, step=2170]Training:   1%|          | 2171/200000 [47:09<72:13:35,  1.31s/it, loss=0.0733, lr=1.08e-05, step=2170]Training:   1%|          | 2171/200000 [47:09<72:13:35,  1.31s/it, loss=0.0309, lr=1.09e-05, step=2171]Training:   1%|          | 2172/200000 [47:10<68:20:40,  1.24s/it, loss=0.0309, lr=1.09e-05, step=2171]Training:   1%|          | 2172/200000 [47:10<68:20:40,  1.24s/it, loss=0.0500, lr=1.09e-05, step=2172]Training:   1%|          | 2173/200000 [47:11<71:59:33,  1.31s/it, loss=0.0500, lr=1.09e-05, step=2172]Training:   1%|          | 2173/200000 [47:11<71:59:33,  1.31s/it, loss=0.0430, lr=1.09e-05, step=2173]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2174/200000 [47:12<68:10:15,  1.24s/it, loss=0.0430, lr=1.09e-05, step=2173]Training:   1%|          | 2174/200000 [47:12<68:10:15,  1.24s/it, loss=0.0602, lr=1.09e-05, step=2174]Training:   1%|          | 2175/200000 [47:13<69:31:31,  1.27s/it, loss=0.0602, lr=1.09e-05, step=2174]Training:   1%|          | 2175/200000 [47:13<69:31:31,  1.27s/it, loss=0.0493, lr=1.09e-05, step=2175]Training:   1%|          | 2176/200000 [47:15<66:29:35,  1.21s/it, loss=0.0493, lr=1.09e-05, step=2175]Training:   1%|          | 2176/200000 [47:15<66:29:35,  1.21s/it, loss=0.0455, lr=1.09e-05, step=2176]Training:   1%|          | 2177/200000 [47:16<64:19:45,  1.17s/it, loss=0.0455, lr=1.09e-05, step=2176]Training:   1%|          | 2177/200000 [47:16<64:19:45,  1.17s/it, loss=0.0272, lr=1.09e-05, step=2177]Training:   1%|          | 2178/200000 [47:17<68:49:48,  1.25s/it, loss=0.0272, lr=1.09e-05, step=2177]Training:   1%|          | 2178/200000 [47:17<68:49:48,  1.25s/it, loss=0.0530, lr=1.09e-05, step=2178]Training:   1%|          | 2179/200000 [47:19<72:16:40,  1.32s/it, loss=0.0530, lr=1.09e-05, step=2178]Training:   1%|          | 2179/200000 [47:19<72:16:40,  1.32s/it, loss=0.0485, lr=1.09e-05, step=2179]Training:   1%|          | 2180/200000 [47:20<73:11:58,  1.33s/it, loss=0.0485, lr=1.09e-05, step=2179]Training:   1%|          | 2180/200000 [47:20<73:11:58,  1.33s/it, loss=0.0438, lr=1.09e-05, step=2180]Training:   1%|          | 2181/200000 [47:21<69:01:30,  1.26s/it, loss=0.0438, lr=1.09e-05, step=2180]Training:   1%|          | 2181/200000 [47:21<69:01:30,  1.26s/it, loss=0.0864, lr=1.09e-05, step=2181]Training:   1%|          | 2182/200000 [47:22<66:05:25,  1.20s/it, loss=0.0864, lr=1.09e-05, step=2181]Training:   1%|          | 2182/200000 [47:22<66:05:25,  1.20s/it, loss=0.0544, lr=1.09e-05, step=2182]Training:   1%|          | 2183/200000 [47:23<68:22:15,  1.24s/it, loss=0.0544, lr=1.09e-05, step=2182]Training:   1%|          | 2183/200000 [47:23<68:22:15,  1.24s/it, loss=0.0345, lr=1.09e-05, step=2183]Training:   1%|          | 2184/200000 [47:25<69:31:31,  1.27s/it, loss=0.0345, lr=1.09e-05, step=2183]Training:   1%|          | 2184/200000 [47:25<69:31:31,  1.27s/it, loss=0.0490, lr=1.09e-05, step=2184]Training:   1%|          | 2185/200000 [47:26<66:27:13,  1.21s/it, loss=0.0490, lr=1.09e-05, step=2184]Training:   1%|          | 2185/200000 [47:26<66:27:13,  1.21s/it, loss=0.0965, lr=1.09e-05, step=2185]Training:   1%|          | 2186/200000 [47:27<69:02:22,  1.26s/it, loss=0.0965, lr=1.09e-05, step=2185]Training:   1%|          | 2186/200000 [47:27<69:02:22,  1.26s/it, loss=0.0325, lr=1.09e-05, step=2186]Training:   1%|          | 2187/200000 [47:28<66:06:58,  1.20s/it, loss=0.0325, lr=1.09e-05, step=2186]Training:   1%|          | 2187/200000 [47:28<66:06:58,  1.20s/it, loss=0.0383, lr=1.09e-05, step=2187]Training:   1%|          | 2188/200000 [47:30<69:05:46,  1.26s/it, loss=0.0383, lr=1.09e-05, step=2187]Training:   1%|          | 2188/200000 [47:30<69:05:46,  1.26s/it, loss=0.0358, lr=1.09e-05, step=2188]Training:   1%|          | 2189/200000 [47:31<70:00:19,  1.27s/it, loss=0.0358, lr=1.09e-05, step=2188]Training:   1%|          | 2189/200000 [47:31<70:00:19,  1.27s/it, loss=0.0582, lr=1.09e-05, step=2189]Training:   1%|          | 2190/200000 [47:32<72:27:34,  1.32s/it, loss=0.0582, lr=1.09e-05, step=2189]Training:   1%|          | 2190/200000 [47:32<72:27:34,  1.32s/it, loss=0.0393, lr=1.09e-05, step=2190]Training:   1%|          | 2191/200000 [47:34<74:34:56,  1.36s/it, loss=0.0393, lr=1.09e-05, step=2190]Training:   1%|          | 2191/200000 [47:34<74:34:56,  1.36s/it, loss=0.0650, lr=1.10e-05, step=2191]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2192/200000 [47:35<69:59:56,  1.27s/it, loss=0.0650, lr=1.10e-05, step=2191]Training:   1%|          | 2192/200000 [47:35<69:59:56,  1.27s/it, loss=0.0577, lr=1.10e-05, step=2192]Training:   1%|          | 2193/200000 [47:36<66:46:23,  1.22s/it, loss=0.0577, lr=1.10e-05, step=2192]Training:   1%|          | 2193/200000 [47:36<66:46:23,  1.22s/it, loss=0.0571, lr=1.10e-05, step=2193]Training:   1%|          | 2194/200000 [47:37<69:54:39,  1.27s/it, loss=0.0571, lr=1.10e-05, step=2193]Training:   1%|          | 2194/200000 [47:37<69:54:39,  1.27s/it, loss=0.0696, lr=1.10e-05, step=2194]Training:   1%|          | 2195/200000 [47:39<72:12:21,  1.31s/it, loss=0.0696, lr=1.10e-05, step=2194]Training:   1%|          | 2195/200000 [47:39<72:12:21,  1.31s/it, loss=0.0291, lr=1.10e-05, step=2195]Training:   1%|          | 2196/200000 [47:40<68:19:47,  1.24s/it, loss=0.0291, lr=1.10e-05, step=2195]Training:   1%|          | 2196/200000 [47:40<68:19:47,  1.24s/it, loss=0.0594, lr=1.10e-05, step=2196]Training:   1%|          | 2197/200000 [47:41<71:36:31,  1.30s/it, loss=0.0594, lr=1.10e-05, step=2196]Training:   1%|          | 2197/200000 [47:41<71:36:31,  1.30s/it, loss=0.0294, lr=1.10e-05, step=2197]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2198/200000 [47:42<67:56:02,  1.24s/it, loss=0.0294, lr=1.10e-05, step=2197]Training:   1%|          | 2198/200000 [47:42<67:56:02,  1.24s/it, loss=0.0615, lr=1.10e-05, step=2198]Training:   1%|          | 2199/200000 [47:44<69:57:51,  1.27s/it, loss=0.0615, lr=1.10e-05, step=2198]Training:   1%|          | 2199/200000 [47:44<69:57:51,  1.27s/it, loss=0.0690, lr=1.10e-05, step=2199]Training:   1%|          | 2200/200000 [47:45<73:31:50,  1.34s/it, loss=0.0690, lr=1.10e-05, step=2199]Training:   1%|          | 2200/200000 [47:45<73:31:50,  1.34s/it, loss=0.0533, lr=1.10e-05, step=2200]23:41:00.571 [I] step=2200 loss=0.0598 lr=1.08e-05 grad_norm=0.96 time=127.3s                     (701675:train_pytorch.py:582)
Training:   1%|          | 2201/200000 [47:47<76:08:52,  1.39s/it, loss=0.0533, lr=1.10e-05, step=2200]Training:   1%|          | 2201/200000 [47:47<76:08:52,  1.39s/it, loss=0.0331, lr=1.10e-05, step=2201]Training:   1%|          | 2202/200000 [47:48<78:32:47,  1.43s/it, loss=0.0331, lr=1.10e-05, step=2201]Training:   1%|          | 2202/200000 [47:48<78:32:47,  1.43s/it, loss=0.0746, lr=1.10e-05, step=2202]Training:   1%|          | 2203/200000 [47:49<72:43:31,  1.32s/it, loss=0.0746, lr=1.10e-05, step=2202]Training:   1%|          | 2203/200000 [47:49<72:43:31,  1.32s/it, loss=0.0403, lr=1.10e-05, step=2203]Training:   1%|          | 2204/200000 [47:50<68:41:43,  1.25s/it, loss=0.0403, lr=1.10e-05, step=2203]Training:   1%|          | 2204/200000 [47:50<68:41:43,  1.25s/it, loss=0.0567, lr=1.10e-05, step=2204]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2205/200000 [47:52<72:36:16,  1.32s/it, loss=0.0567, lr=1.10e-05, step=2204]Training:   1%|          | 2205/200000 [47:52<72:36:16,  1.32s/it, loss=0.0545, lr=1.10e-05, step=2205]Training:   1%|          | 2206/200000 [47:53<68:34:17,  1.25s/it, loss=0.0545, lr=1.10e-05, step=2205]Training:   1%|          | 2206/200000 [47:53<68:34:17,  1.25s/it, loss=0.0400, lr=1.10e-05, step=2206]Training:   1%|          | 2207/200000 [47:54<69:50:08,  1.27s/it, loss=0.0400, lr=1.10e-05, step=2206]Training:   1%|          | 2207/200000 [47:54<69:50:08,  1.27s/it, loss=0.0594, lr=1.10e-05, step=2207]Training:   1%|          | 2208/200000 [47:55<66:38:46,  1.21s/it, loss=0.0594, lr=1.10e-05, step=2207]Training:   1%|          | 2208/200000 [47:55<66:38:46,  1.21s/it, loss=0.0592, lr=1.10e-05, step=2208]Training:   1%|          | 2209/200000 [47:56<64:23:17,  1.17s/it, loss=0.0592, lr=1.10e-05, step=2208]Training:   1%|          | 2209/200000 [47:56<64:23:17,  1.17s/it, loss=0.0316, lr=1.10e-05, step=2209]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2210/200000 [47:58<69:33:02,  1.27s/it, loss=0.0316, lr=1.10e-05, step=2209]Training:   1%|          | 2210/200000 [47:58<69:33:02,  1.27s/it, loss=0.0454, lr=1.10e-05, step=2210]Training:   1%|          | 2211/200000 [47:59<72:57:20,  1.33s/it, loss=0.0454, lr=1.10e-05, step=2210]Training:   1%|          | 2211/200000 [47:59<72:57:20,  1.33s/it, loss=0.0366, lr=1.11e-05, step=2211]Training:   1%|          | 2212/200000 [48:01<73:55:45,  1.35s/it, loss=0.0366, lr=1.11e-05, step=2211]Training:   1%|          | 2212/200000 [48:01<73:55:45,  1.35s/it, loss=0.0243, lr=1.11e-05, step=2212]Training:   1%|          | 2213/200000 [48:02<69:33:03,  1.27s/it, loss=0.0243, lr=1.11e-05, step=2212]Training:   1%|          | 2213/200000 [48:02<69:33:03,  1.27s/it, loss=0.0864, lr=1.11e-05, step=2213]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2214/200000 [48:03<66:30:09,  1.21s/it, loss=0.0864, lr=1.11e-05, step=2213]Training:   1%|          | 2214/200000 [48:03<66:30:09,  1.21s/it, loss=0.0375, lr=1.11e-05, step=2214]Training:   1%|          | 2215/200000 [48:04<68:51:06,  1.25s/it, loss=0.0375, lr=1.11e-05, step=2214]Training:   1%|          | 2215/200000 [48:04<68:51:06,  1.25s/it, loss=0.0715, lr=1.11e-05, step=2215]Training:   1%|          | 2216/200000 [48:06<70:49:41,  1.29s/it, loss=0.0715, lr=1.11e-05, step=2215]Training:   1%|          | 2216/200000 [48:06<70:49:41,  1.29s/it, loss=0.0372, lr=1.11e-05, step=2216]Training:   1%|          | 2217/200000 [48:07<67:21:59,  1.23s/it, loss=0.0372, lr=1.11e-05, step=2216]Training:   1%|          | 2217/200000 [48:07<67:21:59,  1.23s/it, loss=0.0828, lr=1.11e-05, step=2217]Training:   1%|          | 2218/200000 [48:08<70:42:54,  1.29s/it, loss=0.0828, lr=1.11e-05, step=2217]Training:   1%|          | 2218/200000 [48:08<70:42:54,  1.29s/it, loss=0.0878, lr=1.11e-05, step=2218]Training:   1%|          | 2219/200000 [48:09<67:12:31,  1.22s/it, loss=0.0878, lr=1.11e-05, step=2218]Training:   1%|          | 2219/200000 [48:09<67:12:31,  1.22s/it, loss=0.0795, lr=1.11e-05, step=2219]Training:   1%|          | 2220/200000 [48:11<70:23:30,  1.28s/it, loss=0.0795, lr=1.11e-05, step=2219]Training:   1%|          | 2220/200000 [48:11<70:23:30,  1.28s/it, loss=0.0745, lr=1.11e-05, step=2220]Training:   1%|          | 2221/200000 [48:12<67:00:51,  1.22s/it, loss=0.0745, lr=1.11e-05, step=2220]Training:   1%|          | 2221/200000 [48:12<67:00:51,  1.22s/it, loss=0.0540, lr=1.11e-05, step=2221]Training:   1%|          | 2222/200000 [48:13<71:22:15,  1.30s/it, loss=0.0540, lr=1.11e-05, step=2221]Training:   1%|          | 2222/200000 [48:13<71:22:15,  1.30s/it, loss=0.0298, lr=1.11e-05, step=2222]Training:   1%|          | 2223/200000 [48:15<74:52:40,  1.36s/it, loss=0.0298, lr=1.11e-05, step=2222]Training:   1%|          | 2223/200000 [48:15<74:52:40,  1.36s/it, loss=0.0444, lr=1.11e-05, step=2223]Training:   1%|          | 2224/200000 [48:16<70:12:15,  1.28s/it, loss=0.0444, lr=1.11e-05, step=2223]Training:   1%|          | 2224/200000 [48:16<70:12:15,  1.28s/it, loss=0.0404, lr=1.11e-05, step=2224]Training:   1%|          | 2225/200000 [48:17<66:55:35,  1.22s/it, loss=0.0404, lr=1.11e-05, step=2224]Training:   1%|          | 2225/200000 [48:17<66:55:35,  1.22s/it, loss=0.0590, lr=1.11e-05, step=2225]Training:   1%|          | 2226/200000 [48:18<71:00:08,  1.29s/it, loss=0.0590, lr=1.11e-05, step=2225]Training:   1%|          | 2226/200000 [48:18<71:00:08,  1.29s/it, loss=0.0580, lr=1.11e-05, step=2226]Training:   1%|          | 2227/200000 [48:19<67:28:52,  1.23s/it, loss=0.0580, lr=1.11e-05, step=2226]Training:   1%|          | 2227/200000 [48:19<67:28:52,  1.23s/it, loss=0.0325, lr=1.11e-05, step=2227]Training:   1%|          | 2228/200000 [48:21<69:20:18,  1.26s/it, loss=0.0325, lr=1.11e-05, step=2227]Training:   1%|          | 2228/200000 [48:21<69:20:18,  1.26s/it, loss=0.1244, lr=1.11e-05, step=2228]Training:   1%|          | 2229/200000 [48:22<66:17:13,  1.21s/it, loss=0.1244, lr=1.11e-05, step=2228]Training:   1%|          | 2229/200000 [48:22<66:17:13,  1.21s/it, loss=0.0374, lr=1.11e-05, step=2229]Training:   1%|          | 2230/200000 [48:23<64:10:50,  1.17s/it, loss=0.0374, lr=1.11e-05, step=2229]Training:   1%|          | 2230/200000 [48:23<64:10:50,  1.17s/it, loss=0.0286, lr=1.11e-05, step=2230]Training:   1%|          | 2231/200000 [48:24<68:23:08,  1.24s/it, loss=0.0286, lr=1.11e-05, step=2230]Training:   1%|          | 2231/200000 [48:24<68:23:08,  1.24s/it, loss=0.0392, lr=1.12e-05, step=2231]Training:   1%|          | 2232/200000 [48:26<71:08:07,  1.29s/it, loss=0.0392, lr=1.12e-05, step=2231]Training:   1%|          | 2232/200000 [48:26<71:08:07,  1.29s/it, loss=0.0432, lr=1.12e-05, step=2232]Training:   1%|          | 2233/200000 [48:27<73:24:07,  1.34s/it, loss=0.0432, lr=1.12e-05, step=2232]Training:   1%|          | 2233/200000 [48:27<73:24:07,  1.34s/it, loss=0.0271, lr=1.12e-05, step=2233]Training:   1%|          | 2234/200000 [48:28<69:11:24,  1.26s/it, loss=0.0271, lr=1.12e-05, step=2233]Training:   1%|          | 2234/200000 [48:28<69:11:24,  1.26s/it, loss=0.1430, lr=1.12e-05, step=2234]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2235/200000 [48:29<66:13:04,  1.21s/it, loss=0.1430, lr=1.12e-05, step=2234]Training:   1%|          | 2235/200000 [48:29<66:13:04,  1.21s/it, loss=0.0445, lr=1.12e-05, step=2235]Training:   1%|          | 2236/200000 [48:31<68:37:17,  1.25s/it, loss=0.0445, lr=1.12e-05, step=2235]Training:   1%|          | 2236/200000 [48:31<68:37:17,  1.25s/it, loss=0.0306, lr=1.12e-05, step=2236]Training:   1%|          | 2237/200000 [48:32<70:34:18,  1.28s/it, loss=0.0306, lr=1.12e-05, step=2236]Training:   1%|          | 2237/200000 [48:32<70:34:18,  1.28s/it, loss=0.1116, lr=1.12e-05, step=2237]Training:   1%|          | 2238/200000 [48:33<67:11:49,  1.22s/it, loss=0.1116, lr=1.12e-05, step=2237]Training:   1%|          | 2238/200000 [48:33<67:11:49,  1.22s/it, loss=0.0491, lr=1.12e-05, step=2238]Training:   1%|          | 2239/200000 [48:35<69:49:56,  1.27s/it, loss=0.0491, lr=1.12e-05, step=2238]Training:   1%|          | 2239/200000 [48:35<69:49:56,  1.27s/it, loss=0.0850, lr=1.12e-05, step=2239]Training:   1%|          | 2240/200000 [48:36<66:40:21,  1.21s/it, loss=0.0850, lr=1.12e-05, step=2239]Training:   1%|          | 2240/200000 [48:36<66:40:21,  1.21s/it, loss=0.0535, lr=1.12e-05, step=2240]Training:   1%|          | 2241/200000 [48:37<68:49:55,  1.25s/it, loss=0.0535, lr=1.12e-05, step=2240]Training:   1%|          | 2241/200000 [48:37<68:49:55,  1.25s/it, loss=0.0810, lr=1.12e-05, step=2241]Training:   1%|          | 2242/200000 [48:38<72:10:13,  1.31s/it, loss=0.0810, lr=1.12e-05, step=2241]Training:   1%|          | 2242/200000 [48:38<72:10:13,  1.31s/it, loss=0.0442, lr=1.12e-05, step=2242]Training:   1%|          | 2243/200000 [48:40<74:14:36,  1.35s/it, loss=0.0442, lr=1.12e-05, step=2242]Training:   1%|          | 2243/200000 [48:40<74:14:36,  1.35s/it, loss=0.0437, lr=1.12e-05, step=2243]Training:   1%|          | 2244/200000 [48:41<75:54:27,  1.38s/it, loss=0.0437, lr=1.12e-05, step=2243]Training:   1%|          | 2244/200000 [48:41<75:54:27,  1.38s/it, loss=0.0305, lr=1.12e-05, step=2244]Training:   1%|          | 2245/200000 [48:42<70:54:41,  1.29s/it, loss=0.0305, lr=1.12e-05, step=2244]Training:   1%|          | 2245/200000 [48:42<70:54:41,  1.29s/it, loss=0.0404, lr=1.12e-05, step=2245]Training:   1%|          | 2246/200000 [48:44<67:24:09,  1.23s/it, loss=0.0404, lr=1.12e-05, step=2245]Training:   1%|          | 2246/200000 [48:44<67:24:09,  1.23s/it, loss=0.0363, lr=1.12e-05, step=2246]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2247/200000 [48:45<69:32:20,  1.27s/it, loss=0.0363, lr=1.12e-05, step=2246]Training:   1%|          | 2247/200000 [48:45<69:32:20,  1.27s/it, loss=0.0273, lr=1.12e-05, step=2247]Training:   1%|          | 2248/200000 [48:46<72:06:59,  1.31s/it, loss=0.0273, lr=1.12e-05, step=2247]Training:   1%|          | 2248/200000 [48:46<72:06:59,  1.31s/it, loss=0.0519, lr=1.12e-05, step=2248]Training:   1%|          | 2249/200000 [48:47<68:20:04,  1.24s/it, loss=0.0519, lr=1.12e-05, step=2248]Training:   1%|          | 2249/200000 [48:47<68:20:04,  1.24s/it, loss=0.0350, lr=1.12e-05, step=2249]Training:   1%|          | 2250/200000 [48:49<71:46:56,  1.31s/it, loss=0.0350, lr=1.12e-05, step=2249]Training:   1%|          | 2250/200000 [48:49<71:46:56,  1.31s/it, loss=0.0301, lr=1.12e-05, step=2250]Training:   1%|          | 2251/200000 [48:50<68:00:23,  1.24s/it, loss=0.0301, lr=1.12e-05, step=2250]Training:   1%|          | 2251/200000 [48:50<68:00:23,  1.24s/it, loss=0.0835, lr=1.13e-05, step=2251]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2252/200000 [48:51<70:25:53,  1.28s/it, loss=0.0835, lr=1.13e-05, step=2251]Training:   1%|          | 2252/200000 [48:51<70:25:53,  1.28s/it, loss=0.0715, lr=1.13e-05, step=2252]Training:   1%|          | 2253/200000 [48:52<67:03:45,  1.22s/it, loss=0.0715, lr=1.13e-05, step=2252]Training:   1%|          | 2253/200000 [48:52<67:03:45,  1.22s/it, loss=0.0373, lr=1.13e-05, step=2253]Training:   1%|          | 2254/200000 [48:54<71:50:07,  1.31s/it, loss=0.0373, lr=1.13e-05, step=2253]Training:   1%|          | 2254/200000 [48:54<71:50:07,  1.31s/it, loss=0.0373, lr=1.13e-05, step=2254]Training:   1%|          | 2255/200000 [48:55<75:20:34,  1.37s/it, loss=0.0373, lr=1.13e-05, step=2254]Training:   1%|          | 2255/200000 [48:55<75:20:34,  1.37s/it, loss=0.0291, lr=1.13e-05, step=2255]Training:   1%|          | 2256/200000 [48:56<70:29:21,  1.28s/it, loss=0.0291, lr=1.13e-05, step=2255]Training:   1%|          | 2256/200000 [48:56<70:29:21,  1.28s/it, loss=0.0584, lr=1.13e-05, step=2256]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2257/200000 [48:58<67:06:12,  1.22s/it, loss=0.0584, lr=1.13e-05, step=2256]Training:   1%|          | 2257/200000 [48:58<67:06:12,  1.22s/it, loss=0.0506, lr=1.13e-05, step=2257]Training:   1%|          | 2258/200000 [48:59<70:43:04,  1.29s/it, loss=0.0506, lr=1.13e-05, step=2257]Training:   1%|          | 2258/200000 [48:59<70:43:04,  1.29s/it, loss=0.0468, lr=1.13e-05, step=2258]Training:   1%|          | 2259/200000 [49:00<67:18:17,  1.23s/it, loss=0.0468, lr=1.13e-05, step=2258]Training:   1%|          | 2259/200000 [49:00<67:18:17,  1.23s/it, loss=0.0426, lr=1.13e-05, step=2259]Training:   1%|          | 2260/200000 [49:01<69:29:05,  1.27s/it, loss=0.0426, lr=1.13e-05, step=2259]Training:   1%|          | 2260/200000 [49:01<69:29:05,  1.27s/it, loss=0.0328, lr=1.13e-05, step=2260]Training:   1%|          | 2261/200000 [49:03<66:24:29,  1.21s/it, loss=0.0328, lr=1.13e-05, step=2260]Training:   1%|          | 2261/200000 [49:03<66:24:29,  1.21s/it, loss=0.0378, lr=1.13e-05, step=2261]Training:   1%|          | 2262/200000 [49:04<64:16:40,  1.17s/it, loss=0.0378, lr=1.13e-05, step=2261]Training:   1%|          | 2262/200000 [49:04<64:16:40,  1.17s/it, loss=0.0403, lr=1.13e-05, step=2262]Training:   1%|          | 2263/200000 [49:05<68:59:01,  1.26s/it, loss=0.0403, lr=1.13e-05, step=2262]Training:   1%|          | 2263/200000 [49:05<68:59:01,  1.26s/it, loss=0.0328, lr=1.13e-05, step=2263]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2264/200000 [49:07<72:32:23,  1.32s/it, loss=0.0328, lr=1.13e-05, step=2263]Training:   1%|          | 2264/200000 [49:07<72:32:23,  1.32s/it, loss=0.0562, lr=1.13e-05, step=2264]Training:   1%|          | 2265/200000 [49:08<74:31:38,  1.36s/it, loss=0.0562, lr=1.13e-05, step=2264]Training:   1%|          | 2265/200000 [49:08<74:31:38,  1.36s/it, loss=0.0311, lr=1.13e-05, step=2265]Training:   1%|          | 2266/200000 [49:09<69:56:23,  1.27s/it, loss=0.0311, lr=1.13e-05, step=2265]Training:   1%|          | 2266/200000 [49:09<69:56:23,  1.27s/it, loss=0.0333, lr=1.13e-05, step=2266]Training:   1%|          | 2267/200000 [49:10<66:43:07,  1.21s/it, loss=0.0333, lr=1.13e-05, step=2266]Training:   1%|          | 2267/200000 [49:10<66:43:07,  1.21s/it, loss=0.0436, lr=1.13e-05, step=2267]Training:   1%|          | 2268/200000 [49:11<69:03:14,  1.26s/it, loss=0.0436, lr=1.13e-05, step=2267]Training:   1%|          | 2268/200000 [49:11<69:03:14,  1.26s/it, loss=0.0523, lr=1.13e-05, step=2268]Training:   1%|          | 2269/200000 [49:13<71:12:57,  1.30s/it, loss=0.0523, lr=1.13e-05, step=2268]Training:   1%|          | 2269/200000 [49:13<71:12:57,  1.30s/it, loss=0.0514, lr=1.13e-05, step=2269]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2270/200000 [49:14<67:37:36,  1.23s/it, loss=0.0514, lr=1.13e-05, step=2269]Training:   1%|          | 2270/200000 [49:14<67:37:36,  1.23s/it, loss=0.0575, lr=1.13e-05, step=2270]Training:   1%|          | 2271/200000 [49:15<71:10:15,  1.30s/it, loss=0.0575, lr=1.13e-05, step=2270]Training:   1%|          | 2271/200000 [49:15<71:10:15,  1.30s/it, loss=0.0362, lr=1.14e-05, step=2271]Training:   1%|          | 2272/200000 [49:16<67:35:01,  1.23s/it, loss=0.0362, lr=1.14e-05, step=2271]Training:   1%|          | 2272/200000 [49:16<67:35:01,  1.23s/it, loss=0.0774, lr=1.14e-05, step=2272]Training:   1%|          | 2273/200000 [49:18<70:44:10,  1.29s/it, loss=0.0774, lr=1.14e-05, step=2272]Training:   1%|          | 2273/200000 [49:18<70:44:10,  1.29s/it, loss=0.0427, lr=1.14e-05, step=2273]Training:   1%|          | 2274/200000 [49:19<67:20:04,  1.23s/it, loss=0.0427, lr=1.14e-05, step=2273]Training:   1%|          | 2274/200000 [49:19<67:20:04,  1.23s/it, loss=0.0257, lr=1.14e-05, step=2274]Training:   1%|          | 2275/200000 [49:20<71:31:04,  1.30s/it, loss=0.0257, lr=1.14e-05, step=2274]Training:   1%|          | 2275/200000 [49:20<71:31:04,  1.30s/it, loss=0.0435, lr=1.14e-05, step=2275]Training:   1%|          | 2276/200000 [49:22<74:55:14,  1.36s/it, loss=0.0435, lr=1.14e-05, step=2275]Training:   1%|          | 2276/200000 [49:22<74:55:14,  1.36s/it, loss=0.0377, lr=1.14e-05, step=2276]Training:   1%|          | 2277/200000 [49:23<70:11:45,  1.28s/it, loss=0.0377, lr=1.14e-05, step=2276]Training:   1%|          | 2277/200000 [49:23<70:11:45,  1.28s/it, loss=0.0478, lr=1.14e-05, step=2277]Training:   1%|          | 2278/200000 [49:24<66:57:32,  1.22s/it, loss=0.0478, lr=1.14e-05, step=2277]Training:   1%|          | 2278/200000 [49:24<66:57:32,  1.22s/it, loss=0.0365, lr=1.14e-05, step=2278]Training:   1%|          | 2279/200000 [49:26<70:15:22,  1.28s/it, loss=0.0365, lr=1.14e-05, step=2278]Training:   1%|          | 2279/200000 [49:26<70:15:22,  1.28s/it, loss=0.0273, lr=1.14e-05, step=2279]Training:   1%|          | 2280/200000 [49:27<66:58:30,  1.22s/it, loss=0.0273, lr=1.14e-05, step=2279]Training:   1%|          | 2280/200000 [49:27<66:58:30,  1.22s/it, loss=0.0455, lr=1.14e-05, step=2280]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2281/200000 [49:28<68:18:49,  1.24s/it, loss=0.0455, lr=1.14e-05, step=2280]Training:   1%|          | 2281/200000 [49:28<68:18:49,  1.24s/it, loss=0.0456, lr=1.14e-05, step=2281]Training:   1%|          | 2282/200000 [49:29<65:36:13,  1.19s/it, loss=0.0456, lr=1.14e-05, step=2281]Training:   1%|          | 2282/200000 [49:29<65:36:13,  1.19s/it, loss=0.1275, lr=1.14e-05, step=2282]Training:   1%|          | 2283/200000 [49:30<63:43:59,  1.16s/it, loss=0.1275, lr=1.14e-05, step=2282]Training:   1%|          | 2283/200000 [49:30<63:43:59,  1.16s/it, loss=0.0420, lr=1.14e-05, step=2283]Training:   1%|          | 2284/200000 [49:32<68:18:16,  1.24s/it, loss=0.0420, lr=1.14e-05, step=2283]Training:   1%|          | 2284/200000 [49:32<68:18:16,  1.24s/it, loss=0.0470, lr=1.14e-05, step=2284]Training:   1%|          | 2285/200000 [49:33<71:12:08,  1.30s/it, loss=0.0470, lr=1.14e-05, step=2284]Training:   1%|          | 2285/200000 [49:33<71:12:08,  1.30s/it, loss=0.0525, lr=1.14e-05, step=2285]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2286/200000 [49:34<73:20:36,  1.34s/it, loss=0.0525, lr=1.14e-05, step=2285]Training:   1%|          | 2286/200000 [49:34<73:20:36,  1.34s/it, loss=0.0452, lr=1.14e-05, step=2286]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2287/200000 [49:35<69:07:15,  1.26s/it, loss=0.0452, lr=1.14e-05, step=2286]Training:   1%|          | 2287/200000 [49:35<69:07:15,  1.26s/it, loss=0.0389, lr=1.14e-05, step=2287]Training:   1%|          | 2288/200000 [49:37<66:10:09,  1.20s/it, loss=0.0389, lr=1.14e-05, step=2287]Training:   1%|          | 2288/200000 [49:37<66:10:09,  1.20s/it, loss=0.0489, lr=1.14e-05, step=2288]Training:   1%|          | 2289/200000 [49:38<68:23:15,  1.25s/it, loss=0.0489, lr=1.14e-05, step=2288]Training:   1%|          | 2289/200000 [49:38<68:23:15,  1.25s/it, loss=0.0316, lr=1.14e-05, step=2289]Training:   1%|          | 2290/200000 [49:39<69:37:47,  1.27s/it, loss=0.0316, lr=1.14e-05, step=2289]Training:   1%|          | 2290/200000 [49:39<69:37:47,  1.27s/it, loss=0.0290, lr=1.14e-05, step=2290]Training:   1%|          | 2291/200000 [49:40<66:30:40,  1.21s/it, loss=0.0290, lr=1.14e-05, step=2290]Training:   1%|          | 2291/200000 [49:40<66:30:40,  1.21s/it, loss=0.0384, lr=1.15e-05, step=2291]Training:   1%|          | 2292/200000 [49:42<69:04:58,  1.26s/it, loss=0.0384, lr=1.15e-05, step=2291]Training:   1%|          | 2292/200000 [49:42<69:04:58,  1.26s/it, loss=0.0320, lr=1.15e-05, step=2292]Training:   1%|          | 2293/200000 [49:43<66:06:53,  1.20s/it, loss=0.0320, lr=1.15e-05, step=2292]Training:   1%|          | 2293/200000 [49:43<66:06:53,  1.20s/it, loss=0.0291, lr=1.15e-05, step=2293]Training:   1%|          | 2294/200000 [49:44<68:23:57,  1.25s/it, loss=0.0291, lr=1.15e-05, step=2293]Training:   1%|          | 2294/200000 [49:44<68:23:57,  1.25s/it, loss=0.0308, lr=1.15e-05, step=2294]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2295/200000 [49:46<72:10:00,  1.31s/it, loss=0.0308, lr=1.15e-05, step=2294]Training:   1%|          | 2295/200000 [49:46<72:10:00,  1.31s/it, loss=0.0615, lr=1.15e-05, step=2295]Training:   1%|          | 2296/200000 [49:47<73:55:06,  1.35s/it, loss=0.0615, lr=1.15e-05, step=2295]Training:   1%|          | 2296/200000 [49:47<73:55:06,  1.35s/it, loss=0.0262, lr=1.15e-05, step=2296]Training:   1%|          | 2297/200000 [49:48<75:37:09,  1.38s/it, loss=0.0262, lr=1.15e-05, step=2296]Training:   1%|          | 2297/200000 [49:48<75:37:09,  1.38s/it, loss=0.0400, lr=1.15e-05, step=2297]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2298/200000 [49:49<70:40:31,  1.29s/it, loss=0.0400, lr=1.15e-05, step=2297]Training:   1%|          | 2298/200000 [49:49<70:40:31,  1.29s/it, loss=0.0382, lr=1.15e-05, step=2298]Training:   1%|          | 2299/200000 [49:51<67:15:42,  1.22s/it, loss=0.0382, lr=1.15e-05, step=2298]Training:   1%|          | 2299/200000 [49:51<67:15:42,  1.22s/it, loss=0.0341, lr=1.15e-05, step=2299]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2300/200000 [49:52<69:28:15,  1.27s/it, loss=0.0341, lr=1.15e-05, step=2299]Training:   1%|          | 2300/200000 [49:52<69:28:15,  1.27s/it, loss=0.0379, lr=1.15e-05, step=2300]23:43:07.158 [I] step=2300 loss=0.0490 lr=1.13e-05 grad_norm=0.94 time=126.6s                     (701675:train_pytorch.py:582)
Training:   1%|          | 2301/200000 [49:53<72:03:58,  1.31s/it, loss=0.0379, lr=1.15e-05, step=2300]Training:   1%|          | 2301/200000 [49:53<72:03:58,  1.31s/it, loss=0.0406, lr=1.15e-05, step=2301]Training:   1%|          | 2302/200000 [49:54<68:11:22,  1.24s/it, loss=0.0406, lr=1.15e-05, step=2301]Training:   1%|          | 2302/200000 [49:54<68:11:22,  1.24s/it, loss=0.0292, lr=1.15e-05, step=2302]Training:   1%|          | 2303/200000 [49:56<71:40:13,  1.31s/it, loss=0.0292, lr=1.15e-05, step=2302]Training:   1%|          | 2303/200000 [49:56<71:40:13,  1.31s/it, loss=0.0315, lr=1.15e-05, step=2303]Training:   1%|          | 2304/200000 [49:57<67:55:55,  1.24s/it, loss=0.0315, lr=1.15e-05, step=2303]Training:   1%|          | 2304/200000 [49:57<67:55:55,  1.24s/it, loss=0.0468, lr=1.15e-05, step=2304]Training:   1%|          | 2305/200000 [49:58<70:16:48,  1.28s/it, loss=0.0468, lr=1.15e-05, step=2304]Training:   1%|          | 2305/200000 [49:58<70:16:48,  1.28s/it, loss=0.0343, lr=1.15e-05, step=2305]Training:   1%|          | 2306/200000 [49:59<66:59:47,  1.22s/it, loss=0.0343, lr=1.15e-05, step=2305]Training:   1%|          | 2306/200000 [49:59<66:59:47,  1.22s/it, loss=0.1838, lr=1.15e-05, step=2306]Training:   1%|          | 2307/200000 [50:01<71:40:39,  1.31s/it, loss=0.1838, lr=1.15e-05, step=2306]Training:   1%|          | 2307/200000 [50:01<71:40:39,  1.31s/it, loss=0.0960, lr=1.15e-05, step=2307]Training:   1%|          | 2308/200000 [50:02<75:20:23,  1.37s/it, loss=0.0960, lr=1.15e-05, step=2307]Training:   1%|          | 2308/200000 [50:02<75:20:23,  1.37s/it, loss=0.0370, lr=1.15e-05, step=2308]Training:   1%|          | 2309/200000 [50:04<70:31:36,  1.28s/it, loss=0.0370, lr=1.15e-05, step=2308]Training:   1%|          | 2309/200000 [50:04<70:31:36,  1.28s/it, loss=0.0475, lr=1.15e-05, step=2309]Training:   1%|          | 2310/200000 [50:05<67:06:39,  1.22s/it, loss=0.0475, lr=1.15e-05, step=2309]Training:   1%|          | 2310/200000 [50:05<67:06:39,  1.22s/it, loss=0.1106, lr=1.15e-05, step=2310]Training:   1%|          | 2311/200000 [50:06<70:42:47,  1.29s/it, loss=0.1106, lr=1.15e-05, step=2310]Training:   1%|          | 2311/200000 [50:06<70:42:47,  1.29s/it, loss=0.0305, lr=1.16e-05, step=2311]Training:   1%|          | 2312/200000 [50:07<67:16:44,  1.23s/it, loss=0.0305, lr=1.16e-05, step=2311]Training:   1%|          | 2312/200000 [50:07<67:16:44,  1.23s/it, loss=0.0810, lr=1.16e-05, step=2312]Training:   1%|          | 2313/200000 [50:08<68:58:22,  1.26s/it, loss=0.0810, lr=1.16e-05, step=2312]Training:   1%|          | 2313/200000 [50:08<68:58:22,  1.26s/it, loss=0.0358, lr=1.16e-05, step=2313]Training:   1%|          | 2314/200000 [50:10<66:04:27,  1.20s/it, loss=0.0358, lr=1.16e-05, step=2313]Training:   1%|          | 2314/200000 [50:10<66:04:27,  1.20s/it, loss=0.0400, lr=1.16e-05, step=2314]Training:   1%|          | 2315/200000 [50:11<63:59:18,  1.17s/it, loss=0.0400, lr=1.16e-05, step=2314]Training:   1%|          | 2315/200000 [50:11<63:59:18,  1.17s/it, loss=0.0292, lr=1.16e-05, step=2315]Training:   1%|          | 2316/200000 [50:12<69:17:59,  1.26s/it, loss=0.0292, lr=1.16e-05, step=2315]Training:   1%|          | 2316/200000 [50:12<69:17:59,  1.26s/it, loss=0.0407, lr=1.16e-05, step=2316]Training:   1%|          | 2317/200000 [50:14<72:34:36,  1.32s/it, loss=0.0407, lr=1.16e-05, step=2316]Training:   1%|          | 2317/200000 [50:14<72:34:36,  1.32s/it, loss=0.0587, lr=1.16e-05, step=2317]Training:   1%|          | 2318/200000 [50:15<74:35:34,  1.36s/it, loss=0.0587, lr=1.16e-05, step=2317]Training:   1%|          | 2318/200000 [50:15<74:35:34,  1.36s/it, loss=0.0241, lr=1.16e-05, step=2318]Training:   1%|          | 2319/200000 [50:16<70:00:23,  1.27s/it, loss=0.0241, lr=1.16e-05, step=2318]Training:   1%|          | 2319/200000 [50:16<70:00:23,  1.27s/it, loss=0.1587, lr=1.16e-05, step=2319]Training:   1%|          | 2320/200000 [50:17<66:46:22,  1.22s/it, loss=0.1587, lr=1.16e-05, step=2319]Training:   1%|          | 2320/200000 [50:17<66:46:22,  1.22s/it, loss=0.0371, lr=1.16e-05, step=2320]Training:   1%|          | 2321/200000 [50:19<69:14:35,  1.26s/it, loss=0.0371, lr=1.16e-05, step=2320]Training:   1%|          | 2321/200000 [50:19<69:14:35,  1.26s/it, loss=0.0363, lr=1.16e-05, step=2321]Training:   1%|          | 2322/200000 [50:20<71:08:25,  1.30s/it, loss=0.0363, lr=1.16e-05, step=2321]Training:   1%|          | 2322/200000 [50:20<71:08:25,  1.30s/it, loss=0.0442, lr=1.16e-05, step=2322]Training:   1%|          | 2323/200000 [50:21<67:59:22,  1.24s/it, loss=0.0442, lr=1.16e-05, step=2322]Training:   1%|          | 2323/200000 [50:21<67:59:22,  1.24s/it, loss=0.0407, lr=1.16e-05, step=2323]Training:   1%|          | 2324/200000 [50:22<71:05:43,  1.29s/it, loss=0.0407, lr=1.16e-05, step=2323]Training:   1%|          | 2324/200000 [50:22<71:05:43,  1.29s/it, loss=0.0371, lr=1.16e-05, step=2324]Training:   1%|          | 2325/200000 [50:24<67:34:34,  1.23s/it, loss=0.0371, lr=1.16e-05, step=2324]Training:   1%|          | 2325/200000 [50:24<67:34:34,  1.23s/it, loss=0.0312, lr=1.16e-05, step=2325]Training:   1%|          | 2326/200000 [50:25<70:39:34,  1.29s/it, loss=0.0312, lr=1.16e-05, step=2325]Training:   1%|          | 2326/200000 [50:25<70:39:34,  1.29s/it, loss=0.0261, lr=1.16e-05, step=2326]Training:   1%|          | 2327/200000 [50:26<67:12:05,  1.22s/it, loss=0.0261, lr=1.16e-05, step=2326]Training:   1%|          | 2327/200000 [50:26<67:12:05,  1.22s/it, loss=0.0531, lr=1.16e-05, step=2327]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2328/200000 [50:27<71:25:19,  1.30s/it, loss=0.0531, lr=1.16e-05, step=2327]Training:   1%|          | 2328/200000 [50:27<71:25:19,  1.30s/it, loss=0.0230, lr=1.16e-05, step=2328]Training:   1%|          | 2329/200000 [50:29<74:03:35,  1.35s/it, loss=0.0230, lr=1.16e-05, step=2328]Training:   1%|          | 2329/200000 [50:29<74:03:35,  1.35s/it, loss=0.0325, lr=1.16e-05, step=2329]Training:   1%|          | 2330/200000 [50:30<69:35:35,  1.27s/it, loss=0.0325, lr=1.16e-05, step=2329]Training:   1%|          | 2330/200000 [50:30<69:35:35,  1.27s/it, loss=0.0403, lr=1.16e-05, step=2330]Training:   1%|          | 2331/200000 [50:31<66:28:53,  1.21s/it, loss=0.0403, lr=1.16e-05, step=2330]Training:   1%|          | 2331/200000 [50:31<66:28:53,  1.21s/it, loss=0.0500, lr=1.17e-05, step=2331]Training:   1%|          | 2332/200000 [50:33<69:58:10,  1.27s/it, loss=0.0500, lr=1.17e-05, step=2331]Training:   1%|          | 2332/200000 [50:33<69:58:10,  1.27s/it, loss=0.0651, lr=1.17e-05, step=2332]Training:   1%|          | 2333/200000 [50:34<66:44:01,  1.22s/it, loss=0.0651, lr=1.17e-05, step=2332]Training:   1%|          | 2333/200000 [50:34<66:44:01,  1.22s/it, loss=0.0451, lr=1.17e-05, step=2333]Training:   1%|          | 2334/200000 [50:35<68:23:18,  1.25s/it, loss=0.0451, lr=1.17e-05, step=2333]Training:   1%|          | 2334/200000 [50:35<68:23:18,  1.25s/it, loss=0.0338, lr=1.17e-05, step=2334]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2335/200000 [50:36<65:36:10,  1.19s/it, loss=0.0338, lr=1.17e-05, step=2334]Training:   1%|          | 2335/200000 [50:36<65:36:10,  1.19s/it, loss=0.0541, lr=1.17e-05, step=2335]Training:   1%|          | 2336/200000 [50:37<63:41:20,  1.16s/it, loss=0.0541, lr=1.17e-05, step=2335]Training:   1%|          | 2336/200000 [50:37<63:41:20,  1.16s/it, loss=0.0424, lr=1.17e-05, step=2336]Training:   1%|          | 2337/200000 [50:38<67:57:06,  1.24s/it, loss=0.0424, lr=1.17e-05, step=2336]Training:   1%|          | 2337/200000 [50:38<67:57:06,  1.24s/it, loss=0.0373, lr=1.17e-05, step=2337]Training:   1%|          | 2338/200000 [50:40<71:15:42,  1.30s/it, loss=0.0373, lr=1.17e-05, step=2337]Training:   1%|          | 2338/200000 [50:40<71:15:42,  1.30s/it, loss=0.0656, lr=1.17e-05, step=2338]Training:   1%|          | 2339/200000 [50:41<73:29:41,  1.34s/it, loss=0.0656, lr=1.17e-05, step=2338]Training:   1%|          | 2339/200000 [50:41<73:29:41,  1.34s/it, loss=0.0396, lr=1.17e-05, step=2339]Training:   1%|          | 2340/200000 [50:42<69:12:37,  1.26s/it, loss=0.0396, lr=1.17e-05, step=2339]Training:   1%|          | 2340/200000 [50:42<69:12:37,  1.26s/it, loss=0.0542, lr=1.17e-05, step=2340]Training:   1%|          | 2341/200000 [50:44<66:11:43,  1.21s/it, loss=0.0542, lr=1.17e-05, step=2340]Training:   1%|          | 2341/200000 [50:44<66:11:43,  1.21s/it, loss=0.1081, lr=1.17e-05, step=2341]Training:   1%|          | 2342/200000 [50:45<68:21:55,  1.25s/it, loss=0.1081, lr=1.17e-05, step=2341]Training:   1%|          | 2342/200000 [50:45<68:21:55,  1.25s/it, loss=0.0455, lr=1.17e-05, step=2342]Training:   1%|          | 2343/200000 [50:46<69:36:57,  1.27s/it, loss=0.0455, lr=1.17e-05, step=2342]Training:   1%|          | 2343/200000 [50:46<69:36:57,  1.27s/it, loss=0.0205, lr=1.17e-05, step=2343]Training:   1%|          | 2344/200000 [50:47<66:34:22,  1.21s/it, loss=0.0205, lr=1.17e-05, step=2343]Training:   1%|          | 2344/200000 [50:47<66:34:22,  1.21s/it, loss=0.0418, lr=1.17e-05, step=2344]Training:   1%|          | 2345/200000 [50:49<69:19:07,  1.26s/it, loss=0.0418, lr=1.17e-05, step=2344]Training:   1%|          | 2345/200000 [50:49<69:19:07,  1.26s/it, loss=0.0485, lr=1.17e-05, step=2345]Training:   1%|          | 2346/200000 [50:50<66:15:51,  1.21s/it, loss=0.0485, lr=1.17e-05, step=2345]Training:   1%|          | 2346/200000 [50:50<66:15:51,  1.21s/it, loss=0.0498, lr=1.17e-05, step=2346]Training:   1%|          | 2347/200000 [50:51<69:07:13,  1.26s/it, loss=0.0498, lr=1.17e-05, step=2346]Training:   1%|          | 2347/200000 [50:51<69:07:13,  1.26s/it, loss=0.0479, lr=1.17e-05, step=2347]Training:   1%|          | 2348/200000 [50:53<72:02:26,  1.31s/it, loss=0.0479, lr=1.17e-05, step=2347]Training:   1%|          | 2348/200000 [50:53<72:02:26,  1.31s/it, loss=0.0503, lr=1.17e-05, step=2348]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2349/200000 [50:54<73:49:12,  1.34s/it, loss=0.0503, lr=1.17e-05, step=2348]Training:   1%|          | 2349/200000 [50:54<73:49:12,  1.34s/it, loss=0.0426, lr=1.17e-05, step=2349]Training:   1%|          | 2350/200000 [50:55<75:31:31,  1.38s/it, loss=0.0426, lr=1.17e-05, step=2349]Training:   1%|          | 2350/200000 [50:55<75:31:31,  1.38s/it, loss=0.0807, lr=1.17e-05, step=2350]Training:   1%|          | 2351/200000 [50:56<70:35:29,  1.29s/it, loss=0.0807, lr=1.17e-05, step=2350]Training:   1%|          | 2351/200000 [50:56<70:35:29,  1.29s/it, loss=0.0287, lr=1.18e-05, step=2351]Training:   1%|          | 2352/200000 [50:58<67:11:15,  1.22s/it, loss=0.0287, lr=1.18e-05, step=2351]Training:   1%|          | 2352/200000 [50:58<67:11:15,  1.22s/it, loss=0.0554, lr=1.18e-05, step=2352]Training:   1%|          | 2353/200000 [50:59<69:24:13,  1.26s/it, loss=0.0554, lr=1.18e-05, step=2352]Training:   1%|          | 2353/200000 [50:59<69:24:13,  1.26s/it, loss=0.0316, lr=1.18e-05, step=2353]Training:   1%|          | 2354/200000 [51:00<72:09:19,  1.31s/it, loss=0.0316, lr=1.18e-05, step=2353]Training:   1%|          | 2354/200000 [51:00<72:09:19,  1.31s/it, loss=0.0239, lr=1.18e-05, step=2354]Training:   1%|          | 2355/200000 [51:01<68:15:25,  1.24s/it, loss=0.0239, lr=1.18e-05, step=2354]Training:   1%|          | 2355/200000 [51:01<68:15:25,  1.24s/it, loss=0.0561, lr=1.18e-05, step=2355]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2356/200000 [51:03<71:48:40,  1.31s/it, loss=0.0561, lr=1.18e-05, step=2355]Training:   1%|          | 2356/200000 [51:03<71:48:40,  1.31s/it, loss=0.0596, lr=1.18e-05, step=2356]Training:   1%|          | 2357/200000 [51:04<68:03:05,  1.24s/it, loss=0.0596, lr=1.18e-05, step=2356]Training:   1%|          | 2357/200000 [51:04<68:03:05,  1.24s/it, loss=0.0687, lr=1.18e-05, step=2357]Training:   1%|          | 2358/200000 [51:05<70:26:32,  1.28s/it, loss=0.0687, lr=1.18e-05, step=2357]Training:   1%|          | 2358/200000 [51:05<70:26:32,  1.28s/it, loss=0.0453, lr=1.18e-05, step=2358]Training:   1%|          | 2359/200000 [51:06<67:04:29,  1.22s/it, loss=0.0453, lr=1.18e-05, step=2358]Training:   1%|          | 2359/200000 [51:06<67:04:29,  1.22s/it, loss=0.0259, lr=1.18e-05, step=2359]Training:   1%|          | 2360/200000 [51:08<71:38:11,  1.30s/it, loss=0.0259, lr=1.18e-05, step=2359]Training:   1%|          | 2360/200000 [51:08<71:38:11,  1.30s/it, loss=0.0584, lr=1.18e-05, step=2360]Training:   1%|          | 2361/200000 [51:09<74:39:11,  1.36s/it, loss=0.0584, lr=1.18e-05, step=2360]Training:   1%|          | 2361/200000 [51:09<74:39:11,  1.36s/it, loss=0.0585, lr=1.18e-05, step=2361]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2362/200000 [51:10<70:01:08,  1.28s/it, loss=0.0585, lr=1.18e-05, step=2361]Training:   1%|          | 2362/200000 [51:10<70:01:08,  1.28s/it, loss=0.0581, lr=1.18e-05, step=2362]Training:   1%|          | 2363/200000 [51:12<66:46:20,  1.22s/it, loss=0.0581, lr=1.18e-05, step=2362]Training:   1%|          | 2363/200000 [51:12<66:46:20,  1.22s/it, loss=0.0641, lr=1.18e-05, step=2363]Training:   1%|          | 2364/200000 [51:13<70:28:28,  1.28s/it, loss=0.0641, lr=1.18e-05, step=2363]Training:   1%|          | 2364/200000 [51:13<70:28:28,  1.28s/it, loss=0.0681, lr=1.18e-05, step=2364]Training:   1%|          | 2365/200000 [51:14<67:05:05,  1.22s/it, loss=0.0681, lr=1.18e-05, step=2364]Training:   1%|          | 2365/200000 [51:14<67:05:05,  1.22s/it, loss=0.0443, lr=1.18e-05, step=2365]Training:   1%|          | 2366/200000 [51:15<69:06:34,  1.26s/it, loss=0.0443, lr=1.18e-05, step=2365]Training:   1%|          | 2366/200000 [51:15<69:06:34,  1.26s/it, loss=0.0569, lr=1.18e-05, step=2366]Training:   1%|          | 2367/200000 [51:17<66:06:20,  1.20s/it, loss=0.0569, lr=1.18e-05, step=2366]Training:   1%|          | 2367/200000 [51:17<66:06:20,  1.20s/it, loss=0.0284, lr=1.18e-05, step=2367]Training:   1%|          | 2368/200000 [51:18<64:01:18,  1.17s/it, loss=0.0284, lr=1.18e-05, step=2367]Training:   1%|          | 2368/200000 [51:18<64:01:18,  1.17s/it, loss=0.0327, lr=1.18e-05, step=2368]Training:   1%|          | 2369/200000 [51:19<68:59:53,  1.26s/it, loss=0.0327, lr=1.18e-05, step=2368]Training:   1%|          | 2369/200000 [51:19<68:59:53,  1.26s/it, loss=0.0332, lr=1.18e-05, step=2369]Training:   1%|          | 2370/200000 [51:21<72:07:45,  1.31s/it, loss=0.0332, lr=1.18e-05, step=2369]Training:   1%|          | 2370/200000 [51:21<72:07:45,  1.31s/it, loss=0.0371, lr=1.18e-05, step=2370]Training:   1%|          | 2371/200000 [51:22<74:08:01,  1.35s/it, loss=0.0371, lr=1.18e-05, step=2370]Training:   1%|          | 2371/200000 [51:22<74:08:01,  1.35s/it, loss=0.0501, lr=1.19e-05, step=2371]Training:   1%|          | 2372/200000 [51:23<69:36:45,  1.27s/it, loss=0.0501, lr=1.19e-05, step=2371]Training:   1%|          | 2372/200000 [51:23<69:36:45,  1.27s/it, loss=0.0459, lr=1.19e-05, step=2372]Training:   1%|          | 2373/200000 [51:24<66:27:11,  1.21s/it, loss=0.0459, lr=1.19e-05, step=2372]Training:   1%|          | 2373/200000 [51:24<66:27:11,  1.21s/it, loss=0.0375, lr=1.19e-05, step=2373]Training:   1%|          | 2374/200000 [51:25<68:43:54,  1.25s/it, loss=0.0375, lr=1.19e-05, step=2373]Training:   1%|          | 2374/200000 [51:25<68:43:54,  1.25s/it, loss=0.0597, lr=1.19e-05, step=2374]Training:   1%|          | 2375/200000 [51:27<71:03:33,  1.29s/it, loss=0.0597, lr=1.19e-05, step=2374]Training:   1%|          | 2375/200000 [51:27<71:03:33,  1.29s/it, loss=0.0360, lr=1.19e-05, step=2375]Training:   1%|          | 2376/200000 [51:28<67:31:11,  1.23s/it, loss=0.0360, lr=1.19e-05, step=2375]Training:   1%|          | 2376/200000 [51:28<67:31:11,  1.23s/it, loss=0.0523, lr=1.19e-05, step=2376]Training:   1%|          | 2377/200000 [51:29<70:50:25,  1.29s/it, loss=0.0523, lr=1.19e-05, step=2376]Training:   1%|          | 2377/200000 [51:29<70:50:25,  1.29s/it, loss=0.0283, lr=1.19e-05, step=2377]Training:   1%|          | 2378/200000 [51:30<67:20:33,  1.23s/it, loss=0.0283, lr=1.19e-05, step=2377]Training:   1%|          | 2378/200000 [51:30<67:20:33,  1.23s/it, loss=0.0556, lr=1.19e-05, step=2378]Training:   1%|          | 2379/200000 [51:32<70:24:45,  1.28s/it, loss=0.0556, lr=1.19e-05, step=2378]Training:   1%|          | 2379/200000 [51:32<70:24:45,  1.28s/it, loss=0.0725, lr=1.19e-05, step=2379]Training:   1%|          | 2380/200000 [51:33<67:00:51,  1.22s/it, loss=0.0725, lr=1.19e-05, step=2379]Training:   1%|          | 2380/200000 [51:33<67:00:51,  1.22s/it, loss=0.0323, lr=1.19e-05, step=2380]Training:   1%|          | 2381/200000 [51:34<71:20:09,  1.30s/it, loss=0.0323, lr=1.19e-05, step=2380]Training:   1%|          | 2381/200000 [51:34<71:20:09,  1.30s/it, loss=0.0522, lr=1.19e-05, step=2381]Training:   1%|          | 2382/200000 [51:36<74:51:01,  1.36s/it, loss=0.0522, lr=1.19e-05, step=2381]Training:   1%|          | 2382/200000 [51:36<74:51:01,  1.36s/it, loss=0.0290, lr=1.19e-05, step=2382]Training:   1%|          | 2383/200000 [51:37<70:07:58,  1.28s/it, loss=0.0290, lr=1.19e-05, step=2382]Training:   1%|          | 2383/200000 [51:37<70:07:58,  1.28s/it, loss=0.0374, lr=1.19e-05, step=2383]Training:   1%|          | 2384/200000 [51:38<66:50:35,  1.22s/it, loss=0.0374, lr=1.19e-05, step=2383]Training:   1%|          | 2384/200000 [51:38<66:50:35,  1.22s/it, loss=0.0507, lr=1.19e-05, step=2384]Training:   1%|          | 2385/200000 [51:40<70:53:58,  1.29s/it, loss=0.0507, lr=1.19e-05, step=2384]Training:   1%|          | 2385/200000 [51:40<70:53:58,  1.29s/it, loss=0.0515, lr=1.19e-05, step=2385]Training:   1%|          | 2386/200000 [51:41<67:21:14,  1.23s/it, loss=0.0515, lr=1.19e-05, step=2385]Training:   1%|          | 2386/200000 [51:41<67:21:14,  1.23s/it, loss=0.0511, lr=1.19e-05, step=2386]Training:   1%|          | 2387/200000 [51:42<69:10:23,  1.26s/it, loss=0.0511, lr=1.19e-05, step=2386]Training:   1%|          | 2387/200000 [51:42<69:10:23,  1.26s/it, loss=0.0398, lr=1.19e-05, step=2387]Training:   1%|          | 2388/200000 [51:43<66:09:28,  1.21s/it, loss=0.0398, lr=1.19e-05, step=2387]Training:   1%|          | 2388/200000 [51:43<66:09:28,  1.21s/it, loss=0.0921, lr=1.19e-05, step=2388]Training:   1%|          | 2389/200000 [51:44<64:02:17,  1.17s/it, loss=0.0921, lr=1.19e-05, step=2388]Training:   1%|          | 2389/200000 [51:44<64:02:17,  1.17s/it, loss=0.0408, lr=1.19e-05, step=2389]Training:   1%|          | 2390/200000 [51:46<68:25:26,  1.25s/it, loss=0.0408, lr=1.19e-05, step=2389]Training:   1%|          | 2390/200000 [51:46<68:25:26,  1.25s/it, loss=0.1439, lr=1.19e-05, step=2390]Training:   1%|          | 2391/200000 [51:47<71:54:34,  1.31s/it, loss=0.1439, lr=1.19e-05, step=2390]Training:   1%|          | 2391/200000 [51:47<71:54:34,  1.31s/it, loss=0.0443, lr=1.20e-05, step=2391]Training:   1%|          | 2392/200000 [51:48<72:48:21,  1.33s/it, loss=0.0443, lr=1.20e-05, step=2391]Training:   1%|          | 2392/200000 [51:48<72:48:21,  1.33s/it, loss=0.0405, lr=1.20e-05, step=2392]Training:   1%|          | 2393/200000 [51:49<68:43:11,  1.25s/it, loss=0.0405, lr=1.20e-05, step=2392]Training:   1%|          | 2393/200000 [51:49<68:43:11,  1.25s/it, loss=0.0447, lr=1.20e-05, step=2393]Training:   1%|          | 2394/200000 [51:51<65:53:40,  1.20s/it, loss=0.0447, lr=1.20e-05, step=2393]Training:   1%|          | 2394/200000 [51:51<65:53:40,  1.20s/it, loss=0.0448, lr=1.20e-05, step=2394]Training:   1%|          | 2395/200000 [51:52<68:16:14,  1.24s/it, loss=0.0448, lr=1.20e-05, step=2394]Training:   1%|          | 2395/200000 [51:52<68:16:14,  1.24s/it, loss=0.0281, lr=1.20e-05, step=2395]Training:   1%|          | 2396/200000 [51:53<69:36:27,  1.27s/it, loss=0.0281, lr=1.20e-05, step=2395]Training:   1%|          | 2396/200000 [51:53<69:36:27,  1.27s/it, loss=0.0397, lr=1.20e-05, step=2396]Training:   1%|          | 2397/200000 [51:54<66:57:09,  1.22s/it, loss=0.0397, lr=1.20e-05, step=2396]Training:   1%|          | 2397/200000 [51:54<66:57:09,  1.22s/it, loss=0.0576, lr=1.20e-05, step=2397]Training:   1%|          | 2398/200000 [51:56<69:27:14,  1.27s/it, loss=0.0576, lr=1.20e-05, step=2397]Training:   1%|          | 2398/200000 [51:56<69:27:14,  1.27s/it, loss=0.0446, lr=1.20e-05, step=2398]Training:   1%|          | 2399/200000 [51:57<66:30:12,  1.21s/it, loss=0.0446, lr=1.20e-05, step=2398]Training:   1%|          | 2399/200000 [51:57<66:30:12,  1.21s/it, loss=0.0261, lr=1.20e-05, step=2399]Training:   1%|          | 2400/200000 [51:58<68:38:46,  1.25s/it, loss=0.0261, lr=1.20e-05, step=2399]Training:   1%|          | 2400/200000 [51:58<68:38:46,  1.25s/it, loss=0.1063, lr=1.20e-05, step=2400]23:45:13.390 [I] step=2400 loss=0.0506 lr=1.18e-05 grad_norm=0.93 time=126.2s                     (701675:train_pytorch.py:582)
Training:   1%|          | 2401/200000 [52:00<72:33:44,  1.32s/it, loss=0.1063, lr=1.20e-05, step=2400]Training:   1%|          | 2401/200000 [52:00<72:33:44,  1.32s/it, loss=0.0506, lr=1.20e-05, step=2401]Training:   1%|          | 2402/200000 [52:01<74:14:52,  1.35s/it, loss=0.0506, lr=1.20e-05, step=2401]Training:   1%|          | 2402/200000 [52:01<74:14:52,  1.35s/it, loss=0.0357, lr=1.20e-05, step=2402]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2403/200000 [52:02<75:57:46,  1.38s/it, loss=0.0357, lr=1.20e-05, step=2402]Training:   1%|          | 2403/200000 [52:02<75:57:46,  1.38s/it, loss=0.0329, lr=1.20e-05, step=2403]Training:   1%|          | 2404/200000 [52:04<70:58:47,  1.29s/it, loss=0.0329, lr=1.20e-05, step=2403]Training:   1%|          | 2404/200000 [52:04<70:58:47,  1.29s/it, loss=0.0511, lr=1.20e-05, step=2404]Training:   1%|          | 2405/200000 [52:05<67:27:49,  1.23s/it, loss=0.0511, lr=1.20e-05, step=2404]Training:   1%|          | 2405/200000 [52:05<67:27:49,  1.23s/it, loss=0.0384, lr=1.20e-05, step=2405]Training:   1%|          | 2406/200000 [52:06<69:42:00,  1.27s/it, loss=0.0384, lr=1.20e-05, step=2405]Training:   1%|          | 2406/200000 [52:06<69:42:00,  1.27s/it, loss=0.0510, lr=1.20e-05, step=2406]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2407/200000 [52:07<72:08:10,  1.31s/it, loss=0.0510, lr=1.20e-05, step=2406]Training:   1%|          | 2407/200000 [52:07<72:08:10,  1.31s/it, loss=0.0509, lr=1.20e-05, step=2407]Training:   1%|          | 2408/200000 [52:08<68:16:22,  1.24s/it, loss=0.0509, lr=1.20e-05, step=2407]Training:   1%|          | 2408/200000 [52:08<68:16:22,  1.24s/it, loss=0.0273, lr=1.20e-05, step=2408]Training:   1%|          | 2409/200000 [52:10<71:37:08,  1.30s/it, loss=0.0273, lr=1.20e-05, step=2408]Training:   1%|          | 2409/200000 [52:10<71:37:08,  1.30s/it, loss=0.0373, lr=1.20e-05, step=2409]Training:   1%|          | 2410/200000 [52:11<67:59:26,  1.24s/it, loss=0.0373, lr=1.20e-05, step=2409]Training:   1%|          | 2410/200000 [52:11<67:59:26,  1.24s/it, loss=0.0737, lr=1.20e-05, step=2410]Training:   1%|          | 2411/200000 [52:12<70:29:59,  1.28s/it, loss=0.0737, lr=1.20e-05, step=2410]Training:   1%|          | 2411/200000 [52:12<70:29:59,  1.28s/it, loss=0.0496, lr=1.21e-05, step=2411]Training:   1%|          | 2412/200000 [52:13<67:10:08,  1.22s/it, loss=0.0496, lr=1.21e-05, step=2411]Training:   1%|          | 2412/200000 [52:13<67:10:08,  1.22s/it, loss=0.0352, lr=1.21e-05, step=2412]Training:   1%|          | 2413/200000 [52:15<71:50:55,  1.31s/it, loss=0.0352, lr=1.21e-05, step=2412]Training:   1%|          | 2413/200000 [52:15<71:50:55,  1.31s/it, loss=0.0406, lr=1.21e-05, step=2413]Training:   1%|          | 2414/200000 [52:17<75:26:45,  1.37s/it, loss=0.0406, lr=1.21e-05, step=2413]Training:   1%|          | 2414/200000 [52:17<75:26:45,  1.37s/it, loss=0.0467, lr=1.21e-05, step=2414]Training:   1%|          | 2415/200000 [52:18<70:39:08,  1.29s/it, loss=0.0467, lr=1.21e-05, step=2414]Training:   1%|          | 2415/200000 [52:18<70:39:08,  1.29s/it, loss=0.0357, lr=1.21e-05, step=2415]Training:   1%|          | 2416/200000 [52:19<67:20:49,  1.23s/it, loss=0.0357, lr=1.21e-05, step=2415]Training:   1%|          | 2416/200000 [52:19<67:20:49,  1.23s/it, loss=0.0353, lr=1.21e-05, step=2416]Training:   1%|          | 2417/200000 [52:20<71:41:36,  1.31s/it, loss=0.0353, lr=1.21e-05, step=2416]Training:   1%|          | 2417/200000 [52:20<71:41:36,  1.31s/it, loss=0.0567, lr=1.21e-05, step=2417]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2418/200000 [52:21<67:57:17,  1.24s/it, loss=0.0567, lr=1.21e-05, step=2417]Training:   1%|          | 2418/200000 [52:21<67:57:17,  1.24s/it, loss=0.0483, lr=1.21e-05, step=2418]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2419/200000 [52:23<69:28:13,  1.27s/it, loss=0.0483, lr=1.21e-05, step=2418]Training:   1%|          | 2419/200000 [52:23<69:28:13,  1.27s/it, loss=0.0294, lr=1.21e-05, step=2419]Training:   1%|          | 2420/200000 [52:24<66:26:13,  1.21s/it, loss=0.0294, lr=1.21e-05, step=2419]Training:   1%|          | 2420/200000 [52:24<66:26:13,  1.21s/it, loss=0.0454, lr=1.21e-05, step=2420]Training:   1%|          | 2421/200000 [52:25<64:21:39,  1.17s/it, loss=0.0454, lr=1.21e-05, step=2420]Training:   1%|          | 2421/200000 [52:25<64:21:39,  1.17s/it, loss=0.0637, lr=1.21e-05, step=2421]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2422/200000 [52:26<68:52:55,  1.26s/it, loss=0.0637, lr=1.21e-05, step=2421]Training:   1%|          | 2422/200000 [52:26<68:52:55,  1.26s/it, loss=0.0357, lr=1.21e-05, step=2422]Training:   1%|          | 2423/200000 [52:28<72:29:48,  1.32s/it, loss=0.0357, lr=1.21e-05, step=2422]Training:   1%|          | 2423/200000 [52:28<72:29:48,  1.32s/it, loss=0.0378, lr=1.21e-05, step=2423]Training:   1%|          | 2424/200000 [52:29<74:17:52,  1.35s/it, loss=0.0378, lr=1.21e-05, step=2423]Training:   1%|          | 2424/200000 [52:29<74:17:52,  1.35s/it, loss=0.0613, lr=1.21e-05, step=2424]Training:   1%|          | 2425/200000 [52:30<69:50:44,  1.27s/it, loss=0.0613, lr=1.21e-05, step=2424]Training:   1%|          | 2425/200000 [52:30<69:50:44,  1.27s/it, loss=0.0409, lr=1.21e-05, step=2425]Training:   1%|          | 2426/200000 [52:31<66:44:01,  1.22s/it, loss=0.0409, lr=1.21e-05, step=2425]Training:   1%|          | 2426/200000 [52:31<66:44:01,  1.22s/it, loss=0.0336, lr=1.21e-05, step=2426]Training:   1%|          | 2427/200000 [52:33<69:05:46,  1.26s/it, loss=0.0336, lr=1.21e-05, step=2426]Training:   1%|          | 2427/200000 [52:33<69:05:46,  1.26s/it, loss=0.0305, lr=1.21e-05, step=2427]Training:   1%|          | 2428/200000 [52:34<71:30:36,  1.30s/it, loss=0.0305, lr=1.21e-05, step=2427]Training:   1%|          | 2428/200000 [52:34<71:30:36,  1.30s/it, loss=0.0562, lr=1.21e-05, step=2428]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2429/200000 [52:35<67:54:37,  1.24s/it, loss=0.0562, lr=1.21e-05, step=2428]Training:   1%|          | 2429/200000 [52:35<67:54:37,  1.24s/it, loss=0.0414, lr=1.21e-05, step=2429]Training:   1%|          | 2430/200000 [52:37<71:22:58,  1.30s/it, loss=0.0414, lr=1.21e-05, step=2429]Training:   1%|          | 2430/200000 [52:37<71:22:58,  1.30s/it, loss=0.0609, lr=1.21e-05, step=2430]Training:   1%|          | 2431/200000 [52:38<67:48:49,  1.24s/it, loss=0.0609, lr=1.21e-05, step=2430]Training:   1%|          | 2431/200000 [52:38<67:48:49,  1.24s/it, loss=0.0323, lr=1.22e-05, step=2431]Training:   1%|          | 2432/200000 [52:39<70:42:01,  1.29s/it, loss=0.0323, lr=1.22e-05, step=2431]Training:   1%|          | 2432/200000 [52:39<70:42:01,  1.29s/it, loss=0.0696, lr=1.22e-05, step=2432]Training:   1%|          | 2433/200000 [52:40<67:19:14,  1.23s/it, loss=0.0696, lr=1.22e-05, step=2432]Training:   1%|          | 2433/200000 [52:40<67:19:14,  1.23s/it, loss=0.0465, lr=1.22e-05, step=2433]Training:   1%|          | 2434/200000 [52:42<71:32:11,  1.30s/it, loss=0.0465, lr=1.22e-05, step=2433]Training:   1%|          | 2434/200000 [52:42<71:32:11,  1.30s/it, loss=0.0317, lr=1.22e-05, step=2434]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2435/200000 [52:43<75:03:05,  1.37s/it, loss=0.0317, lr=1.22e-05, step=2434]Training:   1%|          | 2435/200000 [52:43<75:03:05,  1.37s/it, loss=0.1117, lr=1.22e-05, step=2435]Training:   1%|          | 2436/200000 [52:44<70:21:49,  1.28s/it, loss=0.1117, lr=1.22e-05, step=2435]Training:   1%|          | 2436/200000 [52:44<70:21:49,  1.28s/it, loss=0.0338, lr=1.22e-05, step=2436]Training:   1%|          | 2437/200000 [52:45<67:05:44,  1.22s/it, loss=0.0338, lr=1.22e-05, step=2436]Training:   1%|          | 2437/200000 [52:45<67:05:44,  1.22s/it, loss=0.0425, lr=1.22e-05, step=2437]Training:   1%|          | 2438/200000 [52:47<71:10:31,  1.30s/it, loss=0.0425, lr=1.22e-05, step=2437]Training:   1%|          | 2438/200000 [52:47<71:10:31,  1.30s/it, loss=0.0354, lr=1.22e-05, step=2438]Training:   1%|          | 2439/200000 [52:48<67:37:37,  1.23s/it, loss=0.0354, lr=1.22e-05, step=2438]Training:   1%|          | 2439/200000 [52:48<67:37:37,  1.23s/it, loss=0.0289, lr=1.22e-05, step=2439]Training:   1%|          | 2440/200000 [52:49<68:55:18,  1.26s/it, loss=0.0289, lr=1.22e-05, step=2439]Training:   1%|          | 2440/200000 [52:49<68:55:18,  1.26s/it, loss=0.0325, lr=1.22e-05, step=2440]Training:   1%|          | 2441/200000 [52:50<66:05:21,  1.20s/it, loss=0.0325, lr=1.22e-05, step=2440]Training:   1%|          | 2441/200000 [52:50<66:05:21,  1.20s/it, loss=0.0331, lr=1.22e-05, step=2441]Training:   1%|          | 2442/200000 [52:51<64:03:05,  1.17s/it, loss=0.0331, lr=1.22e-05, step=2441]Training:   1%|          | 2442/200000 [52:51<64:03:05,  1.17s/it, loss=0.0703, lr=1.22e-05, step=2442]Training:   1%|          | 2443/200000 [52:53<68:19:37,  1.25s/it, loss=0.0703, lr=1.22e-05, step=2442]Training:   1%|          | 2443/200000 [52:53<68:19:37,  1.25s/it, loss=0.0584, lr=1.22e-05, step=2443]Training:   1%|          | 2444/200000 [52:54<71:53:40,  1.31s/it, loss=0.0584, lr=1.22e-05, step=2443]Training:   1%|          | 2444/200000 [52:54<71:53:40,  1.31s/it, loss=0.0515, lr=1.22e-05, step=2444]Training:   1%|          | 2445/200000 [52:56<74:06:51,  1.35s/it, loss=0.0515, lr=1.22e-05, step=2444]Training:   1%|          | 2445/200000 [52:56<74:06:51,  1.35s/it, loss=0.0459, lr=1.22e-05, step=2445]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2446/200000 [52:57<69:39:51,  1.27s/it, loss=0.0459, lr=1.22e-05, step=2445]Training:   1%|          | 2446/200000 [52:57<69:39:51,  1.27s/it, loss=0.0617, lr=1.22e-05, step=2446]Training:   1%|          | 2447/200000 [52:58<66:37:10,  1.21s/it, loss=0.0617, lr=1.22e-05, step=2446]Training:   1%|          | 2447/200000 [52:58<66:37:10,  1.21s/it, loss=0.0670, lr=1.22e-05, step=2447]Training:   1%|          | 2448/200000 [52:59<68:30:17,  1.25s/it, loss=0.0670, lr=1.22e-05, step=2447]Training:   1%|          | 2448/200000 [52:59<68:30:17,  1.25s/it, loss=0.0849, lr=1.22e-05, step=2448]Training:   1%|          | 2449/200000 [53:01<69:53:57,  1.27s/it, loss=0.0849, lr=1.22e-05, step=2448]Training:   1%|          | 2449/200000 [53:01<69:53:57,  1.27s/it, loss=0.0247, lr=1.22e-05, step=2449]Training:   1%|          | 2450/200000 [53:02<66:46:19,  1.22s/it, loss=0.0247, lr=1.22e-05, step=2449]Training:   1%|          | 2450/200000 [53:02<66:46:19,  1.22s/it, loss=0.0419, lr=1.22e-05, step=2450]Training:   1%|          | 2451/200000 [53:03<68:55:54,  1.26s/it, loss=0.0419, lr=1.22e-05, step=2450]Training:   1%|          | 2451/200000 [53:03<68:55:54,  1.26s/it, loss=0.0315, lr=1.23e-05, step=2451]Training:   1%|          | 2452/200000 [53:04<66:02:02,  1.20s/it, loss=0.0315, lr=1.23e-05, step=2451]Training:   1%|          | 2452/200000 [53:04<66:02:02,  1.20s/it, loss=0.0615, lr=1.23e-05, step=2452]Training:   1%|          | 2453/200000 [53:05<69:07:14,  1.26s/it, loss=0.0615, lr=1.23e-05, step=2452]Training:   1%|          | 2453/200000 [53:05<69:07:14,  1.26s/it, loss=0.0431, lr=1.23e-05, step=2453]Training:   1%|          | 2454/200000 [53:07<72:36:12,  1.32s/it, loss=0.0431, lr=1.23e-05, step=2453]Training:   1%|          | 2454/200000 [53:07<72:36:12,  1.32s/it, loss=0.0435, lr=1.23e-05, step=2454]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2455/200000 [53:08<74:15:19,  1.35s/it, loss=0.0435, lr=1.23e-05, step=2454]Training:   1%|          | 2455/200000 [53:08<74:15:19,  1.35s/it, loss=0.0340, lr=1.23e-05, step=2455]Training:   1%|          | 2456/200000 [53:10<75:54:23,  1.38s/it, loss=0.0340, lr=1.23e-05, step=2455]Training:   1%|          | 2456/200000 [53:10<75:54:23,  1.38s/it, loss=0.0430, lr=1.23e-05, step=2456]Training:   1%|          | 2457/200000 [53:11<71:00:35,  1.29s/it, loss=0.0430, lr=1.23e-05, step=2456]Training:   1%|          | 2457/200000 [53:11<71:00:35,  1.29s/it, loss=0.0429, lr=1.23e-05, step=2457]Training:   1%|          | 2458/200000 [53:12<67:28:57,  1.23s/it, loss=0.0429, lr=1.23e-05, step=2457]Training:   1%|          | 2458/200000 [53:12<67:28:57,  1.23s/it, loss=0.0284, lr=1.23e-05, step=2458]Training:   1%|          | 2459/200000 [53:13<70:27:19,  1.28s/it, loss=0.0284, lr=1.23e-05, step=2458]Training:   1%|          | 2459/200000 [53:13<70:27:19,  1.28s/it, loss=0.0899, lr=1.23e-05, step=2459]Training:   1%|          | 2460/200000 [53:15<72:24:20,  1.32s/it, loss=0.0899, lr=1.23e-05, step=2459]Training:   1%|          | 2460/200000 [53:15<72:24:20,  1.32s/it, loss=0.0301, lr=1.23e-05, step=2460]Training:   1%|          | 2461/200000 [53:16<68:30:42,  1.25s/it, loss=0.0301, lr=1.23e-05, step=2460]Training:   1%|          | 2461/200000 [53:16<68:30:42,  1.25s/it, loss=0.0291, lr=1.23e-05, step=2461]Training:   1%|          | 2462/200000 [53:17<72:04:03,  1.31s/it, loss=0.0291, lr=1.23e-05, step=2461]Training:   1%|          | 2462/200000 [53:17<72:04:03,  1.31s/it, loss=0.0282, lr=1.23e-05, step=2462]Training:   1%|          | 2463/200000 [53:18<68:17:22,  1.24s/it, loss=0.0282, lr=1.23e-05, step=2462]Training:   1%|          | 2463/200000 [53:18<68:17:22,  1.24s/it, loss=0.0608, lr=1.23e-05, step=2463]Training:   1%|          | 2464/200000 [53:20<70:27:11,  1.28s/it, loss=0.0608, lr=1.23e-05, step=2463]Training:   1%|          | 2464/200000 [53:20<70:27:11,  1.28s/it, loss=0.0331, lr=1.23e-05, step=2464]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2465/200000 [53:21<67:09:05,  1.22s/it, loss=0.0331, lr=1.23e-05, step=2464]Training:   1%|          | 2465/200000 [53:21<67:09:05,  1.22s/it, loss=0.0433, lr=1.23e-05, step=2465]Training:   1%|          | 2466/200000 [53:22<71:51:03,  1.31s/it, loss=0.0433, lr=1.23e-05, step=2465]Training:   1%|          | 2466/200000 [53:22<71:51:03,  1.31s/it, loss=0.0459, lr=1.23e-05, step=2466]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2467/200000 [53:24<75:36:17,  1.38s/it, loss=0.0459, lr=1.23e-05, step=2466]Training:   1%|          | 2467/200000 [53:24<75:36:17,  1.38s/it, loss=0.0553, lr=1.23e-05, step=2467]Training:   1%|          | 2468/200000 [53:25<70:43:55,  1.29s/it, loss=0.0553, lr=1.23e-05, step=2467]Training:   1%|          | 2468/200000 [53:25<70:43:55,  1.29s/it, loss=0.0331, lr=1.23e-05, step=2468]Training:   1%|          | 2469/200000 [53:26<67:21:01,  1.23s/it, loss=0.0331, lr=1.23e-05, step=2468]Training:   1%|          | 2469/200000 [53:26<67:21:01,  1.23s/it, loss=0.0245, lr=1.23e-05, step=2469]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2470/200000 [53:27<70:50:55,  1.29s/it, loss=0.0245, lr=1.23e-05, step=2469]Training:   1%|          | 2470/200000 [53:27<70:50:55,  1.29s/it, loss=0.0394, lr=1.23e-05, step=2470]Training:   1%|          | 2471/200000 [53:29<67:21:28,  1.23s/it, loss=0.0394, lr=1.23e-05, step=2470]Training:   1%|          | 2471/200000 [53:29<67:21:28,  1.23s/it, loss=0.0422, lr=1.24e-05, step=2471]Training:   1%|          | 2472/200000 [53:30<69:00:31,  1.26s/it, loss=0.0422, lr=1.24e-05, step=2471]Training:   1%|          | 2472/200000 [53:30<69:00:31,  1.26s/it, loss=0.0218, lr=1.24e-05, step=2472]Training:   1%|          | 2473/200000 [53:31<66:05:34,  1.20s/it, loss=0.0218, lr=1.24e-05, step=2472]Training:   1%|          | 2473/200000 [53:31<66:05:34,  1.20s/it, loss=0.0415, lr=1.24e-05, step=2473]Training:   1%|          | 2474/200000 [53:32<64:05:59,  1.17s/it, loss=0.0415, lr=1.24e-05, step=2473]Training:   1%|          | 2474/200000 [53:32<64:05:59,  1.17s/it, loss=0.0335, lr=1.24e-05, step=2474]Training:   1%|          | 2475/200000 [53:34<68:43:22,  1.25s/it, loss=0.0335, lr=1.24e-05, step=2474]Training:   1%|          | 2475/200000 [53:34<68:43:22,  1.25s/it, loss=0.0628, lr=1.24e-05, step=2475]Training:   1%|          | 2476/200000 [53:35<71:13:25,  1.30s/it, loss=0.0628, lr=1.24e-05, step=2475]Training:   1%|          | 2476/200000 [53:35<71:13:25,  1.30s/it, loss=0.0794, lr=1.24e-05, step=2476]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2477/200000 [53:36<73:37:53,  1.34s/it, loss=0.0794, lr=1.24e-05, step=2476]Training:   1%|          | 2477/200000 [53:36<73:37:53,  1.34s/it, loss=0.1079, lr=1.24e-05, step=2477]Training:   1%|          | 2478/200000 [53:37<69:21:27,  1.26s/it, loss=0.1079, lr=1.24e-05, step=2477]Training:   1%|          | 2478/200000 [53:37<69:21:27,  1.26s/it, loss=0.0522, lr=1.24e-05, step=2478]Training:   1%|          | 2479/200000 [53:39<66:21:50,  1.21s/it, loss=0.0522, lr=1.24e-05, step=2478]Training:   1%|          | 2479/200000 [53:39<66:21:50,  1.21s/it, loss=0.0258, lr=1.24e-05, step=2479]Training:   1%|          | 2480/200000 [53:40<68:45:05,  1.25s/it, loss=0.0258, lr=1.24e-05, step=2479]Training:   1%|          | 2480/200000 [53:40<68:45:05,  1.25s/it, loss=0.0752, lr=1.24e-05, step=2480]Training:   1%|          | 2481/200000 [53:41<71:04:40,  1.30s/it, loss=0.0752, lr=1.24e-05, step=2480]Training:   1%|          | 2481/200000 [53:41<71:04:40,  1.30s/it, loss=0.0239, lr=1.24e-05, step=2481]Training:   1%|          | 2482/200000 [53:42<67:33:07,  1.23s/it, loss=0.0239, lr=1.24e-05, step=2481]Training:   1%|          | 2482/200000 [53:42<67:33:07,  1.23s/it, loss=0.0384, lr=1.24e-05, step=2482]Training:   1%|          | 2483/200000 [53:44<70:17:46,  1.28s/it, loss=0.0384, lr=1.24e-05, step=2482]Training:   1%|          | 2483/200000 [53:44<70:17:46,  1.28s/it, loss=0.0456, lr=1.24e-05, step=2483]Training:   1%|          | 2484/200000 [53:45<67:02:52,  1.22s/it, loss=0.0456, lr=1.24e-05, step=2483]Training:   1%|          | 2484/200000 [53:45<67:02:52,  1.22s/it, loss=0.0298, lr=1.24e-05, step=2484]Training:   1%|          | 2485/200000 [53:46<70:08:21,  1.28s/it, loss=0.0298, lr=1.24e-05, step=2484]Training:   1%|          | 2485/200000 [53:46<70:08:21,  1.28s/it, loss=0.0357, lr=1.24e-05, step=2485]Training:   1%|          | 2486/200000 [53:47<66:58:00,  1.22s/it, loss=0.0357, lr=1.24e-05, step=2485]Training:   1%|          | 2486/200000 [53:47<66:58:00,  1.22s/it, loss=0.0494, lr=1.24e-05, step=2486]Training:   1%|          | 2487/200000 [53:49<71:22:51,  1.30s/it, loss=0.0494, lr=1.24e-05, step=2486]Training:   1%|          | 2487/200000 [53:49<71:22:51,  1.30s/it, loss=0.0294, lr=1.24e-05, step=2487]Training:   1%|          | 2488/200000 [53:50<74:48:38,  1.36s/it, loss=0.0294, lr=1.24e-05, step=2487]Training:   1%|          | 2488/200000 [53:50<74:48:38,  1.36s/it, loss=0.0375, lr=1.24e-05, step=2488]Training:   1%|          | 2489/200000 [53:51<70:12:17,  1.28s/it, loss=0.0375, lr=1.24e-05, step=2488]Training:   1%|          | 2489/200000 [53:51<70:12:17,  1.28s/it, loss=0.0270, lr=1.24e-05, step=2489]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2490/200000 [53:52<66:55:46,  1.22s/it, loss=0.0270, lr=1.24e-05, step=2489]Training:   1%|          | 2490/200000 [53:52<66:55:46,  1.22s/it, loss=0.0286, lr=1.24e-05, step=2490]Training:   1%|          | 2491/200000 [53:54<70:24:57,  1.28s/it, loss=0.0286, lr=1.24e-05, step=2490]Training:   1%|          | 2491/200000 [53:54<70:24:57,  1.28s/it, loss=0.0545, lr=1.25e-05, step=2491]Training:   1%|          | 2492/200000 [53:55<67:05:02,  1.22s/it, loss=0.0545, lr=1.25e-05, step=2491]Training:   1%|          | 2492/200000 [53:55<67:05:02,  1.22s/it, loss=0.0768, lr=1.25e-05, step=2492]Training:   1%|          | 2493/200000 [53:56<68:07:42,  1.24s/it, loss=0.0768, lr=1.25e-05, step=2492]Training:   1%|          | 2493/200000 [53:56<68:07:42,  1.24s/it, loss=0.0223, lr=1.25e-05, step=2493]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|          | 2494/200000 [53:57<65:24:55,  1.19s/it, loss=0.0223, lr=1.25e-05, step=2493]Training:   1%|          | 2494/200000 [53:57<65:24:55,  1.19s/it, loss=0.0407, lr=1.25e-05, step=2494]Training:   1%|          | 2495/200000 [53:58<63:38:05,  1.16s/it, loss=0.0407, lr=1.25e-05, step=2494]Training:   1%|          | 2495/200000 [53:58<63:38:05,  1.16s/it, loss=0.0459, lr=1.25e-05, step=2495]Training:   1%|          | 2496/200000 [54:00<68:41:58,  1.25s/it, loss=0.0459, lr=1.25e-05, step=2495]Training:   1%|          | 2496/200000 [54:00<68:41:58,  1.25s/it, loss=0.0463, lr=1.25e-05, step=2496]Training:   1%|          | 2497/200000 [54:01<71:20:00,  1.30s/it, loss=0.0463, lr=1.25e-05, step=2496]Training:   1%|          | 2497/200000 [54:01<71:20:00,  1.30s/it, loss=0.0588, lr=1.25e-05, step=2497]Training:   1%|          | 2498/200000 [54:03<73:30:00,  1.34s/it, loss=0.0588, lr=1.25e-05, step=2497]Training:   1%|          | 2498/200000 [54:03<73:30:00,  1.34s/it, loss=0.0261, lr=1.25e-05, step=2498]Training:   1%|          | 2499/200000 [54:04<69:16:42,  1.26s/it, loss=0.0261, lr=1.25e-05, step=2498]Training:   1%|          | 2499/200000 [54:04<69:16:42,  1.26s/it, loss=0.0623, lr=1.25e-05, step=2499]Training:   1%|‚ñè         | 2500/200000 [54:05<66:21:10,  1.21s/it, loss=0.0623, lr=1.25e-05, step=2499]Training:   1%|‚ñè         | 2500/200000 [54:05<66:21:10,  1.21s/it, loss=0.0384, lr=1.25e-05, step=2500]23:47:20.075 [I] step=2500 loss=0.0456 lr=1.23e-05 grad_norm=0.88 time=126.7s                     (701675:train_pytorch.py:582)
Training:   1%|‚ñè         | 2501/200000 [54:06<68:26:12,  1.25s/it, loss=0.0384, lr=1.25e-05, step=2500]Training:   1%|‚ñè         | 2501/200000 [54:06<68:26:12,  1.25s/it, loss=0.0692, lr=1.25e-05, step=2501]Training:   1%|‚ñè         | 2502/200000 [54:08<69:32:02,  1.27s/it, loss=0.0692, lr=1.25e-05, step=2501]Training:   1%|‚ñè         | 2502/200000 [54:08<69:32:02,  1.27s/it, loss=0.0247, lr=1.25e-05, step=2502]Training:   1%|‚ñè         | 2503/200000 [54:09<66:35:05,  1.21s/it, loss=0.0247, lr=1.25e-05, step=2502]Training:   1%|‚ñè         | 2503/200000 [54:09<66:35:05,  1.21s/it, loss=0.0347, lr=1.25e-05, step=2503]Training:   1%|‚ñè         | 2504/200000 [54:10<69:17:33,  1.26s/it, loss=0.0347, lr=1.25e-05, step=2503]Training:   1%|‚ñè         | 2504/200000 [54:10<69:17:33,  1.26s/it, loss=0.0229, lr=1.25e-05, step=2504]Training:   1%|‚ñè         | 2505/200000 [54:11<66:20:48,  1.21s/it, loss=0.0229, lr=1.25e-05, step=2504]Training:   1%|‚ñè         | 2505/200000 [54:11<66:20:48,  1.21s/it, loss=0.0286, lr=1.25e-05, step=2505]Training:   1%|‚ñè         | 2506/200000 [54:13<69:22:29,  1.26s/it, loss=0.0286, lr=1.25e-05, step=2505]Training:   1%|‚ñè         | 2506/200000 [54:13<69:22:29,  1.26s/it, loss=0.0265, lr=1.25e-05, step=2506]Training:   1%|‚ñè         | 2507/200000 [54:14<72:41:39,  1.33s/it, loss=0.0265, lr=1.25e-05, step=2506]Training:   1%|‚ñè         | 2507/200000 [54:14<72:41:39,  1.33s/it, loss=0.0325, lr=1.25e-05, step=2507]Training:   1%|‚ñè         | 2508/200000 [54:15<74:15:42,  1.35s/it, loss=0.0325, lr=1.25e-05, step=2507]Training:   1%|‚ñè         | 2508/200000 [54:15<74:15:42,  1.35s/it, loss=0.0328, lr=1.25e-05, step=2508]Training:   1%|‚ñè         | 2509/200000 [54:17<75:36:33,  1.38s/it, loss=0.0328, lr=1.25e-05, step=2508]Training:   1%|‚ñè         | 2509/200000 [54:17<75:36:33,  1.38s/it, loss=0.0283, lr=1.25e-05, step=2509]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2510/200000 [54:18<70:39:58,  1.29s/it, loss=0.0283, lr=1.25e-05, step=2509]Training:   1%|‚ñè         | 2510/200000 [54:18<70:39:58,  1.29s/it, loss=0.0347, lr=1.25e-05, step=2510]Training:   1%|‚ñè         | 2511/200000 [54:19<67:16:16,  1.23s/it, loss=0.0347, lr=1.25e-05, step=2510]Training:   1%|‚ñè         | 2511/200000 [54:19<67:16:16,  1.23s/it, loss=0.0311, lr=1.26e-05, step=2511]Training:   1%|‚ñè         | 2512/200000 [54:20<70:20:54,  1.28s/it, loss=0.0311, lr=1.26e-05, step=2511]Training:   1%|‚ñè         | 2512/200000 [54:20<70:20:54,  1.28s/it, loss=0.1058, lr=1.26e-05, step=2512]Training:   1%|‚ñè         | 2513/200000 [54:22<72:23:54,  1.32s/it, loss=0.1058, lr=1.26e-05, step=2512]Training:   1%|‚ñè         | 2513/200000 [54:22<72:23:54,  1.32s/it, loss=0.0473, lr=1.26e-05, step=2513]Training:   1%|‚ñè         | 2514/200000 [54:23<68:29:01,  1.25s/it, loss=0.0473, lr=1.26e-05, step=2513]Training:   1%|‚ñè         | 2514/200000 [54:23<68:29:01,  1.25s/it, loss=0.0267, lr=1.26e-05, step=2514]Training:   1%|‚ñè         | 2515/200000 [54:24<71:08:57,  1.30s/it, loss=0.0267, lr=1.26e-05, step=2514]Training:   1%|‚ñè         | 2515/200000 [54:24<71:08:57,  1.30s/it, loss=0.0439, lr=1.26e-05, step=2515]Training:   1%|‚ñè         | 2516/200000 [54:25<67:35:33,  1.23s/it, loss=0.0439, lr=1.26e-05, step=2515]Training:   1%|‚ñè         | 2516/200000 [54:25<67:35:33,  1.23s/it, loss=0.0773, lr=1.26e-05, step=2516]Training:   1%|‚ñè         | 2517/200000 [54:27<70:03:36,  1.28s/it, loss=0.0773, lr=1.26e-05, step=2516]Training:   1%|‚ñè         | 2517/200000 [54:27<70:03:36,  1.28s/it, loss=0.0354, lr=1.26e-05, step=2517]Training:   1%|‚ñè         | 2518/200000 [54:28<66:50:30,  1.22s/it, loss=0.0354, lr=1.26e-05, step=2517]Training:   1%|‚ñè         | 2518/200000 [54:28<66:50:30,  1.22s/it, loss=0.0390, lr=1.26e-05, step=2518]Training:   1%|‚ñè         | 2519/200000 [54:29<71:29:55,  1.30s/it, loss=0.0390, lr=1.26e-05, step=2518]Training:   1%|‚ñè         | 2519/200000 [54:29<71:29:55,  1.30s/it, loss=0.0324, lr=1.26e-05, step=2519]Training:   1%|‚ñè         | 2520/200000 [54:31<75:13:16,  1.37s/it, loss=0.0324, lr=1.26e-05, step=2519]Training:   1%|‚ñè         | 2520/200000 [54:31<75:13:16,  1.37s/it, loss=0.0446, lr=1.26e-05, step=2520]Training:   1%|‚ñè         | 2521/200000 [54:32<70:27:55,  1.28s/it, loss=0.0446, lr=1.26e-05, step=2520]Training:   1%|‚ñè         | 2521/200000 [54:32<70:27:55,  1.28s/it, loss=0.0259, lr=1.26e-05, step=2521]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2522/200000 [54:33<67:10:35,  1.22s/it, loss=0.0259, lr=1.26e-05, step=2521]Training:   1%|‚ñè         | 2522/200000 [54:33<67:10:35,  1.22s/it, loss=0.0284, lr=1.26e-05, step=2522]Training:   1%|‚ñè         | 2523/200000 [54:35<71:33:04,  1.30s/it, loss=0.0284, lr=1.26e-05, step=2522]Training:   1%|‚ñè         | 2523/200000 [54:35<71:33:04,  1.30s/it, loss=0.0404, lr=1.26e-05, step=2523]Training:   1%|‚ñè         | 2524/200000 [54:36<67:55:26,  1.24s/it, loss=0.0404, lr=1.26e-05, step=2523]Training:   1%|‚ñè         | 2524/200000 [54:36<67:55:26,  1.24s/it, loss=0.0334, lr=1.26e-05, step=2524]Training:   1%|‚ñè         | 2525/200000 [54:37<69:30:29,  1.27s/it, loss=0.0334, lr=1.26e-05, step=2524]Training:   1%|‚ñè         | 2525/200000 [54:37<69:30:29,  1.27s/it, loss=0.0243, lr=1.26e-05, step=2525]Training:   1%|‚ñè         | 2526/200000 [54:38<66:26:23,  1.21s/it, loss=0.0243, lr=1.26e-05, step=2525]Training:   1%|‚ñè         | 2526/200000 [54:38<66:26:23,  1.21s/it, loss=0.0207, lr=1.26e-05, step=2526]Training:   1%|‚ñè         | 2527/200000 [54:39<64:19:05,  1.17s/it, loss=0.0207, lr=1.26e-05, step=2526]Training:   1%|‚ñè         | 2527/200000 [54:39<64:19:05,  1.17s/it, loss=0.0282, lr=1.26e-05, step=2527]Training:   1%|‚ñè         | 2528/200000 [54:41<69:05:35,  1.26s/it, loss=0.0282, lr=1.26e-05, step=2527]Training:   1%|‚ñè         | 2528/200000 [54:41<69:05:35,  1.26s/it, loss=0.0604, lr=1.26e-05, step=2528]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2529/200000 [54:42<72:37:18,  1.32s/it, loss=0.0604, lr=1.26e-05, step=2528]Training:   1%|‚ñè         | 2529/200000 [54:42<72:37:18,  1.32s/it, loss=0.0484, lr=1.26e-05, step=2529]Training:   1%|‚ñè         | 2530/200000 [54:43<73:50:18,  1.35s/it, loss=0.0484, lr=1.26e-05, step=2529]Training:   1%|‚ñè         | 2530/200000 [54:43<73:50:18,  1.35s/it, loss=0.0474, lr=1.26e-05, step=2530]Training:   1%|‚ñè         | 2531/200000 [54:45<69:35:41,  1.27s/it, loss=0.0474, lr=1.26e-05, step=2530]Training:   1%|‚ñè         | 2531/200000 [54:45<69:35:41,  1.27s/it, loss=0.0378, lr=1.27e-05, step=2531]Training:   1%|‚ñè         | 2532/200000 [54:46<66:29:16,  1.21s/it, loss=0.0378, lr=1.27e-05, step=2531]Training:   1%|‚ñè         | 2532/200000 [54:46<66:29:16,  1.21s/it, loss=0.0299, lr=1.27e-05, step=2532]Training:   1%|‚ñè         | 2533/200000 [54:47<68:58:09,  1.26s/it, loss=0.0299, lr=1.27e-05, step=2532]Training:   1%|‚ñè         | 2533/200000 [54:47<68:58:09,  1.26s/it, loss=0.0411, lr=1.27e-05, step=2533]Training:   1%|‚ñè         | 2534/200000 [54:48<71:53:16,  1.31s/it, loss=0.0411, lr=1.27e-05, step=2533]Training:   1%|‚ñè         | 2534/200000 [54:48<71:53:16,  1.31s/it, loss=0.0223, lr=1.27e-05, step=2534]Training:   1%|‚ñè         | 2535/200000 [54:50<68:09:28,  1.24s/it, loss=0.0223, lr=1.27e-05, step=2534]Training:   1%|‚ñè         | 2535/200000 [54:50<68:09:28,  1.24s/it, loss=0.0426, lr=1.27e-05, step=2535]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2536/200000 [54:51<70:45:12,  1.29s/it, loss=0.0426, lr=1.27e-05, step=2535]Training:   1%|‚ñè         | 2536/200000 [54:51<70:45:12,  1.29s/it, loss=0.0315, lr=1.27e-05, step=2536]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2537/200000 [54:52<67:19:21,  1.23s/it, loss=0.0315, lr=1.27e-05, step=2536]Training:   1%|‚ñè         | 2537/200000 [54:52<67:19:21,  1.23s/it, loss=0.0988, lr=1.27e-05, step=2537]Training:   1%|‚ñè         | 2538/200000 [54:53<70:15:32,  1.28s/it, loss=0.0988, lr=1.27e-05, step=2537]Training:   1%|‚ñè         | 2538/200000 [54:53<70:15:32,  1.28s/it, loss=0.0519, lr=1.27e-05, step=2538]Training:   1%|‚ñè         | 2539/200000 [54:54<66:55:48,  1.22s/it, loss=0.0519, lr=1.27e-05, step=2538]Training:   1%|‚ñè         | 2539/200000 [54:54<66:55:48,  1.22s/it, loss=0.0546, lr=1.27e-05, step=2539]Training:   1%|‚ñè         | 2540/200000 [54:56<71:20:04,  1.30s/it, loss=0.0546, lr=1.27e-05, step=2539]Training:   1%|‚ñè         | 2540/200000 [54:56<71:20:04,  1.30s/it, loss=0.0316, lr=1.27e-05, step=2540]Training:   1%|‚ñè         | 2541/200000 [54:57<74:14:10,  1.35s/it, loss=0.0316, lr=1.27e-05, step=2540]Training:   1%|‚ñè         | 2541/200000 [54:57<74:14:10,  1.35s/it, loss=0.0632, lr=1.27e-05, step=2541]Training:   1%|‚ñè         | 2542/200000 [54:59<69:46:54,  1.27s/it, loss=0.0632, lr=1.27e-05, step=2541]Training:   1%|‚ñè         | 2542/200000 [54:59<69:46:54,  1.27s/it, loss=0.0634, lr=1.27e-05, step=2542]Training:   1%|‚ñè         | 2543/200000 [55:00<66:40:35,  1.22s/it, loss=0.0634, lr=1.27e-05, step=2542]Training:   1%|‚ñè         | 2543/200000 [55:00<66:40:35,  1.22s/it, loss=0.0199, lr=1.27e-05, step=2543]Training:   1%|‚ñè         | 2544/200000 [55:01<70:54:44,  1.29s/it, loss=0.0199, lr=1.27e-05, step=2543]Training:   1%|‚ñè         | 2544/200000 [55:01<70:54:44,  1.29s/it, loss=0.0285, lr=1.27e-05, step=2544]Training:   1%|‚ñè         | 2545/200000 [55:02<67:25:57,  1.23s/it, loss=0.0285, lr=1.27e-05, step=2544]Training:   1%|‚ñè         | 2545/200000 [55:02<67:25:57,  1.23s/it, loss=0.0358, lr=1.27e-05, step=2545]Training:   1%|‚ñè         | 2546/200000 [55:04<69:09:26,  1.26s/it, loss=0.0358, lr=1.27e-05, step=2545]Training:   1%|‚ñè         | 2546/200000 [55:04<69:09:26,  1.26s/it, loss=0.0450, lr=1.27e-05, step=2546]Training:   1%|‚ñè         | 2547/200000 [55:05<66:15:11,  1.21s/it, loss=0.0450, lr=1.27e-05, step=2546]Training:   1%|‚ñè         | 2547/200000 [55:05<66:15:11,  1.21s/it, loss=0.0391, lr=1.27e-05, step=2547]Training:   1%|‚ñè         | 2548/200000 [55:06<64:11:23,  1.17s/it, loss=0.0391, lr=1.27e-05, step=2547]Training:   1%|‚ñè         | 2548/200000 [55:06<64:11:23,  1.17s/it, loss=0.0300, lr=1.27e-05, step=2548]Training:   1%|‚ñè         | 2549/200000 [55:07<68:26:39,  1.25s/it, loss=0.0300, lr=1.27e-05, step=2548]Training:   1%|‚ñè         | 2549/200000 [55:07<68:26:39,  1.25s/it, loss=0.0338, lr=1.27e-05, step=2549]Training:   1%|‚ñè         | 2550/200000 [55:09<71:04:43,  1.30s/it, loss=0.0338, lr=1.27e-05, step=2549]Training:   1%|‚ñè         | 2550/200000 [55:09<71:04:43,  1.30s/it, loss=0.0330, lr=1.27e-05, step=2550]Training:   1%|‚ñè         | 2551/200000 [55:10<73:09:33,  1.33s/it, loss=0.0330, lr=1.27e-05, step=2550]Training:   1%|‚ñè         | 2551/200000 [55:10<73:09:33,  1.33s/it, loss=0.0448, lr=1.28e-05, step=2551]Training:   1%|‚ñè         | 2552/200000 [55:11<69:01:21,  1.26s/it, loss=0.0448, lr=1.28e-05, step=2551]Training:   1%|‚ñè         | 2552/200000 [55:11<69:01:21,  1.26s/it, loss=0.0245, lr=1.28e-05, step=2552]Training:   1%|‚ñè         | 2553/200000 [55:12<66:09:59,  1.21s/it, loss=0.0245, lr=1.28e-05, step=2552]Training:   1%|‚ñè         | 2553/200000 [55:12<66:09:59,  1.21s/it, loss=0.0310, lr=1.28e-05, step=2553]Training:   1%|‚ñè         | 2554/200000 [55:13<68:36:41,  1.25s/it, loss=0.0310, lr=1.28e-05, step=2553]Training:   1%|‚ñè         | 2554/200000 [55:13<68:36:41,  1.25s/it, loss=0.0335, lr=1.28e-05, step=2554]Training:   1%|‚ñè         | 2555/200000 [55:15<70:36:16,  1.29s/it, loss=0.0335, lr=1.28e-05, step=2554]Training:   1%|‚ñè         | 2555/200000 [55:15<70:36:16,  1.29s/it, loss=0.0359, lr=1.28e-05, step=2555]Training:   1%|‚ñè         | 2556/200000 [55:16<67:15:34,  1.23s/it, loss=0.0359, lr=1.28e-05, step=2555]Training:   1%|‚ñè         | 2556/200000 [55:16<67:15:34,  1.23s/it, loss=0.0357, lr=1.28e-05, step=2556]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2557/200000 [55:17<69:27:08,  1.27s/it, loss=0.0357, lr=1.28e-05, step=2556]Training:   1%|‚ñè         | 2557/200000 [55:17<69:27:08,  1.27s/it, loss=0.0188, lr=1.28e-05, step=2557]Training:   1%|‚ñè         | 2558/200000 [55:18<66:25:17,  1.21s/it, loss=0.0188, lr=1.28e-05, step=2557]Training:   1%|‚ñè         | 2558/200000 [55:18<66:25:17,  1.21s/it, loss=0.0460, lr=1.28e-05, step=2558]Training:   1%|‚ñè         | 2559/200000 [55:20<69:28:33,  1.27s/it, loss=0.0460, lr=1.28e-05, step=2558]Training:   1%|‚ñè         | 2559/200000 [55:20<69:28:33,  1.27s/it, loss=0.0267, lr=1.28e-05, step=2559]Training:   1%|‚ñè         | 2560/200000 [55:21<72:41:01,  1.33s/it, loss=0.0267, lr=1.28e-05, step=2559]Training:   1%|‚ñè         | 2560/200000 [55:21<72:41:01,  1.33s/it, loss=0.0269, lr=1.28e-05, step=2560]Training:   1%|‚ñè         | 2561/200000 [55:23<74:18:18,  1.35s/it, loss=0.0269, lr=1.28e-05, step=2560]Training:   1%|‚ñè         | 2561/200000 [55:23<74:18:18,  1.35s/it, loss=0.0285, lr=1.28e-05, step=2561]Training:   1%|‚ñè         | 2562/200000 [55:24<75:50:39,  1.38s/it, loss=0.0285, lr=1.28e-05, step=2561]Training:   1%|‚ñè         | 2562/200000 [55:24<75:50:39,  1.38s/it, loss=0.0480, lr=1.28e-05, step=2562]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2563/200000 [55:25<70:52:35,  1.29s/it, loss=0.0480, lr=1.28e-05, step=2562]Training:   1%|‚ñè         | 2563/200000 [55:25<70:52:35,  1.29s/it, loss=0.0442, lr=1.28e-05, step=2563]Training:   1%|‚ñè         | 2564/200000 [55:26<67:24:45,  1.23s/it, loss=0.0442, lr=1.28e-05, step=2563]Training:   1%|‚ñè         | 2564/200000 [55:26<67:24:45,  1.23s/it, loss=0.0377, lr=1.28e-05, step=2564]Training:   1%|‚ñè         | 2565/200000 [55:28<70:24:33,  1.28s/it, loss=0.0377, lr=1.28e-05, step=2564]Training:   1%|‚ñè         | 2565/200000 [55:28<70:24:33,  1.28s/it, loss=0.0494, lr=1.28e-05, step=2565]Training:   1%|‚ñè         | 2566/200000 [55:29<73:04:14,  1.33s/it, loss=0.0494, lr=1.28e-05, step=2565]Training:   1%|‚ñè         | 2566/200000 [55:29<73:04:14,  1.33s/it, loss=0.0329, lr=1.28e-05, step=2566]Training:   1%|‚ñè         | 2567/200000 [55:30<68:57:20,  1.26s/it, loss=0.0329, lr=1.28e-05, step=2566]Training:   1%|‚ñè         | 2567/200000 [55:30<68:57:20,  1.26s/it, loss=0.0294, lr=1.28e-05, step=2567]Training:   1%|‚ñè         | 2568/200000 [55:32<71:30:16,  1.30s/it, loss=0.0294, lr=1.28e-05, step=2567]Training:   1%|‚ñè         | 2568/200000 [55:32<71:30:16,  1.30s/it, loss=0.0358, lr=1.28e-05, step=2568]Training:   1%|‚ñè         | 2569/200000 [55:33<67:53:34,  1.24s/it, loss=0.0358, lr=1.28e-05, step=2568]Training:   1%|‚ñè         | 2569/200000 [55:33<67:53:34,  1.24s/it, loss=0.0613, lr=1.28e-05, step=2569]Training:   1%|‚ñè         | 2570/200000 [55:34<70:48:24,  1.29s/it, loss=0.0613, lr=1.28e-05, step=2569]Training:   1%|‚ñè         | 2570/200000 [55:34<70:48:24,  1.29s/it, loss=0.0442, lr=1.28e-05, step=2570]Training:   1%|‚ñè         | 2571/200000 [55:35<67:24:30,  1.23s/it, loss=0.0442, lr=1.28e-05, step=2570]Training:   1%|‚ñè         | 2571/200000 [55:35<67:24:30,  1.23s/it, loss=0.0329, lr=1.29e-05, step=2571]Training:   1%|‚ñè         | 2572/200000 [55:37<71:58:47,  1.31s/it, loss=0.0329, lr=1.29e-05, step=2571]Training:   1%|‚ñè         | 2572/200000 [55:37<71:58:47,  1.31s/it, loss=0.1628, lr=1.29e-05, step=2572]Training:   1%|‚ñè         | 2573/200000 [55:38<75:30:56,  1.38s/it, loss=0.1628, lr=1.29e-05, step=2572]Training:   1%|‚ñè         | 2573/200000 [55:38<75:30:56,  1.38s/it, loss=0.0242, lr=1.29e-05, step=2573]Training:   1%|‚ñè         | 2574/200000 [55:39<70:36:25,  1.29s/it, loss=0.0242, lr=1.29e-05, step=2573]Training:   1%|‚ñè         | 2574/200000 [55:39<70:36:25,  1.29s/it, loss=0.0304, lr=1.29e-05, step=2574]Training:   1%|‚ñè         | 2575/200000 [55:40<67:14:39,  1.23s/it, loss=0.0304, lr=1.29e-05, step=2574]Training:   1%|‚ñè         | 2575/200000 [55:40<67:14:39,  1.23s/it, loss=0.0271, lr=1.29e-05, step=2575]Training:   1%|‚ñè         | 2576/200000 [55:42<71:04:30,  1.30s/it, loss=0.0271, lr=1.29e-05, step=2575]Training:   1%|‚ñè         | 2576/200000 [55:42<71:04:30,  1.30s/it, loss=0.0515, lr=1.29e-05, step=2576]Training:   1%|‚ñè         | 2577/200000 [55:43<67:36:40,  1.23s/it, loss=0.0515, lr=1.29e-05, step=2576]Training:   1%|‚ñè         | 2577/200000 [55:43<67:36:40,  1.23s/it, loss=0.0641, lr=1.29e-05, step=2577]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2578/200000 [55:44<68:57:40,  1.26s/it, loss=0.0641, lr=1.29e-05, step=2577]Training:   1%|‚ñè         | 2578/200000 [55:44<68:57:40,  1.26s/it, loss=0.0307, lr=1.29e-05, step=2578]Training:   1%|‚ñè         | 2579/200000 [55:45<66:05:12,  1.21s/it, loss=0.0307, lr=1.29e-05, step=2578]Training:   1%|‚ñè         | 2579/200000 [55:45<66:05:12,  1.21s/it, loss=0.0260, lr=1.29e-05, step=2579]Training:   1%|‚ñè         | 2580/200000 [55:46<64:03:41,  1.17s/it, loss=0.0260, lr=1.29e-05, step=2579]Training:   1%|‚ñè         | 2580/200000 [55:46<64:03:41,  1.17s/it, loss=0.0347, lr=1.29e-05, step=2580]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2581/200000 [55:48<68:42:20,  1.25s/it, loss=0.0347, lr=1.29e-05, step=2580]Training:   1%|‚ñè         | 2581/200000 [55:48<68:42:20,  1.25s/it, loss=0.0343, lr=1.29e-05, step=2581]Training:   1%|‚ñè         | 2582/200000 [55:49<71:44:00,  1.31s/it, loss=0.0343, lr=1.29e-05, step=2581]Training:   1%|‚ñè         | 2582/200000 [55:49<71:44:00,  1.31s/it, loss=0.0391, lr=1.29e-05, step=2582]Training:   1%|‚ñè         | 2583/200000 [55:51<73:48:15,  1.35s/it, loss=0.0391, lr=1.29e-05, step=2582]Training:   1%|‚ñè         | 2583/200000 [55:51<73:48:15,  1.35s/it, loss=0.0431, lr=1.29e-05, step=2583]Training:   1%|‚ñè         | 2584/200000 [55:52<69:29:50,  1.27s/it, loss=0.0431, lr=1.29e-05, step=2583]Training:   1%|‚ñè         | 2584/200000 [55:52<69:29:50,  1.27s/it, loss=0.0589, lr=1.29e-05, step=2584]Training:   1%|‚ñè         | 2585/200000 [55:53<66:27:31,  1.21s/it, loss=0.0589, lr=1.29e-05, step=2584]Training:   1%|‚ñè         | 2585/200000 [55:53<66:27:31,  1.21s/it, loss=0.0424, lr=1.29e-05, step=2585]Training:   1%|‚ñè         | 2586/200000 [55:54<68:45:12,  1.25s/it, loss=0.0424, lr=1.29e-05, step=2585]Training:   1%|‚ñè         | 2586/200000 [55:54<68:45:12,  1.25s/it, loss=0.0376, lr=1.29e-05, step=2586]Training:   1%|‚ñè         | 2587/200000 [55:56<71:48:18,  1.31s/it, loss=0.0376, lr=1.29e-05, step=2586]Training:   1%|‚ñè         | 2587/200000 [55:56<71:48:18,  1.31s/it, loss=0.0519, lr=1.29e-05, step=2587]Training:   1%|‚ñè         | 2588/200000 [55:57<68:05:06,  1.24s/it, loss=0.0519, lr=1.29e-05, step=2587]Training:   1%|‚ñè         | 2588/200000 [55:57<68:05:06,  1.24s/it, loss=0.0437, lr=1.29e-05, step=2588]Training:   1%|‚ñè         | 2589/200000 [55:58<70:40:56,  1.29s/it, loss=0.0437, lr=1.29e-05, step=2588]Training:   1%|‚ñè         | 2589/200000 [55:58<70:40:56,  1.29s/it, loss=0.1846, lr=1.29e-05, step=2589]Training:   1%|‚ñè         | 2590/200000 [55:59<67:16:13,  1.23s/it, loss=0.1846, lr=1.29e-05, step=2589]Training:   1%|‚ñè         | 2590/200000 [55:59<67:16:13,  1.23s/it, loss=0.0477, lr=1.29e-05, step=2590]Training:   1%|‚ñè         | 2591/200000 [56:01<69:59:19,  1.28s/it, loss=0.0477, lr=1.29e-05, step=2590]Training:   1%|‚ñè         | 2591/200000 [56:01<69:59:19,  1.28s/it, loss=0.0425, lr=1.30e-05, step=2591]Training:   1%|‚ñè         | 2592/200000 [56:02<66:46:36,  1.22s/it, loss=0.0425, lr=1.30e-05, step=2591]Training:   1%|‚ñè         | 2592/200000 [56:02<66:46:36,  1.22s/it, loss=0.0302, lr=1.30e-05, step=2592]Training:   1%|‚ñè         | 2593/200000 [56:03<71:16:39,  1.30s/it, loss=0.0302, lr=1.30e-05, step=2592]Training:   1%|‚ñè         | 2593/200000 [56:03<71:16:39,  1.30s/it, loss=0.0233, lr=1.30e-05, step=2593]Training:   1%|‚ñè         | 2594/200000 [56:05<74:23:07,  1.36s/it, loss=0.0233, lr=1.30e-05, step=2593]Training:   1%|‚ñè         | 2594/200000 [56:05<74:23:07,  1.36s/it, loss=0.0476, lr=1.30e-05, step=2594]Training:   1%|‚ñè         | 2595/200000 [56:06<69:54:40,  1.27s/it, loss=0.0476, lr=1.30e-05, step=2594]Training:   1%|‚ñè         | 2595/200000 [56:06<69:54:40,  1.27s/it, loss=0.1343, lr=1.30e-05, step=2595]Training:   1%|‚ñè         | 2596/200000 [56:07<66:47:25,  1.22s/it, loss=0.1343, lr=1.30e-05, step=2595]Training:   1%|‚ñè         | 2596/200000 [56:07<66:47:25,  1.22s/it, loss=0.0587, lr=1.30e-05, step=2596]Training:   1%|‚ñè         | 2597/200000 [56:08<71:00:36,  1.29s/it, loss=0.0587, lr=1.30e-05, step=2596]Training:   1%|‚ñè         | 2597/200000 [56:08<71:00:36,  1.29s/it, loss=0.0289, lr=1.30e-05, step=2597]Training:   1%|‚ñè         | 2598/200000 [56:09<67:30:28,  1.23s/it, loss=0.0289, lr=1.30e-05, step=2597]Training:   1%|‚ñè         | 2598/200000 [56:09<67:30:28,  1.23s/it, loss=0.0387, lr=1.30e-05, step=2598]Training:   1%|‚ñè         | 2599/200000 [56:11<68:49:39,  1.26s/it, loss=0.0387, lr=1.30e-05, step=2598]Training:   1%|‚ñè         | 2599/200000 [56:11<68:49:39,  1.26s/it, loss=0.0787, lr=1.30e-05, step=2599]Training:   1%|‚ñè         | 2600/200000 [56:12<65:57:18,  1.20s/it, loss=0.0787, lr=1.30e-05, step=2599]Training:   1%|‚ñè         | 2600/200000 [56:12<65:57:18,  1.20s/it, loss=0.0475, lr=1.30e-05, step=2600]23:49:26.711 [I] step=2600 loss=0.0431 lr=1.28e-05 grad_norm=0.81 time=126.6s                     (701675:train_pytorch.py:582)
Training:   1%|‚ñè         | 2601/200000 [56:13<64:01:17,  1.17s/it, loss=0.0475, lr=1.30e-05, step=2600]Training:   1%|‚ñè         | 2601/200000 [56:13<64:01:17,  1.17s/it, loss=0.0330, lr=1.30e-05, step=2601]Training:   1%|‚ñè         | 2602/200000 [56:14<68:52:45,  1.26s/it, loss=0.0330, lr=1.30e-05, step=2601]Training:   1%|‚ñè         | 2602/200000 [56:14<68:52:45,  1.26s/it, loss=0.0265, lr=1.30e-05, step=2602]Training:   1%|‚ñè         | 2603/200000 [56:16<71:55:41,  1.31s/it, loss=0.0265, lr=1.30e-05, step=2602]Training:   1%|‚ñè         | 2603/200000 [56:16<71:55:41,  1.31s/it, loss=0.0252, lr=1.30e-05, step=2603]Training:   1%|‚ñè         | 2604/200000 [56:17<74:02:50,  1.35s/it, loss=0.0252, lr=1.30e-05, step=2603]Training:   1%|‚ñè         | 2604/200000 [56:17<74:02:50,  1.35s/it, loss=0.0314, lr=1.30e-05, step=2604]Training:   1%|‚ñè         | 2605/200000 [56:18<69:38:39,  1.27s/it, loss=0.0314, lr=1.30e-05, step=2604]Training:   1%|‚ñè         | 2605/200000 [56:18<69:38:39,  1.27s/it, loss=0.0195, lr=1.30e-05, step=2605]Training:   1%|‚ñè         | 2606/200000 [56:19<66:34:55,  1.21s/it, loss=0.0195, lr=1.30e-05, step=2605]Training:   1%|‚ñè         | 2606/200000 [56:19<66:34:55,  1.21s/it, loss=0.0593, lr=1.30e-05, step=2606]Training:   1%|‚ñè         | 2607/200000 [56:21<68:44:49,  1.25s/it, loss=0.0593, lr=1.30e-05, step=2606]Training:   1%|‚ñè         | 2607/200000 [56:21<68:44:49,  1.25s/it, loss=0.0448, lr=1.30e-05, step=2607]Training:   1%|‚ñè         | 2608/200000 [56:22<70:10:00,  1.28s/it, loss=0.0448, lr=1.30e-05, step=2607]Training:   1%|‚ñè         | 2608/200000 [56:22<70:10:00,  1.28s/it, loss=0.0518, lr=1.30e-05, step=2608]Training:   1%|‚ñè         | 2609/200000 [56:23<66:56:38,  1.22s/it, loss=0.0518, lr=1.30e-05, step=2608]Training:   1%|‚ñè         | 2609/200000 [56:23<66:56:38,  1.22s/it, loss=0.0253, lr=1.30e-05, step=2609]Training:   1%|‚ñè         | 2610/200000 [56:25<68:41:58,  1.25s/it, loss=0.0253, lr=1.30e-05, step=2609]Training:   1%|‚ñè         | 2610/200000 [56:25<68:41:58,  1.25s/it, loss=0.0284, lr=1.30e-05, step=2610]Training:   1%|‚ñè         | 2611/200000 [56:26<65:54:28,  1.20s/it, loss=0.0284, lr=1.30e-05, step=2610]Training:   1%|‚ñè         | 2611/200000 [56:26<65:54:28,  1.20s/it, loss=0.0388, lr=1.31e-05, step=2611]Training:   1%|‚ñè         | 2612/200000 [56:27<68:29:56,  1.25s/it, loss=0.0388, lr=1.31e-05, step=2611]Training:   1%|‚ñè         | 2612/200000 [56:27<68:29:56,  1.25s/it, loss=0.0665, lr=1.31e-05, step=2612]Training:   1%|‚ñè         | 2613/200000 [56:28<71:54:55,  1.31s/it, loss=0.0665, lr=1.31e-05, step=2612]Training:   1%|‚ñè         | 2613/200000 [56:28<71:54:55,  1.31s/it, loss=0.0351, lr=1.31e-05, step=2613]Training:   1%|‚ñè         | 2614/200000 [56:30<74:01:43,  1.35s/it, loss=0.0351, lr=1.31e-05, step=2613]Training:   1%|‚ñè         | 2614/200000 [56:30<74:01:43,  1.35s/it, loss=0.0329, lr=1.31e-05, step=2614]Training:   1%|‚ñè         | 2615/200000 [56:31<75:45:20,  1.38s/it, loss=0.0329, lr=1.31e-05, step=2614]Training:   1%|‚ñè         | 2615/200000 [56:31<75:45:20,  1.38s/it, loss=0.0481, lr=1.31e-05, step=2615]Training:   1%|‚ñè         | 2616/200000 [56:32<70:50:36,  1.29s/it, loss=0.0481, lr=1.31e-05, step=2615]Training:   1%|‚ñè         | 2616/200000 [56:32<70:50:36,  1.29s/it, loss=0.0283, lr=1.31e-05, step=2616]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2617/200000 [56:33<67:23:34,  1.23s/it, loss=0.0283, lr=1.31e-05, step=2616]Training:   1%|‚ñè         | 2617/200000 [56:33<67:23:34,  1.23s/it, loss=0.0372, lr=1.31e-05, step=2617]Training:   1%|‚ñè         | 2618/200000 [56:35<70:16:16,  1.28s/it, loss=0.0372, lr=1.31e-05, step=2617]Training:   1%|‚ñè         | 2618/200000 [56:35<70:16:16,  1.28s/it, loss=0.0455, lr=1.31e-05, step=2618]Training:   1%|‚ñè         | 2619/200000 [56:36<72:22:23,  1.32s/it, loss=0.0455, lr=1.31e-05, step=2618]Training:   1%|‚ñè         | 2619/200000 [56:36<72:22:23,  1.32s/it, loss=0.3369, lr=1.31e-05, step=2619]Training:   1%|‚ñè         | 2620/200000 [56:37<68:28:24,  1.25s/it, loss=0.3369, lr=1.31e-05, step=2619]Training:   1%|‚ñè         | 2620/200000 [56:37<68:28:24,  1.25s/it, loss=0.0401, lr=1.31e-05, step=2620]Training:   1%|‚ñè         | 2621/200000 [56:39<71:40:41,  1.31s/it, loss=0.0401, lr=1.31e-05, step=2620]Training:   1%|‚ñè         | 2621/200000 [56:39<71:40:41,  1.31s/it, loss=0.0655, lr=1.31e-05, step=2621]Training:   1%|‚ñè         | 2622/200000 [56:40<67:57:59,  1.24s/it, loss=0.0655, lr=1.31e-05, step=2621]Training:   1%|‚ñè         | 2622/200000 [56:40<67:57:59,  1.24s/it, loss=0.0336, lr=1.31e-05, step=2622]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2623/200000 [56:41<70:02:59,  1.28s/it, loss=0.0336, lr=1.31e-05, step=2622]Training:   1%|‚ñè         | 2623/200000 [56:41<70:02:59,  1.28s/it, loss=0.0277, lr=1.31e-05, step=2623]Training:   1%|‚ñè         | 2624/200000 [56:42<66:51:40,  1.22s/it, loss=0.0277, lr=1.31e-05, step=2623]Training:   1%|‚ñè         | 2624/200000 [56:42<66:51:40,  1.22s/it, loss=0.0260, lr=1.31e-05, step=2624]Training:   1%|‚ñè         | 2625/200000 [56:44<71:34:16,  1.31s/it, loss=0.0260, lr=1.31e-05, step=2624]Training:   1%|‚ñè         | 2625/200000 [56:44<71:34:16,  1.31s/it, loss=0.0592, lr=1.31e-05, step=2625]Training:   1%|‚ñè         | 2626/200000 [56:45<75:13:24,  1.37s/it, loss=0.0592, lr=1.31e-05, step=2625]Training:   1%|‚ñè         | 2626/200000 [56:45<75:13:24,  1.37s/it, loss=0.0285, lr=1.31e-05, step=2626]Training:   1%|‚ñè         | 2627/200000 [56:46<70:28:32,  1.29s/it, loss=0.0285, lr=1.31e-05, step=2626]Training:   1%|‚ñè         | 2627/200000 [56:46<70:28:32,  1.29s/it, loss=0.0378, lr=1.31e-05, step=2627]Training:   1%|‚ñè         | 2628/200000 [56:48<67:07:39,  1.22s/it, loss=0.0378, lr=1.31e-05, step=2627]Training:   1%|‚ñè         | 2628/200000 [56:48<67:07:39,  1.22s/it, loss=0.0215, lr=1.31e-05, step=2628]Training:   1%|‚ñè         | 2629/200000 [56:49<71:29:51,  1.30s/it, loss=0.0215, lr=1.31e-05, step=2628]Training:   1%|‚ñè         | 2629/200000 [56:49<71:29:51,  1.30s/it, loss=0.0331, lr=1.31e-05, step=2629]Training:   1%|‚ñè         | 2630/200000 [56:50<67:49:35,  1.24s/it, loss=0.0331, lr=1.31e-05, step=2629]Training:   1%|‚ñè         | 2630/200000 [56:50<67:49:35,  1.24s/it, loss=0.0398, lr=1.31e-05, step=2630]Training:   1%|‚ñè         | 2631/200000 [56:51<68:54:20,  1.26s/it, loss=0.0398, lr=1.31e-05, step=2630]Training:   1%|‚ñè         | 2631/200000 [56:51<68:54:20,  1.26s/it, loss=0.0561, lr=1.32e-05, step=2631]Training:   1%|‚ñè         | 2632/200000 [56:52<66:03:18,  1.20s/it, loss=0.0561, lr=1.32e-05, step=2631]Training:   1%|‚ñè         | 2632/200000 [56:53<66:03:18,  1.20s/it, loss=0.0297, lr=1.32e-05, step=2632]Training:   1%|‚ñè         | 2633/200000 [56:54<64:02:02,  1.17s/it, loss=0.0297, lr=1.32e-05, step=2632]Training:   1%|‚ñè         | 2633/200000 [56:54<64:02:02,  1.17s/it, loss=0.0530, lr=1.32e-05, step=2633]Training:   1%|‚ñè         | 2634/200000 [56:55<69:18:43,  1.26s/it, loss=0.0530, lr=1.32e-05, step=2633]Training:   1%|‚ñè         | 2634/200000 [56:55<69:18:43,  1.26s/it, loss=0.0254, lr=1.32e-05, step=2634]Training:   1%|‚ñè         | 2635/200000 [56:56<71:51:39,  1.31s/it, loss=0.0254, lr=1.32e-05, step=2634]Training:   1%|‚ñè         | 2635/200000 [56:56<71:51:39,  1.31s/it, loss=0.0395, lr=1.32e-05, step=2635]Training:   1%|‚ñè         | 2636/200000 [56:58<73:56:20,  1.35s/it, loss=0.0395, lr=1.32e-05, step=2635]Training:   1%|‚ñè         | 2636/200000 [56:58<73:56:20,  1.35s/it, loss=0.0406, lr=1.32e-05, step=2636]Training:   1%|‚ñè         | 2637/200000 [56:59<69:33:38,  1.27s/it, loss=0.0406, lr=1.32e-05, step=2636]Training:   1%|‚ñè         | 2637/200000 [56:59<69:33:38,  1.27s/it, loss=0.0349, lr=1.32e-05, step=2637]Training:   1%|‚ñè         | 2638/200000 [57:00<66:30:58,  1.21s/it, loss=0.0349, lr=1.32e-05, step=2637]Training:   1%|‚ñè         | 2638/200000 [57:00<66:30:58,  1.21s/it, loss=0.0598, lr=1.32e-05, step=2638]Training:   1%|‚ñè         | 2639/200000 [57:01<69:01:09,  1.26s/it, loss=0.0598, lr=1.32e-05, step=2638]Training:   1%|‚ñè         | 2639/200000 [57:01<69:01:09,  1.26s/it, loss=0.0301, lr=1.32e-05, step=2639]Training:   1%|‚ñè         | 2640/200000 [57:03<72:01:41,  1.31s/it, loss=0.0301, lr=1.32e-05, step=2639]Training:   1%|‚ñè         | 2640/200000 [57:03<72:01:41,  1.31s/it, loss=0.0406, lr=1.32e-05, step=2640]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2641/200000 [57:04<68:13:34,  1.24s/it, loss=0.0406, lr=1.32e-05, step=2640]Training:   1%|‚ñè         | 2641/200000 [57:04<68:13:34,  1.24s/it, loss=0.0460, lr=1.32e-05, step=2641]Training:   1%|‚ñè         | 2642/200000 [57:05<70:48:38,  1.29s/it, loss=0.0460, lr=1.32e-05, step=2641]Training:   1%|‚ñè         | 2642/200000 [57:05<70:48:38,  1.29s/it, loss=0.0499, lr=1.32e-05, step=2642]Training:   1%|‚ñè         | 2643/200000 [57:06<67:25:55,  1.23s/it, loss=0.0499, lr=1.32e-05, step=2642]Training:   1%|‚ñè         | 2643/200000 [57:06<67:25:55,  1.23s/it, loss=0.0361, lr=1.32e-05, step=2643]Training:   1%|‚ñè         | 2644/200000 [57:08<70:13:16,  1.28s/it, loss=0.0361, lr=1.32e-05, step=2643]Training:   1%|‚ñè         | 2644/200000 [57:08<70:13:16,  1.28s/it, loss=0.0275, lr=1.32e-05, step=2644]Training:   1%|‚ñè         | 2645/200000 [57:09<66:58:30,  1.22s/it, loss=0.0275, lr=1.32e-05, step=2644]Training:   1%|‚ñè         | 2645/200000 [57:09<66:58:30,  1.22s/it, loss=0.0715, lr=1.32e-05, step=2645]Training:   1%|‚ñè         | 2646/200000 [57:10<71:54:40,  1.31s/it, loss=0.0715, lr=1.32e-05, step=2645]Training:   1%|‚ñè         | 2646/200000 [57:10<71:54:40,  1.31s/it, loss=0.0265, lr=1.32e-05, step=2646]Training:   1%|‚ñè         | 2647/200000 [57:12<75:16:32,  1.37s/it, loss=0.0265, lr=1.32e-05, step=2646]Training:   1%|‚ñè         | 2647/200000 [57:12<75:16:32,  1.37s/it, loss=0.0264, lr=1.32e-05, step=2647]Training:   1%|‚ñè         | 2648/200000 [57:13<70:29:18,  1.29s/it, loss=0.0264, lr=1.32e-05, step=2647]Training:   1%|‚ñè         | 2648/200000 [57:13<70:29:18,  1.29s/it, loss=0.0277, lr=1.32e-05, step=2648]Training:   1%|‚ñè         | 2649/200000 [57:14<67:11:03,  1.23s/it, loss=0.0277, lr=1.32e-05, step=2648]Training:   1%|‚ñè         | 2649/200000 [57:14<67:11:03,  1.23s/it, loss=0.0326, lr=1.32e-05, step=2649]Training:   1%|‚ñè         | 2650/200000 [57:16<71:10:02,  1.30s/it, loss=0.0326, lr=1.32e-05, step=2649]Training:   1%|‚ñè         | 2650/200000 [57:16<71:10:02,  1.30s/it, loss=0.0453, lr=1.32e-05, step=2650]Training:   1%|‚ñè         | 2651/200000 [57:17<67:38:29,  1.23s/it, loss=0.0453, lr=1.32e-05, step=2650]Training:   1%|‚ñè         | 2651/200000 [57:17<67:38:29,  1.23s/it, loss=0.0560, lr=1.33e-05, step=2651]Training:   1%|‚ñè         | 2652/200000 [57:18<68:38:34,  1.25s/it, loss=0.0560, lr=1.33e-05, step=2651]Training:   1%|‚ñè         | 2652/200000 [57:18<68:38:34,  1.25s/it, loss=0.1318, lr=1.33e-05, step=2652]Training:   1%|‚ñè         | 2653/200000 [57:19<65:50:52,  1.20s/it, loss=0.1318, lr=1.33e-05, step=2652]Training:   1%|‚ñè         | 2653/200000 [57:19<65:50:52,  1.20s/it, loss=0.0298, lr=1.33e-05, step=2653]Training:   1%|‚ñè         | 2654/200000 [57:20<63:56:14,  1.17s/it, loss=0.0298, lr=1.33e-05, step=2653]Training:   1%|‚ñè         | 2654/200000 [57:20<63:56:14,  1.17s/it, loss=0.0682, lr=1.33e-05, step=2654]Training:   1%|‚ñè         | 2655/200000 [57:22<68:12:55,  1.24s/it, loss=0.0682, lr=1.33e-05, step=2654]Training:   1%|‚ñè         | 2655/200000 [57:22<68:12:55,  1.24s/it, loss=0.0392, lr=1.33e-05, step=2655]Training:   1%|‚ñè         | 2656/200000 [57:23<70:47:37,  1.29s/it, loss=0.0392, lr=1.33e-05, step=2655]Training:   1%|‚ñè         | 2656/200000 [57:23<70:47:37,  1.29s/it, loss=0.0346, lr=1.33e-05, step=2656]Training:   1%|‚ñè         | 2657/200000 [57:24<73:11:30,  1.34s/it, loss=0.0346, lr=1.33e-05, step=2656]Training:   1%|‚ñè         | 2657/200000 [57:24<73:11:30,  1.34s/it, loss=0.0401, lr=1.33e-05, step=2657]Training:   1%|‚ñè         | 2658/200000 [57:26<69:01:04,  1.26s/it, loss=0.0401, lr=1.33e-05, step=2657]Training:   1%|‚ñè         | 2658/200000 [57:26<69:01:04,  1.26s/it, loss=0.0306, lr=1.33e-05, step=2658]Training:   1%|‚ñè         | 2659/200000 [57:27<66:11:01,  1.21s/it, loss=0.0306, lr=1.33e-05, step=2658]Training:   1%|‚ñè         | 2659/200000 [57:27<66:11:01,  1.21s/it, loss=0.0613, lr=1.33e-05, step=2659]Training:   1%|‚ñè         | 2660/200000 [57:28<68:37:30,  1.25s/it, loss=0.0613, lr=1.33e-05, step=2659]Training:   1%|‚ñè         | 2660/200000 [57:28<68:37:30,  1.25s/it, loss=0.0250, lr=1.33e-05, step=2660]Training:   1%|‚ñè         | 2661/200000 [57:29<70:30:43,  1.29s/it, loss=0.0250, lr=1.33e-05, step=2660]Training:   1%|‚ñè         | 2661/200000 [57:29<70:30:43,  1.29s/it, loss=0.0379, lr=1.33e-05, step=2661]Training:   1%|‚ñè         | 2662/200000 [57:30<67:11:05,  1.23s/it, loss=0.0379, lr=1.33e-05, step=2661]Training:   1%|‚ñè         | 2662/200000 [57:30<67:11:05,  1.23s/it, loss=0.0424, lr=1.33e-05, step=2662]Training:   1%|‚ñè         | 2663/200000 [57:32<69:05:04,  1.26s/it, loss=0.0424, lr=1.33e-05, step=2662]Training:   1%|‚ñè         | 2663/200000 [57:32<69:05:04,  1.26s/it, loss=0.0338, lr=1.33e-05, step=2663]Training:   1%|‚ñè         | 2664/200000 [57:33<66:09:55,  1.21s/it, loss=0.0338, lr=1.33e-05, step=2663]Training:   1%|‚ñè         | 2664/200000 [57:33<66:09:55,  1.21s/it, loss=0.0422, lr=1.33e-05, step=2664]Training:   1%|‚ñè         | 2665/200000 [57:34<69:15:26,  1.26s/it, loss=0.0422, lr=1.33e-05, step=2664]Training:   1%|‚ñè         | 2665/200000 [57:34<69:15:26,  1.26s/it, loss=0.0349, lr=1.33e-05, step=2665]Training:   1%|‚ñè         | 2666/200000 [57:36<72:39:49,  1.33s/it, loss=0.0349, lr=1.33e-05, step=2665]Training:   1%|‚ñè         | 2666/200000 [57:36<72:39:49,  1.33s/it, loss=0.0391, lr=1.33e-05, step=2666]Training:   1%|‚ñè         | 2667/200000 [57:37<74:11:41,  1.35s/it, loss=0.0391, lr=1.33e-05, step=2666]Training:   1%|‚ñè         | 2667/200000 [57:37<74:11:41,  1.35s/it, loss=0.0635, lr=1.33e-05, step=2667]Training:   1%|‚ñè         | 2668/200000 [57:39<75:21:11,  1.37s/it, loss=0.0635, lr=1.33e-05, step=2667]Training:   1%|‚ñè         | 2668/200000 [57:39<75:21:11,  1.37s/it, loss=0.0231, lr=1.33e-05, step=2668]Training:   1%|‚ñè         | 2669/200000 [57:40<70:34:01,  1.29s/it, loss=0.0231, lr=1.33e-05, step=2668]Training:   1%|‚ñè         | 2669/200000 [57:40<70:34:01,  1.29s/it, loss=0.0423, lr=1.33e-05, step=2669]Training:   1%|‚ñè         | 2670/200000 [57:41<67:09:26,  1.23s/it, loss=0.0423, lr=1.33e-05, step=2669]Training:   1%|‚ñè         | 2670/200000 [57:41<67:09:26,  1.23s/it, loss=0.0707, lr=1.33e-05, step=2670]Training:   1%|‚ñè         | 2671/200000 [57:42<70:13:21,  1.28s/it, loss=0.0707, lr=1.33e-05, step=2670]Training:   1%|‚ñè         | 2671/200000 [57:42<70:13:21,  1.28s/it, loss=0.0496, lr=1.34e-05, step=2671]Training:   1%|‚ñè         | 2672/200000 [57:44<72:26:12,  1.32s/it, loss=0.0496, lr=1.34e-05, step=2671]Training:   1%|‚ñè         | 2672/200000 [57:44<72:26:12,  1.32s/it, loss=0.0434, lr=1.34e-05, step=2672]Training:   1%|‚ñè         | 2673/200000 [57:45<68:32:33,  1.25s/it, loss=0.0434, lr=1.34e-05, step=2672]Training:   1%|‚ñè         | 2673/200000 [57:45<68:32:33,  1.25s/it, loss=0.0399, lr=1.34e-05, step=2673]Training:   1%|‚ñè         | 2674/200000 [57:46<71:14:35,  1.30s/it, loss=0.0399, lr=1.34e-05, step=2673]Training:   1%|‚ñè         | 2674/200000 [57:46<71:14:35,  1.30s/it, loss=0.0603, lr=1.34e-05, step=2674]Training:   1%|‚ñè         | 2675/200000 [57:47<67:40:50,  1.23s/it, loss=0.0603, lr=1.34e-05, step=2674]Training:   1%|‚ñè         | 2675/200000 [57:47<67:40:50,  1.23s/it, loss=0.0385, lr=1.34e-05, step=2675]Training:   1%|‚ñè         | 2676/200000 [57:48<70:03:34,  1.28s/it, loss=0.0385, lr=1.34e-05, step=2675]Training:   1%|‚ñè         | 2676/200000 [57:48<70:03:34,  1.28s/it, loss=0.0423, lr=1.34e-05, step=2676]Training:   1%|‚ñè         | 2677/200000 [57:50<66:54:02,  1.22s/it, loss=0.0423, lr=1.34e-05, step=2676]Training:   1%|‚ñè         | 2677/200000 [57:50<66:54:02,  1.22s/it, loss=0.0312, lr=1.34e-05, step=2677]Training:   1%|‚ñè         | 2678/200000 [57:51<71:38:00,  1.31s/it, loss=0.0312, lr=1.34e-05, step=2677]Training:   1%|‚ñè         | 2678/200000 [57:51<71:38:00,  1.31s/it, loss=0.0584, lr=1.34e-05, step=2678]Training:   1%|‚ñè         | 2679/200000 [57:53<75:14:25,  1.37s/it, loss=0.0584, lr=1.34e-05, step=2678]Training:   1%|‚ñè         | 2679/200000 [57:53<75:14:25,  1.37s/it, loss=0.0350, lr=1.34e-05, step=2679]Training:   1%|‚ñè         | 2680/200000 [57:54<70:28:13,  1.29s/it, loss=0.0350, lr=1.34e-05, step=2679]Training:   1%|‚ñè         | 2680/200000 [57:54<70:28:13,  1.29s/it, loss=0.0443, lr=1.34e-05, step=2680]Training:   1%|‚ñè         | 2681/200000 [57:55<67:09:21,  1.23s/it, loss=0.0443, lr=1.34e-05, step=2680]Training:   1%|‚ñè         | 2681/200000 [57:55<67:09:21,  1.23s/it, loss=0.0568, lr=1.34e-05, step=2681]Training:   1%|‚ñè         | 2682/200000 [57:56<71:29:33,  1.30s/it, loss=0.0568, lr=1.34e-05, step=2681]Training:   1%|‚ñè         | 2682/200000 [57:56<71:29:33,  1.30s/it, loss=0.0231, lr=1.34e-05, step=2682]Training:   1%|‚ñè         | 2683/200000 [57:57<67:53:23,  1.24s/it, loss=0.0231, lr=1.34e-05, step=2682]Training:   1%|‚ñè         | 2683/200000 [57:57<67:53:23,  1.24s/it, loss=0.0383, lr=1.34e-05, step=2683]Training:   1%|‚ñè         | 2684/200000 [57:59<69:03:13,  1.26s/it, loss=0.0383, lr=1.34e-05, step=2683]Training:   1%|‚ñè         | 2684/200000 [57:59<69:03:13,  1.26s/it, loss=0.0418, lr=1.34e-05, step=2684]Training:   1%|‚ñè         | 2685/200000 [58:00<66:10:08,  1.21s/it, loss=0.0418, lr=1.34e-05, step=2684]Training:   1%|‚ñè         | 2685/200000 [58:00<66:10:08,  1.21s/it, loss=0.0358, lr=1.34e-05, step=2685]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2686/200000 [58:01<64:06:37,  1.17s/it, loss=0.0358, lr=1.34e-05, step=2685]Training:   1%|‚ñè         | 2686/200000 [58:01<64:06:37,  1.17s/it, loss=0.0518, lr=1.34e-05, step=2686]Training:   1%|‚ñè         | 2687/200000 [58:02<68:42:10,  1.25s/it, loss=0.0518, lr=1.34e-05, step=2686]Training:   1%|‚ñè         | 2687/200000 [58:02<68:42:10,  1.25s/it, loss=0.0249, lr=1.34e-05, step=2687]Training:   1%|‚ñè         | 2688/200000 [58:04<71:22:01,  1.30s/it, loss=0.0249, lr=1.34e-05, step=2687]Training:   1%|‚ñè         | 2688/200000 [58:04<71:22:01,  1.30s/it, loss=0.0350, lr=1.34e-05, step=2688]Training:   1%|‚ñè         | 2689/200000 [58:05<73:47:02,  1.35s/it, loss=0.0350, lr=1.34e-05, step=2688]Training:   1%|‚ñè         | 2689/200000 [58:05<73:47:02,  1.35s/it, loss=0.0555, lr=1.34e-05, step=2689]Training:   1%|‚ñè         | 2690/200000 [58:06<69:24:21,  1.27s/it, loss=0.0555, lr=1.34e-05, step=2689]Training:   1%|‚ñè         | 2690/200000 [58:06<69:24:21,  1.27s/it, loss=0.0515, lr=1.34e-05, step=2690]Training:   1%|‚ñè         | 2691/200000 [58:07<66:24:15,  1.21s/it, loss=0.0515, lr=1.34e-05, step=2690]Training:   1%|‚ñè         | 2691/200000 [58:07<66:24:15,  1.21s/it, loss=0.0317, lr=1.35e-05, step=2691]Training:   1%|‚ñè         | 2692/200000 [58:09<68:53:13,  1.26s/it, loss=0.0317, lr=1.35e-05, step=2691]Training:   1%|‚ñè         | 2692/200000 [58:09<68:53:13,  1.26s/it, loss=0.0644, lr=1.35e-05, step=2692]Training:   1%|‚ñè         | 2693/200000 [58:10<71:17:40,  1.30s/it, loss=0.0644, lr=1.35e-05, step=2692]Training:   1%|‚ñè         | 2693/200000 [58:10<71:17:40,  1.30s/it, loss=0.0733, lr=1.35e-05, step=2693]Training:   1%|‚ñè         | 2694/200000 [58:11<67:41:07,  1.23s/it, loss=0.0733, lr=1.35e-05, step=2693]Training:   1%|‚ñè         | 2694/200000 [58:11<67:41:07,  1.23s/it, loss=0.0733, lr=1.35e-05, step=2694]Training:   1%|‚ñè         | 2695/200000 [58:13<70:25:38,  1.29s/it, loss=0.0733, lr=1.35e-05, step=2694]Training:   1%|‚ñè         | 2695/200000 [58:13<70:25:38,  1.29s/it, loss=0.0859, lr=1.35e-05, step=2695]Training:   1%|‚ñè         | 2696/200000 [58:14<67:07:07,  1.22s/it, loss=0.0859, lr=1.35e-05, step=2695]Training:   1%|‚ñè         | 2696/200000 [58:14<67:07:07,  1.22s/it, loss=0.0185, lr=1.35e-05, step=2696]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2697/200000 [58:15<70:12:28,  1.28s/it, loss=0.0185, lr=1.35e-05, step=2696]Training:   1%|‚ñè         | 2697/200000 [58:15<70:12:28,  1.28s/it, loss=0.0402, lr=1.35e-05, step=2697]Training:   1%|‚ñè         | 2698/200000 [58:16<66:55:00,  1.22s/it, loss=0.0402, lr=1.35e-05, step=2697]Training:   1%|‚ñè         | 2698/200000 [58:16<66:55:00,  1.22s/it, loss=0.0444, lr=1.35e-05, step=2698]Training:   1%|‚ñè         | 2699/200000 [58:18<71:20:41,  1.30s/it, loss=0.0444, lr=1.35e-05, step=2698]Training:   1%|‚ñè         | 2699/200000 [58:18<71:20:41,  1.30s/it, loss=0.0452, lr=1.35e-05, step=2699]Training:   1%|‚ñè         | 2700/200000 [58:19<74:01:42,  1.35s/it, loss=0.0452, lr=1.35e-05, step=2699]Training:   1%|‚ñè         | 2700/200000 [58:19<74:01:42,  1.35s/it, loss=0.0374, lr=1.35e-05, step=2700]23:51:33.989 [I] step=2700 loss=0.0454 lr=1.33e-05 grad_norm=0.88 time=127.3s                     (701675:train_pytorch.py:582)
Training:   1%|‚ñè         | 2701/200000 [58:20<69:38:36,  1.27s/it, loss=0.0374, lr=1.35e-05, step=2700]Training:   1%|‚ñè         | 2701/200000 [58:20<69:38:36,  1.27s/it, loss=0.0281, lr=1.35e-05, step=2701]Training:   1%|‚ñè         | 2702/200000 [58:21<66:33:22,  1.21s/it, loss=0.0281, lr=1.35e-05, step=2701]Training:   1%|‚ñè         | 2702/200000 [58:21<66:33:22,  1.21s/it, loss=0.0447, lr=1.35e-05, step=2702]Training:   1%|‚ñè         | 2703/200000 [58:23<70:49:32,  1.29s/it, loss=0.0447, lr=1.35e-05, step=2702]Training:   1%|‚ñè         | 2703/200000 [58:23<70:49:32,  1.29s/it, loss=0.0265, lr=1.35e-05, step=2703]Training:   1%|‚ñè         | 2704/200000 [58:24<67:20:36,  1.23s/it, loss=0.0265, lr=1.35e-05, step=2703]Training:   1%|‚ñè         | 2704/200000 [58:24<67:20:36,  1.23s/it, loss=0.0415, lr=1.35e-05, step=2704]Training:   1%|‚ñè         | 2705/200000 [58:25<68:15:07,  1.25s/it, loss=0.0415, lr=1.35e-05, step=2704]Training:   1%|‚ñè         | 2705/200000 [58:25<68:15:07,  1.25s/it, loss=0.0354, lr=1.35e-05, step=2705]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2706/200000 [58:26<65:36:02,  1.20s/it, loss=0.0354, lr=1.35e-05, step=2705]Training:   1%|‚ñè         | 2706/200000 [58:26<65:36:02,  1.20s/it, loss=0.0264, lr=1.35e-05, step=2706]Training:   1%|‚ñè         | 2707/200000 [58:27<63:44:30,  1.16s/it, loss=0.0264, lr=1.35e-05, step=2706]Training:   1%|‚ñè         | 2707/200000 [58:27<63:44:30,  1.16s/it, loss=0.0801, lr=1.35e-05, step=2707]Training:   1%|‚ñè         | 2708/200000 [58:29<68:09:26,  1.24s/it, loss=0.0801, lr=1.35e-05, step=2707]Training:   1%|‚ñè         | 2708/200000 [58:29<68:09:26,  1.24s/it, loss=0.0556, lr=1.35e-05, step=2708]Training:   1%|‚ñè         | 2709/200000 [58:30<71:03:17,  1.30s/it, loss=0.0556, lr=1.35e-05, step=2708]Training:   1%|‚ñè         | 2709/200000 [58:30<71:03:17,  1.30s/it, loss=0.0689, lr=1.35e-05, step=2709]Training:   1%|‚ñè         | 2710/200000 [58:32<73:03:18,  1.33s/it, loss=0.0689, lr=1.35e-05, step=2709]Training:   1%|‚ñè         | 2710/200000 [58:32<73:03:18,  1.33s/it, loss=0.0290, lr=1.35e-05, step=2710]Training:   1%|‚ñè         | 2711/200000 [58:33<68:55:47,  1.26s/it, loss=0.0290, lr=1.35e-05, step=2710]Training:   1%|‚ñè         | 2711/200000 [58:33<68:55:47,  1.26s/it, loss=0.0560, lr=1.36e-05, step=2711]Training:   1%|‚ñè         | 2712/200000 [58:34<66:01:06,  1.20s/it, loss=0.0560, lr=1.36e-05, step=2711]Training:   1%|‚ñè         | 2712/200000 [58:34<66:01:06,  1.20s/it, loss=0.0326, lr=1.36e-05, step=2712]Training:   1%|‚ñè         | 2713/200000 [58:35<68:14:46,  1.25s/it, loss=0.0326, lr=1.36e-05, step=2712]Training:   1%|‚ñè         | 2713/200000 [58:35<68:14:46,  1.25s/it, loss=0.0323, lr=1.36e-05, step=2713]Training:   1%|‚ñè         | 2714/200000 [58:36<69:43:10,  1.27s/it, loss=0.0323, lr=1.36e-05, step=2713]Training:   1%|‚ñè         | 2714/200000 [58:36<69:43:10,  1.27s/it, loss=0.0244, lr=1.36e-05, step=2714]Training:   1%|‚ñè         | 2715/200000 [58:37<66:38:29,  1.22s/it, loss=0.0244, lr=1.36e-05, step=2714]Training:   1%|‚ñè         | 2715/200000 [58:37<66:38:29,  1.22s/it, loss=0.0418, lr=1.36e-05, step=2715]Training:   1%|‚ñè         | 2716/200000 [58:39<68:26:55,  1.25s/it, loss=0.0418, lr=1.36e-05, step=2715]Training:   1%|‚ñè         | 2716/200000 [58:39<68:26:55,  1.25s/it, loss=0.0337, lr=1.36e-05, step=2716]Training:   1%|‚ñè         | 2717/200000 [58:40<65:41:34,  1.20s/it, loss=0.0337, lr=1.36e-05, step=2716]Training:   1%|‚ñè         | 2717/200000 [58:40<65:41:34,  1.20s/it, loss=0.0594, lr=1.36e-05, step=2717]Training:   1%|‚ñè         | 2718/200000 [58:41<68:29:59,  1.25s/it, loss=0.0594, lr=1.36e-05, step=2717]Training:   1%|‚ñè         | 2718/200000 [58:41<68:29:59,  1.25s/it, loss=0.0392, lr=1.36e-05, step=2718]Training:   1%|‚ñè         | 2719/200000 [58:43<72:27:55,  1.32s/it, loss=0.0392, lr=1.36e-05, step=2718]Training:   1%|‚ñè         | 2719/200000 [58:43<72:27:55,  1.32s/it, loss=0.0263, lr=1.36e-05, step=2719]Training:   1%|‚ñè         | 2720/200000 [58:44<74:07:53,  1.35s/it, loss=0.0263, lr=1.36e-05, step=2719]Training:   1%|‚ñè         | 2720/200000 [58:44<74:07:53,  1.35s/it, loss=0.0218, lr=1.36e-05, step=2720]Training:   1%|‚ñè         | 2721/200000 [58:46<75:27:51,  1.38s/it, loss=0.0218, lr=1.36e-05, step=2720]Training:   1%|‚ñè         | 2721/200000 [58:46<75:27:51,  1.38s/it, loss=0.0269, lr=1.36e-05, step=2721]Training:   1%|‚ñè         | 2722/200000 [58:47<70:35:57,  1.29s/it, loss=0.0269, lr=1.36e-05, step=2721]Training:   1%|‚ñè         | 2722/200000 [58:47<70:35:57,  1.29s/it, loss=0.0383, lr=1.36e-05, step=2722]Training:   1%|‚ñè         | 2723/200000 [58:48<67:08:37,  1.23s/it, loss=0.0383, lr=1.36e-05, step=2722]Training:   1%|‚ñè         | 2723/200000 [58:48<67:08:37,  1.23s/it, loss=0.0291, lr=1.36e-05, step=2723]Training:   1%|‚ñè         | 2724/200000 [58:49<70:12:12,  1.28s/it, loss=0.0291, lr=1.36e-05, step=2723]Training:   1%|‚ñè         | 2724/200000 [58:49<70:12:12,  1.28s/it, loss=0.0375, lr=1.36e-05, step=2724]Training:   1%|‚ñè         | 2725/200000 [58:51<72:22:04,  1.32s/it, loss=0.0375, lr=1.36e-05, step=2724]Training:   1%|‚ñè         | 2725/200000 [58:51<72:22:04,  1.32s/it, loss=0.0424, lr=1.36e-05, step=2725]Training:   1%|‚ñè         | 2726/200000 [58:52<68:18:38,  1.25s/it, loss=0.0424, lr=1.36e-05, step=2725]Training:   1%|‚ñè         | 2726/200000 [58:52<68:18:38,  1.25s/it, loss=0.0333, lr=1.36e-05, step=2726]Training:   1%|‚ñè         | 2727/200000 [58:53<70:52:00,  1.29s/it, loss=0.0333, lr=1.36e-05, step=2726]Training:   1%|‚ñè         | 2727/200000 [58:53<70:52:00,  1.29s/it, loss=0.0357, lr=1.36e-05, step=2727]Training:   1%|‚ñè         | 2728/200000 [58:54<67:21:42,  1.23s/it, loss=0.0357, lr=1.36e-05, step=2727]Training:   1%|‚ñè         | 2728/200000 [58:54<67:21:42,  1.23s/it, loss=0.0234, lr=1.36e-05, step=2728]Training:   1%|‚ñè         | 2729/200000 [58:55<69:33:38,  1.27s/it, loss=0.0234, lr=1.36e-05, step=2728]Training:   1%|‚ñè         | 2729/200000 [58:55<69:33:38,  1.27s/it, loss=0.0715, lr=1.36e-05, step=2729]Training:   1%|‚ñè         | 2730/200000 [58:57<66:23:24,  1.21s/it, loss=0.0715, lr=1.36e-05, step=2729]Training:   1%|‚ñè         | 2730/200000 [58:57<66:23:24,  1.21s/it, loss=0.0452, lr=1.36e-05, step=2730]Training:   1%|‚ñè         | 2731/200000 [58:58<71:22:09,  1.30s/it, loss=0.0452, lr=1.36e-05, step=2730]Training:   1%|‚ñè         | 2731/200000 [58:58<71:22:09,  1.30s/it, loss=0.0329, lr=1.37e-05, step=2731]Training:   1%|‚ñè         | 2732/200000 [59:00<74:14:44,  1.35s/it, loss=0.0329, lr=1.37e-05, step=2731]Training:   1%|‚ñè         | 2732/200000 [59:00<74:14:44,  1.35s/it, loss=0.0272, lr=1.37e-05, step=2732]Training:   1%|‚ñè         | 2733/200000 [59:01<69:40:05,  1.27s/it, loss=0.0272, lr=1.37e-05, step=2732]Training:   1%|‚ñè         | 2733/200000 [59:01<69:40:05,  1.27s/it, loss=0.0379, lr=1.37e-05, step=2733]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2734/200000 [59:02<66:29:09,  1.21s/it, loss=0.0379, lr=1.37e-05, step=2733]Training:   1%|‚ñè         | 2734/200000 [59:02<66:29:09,  1.21s/it, loss=0.0345, lr=1.37e-05, step=2734]Training:   1%|‚ñè         | 2735/200000 [59:03<70:19:52,  1.28s/it, loss=0.0345, lr=1.37e-05, step=2734]Training:   1%|‚ñè         | 2735/200000 [59:03<70:19:52,  1.28s/it, loss=0.0391, lr=1.37e-05, step=2735]Training:   1%|‚ñè         | 2736/200000 [59:04<66:57:29,  1.22s/it, loss=0.0391, lr=1.37e-05, step=2735]Training:   1%|‚ñè         | 2736/200000 [59:04<66:57:29,  1.22s/it, loss=0.0502, lr=1.37e-05, step=2736]Training:   1%|‚ñè         | 2737/200000 [59:06<68:12:22,  1.24s/it, loss=0.0502, lr=1.37e-05, step=2736]Training:   1%|‚ñè         | 2737/200000 [59:06<68:12:22,  1.24s/it, loss=0.0326, lr=1.37e-05, step=2737]Training:   1%|‚ñè         | 2738/200000 [59:07<65:24:08,  1.19s/it, loss=0.0326, lr=1.37e-05, step=2737]Training:   1%|‚ñè         | 2738/200000 [59:07<65:24:08,  1.19s/it, loss=0.0336, lr=1.37e-05, step=2738]Training:   1%|‚ñè         | 2739/200000 [59:08<63:28:40,  1.16s/it, loss=0.0336, lr=1.37e-05, step=2738]Training:   1%|‚ñè         | 2739/200000 [59:08<63:28:40,  1.16s/it, loss=0.0603, lr=1.37e-05, step=2739]Training:   1%|‚ñè         | 2740/200000 [59:09<68:30:43,  1.25s/it, loss=0.0603, lr=1.37e-05, step=2739]Training:   1%|‚ñè         | 2740/200000 [59:09<68:30:43,  1.25s/it, loss=0.0281, lr=1.37e-05, step=2740]Training:   1%|‚ñè         | 2741/200000 [59:11<70:52:05,  1.29s/it, loss=0.0281, lr=1.37e-05, step=2740]Training:   1%|‚ñè         | 2741/200000 [59:11<70:52:05,  1.29s/it, loss=0.0350, lr=1.37e-05, step=2741]Training:   1%|‚ñè         | 2742/200000 [59:12<73:19:16,  1.34s/it, loss=0.0350, lr=1.37e-05, step=2741]Training:   1%|‚ñè         | 2742/200000 [59:12<73:19:16,  1.34s/it, loss=0.0170, lr=1.37e-05, step=2742]Training:   1%|‚ñè         | 2743/200000 [59:13<69:00:52,  1.26s/it, loss=0.0170, lr=1.37e-05, step=2742]Training:   1%|‚ñè         | 2743/200000 [59:13<69:00:52,  1.26s/it, loss=0.0251, lr=1.37e-05, step=2743]Training:   1%|‚ñè         | 2744/200000 [59:14<66:03:35,  1.21s/it, loss=0.0251, lr=1.37e-05, step=2743]Training:   1%|‚ñè         | 2744/200000 [59:14<66:03:35,  1.21s/it, loss=0.0715, lr=1.37e-05, step=2744]Training:   1%|‚ñè         | 2745/200000 [59:16<68:33:00,  1.25s/it, loss=0.0715, lr=1.37e-05, step=2744]Training:   1%|‚ñè         | 2745/200000 [59:16<68:33:00,  1.25s/it, loss=0.0345, lr=1.37e-05, step=2745]Training:   1%|‚ñè         | 2746/200000 [59:17<71:08:27,  1.30s/it, loss=0.0345, lr=1.37e-05, step=2745]Training:   1%|‚ñè         | 2746/200000 [59:17<71:08:27,  1.30s/it, loss=0.0255, lr=1.37e-05, step=2746]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2747/200000 [59:18<67:25:51,  1.23s/it, loss=0.0255, lr=1.37e-05, step=2746]Training:   1%|‚ñè         | 2747/200000 [59:18<67:25:51,  1.23s/it, loss=0.0375, lr=1.37e-05, step=2747]Training:   1%|‚ñè         | 2748/200000 [59:19<70:06:39,  1.28s/it, loss=0.0375, lr=1.37e-05, step=2747]Training:   1%|‚ñè         | 2748/200000 [59:19<70:06:39,  1.28s/it, loss=0.0706, lr=1.37e-05, step=2748]Training:   1%|‚ñè         | 2749/200000 [59:20<66:44:40,  1.22s/it, loss=0.0706, lr=1.37e-05, step=2748]Training:   1%|‚ñè         | 2749/200000 [59:20<66:44:40,  1.22s/it, loss=0.0815, lr=1.37e-05, step=2749]Training:   1%|‚ñè         | 2750/200000 [59:22<69:50:14,  1.27s/it, loss=0.0815, lr=1.37e-05, step=2749]Training:   1%|‚ñè         | 2750/200000 [59:22<69:50:14,  1.27s/it, loss=0.0306, lr=1.37e-05, step=2750]Training:   1%|‚ñè         | 2751/200000 [59:23<66:34:10,  1.21s/it, loss=0.0306, lr=1.37e-05, step=2750]Training:   1%|‚ñè         | 2751/200000 [59:23<66:34:10,  1.21s/it, loss=0.0504, lr=1.38e-05, step=2751]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2752/200000 [59:24<70:52:00,  1.29s/it, loss=0.0504, lr=1.38e-05, step=2751]Training:   1%|‚ñè         | 2752/200000 [59:24<70:52:00,  1.29s/it, loss=0.0240, lr=1.38e-05, step=2752]Training:   1%|‚ñè         | 2753/200000 [59:26<74:23:43,  1.36s/it, loss=0.0240, lr=1.38e-05, step=2752]Training:   1%|‚ñè         | 2753/200000 [59:26<74:23:43,  1.36s/it, loss=0.0249, lr=1.38e-05, step=2753]Training:   1%|‚ñè         | 2754/200000 [59:27<69:49:17,  1.27s/it, loss=0.0249, lr=1.38e-05, step=2753]Training:   1%|‚ñè         | 2754/200000 [59:27<69:49:17,  1.27s/it, loss=0.0355, lr=1.38e-05, step=2754]Training:   1%|‚ñè         | 2755/200000 [59:28<66:35:14,  1.22s/it, loss=0.0355, lr=1.38e-05, step=2754]Training:   1%|‚ñè         | 2755/200000 [59:28<66:35:14,  1.22s/it, loss=0.0588, lr=1.38e-05, step=2755]Training:   1%|‚ñè         | 2756/200000 [59:30<70:44:01,  1.29s/it, loss=0.0588, lr=1.38e-05, step=2755]Training:   1%|‚ñè         | 2756/200000 [59:30<70:44:01,  1.29s/it, loss=0.0448, lr=1.38e-05, step=2756]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2757/200000 [59:31<67:13:11,  1.23s/it, loss=0.0448, lr=1.38e-05, step=2756]Training:   1%|‚ñè         | 2757/200000 [59:31<67:13:11,  1.23s/it, loss=0.0434, lr=1.38e-05, step=2757]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2758/200000 [59:32<68:19:58,  1.25s/it, loss=0.0434, lr=1.38e-05, step=2757]Training:   1%|‚ñè         | 2758/200000 [59:32<68:19:58,  1.25s/it, loss=0.0591, lr=1.38e-05, step=2758]Training:   1%|‚ñè         | 2759/200000 [59:33<65:32:43,  1.20s/it, loss=0.0591, lr=1.38e-05, step=2758]Training:   1%|‚ñè         | 2759/200000 [59:33<65:32:43,  1.20s/it, loss=0.0360, lr=1.38e-05, step=2759]Training:   1%|‚ñè         | 2760/200000 [59:34<63:39:19,  1.16s/it, loss=0.0360, lr=1.38e-05, step=2759]Training:   1%|‚ñè         | 2760/200000 [59:34<63:39:19,  1.16s/it, loss=0.0294, lr=1.38e-05, step=2760]Training:   1%|‚ñè         | 2761/200000 [59:35<67:52:48,  1.24s/it, loss=0.0294, lr=1.38e-05, step=2760]Training:   1%|‚ñè         | 2761/200000 [59:35<67:52:48,  1.24s/it, loss=0.0344, lr=1.38e-05, step=2761]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2762/200000 [59:37<70:41:17,  1.29s/it, loss=0.0344, lr=1.38e-05, step=2761]Training:   1%|‚ñè         | 2762/200000 [59:37<70:41:17,  1.29s/it, loss=0.0393, lr=1.38e-05, step=2762]Training:   1%|‚ñè         | 2763/200000 [59:38<72:32:33,  1.32s/it, loss=0.0393, lr=1.38e-05, step=2762]Training:   1%|‚ñè         | 2763/200000 [59:38<72:32:33,  1.32s/it, loss=0.0335, lr=1.38e-05, step=2763]Training:   1%|‚ñè         | 2764/200000 [59:39<68:29:39,  1.25s/it, loss=0.0335, lr=1.38e-05, step=2763]Training:   1%|‚ñè         | 2764/200000 [59:39<68:29:39,  1.25s/it, loss=0.1053, lr=1.38e-05, step=2764]Training:   1%|‚ñè         | 2765/200000 [59:40<65:41:46,  1.20s/it, loss=0.1053, lr=1.38e-05, step=2764]Training:   1%|‚ñè         | 2765/200000 [59:40<65:41:46,  1.20s/it, loss=0.0298, lr=1.38e-05, step=2765]Training:   1%|‚ñè         | 2766/200000 [59:42<68:12:44,  1.25s/it, loss=0.0298, lr=1.38e-05, step=2765]Training:   1%|‚ñè         | 2766/200000 [59:42<68:12:44,  1.25s/it, loss=0.0371, lr=1.38e-05, step=2766]Training:   1%|‚ñè         | 2767/200000 [59:43<69:43:54,  1.27s/it, loss=0.0371, lr=1.38e-05, step=2766]Training:   1%|‚ñè         | 2767/200000 [59:43<69:43:54,  1.27s/it, loss=0.0284, lr=1.38e-05, step=2767]Training:   1%|‚ñè         | 2768/200000 [59:44<66:31:18,  1.21s/it, loss=0.0284, lr=1.38e-05, step=2767]Training:   1%|‚ñè         | 2768/200000 [59:44<66:31:18,  1.21s/it, loss=0.0251, lr=1.38e-05, step=2768]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2769/200000 [59:46<68:39:27,  1.25s/it, loss=0.0251, lr=1.38e-05, step=2768]Training:   1%|‚ñè         | 2769/200000 [59:46<68:39:27,  1.25s/it, loss=0.0473, lr=1.38e-05, step=2769]Training:   1%|‚ñè         | 2770/200000 [59:47<65:48:17,  1.20s/it, loss=0.0473, lr=1.38e-05, step=2769]Training:   1%|‚ñè         | 2770/200000 [59:47<65:48:17,  1.20s/it, loss=0.0382, lr=1.38e-05, step=2770]Training:   1%|‚ñè         | 2771/200000 [59:48<68:51:25,  1.26s/it, loss=0.0382, lr=1.38e-05, step=2770]Training:   1%|‚ñè         | 2771/200000 [59:48<68:51:25,  1.26s/it, loss=0.0345, lr=1.39e-05, step=2771]Training:   1%|‚ñè         | 2772/200000 [59:50<72:13:20,  1.32s/it, loss=0.0345, lr=1.39e-05, step=2771]Training:   1%|‚ñè         | 2772/200000 [59:50<72:13:20,  1.32s/it, loss=0.0264, lr=1.39e-05, step=2772]Training:   1%|‚ñè         | 2773/200000 [59:51<73:51:39,  1.35s/it, loss=0.0264, lr=1.39e-05, step=2772]Training:   1%|‚ñè         | 2773/200000 [59:51<73:51:39,  1.35s/it, loss=0.0351, lr=1.39e-05, step=2773]Training:   1%|‚ñè         | 2774/200000 [59:52<75:32:23,  1.38s/it, loss=0.0351, lr=1.39e-05, step=2773]Training:   1%|‚ñè         | 2774/200000 [59:52<75:32:23,  1.38s/it, loss=0.0393, lr=1.39e-05, step=2774]Training:   1%|‚ñè         | 2775/200000 [59:53<70:34:42,  1.29s/it, loss=0.0393, lr=1.39e-05, step=2774]Training:   1%|‚ñè         | 2775/200000 [59:53<70:34:42,  1.29s/it, loss=0.0336, lr=1.39e-05, step=2775]Training:   1%|‚ñè         | 2776/200000 [59:55<67:06:43,  1.23s/it, loss=0.0336, lr=1.39e-05, step=2775]Training:   1%|‚ñè         | 2776/200000 [59:55<67:06:43,  1.23s/it, loss=0.1012, lr=1.39e-05, step=2776]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2777/200000 [59:56<70:06:50,  1.28s/it, loss=0.1012, lr=1.39e-05, step=2776]Training:   1%|‚ñè         | 2777/200000 [59:56<70:06:50,  1.28s/it, loss=0.0441, lr=1.39e-05, step=2777]Training:   1%|‚ñè         | 2778/200000 [59:57<71:56:26,  1.31s/it, loss=0.0441, lr=1.39e-05, step=2777]Training:   1%|‚ñè         | 2778/200000 [59:57<71:56:26,  1.31s/it, loss=0.0621, lr=1.39e-05, step=2778]Training:   1%|‚ñè         | 2779/200000 [59:58<68:03:21,  1.24s/it, loss=0.0621, lr=1.39e-05, step=2778]Training:   1%|‚ñè         | 2779/200000 [59:58<68:03:21,  1.24s/it, loss=0.0618, lr=1.39e-05, step=2779]Training:   1%|‚ñè         | 2780/200000 [1:00:00<71:29:41,  1.31s/it, loss=0.0618, lr=1.39e-05, step=2779]Training:   1%|‚ñè         | 2780/200000 [1:00:00<71:29:41,  1.31s/it, loss=0.0510, lr=1.39e-05, step=2780]Training:   1%|‚ñè         | 2781/200000 [1:00:01<67:46:42,  1.24s/it, loss=0.0510, lr=1.39e-05, step=2780]Training:   1%|‚ñè         | 2781/200000 [1:00:01<67:46:42,  1.24s/it, loss=0.0287, lr=1.39e-05, step=2781]Training:   1%|‚ñè         | 2782/200000 [1:00:02<70:22:12,  1.28s/it, loss=0.0287, lr=1.39e-05, step=2781]Training:   1%|‚ñè         | 2782/200000 [1:00:02<70:22:12,  1.28s/it, loss=0.0438, lr=1.39e-05, step=2782]Training:   1%|‚ñè         | 2783/200000 [1:00:03<66:57:43,  1.22s/it, loss=0.0438, lr=1.39e-05, step=2782]Training:   1%|‚ñè         | 2783/200000 [1:00:03<66:57:43,  1.22s/it, loss=0.0288, lr=1.39e-05, step=2783]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2784/200000 [1:00:05<72:28:38,  1.32s/it, loss=0.0288, lr=1.39e-05, step=2783]Training:   1%|‚ñè         | 2784/200000 [1:00:05<72:28:38,  1.32s/it, loss=0.0357, lr=1.39e-05, step=2784]Training:   1%|‚ñè         | 2785/200000 [1:00:06<75:51:18,  1.38s/it, loss=0.0357, lr=1.39e-05, step=2784]Training:   1%|‚ñè         | 2785/200000 [1:00:06<75:51:18,  1.38s/it, loss=0.0304, lr=1.39e-05, step=2785]Training:   1%|‚ñè         | 2786/200000 [1:00:08<70:48:11,  1.29s/it, loss=0.0304, lr=1.39e-05, step=2785]Training:   1%|‚ñè         | 2786/200000 [1:00:08<70:48:11,  1.29s/it, loss=0.0194, lr=1.39e-05, step=2786]Training:   1%|‚ñè         | 2787/200000 [1:00:09<67:16:48,  1.23s/it, loss=0.0194, lr=1.39e-05, step=2786]Training:   1%|‚ñè         | 2787/200000 [1:00:09<67:16:48,  1.23s/it, loss=0.0361, lr=1.39e-05, step=2787]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2788/200000 [1:00:10<70:45:32,  1.29s/it, loss=0.0361, lr=1.39e-05, step=2787]Training:   1%|‚ñè         | 2788/200000 [1:00:10<70:45:32,  1.29s/it, loss=0.0267, lr=1.39e-05, step=2788]Training:   1%|‚ñè         | 2789/200000 [1:00:11<67:12:42,  1.23s/it, loss=0.0267, lr=1.39e-05, step=2788]Training:   1%|‚ñè         | 2789/200000 [1:00:11<67:12:42,  1.23s/it, loss=0.0386, lr=1.39e-05, step=2789]Training:   1%|‚ñè         | 2790/200000 [1:00:13<69:11:16,  1.26s/it, loss=0.0386, lr=1.39e-05, step=2789]Training:   1%|‚ñè         | 2790/200000 [1:00:13<69:11:16,  1.26s/it, loss=0.0376, lr=1.39e-05, step=2790]Training:   1%|‚ñè         | 2791/200000 [1:00:14<66:07:31,  1.21s/it, loss=0.0376, lr=1.39e-05, step=2790]Training:   1%|‚ñè         | 2791/200000 [1:00:14<66:07:31,  1.21s/it, loss=0.0312, lr=1.40e-05, step=2791]Training:   1%|‚ñè         | 2792/200000 [1:00:15<64:01:06,  1.17s/it, loss=0.0312, lr=1.40e-05, step=2791]Training:   1%|‚ñè         | 2792/200000 [1:00:15<64:01:06,  1.17s/it, loss=0.0424, lr=1.40e-05, step=2792]Training:   1%|‚ñè         | 2793/200000 [1:00:16<68:35:52,  1.25s/it, loss=0.0424, lr=1.40e-05, step=2792]Training:   1%|‚ñè         | 2793/200000 [1:00:16<68:35:52,  1.25s/it, loss=0.0375, lr=1.40e-05, step=2793]Training:   1%|‚ñè         | 2794/200000 [1:00:18<71:06:20,  1.30s/it, loss=0.0375, lr=1.40e-05, step=2793]Training:   1%|‚ñè         | 2794/200000 [1:00:18<71:06:20,  1.30s/it, loss=0.0238, lr=1.40e-05, step=2794]Training:   1%|‚ñè         | 2795/200000 [1:00:19<72:52:57,  1.33s/it, loss=0.0238, lr=1.40e-05, step=2794]Training:   1%|‚ñè         | 2795/200000 [1:00:19<72:52:57,  1.33s/it, loss=0.0275, lr=1.40e-05, step=2795]Training:   1%|‚ñè         | 2796/200000 [1:00:20<68:45:12,  1.26s/it, loss=0.0275, lr=1.40e-05, step=2795]Training:   1%|‚ñè         | 2796/200000 [1:00:20<68:45:12,  1.26s/it, loss=0.0370, lr=1.40e-05, step=2796]Training:   1%|‚ñè         | 2797/200000 [1:00:21<65:51:33,  1.20s/it, loss=0.0370, lr=1.40e-05, step=2796]Training:   1%|‚ñè         | 2797/200000 [1:00:21<65:51:33,  1.20s/it, loss=0.0299, lr=1.40e-05, step=2797]Training:   1%|‚ñè         | 2798/200000 [1:00:22<68:24:53,  1.25s/it, loss=0.0299, lr=1.40e-05, step=2797]Training:   1%|‚ñè         | 2798/200000 [1:00:22<68:24:53,  1.25s/it, loss=0.0455, lr=1.40e-05, step=2798]Training:   1%|‚ñè         | 2799/200000 [1:00:24<70:06:10,  1.28s/it, loss=0.0455, lr=1.40e-05, step=2798]Training:   1%|‚ñè         | 2799/200000 [1:00:24<70:06:10,  1.28s/it, loss=0.0305, lr=1.40e-05, step=2799]Training:   1%|‚ñè         | 2800/200000 [1:00:25<66:47:13,  1.22s/it, loss=0.0305, lr=1.40e-05, step=2799]Training:   1%|‚ñè         | 2800/200000 [1:00:25<66:47:13,  1.22s/it, loss=0.0285, lr=1.40e-05, step=2800]23:53:40.119 [I] step=2800 loss=0.0397 lr=1.38e-05 grad_norm=0.76 time=126.1s                     (701675:train_pytorch.py:582)
Training:   1%|‚ñè         | 2801/200000 [1:00:26<70:26:13,  1.29s/it, loss=0.0285, lr=1.40e-05, step=2800]Training:   1%|‚ñè         | 2801/200000 [1:00:26<70:26:13,  1.29s/it, loss=0.0325, lr=1.40e-05, step=2801]Training:   1%|‚ñè         | 2802/200000 [1:00:27<67:01:32,  1.22s/it, loss=0.0325, lr=1.40e-05, step=2801]Training:   1%|‚ñè         | 2802/200000 [1:00:27<67:01:32,  1.22s/it, loss=0.0328, lr=1.40e-05, step=2802]Training:   1%|‚ñè         | 2803/200000 [1:00:29<69:32:51,  1.27s/it, loss=0.0328, lr=1.40e-05, step=2802]Training:   1%|‚ñè         | 2803/200000 [1:00:29<69:32:51,  1.27s/it, loss=0.0487, lr=1.40e-05, step=2803]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2804/200000 [1:00:30<66:22:10,  1.21s/it, loss=0.0487, lr=1.40e-05, step=2803]Training:   1%|‚ñè         | 2804/200000 [1:00:30<66:22:10,  1.21s/it, loss=0.0255, lr=1.40e-05, step=2804]Training:   1%|‚ñè         | 2805/200000 [1:00:31<70:49:18,  1.29s/it, loss=0.0255, lr=1.40e-05, step=2804]Training:   1%|‚ñè         | 2805/200000 [1:00:31<70:49:18,  1.29s/it, loss=0.0325, lr=1.40e-05, step=2805]Training:   1%|‚ñè         | 2806/200000 [1:00:33<73:43:20,  1.35s/it, loss=0.0325, lr=1.40e-05, step=2805]Training:   1%|‚ñè         | 2806/200000 [1:00:33<73:43:20,  1.35s/it, loss=0.0484, lr=1.40e-05, step=2806]Training:   1%|‚ñè         | 2807/200000 [1:00:34<69:21:16,  1.27s/it, loss=0.0484, lr=1.40e-05, step=2806]Training:   1%|‚ñè         | 2807/200000 [1:00:34<69:21:16,  1.27s/it, loss=0.0365, lr=1.40e-05, step=2807]Training:   1%|‚ñè         | 2808/200000 [1:00:35<66:19:31,  1.21s/it, loss=0.0365, lr=1.40e-05, step=2807]Training:   1%|‚ñè         | 2808/200000 [1:00:35<66:19:31,  1.21s/it, loss=0.0328, lr=1.40e-05, step=2808]Training:   1%|‚ñè         | 2809/200000 [1:00:36<70:31:37,  1.29s/it, loss=0.0328, lr=1.40e-05, step=2808]Training:   1%|‚ñè         | 2809/200000 [1:00:36<70:31:37,  1.29s/it, loss=0.0418, lr=1.40e-05, step=2809]Training:   1%|‚ñè         | 2810/200000 [1:00:38<67:05:47,  1.22s/it, loss=0.0418, lr=1.40e-05, step=2809]Training:   1%|‚ñè         | 2810/200000 [1:00:38<67:05:47,  1.22s/it, loss=0.0268, lr=1.40e-05, step=2810]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2811/200000 [1:00:39<68:17:43,  1.25s/it, loss=0.0268, lr=1.40e-05, step=2810]Training:   1%|‚ñè         | 2811/200000 [1:00:39<68:17:43,  1.25s/it, loss=0.0351, lr=1.41e-05, step=2811]Training:   1%|‚ñè         | 2812/200000 [1:00:40<65:31:17,  1.20s/it, loss=0.0351, lr=1.41e-05, step=2811]Training:   1%|‚ñè         | 2812/200000 [1:00:40<65:31:17,  1.20s/it, loss=0.0245, lr=1.41e-05, step=2812]Training:   1%|‚ñè         | 2813/200000 [1:00:41<63:36:12,  1.16s/it, loss=0.0245, lr=1.41e-05, step=2812]Training:   1%|‚ñè         | 2813/200000 [1:00:41<63:36:12,  1.16s/it, loss=0.0318, lr=1.41e-05, step=2813]Training:   1%|‚ñè         | 2814/200000 [1:00:42<68:26:16,  1.25s/it, loss=0.0318, lr=1.41e-05, step=2813]Training:   1%|‚ñè         | 2814/200000 [1:00:42<68:26:16,  1.25s/it, loss=0.0462, lr=1.41e-05, step=2814]Training:   1%|‚ñè         | 2815/200000 [1:00:44<71:01:04,  1.30s/it, loss=0.0462, lr=1.41e-05, step=2814]Training:   1%|‚ñè         | 2815/200000 [1:00:44<71:01:04,  1.30s/it, loss=0.0290, lr=1.41e-05, step=2815]Training:   1%|‚ñè         | 2816/200000 [1:00:45<73:14:37,  1.34s/it, loss=0.0290, lr=1.41e-05, step=2815]Training:   1%|‚ñè         | 2816/200000 [1:00:45<73:14:37,  1.34s/it, loss=0.0283, lr=1.41e-05, step=2816]Training:   1%|‚ñè         | 2817/200000 [1:00:46<68:58:31,  1.26s/it, loss=0.0283, lr=1.41e-05, step=2816]Training:   1%|‚ñè         | 2817/200000 [1:00:46<68:58:31,  1.26s/it, loss=0.0244, lr=1.41e-05, step=2817]Training:   1%|‚ñè         | 2818/200000 [1:00:47<66:04:27,  1.21s/it, loss=0.0244, lr=1.41e-05, step=2817]Training:   1%|‚ñè         | 2818/200000 [1:00:47<66:04:27,  1.21s/it, loss=0.0379, lr=1.41e-05, step=2818]Training:   1%|‚ñè         | 2819/200000 [1:00:49<68:20:34,  1.25s/it, loss=0.0379, lr=1.41e-05, step=2818]Training:   1%|‚ñè         | 2819/200000 [1:00:49<68:20:34,  1.25s/it, loss=0.0258, lr=1.41e-05, step=2819]Training:   1%|‚ñè         | 2820/200000 [1:00:50<70:20:27,  1.28s/it, loss=0.0258, lr=1.41e-05, step=2819]Training:   1%|‚ñè         | 2820/200000 [1:00:50<70:20:27,  1.28s/it, loss=0.0296, lr=1.41e-05, step=2820]Training:   1%|‚ñè         | 2821/200000 [1:00:51<66:58:19,  1.22s/it, loss=0.0296, lr=1.41e-05, step=2820]Training:   1%|‚ñè         | 2821/200000 [1:00:51<66:58:19,  1.22s/it, loss=0.0588, lr=1.41e-05, step=2821]Training:   1%|‚ñè         | 2822/200000 [1:00:53<69:40:31,  1.27s/it, loss=0.0588, lr=1.41e-05, step=2821]Training:   1%|‚ñè         | 2822/200000 [1:00:53<69:40:31,  1.27s/it, loss=0.0302, lr=1.41e-05, step=2822]Training:   1%|‚ñè         | 2823/200000 [1:00:54<66:27:39,  1.21s/it, loss=0.0302, lr=1.41e-05, step=2822]Training:   1%|‚ñè         | 2823/200000 [1:00:54<66:27:39,  1.21s/it, loss=0.0368, lr=1.41e-05, step=2823]Training:   1%|‚ñè         | 2824/200000 [1:00:55<69:22:16,  1.27s/it, loss=0.0368, lr=1.41e-05, step=2823]Training:   1%|‚ñè         | 2824/200000 [1:00:55<69:22:16,  1.27s/it, loss=0.0570, lr=1.41e-05, step=2824]Training:   1%|‚ñè         | 2825/200000 [1:00:56<71:56:09,  1.31s/it, loss=0.0570, lr=1.41e-05, step=2824]Training:   1%|‚ñè         | 2825/200000 [1:00:56<71:56:09,  1.31s/it, loss=0.0343, lr=1.41e-05, step=2825]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2826/200000 [1:00:58<73:32:32,  1.34s/it, loss=0.0343, lr=1.41e-05, step=2825]Training:   1%|‚ñè         | 2826/200000 [1:00:58<73:32:32,  1.34s/it, loss=0.0228, lr=1.41e-05, step=2826]Training:   1%|‚ñè         | 2827/200000 [1:00:59<75:19:08,  1.38s/it, loss=0.0228, lr=1.41e-05, step=2826]Training:   1%|‚ñè         | 2827/200000 [1:00:59<75:19:08,  1.38s/it, loss=0.0235, lr=1.41e-05, step=2827]Training:   1%|‚ñè         | 2828/200000 [1:01:00<70:27:15,  1.29s/it, loss=0.0235, lr=1.41e-05, step=2827]Training:   1%|‚ñè         | 2828/200000 [1:01:00<70:27:15,  1.29s/it, loss=0.0262, lr=1.41e-05, step=2828]Training:   1%|‚ñè         | 2829/200000 [1:01:01<67:01:12,  1.22s/it, loss=0.0262, lr=1.41e-05, step=2828]Training:   1%|‚ñè         | 2829/200000 [1:01:02<67:01:12,  1.22s/it, loss=0.0364, lr=1.41e-05, step=2829]Training:   1%|‚ñè         | 2830/200000 [1:01:03<69:53:51,  1.28s/it, loss=0.0364, lr=1.41e-05, step=2829]Training:   1%|‚ñè         | 2830/200000 [1:01:03<69:53:51,  1.28s/it, loss=0.0414, lr=1.41e-05, step=2830]Training:   1%|‚ñè         | 2831/200000 [1:01:04<72:16:32,  1.32s/it, loss=0.0414, lr=1.41e-05, step=2830]Training:   1%|‚ñè         | 2831/200000 [1:01:04<72:16:32,  1.32s/it, loss=0.0429, lr=1.42e-05, step=2831]Training:   1%|‚ñè         | 2832/200000 [1:01:05<68:20:18,  1.25s/it, loss=0.0429, lr=1.42e-05, step=2831]Training:   1%|‚ñè         | 2832/200000 [1:01:05<68:20:18,  1.25s/it, loss=0.0587, lr=1.42e-05, step=2832]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2833/200000 [1:01:07<71:27:43,  1.30s/it, loss=0.0587, lr=1.42e-05, step=2832]Training:   1%|‚ñè         | 2833/200000 [1:01:07<71:27:43,  1.30s/it, loss=0.0414, lr=1.42e-05, step=2833]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2834/200000 [1:01:08<67:45:28,  1.24s/it, loss=0.0414, lr=1.42e-05, step=2833]Training:   1%|‚ñè         | 2834/200000 [1:01:08<67:45:28,  1.24s/it, loss=0.0311, lr=1.42e-05, step=2834]Training:   1%|‚ñè         | 2835/200000 [1:01:09<70:00:32,  1.28s/it, loss=0.0311, lr=1.42e-05, step=2834]Training:   1%|‚ñè         | 2835/200000 [1:01:09<70:00:32,  1.28s/it, loss=0.0542, lr=1.42e-05, step=2835]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2836/200000 [1:01:10<66:46:19,  1.22s/it, loss=0.0542, lr=1.42e-05, step=2835]Training:   1%|‚ñè         | 2836/200000 [1:01:10<66:46:19,  1.22s/it, loss=0.0412, lr=1.42e-05, step=2836]Training:   1%|‚ñè         | 2837/200000 [1:01:12<71:22:01,  1.30s/it, loss=0.0412, lr=1.42e-05, step=2836]Training:   1%|‚ñè         | 2837/200000 [1:01:12<71:22:01,  1.30s/it, loss=0.0384, lr=1.42e-05, step=2837]Training:   1%|‚ñè         | 2838/200000 [1:01:13<75:04:04,  1.37s/it, loss=0.0384, lr=1.42e-05, step=2837]Training:   1%|‚ñè         | 2838/200000 [1:01:13<75:04:04,  1.37s/it, loss=0.0354, lr=1.42e-05, step=2838]Training:   1%|‚ñè         | 2839/200000 [1:01:14<70:16:10,  1.28s/it, loss=0.0354, lr=1.42e-05, step=2838]Training:   1%|‚ñè         | 2839/200000 [1:01:14<70:16:10,  1.28s/it, loss=0.0428, lr=1.42e-05, step=2839]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2840/200000 [1:01:16<66:55:34,  1.22s/it, loss=0.0428, lr=1.42e-05, step=2839]Training:   1%|‚ñè         | 2840/200000 [1:01:16<66:55:34,  1.22s/it, loss=0.0359, lr=1.42e-05, step=2840]Training:   1%|‚ñè         | 2841/200000 [1:01:17<71:14:26,  1.30s/it, loss=0.0359, lr=1.42e-05, step=2840]Training:   1%|‚ñè         | 2841/200000 [1:01:17<71:14:26,  1.30s/it, loss=0.0358, lr=1.42e-05, step=2841]Training:   1%|‚ñè         | 2842/200000 [1:01:18<67:35:36,  1.23s/it, loss=0.0358, lr=1.42e-05, step=2841]Training:   1%|‚ñè         | 2842/200000 [1:01:18<67:35:36,  1.23s/it, loss=0.0273, lr=1.42e-05, step=2842]Training:   1%|‚ñè         | 2843/200000 [1:01:19<69:33:38,  1.27s/it, loss=0.0273, lr=1.42e-05, step=2842]Training:   1%|‚ñè         | 2843/200000 [1:01:19<69:33:38,  1.27s/it, loss=0.0368, lr=1.42e-05, step=2843]Training:   1%|‚ñè         | 2844/200000 [1:01:21<66:21:49,  1.21s/it, loss=0.0368, lr=1.42e-05, step=2843]Training:   1%|‚ñè         | 2844/200000 [1:01:21<66:21:49,  1.21s/it, loss=0.0547, lr=1.42e-05, step=2844]Training:   1%|‚ñè         | 2845/200000 [1:01:22<64:10:36,  1.17s/it, loss=0.0547, lr=1.42e-05, step=2844]Training:   1%|‚ñè         | 2845/200000 [1:01:22<64:10:36,  1.17s/it, loss=0.0327, lr=1.42e-05, step=2845]Training:   1%|‚ñè         | 2846/200000 [1:01:23<69:16:47,  1.27s/it, loss=0.0327, lr=1.42e-05, step=2845]Training:   1%|‚ñè         | 2846/200000 [1:01:23<69:16:47,  1.27s/it, loss=0.1299, lr=1.42e-05, step=2846]Training:   1%|‚ñè         | 2847/200000 [1:01:25<71:44:55,  1.31s/it, loss=0.1299, lr=1.42e-05, step=2846]Training:   1%|‚ñè         | 2847/200000 [1:01:25<71:44:55,  1.31s/it, loss=0.0375, lr=1.42e-05, step=2847]Training:   1%|‚ñè         | 2848/200000 [1:01:26<73:55:43,  1.35s/it, loss=0.0375, lr=1.42e-05, step=2847]Training:   1%|‚ñè         | 2848/200000 [1:01:26<73:55:43,  1.35s/it, loss=0.0336, lr=1.42e-05, step=2848]Training:   1%|‚ñè         | 2849/200000 [1:01:27<69:27:53,  1.27s/it, loss=0.0336, lr=1.42e-05, step=2848]Training:   1%|‚ñè         | 2849/200000 [1:01:27<69:27:53,  1.27s/it, loss=0.0347, lr=1.42e-05, step=2849]Training:   1%|‚ñè         | 2850/200000 [1:01:28<66:21:16,  1.21s/it, loss=0.0347, lr=1.42e-05, step=2849]Training:   1%|‚ñè         | 2850/200000 [1:01:28<66:21:16,  1.21s/it, loss=0.0297, lr=1.42e-05, step=2850]Training:   1%|‚ñè         | 2851/200000 [1:01:29<68:46:19,  1.26s/it, loss=0.0297, lr=1.42e-05, step=2850]Training:   1%|‚ñè         | 2851/200000 [1:01:29<68:46:19,  1.26s/it, loss=0.0296, lr=1.43e-05, step=2851]Training:   1%|‚ñè         | 2852/200000 [1:01:31<71:42:23,  1.31s/it, loss=0.0296, lr=1.43e-05, step=2851]Training:   1%|‚ñè         | 2852/200000 [1:01:31<71:42:23,  1.31s/it, loss=0.0233, lr=1.43e-05, step=2852]Training:   1%|‚ñè         | 2853/200000 [1:01:32<67:56:42,  1.24s/it, loss=0.0233, lr=1.43e-05, step=2852]Training:   1%|‚ñè         | 2853/200000 [1:01:32<67:56:42,  1.24s/it, loss=0.1022, lr=1.43e-05, step=2853]Training:   1%|‚ñè         | 2854/200000 [1:01:33<71:13:45,  1.30s/it, loss=0.1022, lr=1.43e-05, step=2853]Training:   1%|‚ñè         | 2854/200000 [1:01:33<71:13:45,  1.30s/it, loss=0.0432, lr=1.43e-05, step=2854]Training:   1%|‚ñè         | 2855/200000 [1:01:35<67:34:24,  1.23s/it, loss=0.0432, lr=1.43e-05, step=2854]Training:   1%|‚ñè         | 2855/200000 [1:01:35<67:34:24,  1.23s/it, loss=0.0203, lr=1.43e-05, step=2855]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2856/200000 [1:01:36<70:04:59,  1.28s/it, loss=0.0203, lr=1.43e-05, step=2855]Training:   1%|‚ñè         | 2856/200000 [1:01:36<70:04:59,  1.28s/it, loss=0.0609, lr=1.43e-05, step=2856]Training:   1%|‚ñè         | 2857/200000 [1:01:37<66:47:09,  1.22s/it, loss=0.0609, lr=1.43e-05, step=2856]Training:   1%|‚ñè         | 2857/200000 [1:01:37<66:47:09,  1.22s/it, loss=0.0408, lr=1.43e-05, step=2857]Training:   1%|‚ñè         | 2858/200000 [1:01:38<71:05:29,  1.30s/it, loss=0.0408, lr=1.43e-05, step=2857]Training:   1%|‚ñè         | 2858/200000 [1:01:38<71:05:29,  1.30s/it, loss=0.0254, lr=1.43e-05, step=2858]Training:   1%|‚ñè         | 2859/200000 [1:01:40<74:31:45,  1.36s/it, loss=0.0254, lr=1.43e-05, step=2858]Training:   1%|‚ñè         | 2859/200000 [1:01:40<74:31:45,  1.36s/it, loss=0.0653, lr=1.43e-05, step=2859]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2860/200000 [1:01:41<69:53:22,  1.28s/it, loss=0.0653, lr=1.43e-05, step=2859]Training:   1%|‚ñè         | 2860/200000 [1:01:41<69:53:22,  1.28s/it, loss=0.0218, lr=1.43e-05, step=2860]Training:   1%|‚ñè         | 2861/200000 [1:01:42<66:38:23,  1.22s/it, loss=0.0218, lr=1.43e-05, step=2860]Training:   1%|‚ñè         | 2861/200000 [1:01:42<66:38:23,  1.22s/it, loss=0.0484, lr=1.43e-05, step=2861]Training:   1%|‚ñè         | 2862/200000 [1:01:44<70:41:39,  1.29s/it, loss=0.0484, lr=1.43e-05, step=2861]Training:   1%|‚ñè         | 2862/200000 [1:01:44<70:41:39,  1.29s/it, loss=0.0620, lr=1.43e-05, step=2862]Training:   1%|‚ñè         | 2863/200000 [1:01:45<67:12:51,  1.23s/it, loss=0.0620, lr=1.43e-05, step=2862]Training:   1%|‚ñè         | 2863/200000 [1:01:45<67:12:51,  1.23s/it, loss=0.0312, lr=1.43e-05, step=2863]Training:   1%|‚ñè         | 2864/200000 [1:01:46<69:01:39,  1.26s/it, loss=0.0312, lr=1.43e-05, step=2863]Training:   1%|‚ñè         | 2864/200000 [1:01:46<69:01:39,  1.26s/it, loss=0.0274, lr=1.43e-05, step=2864]Training:   1%|‚ñè         | 2865/200000 [1:01:47<66:01:40,  1.21s/it, loss=0.0274, lr=1.43e-05, step=2864]Training:   1%|‚ñè         | 2865/200000 [1:01:47<66:01:40,  1.21s/it, loss=0.0265, lr=1.43e-05, step=2865]Training:   1%|‚ñè         | 2866/200000 [1:01:48<63:54:11,  1.17s/it, loss=0.0265, lr=1.43e-05, step=2865]Training:   1%|‚ñè         | 2866/200000 [1:01:48<63:54:11,  1.17s/it, loss=0.0313, lr=1.43e-05, step=2866]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2867/200000 [1:01:50<68:03:20,  1.24s/it, loss=0.0313, lr=1.43e-05, step=2866]Training:   1%|‚ñè         | 2867/200000 [1:01:50<68:03:20,  1.24s/it, loss=0.0283, lr=1.43e-05, step=2867]Training:   1%|‚ñè         | 2868/200000 [1:01:51<70:46:38,  1.29s/it, loss=0.0283, lr=1.43e-05, step=2867]Training:   1%|‚ñè         | 2868/200000 [1:01:51<70:46:38,  1.29s/it, loss=0.0206, lr=1.43e-05, step=2868]Training:   1%|‚ñè         | 2869/200000 [1:01:52<73:02:40,  1.33s/it, loss=0.0206, lr=1.43e-05, step=2868]Training:   1%|‚ñè         | 2869/200000 [1:01:52<73:02:40,  1.33s/it, loss=0.0455, lr=1.43e-05, step=2869]Training:   1%|‚ñè         | 2870/200000 [1:01:54<68:51:35,  1.26s/it, loss=0.0455, lr=1.43e-05, step=2869]Training:   1%|‚ñè         | 2870/200000 [1:01:54<68:51:35,  1.26s/it, loss=0.0315, lr=1.43e-05, step=2870]Training:   1%|‚ñè         | 2871/200000 [1:01:55<65:54:50,  1.20s/it, loss=0.0315, lr=1.43e-05, step=2870]Training:   1%|‚ñè         | 2871/200000 [1:01:55<65:54:50,  1.20s/it, loss=0.0314, lr=1.44e-05, step=2871]Training:   1%|‚ñè         | 2872/200000 [1:01:56<68:08:41,  1.24s/it, loss=0.0314, lr=1.44e-05, step=2871]Training:   1%|‚ñè         | 2872/200000 [1:01:56<68:08:41,  1.24s/it, loss=0.0454, lr=1.44e-05, step=2872]Training:   1%|‚ñè         | 2873/200000 [1:01:57<69:30:03,  1.27s/it, loss=0.0454, lr=1.44e-05, step=2872]Training:   1%|‚ñè         | 2873/200000 [1:01:57<69:30:03,  1.27s/it, loss=0.0271, lr=1.44e-05, step=2873]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2874/200000 [1:01:58<66:21:30,  1.21s/it, loss=0.0271, lr=1.44e-05, step=2873]Training:   1%|‚ñè         | 2874/200000 [1:01:58<66:21:30,  1.21s/it, loss=0.0228, lr=1.44e-05, step=2874]Training:   1%|‚ñè         | 2875/200000 [1:02:00<68:44:46,  1.26s/it, loss=0.0228, lr=1.44e-05, step=2874]Training:   1%|‚ñè         | 2875/200000 [1:02:00<68:44:46,  1.26s/it, loss=0.0298, lr=1.44e-05, step=2875]Training:   1%|‚ñè         | 2876/200000 [1:02:01<65:49:21,  1.20s/it, loss=0.0298, lr=1.44e-05, step=2875]Training:   1%|‚ñè         | 2876/200000 [1:02:01<65:49:21,  1.20s/it, loss=0.0386, lr=1.44e-05, step=2876]Training:   1%|‚ñè         | 2877/200000 [1:02:02<68:14:14,  1.25s/it, loss=0.0386, lr=1.44e-05, step=2876]Training:   1%|‚ñè         | 2877/200000 [1:02:02<68:14:14,  1.25s/it, loss=0.0424, lr=1.44e-05, step=2877]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2878/200000 [1:02:04<71:06:52,  1.30s/it, loss=0.0424, lr=1.44e-05, step=2877]Training:   1%|‚ñè         | 2878/200000 [1:02:04<71:06:52,  1.30s/it, loss=0.0296, lr=1.44e-05, step=2878]Training:   1%|‚ñè         | 2879/200000 [1:02:05<72:51:46,  1.33s/it, loss=0.0296, lr=1.44e-05, step=2878]Training:   1%|‚ñè         | 2879/200000 [1:02:05<72:51:46,  1.33s/it, loss=0.0751, lr=1.44e-05, step=2879]Training:   1%|‚ñè         | 2880/200000 [1:02:06<74:46:11,  1.37s/it, loss=0.0751, lr=1.44e-05, step=2879]Training:   1%|‚ñè         | 2880/200000 [1:02:06<74:46:11,  1.37s/it, loss=0.1164, lr=1.44e-05, step=2880]Training:   1%|‚ñè         | 2881/200000 [1:02:07<70:02:28,  1.28s/it, loss=0.1164, lr=1.44e-05, step=2880]Training:   1%|‚ñè         | 2881/200000 [1:02:07<70:02:28,  1.28s/it, loss=0.0318, lr=1.44e-05, step=2881]Training:   1%|‚ñè         | 2882/200000 [1:02:09<66:41:53,  1.22s/it, loss=0.0318, lr=1.44e-05, step=2881]Training:   1%|‚ñè         | 2882/200000 [1:02:09<66:41:53,  1.22s/it, loss=0.0608, lr=1.44e-05, step=2882]Training:   1%|‚ñè         | 2883/200000 [1:02:10<69:07:08,  1.26s/it, loss=0.0608, lr=1.44e-05, step=2882]Training:   1%|‚ñè         | 2883/200000 [1:02:10<69:07:08,  1.26s/it, loss=0.0366, lr=1.44e-05, step=2883]Training:   1%|‚ñè         | 2884/200000 [1:02:11<72:00:40,  1.32s/it, loss=0.0366, lr=1.44e-05, step=2883]Training:   1%|‚ñè         | 2884/200000 [1:02:11<72:00:40,  1.32s/it, loss=0.0440, lr=1.44e-05, step=2884]Training:   1%|‚ñè         | 2885/200000 [1:02:12<68:06:21,  1.24s/it, loss=0.0440, lr=1.44e-05, step=2884]Training:   1%|‚ñè         | 2885/200000 [1:02:12<68:06:21,  1.24s/it, loss=0.1118, lr=1.44e-05, step=2885]Training:   1%|‚ñè         | 2886/200000 [1:02:14<71:46:49,  1.31s/it, loss=0.1118, lr=1.44e-05, step=2885]Training:   1%|‚ñè         | 2886/200000 [1:02:14<71:46:49,  1.31s/it, loss=0.0400, lr=1.44e-05, step=2886]Training:   1%|‚ñè         | 2887/200000 [1:02:15<67:57:39,  1.24s/it, loss=0.0400, lr=1.44e-05, step=2886]Training:   1%|‚ñè         | 2887/200000 [1:02:15<67:57:39,  1.24s/it, loss=0.0270, lr=1.44e-05, step=2887]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2888/200000 [1:02:16<70:26:33,  1.29s/it, loss=0.0270, lr=1.44e-05, step=2887]Training:   1%|‚ñè         | 2888/200000 [1:02:16<70:26:33,  1.29s/it, loss=0.0403, lr=1.44e-05, step=2888]Training:   1%|‚ñè         | 2889/200000 [1:02:17<67:00:26,  1.22s/it, loss=0.0403, lr=1.44e-05, step=2888]Training:   1%|‚ñè         | 2889/200000 [1:02:17<67:00:26,  1.22s/it, loss=0.0441, lr=1.44e-05, step=2889]Training:   1%|‚ñè         | 2890/200000 [1:02:19<71:32:46,  1.31s/it, loss=0.0441, lr=1.44e-05, step=2889]Training:   1%|‚ñè         | 2890/200000 [1:02:19<71:32:46,  1.31s/it, loss=0.0261, lr=1.44e-05, step=2890]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2891/200000 [1:02:20<75:12:48,  1.37s/it, loss=0.0261, lr=1.44e-05, step=2890]Training:   1%|‚ñè         | 2891/200000 [1:02:20<75:12:48,  1.37s/it, loss=0.0264, lr=1.45e-05, step=2891]Training:   1%|‚ñè         | 2892/200000 [1:02:22<70:20:54,  1.28s/it, loss=0.0264, lr=1.45e-05, step=2891]Training:   1%|‚ñè         | 2892/200000 [1:02:22<70:20:54,  1.28s/it, loss=0.0251, lr=1.45e-05, step=2892]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2893/200000 [1:02:23<66:56:34,  1.22s/it, loss=0.0251, lr=1.45e-05, step=2892]Training:   1%|‚ñè         | 2893/200000 [1:02:23<66:56:34,  1.22s/it, loss=0.0303, lr=1.45e-05, step=2893]Training:   1%|‚ñè         | 2894/200000 [1:02:24<71:10:39,  1.30s/it, loss=0.0303, lr=1.45e-05, step=2893]Training:   1%|‚ñè         | 2894/200000 [1:02:24<71:10:39,  1.30s/it, loss=0.0282, lr=1.45e-05, step=2894]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2895/200000 [1:02:25<67:33:47,  1.23s/it, loss=0.0282, lr=1.45e-05, step=2894]Training:   1%|‚ñè         | 2895/200000 [1:02:25<67:33:47,  1.23s/it, loss=0.0315, lr=1.45e-05, step=2895]Training:   1%|‚ñè         | 2896/200000 [1:02:27<69:41:08,  1.27s/it, loss=0.0315, lr=1.45e-05, step=2895]Training:   1%|‚ñè         | 2896/200000 [1:02:27<69:41:08,  1.27s/it, loss=0.0221, lr=1.45e-05, step=2896]Training:   1%|‚ñè         | 2897/200000 [1:02:28<66:26:23,  1.21s/it, loss=0.0221, lr=1.45e-05, step=2896]Training:   1%|‚ñè         | 2897/200000 [1:02:28<66:26:23,  1.21s/it, loss=0.0230, lr=1.45e-05, step=2897]Training:   1%|‚ñè         | 2898/200000 [1:02:29<64:16:08,  1.17s/it, loss=0.0230, lr=1.45e-05, step=2897]Training:   1%|‚ñè         | 2898/200000 [1:02:29<64:16:08,  1.17s/it, loss=0.0379, lr=1.45e-05, step=2898]Training:   1%|‚ñè         | 2899/200000 [1:02:30<68:42:59,  1.26s/it, loss=0.0379, lr=1.45e-05, step=2898]Training:   1%|‚ñè         | 2899/200000 [1:02:30<68:42:59,  1.26s/it, loss=0.0542, lr=1.45e-05, step=2899]Training:   1%|‚ñè         | 2900/200000 [1:02:32<71:06:30,  1.30s/it, loss=0.0542, lr=1.45e-05, step=2899]Training:   1%|‚ñè         | 2900/200000 [1:02:32<71:06:30,  1.30s/it, loss=0.0326, lr=1.45e-05, step=2900]23:55:46.744 [I] step=2900 loss=0.0393 lr=1.43e-05 grad_norm=0.84 time=126.6s                     (701675:train_pytorch.py:582)
Training:   1%|‚ñè         | 2901/200000 [1:02:33<72:35:03,  1.33s/it, loss=0.0326, lr=1.45e-05, step=2900]Training:   1%|‚ñè         | 2901/200000 [1:02:33<72:35:03,  1.33s/it, loss=0.0274, lr=1.45e-05, step=2901]Training:   1%|‚ñè         | 2902/200000 [1:02:34<68:29:49,  1.25s/it, loss=0.0274, lr=1.45e-05, step=2901]Training:   1%|‚ñè         | 2902/200000 [1:02:34<68:29:49,  1.25s/it, loss=0.4458, lr=1.45e-05, step=2902]Training:   1%|‚ñè         | 2903/200000 [1:02:35<65:39:37,  1.20s/it, loss=0.4458, lr=1.45e-05, step=2902]Training:   1%|‚ñè         | 2903/200000 [1:02:35<65:39:37,  1.20s/it, loss=0.0320, lr=1.45e-05, step=2903]Training:   1%|‚ñè         | 2904/200000 [1:02:36<68:15:25,  1.25s/it, loss=0.0320, lr=1.45e-05, step=2903]Training:   1%|‚ñè         | 2904/200000 [1:02:36<68:15:25,  1.25s/it, loss=0.0325, lr=1.45e-05, step=2904]Training:   1%|‚ñè         | 2905/200000 [1:02:38<70:32:25,  1.29s/it, loss=0.0325, lr=1.45e-05, step=2904]Training:   1%|‚ñè         | 2905/200000 [1:02:38<70:32:25,  1.29s/it, loss=0.0289, lr=1.45e-05, step=2905]Training:   1%|‚ñè         | 2906/200000 [1:02:39<67:03:47,  1.22s/it, loss=0.0289, lr=1.45e-05, step=2905]Training:   1%|‚ñè         | 2906/200000 [1:02:39<67:03:47,  1.22s/it, loss=0.1482, lr=1.45e-05, step=2906]Training:   1%|‚ñè         | 2907/200000 [1:02:40<70:18:57,  1.28s/it, loss=0.1482, lr=1.45e-05, step=2906]Training:   1%|‚ñè         | 2907/200000 [1:02:40<70:18:57,  1.28s/it, loss=0.0351, lr=1.45e-05, step=2907]Training:   1%|‚ñè         | 2908/200000 [1:02:41<66:55:21,  1.22s/it, loss=0.0351, lr=1.45e-05, step=2907]Training:   1%|‚ñè         | 2908/200000 [1:02:41<66:55:21,  1.22s/it, loss=0.0380, lr=1.45e-05, step=2908]Training:   1%|‚ñè         | 2909/200000 [1:02:43<69:53:30,  1.28s/it, loss=0.0380, lr=1.45e-05, step=2908]Training:   1%|‚ñè         | 2909/200000 [1:02:43<69:53:30,  1.28s/it, loss=0.0309, lr=1.45e-05, step=2909]Training:   1%|‚ñè         | 2910/200000 [1:02:44<66:38:45,  1.22s/it, loss=0.0309, lr=1.45e-05, step=2909]Training:   1%|‚ñè         | 2910/200000 [1:02:44<66:38:45,  1.22s/it, loss=0.0615, lr=1.45e-05, step=2910]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2911/200000 [1:02:45<71:07:52,  1.30s/it, loss=0.0615, lr=1.45e-05, step=2910]Training:   1%|‚ñè         | 2911/200000 [1:02:45<71:07:52,  1.30s/it, loss=0.0317, lr=1.46e-05, step=2911]Training:   1%|‚ñè         | 2912/200000 [1:02:47<74:29:36,  1.36s/it, loss=0.0317, lr=1.46e-05, step=2911]Training:   1%|‚ñè         | 2912/200000 [1:02:47<74:29:36,  1.36s/it, loss=0.0428, lr=1.46e-05, step=2912]Training:   1%|‚ñè         | 2913/200000 [1:02:48<69:52:06,  1.28s/it, loss=0.0428, lr=1.46e-05, step=2912]Training:   1%|‚ñè         | 2913/200000 [1:02:48<69:52:06,  1.28s/it, loss=0.0270, lr=1.46e-05, step=2913]Training:   1%|‚ñè         | 2914/200000 [1:02:49<66:37:50,  1.22s/it, loss=0.0270, lr=1.46e-05, step=2913]Training:   1%|‚ñè         | 2914/200000 [1:02:49<66:37:50,  1.22s/it, loss=0.0291, lr=1.46e-05, step=2914]Training:   1%|‚ñè         | 2915/200000 [1:02:51<70:46:54,  1.29s/it, loss=0.0291, lr=1.46e-05, step=2914]Training:   1%|‚ñè         | 2915/200000 [1:02:51<70:46:54,  1.29s/it, loss=0.0309, lr=1.46e-05, step=2915]Training:   1%|‚ñè         | 2916/200000 [1:02:52<67:11:31,  1.23s/it, loss=0.0309, lr=1.46e-05, step=2915]Training:   1%|‚ñè         | 2916/200000 [1:02:52<67:11:31,  1.23s/it, loss=0.0392, lr=1.46e-05, step=2916]Training:   1%|‚ñè         | 2917/200000 [1:02:53<69:11:30,  1.26s/it, loss=0.0392, lr=1.46e-05, step=2916]Training:   1%|‚ñè         | 2917/200000 [1:02:53<69:11:30,  1.26s/it, loss=0.0217, lr=1.46e-05, step=2917]Training:   1%|‚ñè         | 2918/200000 [1:02:54<66:07:33,  1.21s/it, loss=0.0217, lr=1.46e-05, step=2917]Training:   1%|‚ñè         | 2918/200000 [1:02:54<66:07:33,  1.21s/it, loss=0.0255, lr=1.46e-05, step=2918]Training:   1%|‚ñè         | 2919/200000 [1:02:55<63:58:08,  1.17s/it, loss=0.0255, lr=1.46e-05, step=2918]Training:   1%|‚ñè         | 2919/200000 [1:02:55<63:58:08,  1.17s/it, loss=0.0272, lr=1.46e-05, step=2919]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2920/200000 [1:02:57<68:24:29,  1.25s/it, loss=0.0272, lr=1.46e-05, step=2919]Training:   1%|‚ñè         | 2920/200000 [1:02:57<68:24:29,  1.25s/it, loss=0.0213, lr=1.46e-05, step=2920]Training:   1%|‚ñè         | 2921/200000 [1:02:58<70:59:30,  1.30s/it, loss=0.0213, lr=1.46e-05, step=2920]Training:   1%|‚ñè         | 2921/200000 [1:02:58<70:59:30,  1.30s/it, loss=0.0288, lr=1.46e-05, step=2921]Training:   1%|‚ñè         | 2922/200000 [1:02:59<73:02:06,  1.33s/it, loss=0.0288, lr=1.46e-05, step=2921]Training:   1%|‚ñè         | 2922/200000 [1:02:59<73:02:06,  1.33s/it, loss=0.0240, lr=1.46e-05, step=2922]Training:   1%|‚ñè         | 2923/200000 [1:03:00<68:50:06,  1.26s/it, loss=0.0240, lr=1.46e-05, step=2922]Training:   1%|‚ñè         | 2923/200000 [1:03:00<68:50:06,  1.26s/it, loss=0.0263, lr=1.46e-05, step=2923]Training:   1%|‚ñè         | 2924/200000 [1:03:02<65:52:26,  1.20s/it, loss=0.0263, lr=1.46e-05, step=2923]Training:   1%|‚ñè         | 2924/200000 [1:03:02<65:52:26,  1.20s/it, loss=0.0209, lr=1.46e-05, step=2924]Training:   1%|‚ñè         | 2925/200000 [1:03:03<68:01:14,  1.24s/it, loss=0.0209, lr=1.46e-05, step=2924]Training:   1%|‚ñè         | 2925/200000 [1:03:03<68:01:14,  1.24s/it, loss=0.0351, lr=1.46e-05, step=2925]Training:   1%|‚ñè         | 2926/200000 [1:03:04<69:22:43,  1.27s/it, loss=0.0351, lr=1.46e-05, step=2925]Training:   1%|‚ñè         | 2926/200000 [1:03:04<69:22:43,  1.27s/it, loss=0.0240, lr=1.46e-05, step=2926]Training:   1%|‚ñè         | 2927/200000 [1:03:05<66:13:10,  1.21s/it, loss=0.0240, lr=1.46e-05, step=2926]Training:   1%|‚ñè         | 2927/200000 [1:03:05<66:13:10,  1.21s/it, loss=0.3244, lr=1.46e-05, step=2927]Training:   1%|‚ñè         | 2928/200000 [1:03:07<69:22:46,  1.27s/it, loss=0.3244, lr=1.46e-05, step=2927]Training:   1%|‚ñè         | 2928/200000 [1:03:07<69:22:46,  1.27s/it, loss=0.3619, lr=1.46e-05, step=2928]Training:   1%|‚ñè         | 2929/200000 [1:03:08<66:14:24,  1.21s/it, loss=0.3619, lr=1.46e-05, step=2928]Training:   1%|‚ñè         | 2929/200000 [1:03:08<66:14:24,  1.21s/it, loss=0.0276, lr=1.46e-05, step=2929]Training:   1%|‚ñè         | 2930/200000 [1:03:09<68:21:14,  1.25s/it, loss=0.0276, lr=1.46e-05, step=2929]Training:   1%|‚ñè         | 2930/200000 [1:03:09<68:21:14,  1.25s/it, loss=0.0359, lr=1.46e-05, step=2930]Training:   1%|‚ñè         | 2931/200000 [1:03:11<71:53:41,  1.31s/it, loss=0.0359, lr=1.46e-05, step=2930]Training:   1%|‚ñè         | 2931/200000 [1:03:11<71:53:41,  1.31s/it, loss=0.0436, lr=1.47e-05, step=2931]Training:   1%|‚ñè         | 2932/200000 [1:03:12<73:33:34,  1.34s/it, loss=0.0436, lr=1.47e-05, step=2931]Training:   1%|‚ñè         | 2932/200000 [1:03:12<73:33:34,  1.34s/it, loss=0.0332, lr=1.47e-05, step=2932]Training:   1%|‚ñè         | 2933/200000 [1:03:13<75:09:47,  1.37s/it, loss=0.0332, lr=1.47e-05, step=2932]Training:   1%|‚ñè         | 2933/200000 [1:03:13<75:09:47,  1.37s/it, loss=0.0928, lr=1.47e-05, step=2933]Training:   1%|‚ñè         | 2934/200000 [1:03:14<70:18:07,  1.28s/it, loss=0.0928, lr=1.47e-05, step=2933]Training:   1%|‚ñè         | 2934/200000 [1:03:14<70:18:07,  1.28s/it, loss=0.0255, lr=1.47e-05, step=2934]Training:   1%|‚ñè         | 2935/200000 [1:03:16<66:54:30,  1.22s/it, loss=0.0255, lr=1.47e-05, step=2934]Training:   1%|‚ñè         | 2935/200000 [1:03:16<66:54:30,  1.22s/it, loss=0.0259, lr=1.47e-05, step=2935]Training:   1%|‚ñè         | 2936/200000 [1:03:17<69:49:06,  1.28s/it, loss=0.0259, lr=1.47e-05, step=2935]Training:   1%|‚ñè         | 2936/200000 [1:03:17<69:49:06,  1.28s/it, loss=0.0252, lr=1.47e-05, step=2936]Training:   1%|‚ñè         | 2937/200000 [1:03:18<71:43:38,  1.31s/it, loss=0.0252, lr=1.47e-05, step=2936]Training:   1%|‚ñè         | 2937/200000 [1:03:18<71:43:38,  1.31s/it, loss=0.0361, lr=1.47e-05, step=2937]Training:   1%|‚ñè         | 2938/200000 [1:03:19<67:53:28,  1.24s/it, loss=0.0361, lr=1.47e-05, step=2937]Training:   1%|‚ñè         | 2938/200000 [1:03:19<67:53:28,  1.24s/it, loss=0.0745, lr=1.47e-05, step=2938]Training:   1%|‚ñè         | 2939/200000 [1:03:21<71:28:30,  1.31s/it, loss=0.0745, lr=1.47e-05, step=2938]Training:   1%|‚ñè         | 2939/200000 [1:03:21<71:28:30,  1.31s/it, loss=0.0296, lr=1.47e-05, step=2939]Training:   1%|‚ñè         | 2940/200000 [1:03:22<67:44:49,  1.24s/it, loss=0.0296, lr=1.47e-05, step=2939]Training:   1%|‚ñè         | 2940/200000 [1:03:22<67:44:49,  1.24s/it, loss=0.0479, lr=1.47e-05, step=2940]Training:   1%|‚ñè         | 2941/200000 [1:03:23<69:55:29,  1.28s/it, loss=0.0479, lr=1.47e-05, step=2940]Training:   1%|‚ñè         | 2941/200000 [1:03:23<69:55:29,  1.28s/it, loss=0.0284, lr=1.47e-05, step=2941]Training:   1%|‚ñè         | 2942/200000 [1:03:24<66:39:13,  1.22s/it, loss=0.0284, lr=1.47e-05, step=2941]Training:   1%|‚ñè         | 2942/200000 [1:03:24<66:39:13,  1.22s/it, loss=0.0305, lr=1.47e-05, step=2942]Training:   1%|‚ñè         | 2943/200000 [1:03:26<71:23:23,  1.30s/it, loss=0.0305, lr=1.47e-05, step=2942]Training:   1%|‚ñè         | 2943/200000 [1:03:26<71:23:23,  1.30s/it, loss=0.0346, lr=1.47e-05, step=2943]Training:   1%|‚ñè         | 2944/200000 [1:03:27<75:07:24,  1.37s/it, loss=0.0346, lr=1.47e-05, step=2943]Training:   1%|‚ñè         | 2944/200000 [1:03:27<75:07:24,  1.37s/it, loss=0.0251, lr=1.47e-05, step=2944]Training:   1%|‚ñè         | 2945/200000 [1:03:29<70:16:24,  1.28s/it, loss=0.0251, lr=1.47e-05, step=2944]Training:   1%|‚ñè         | 2945/200000 [1:03:29<70:16:24,  1.28s/it, loss=0.0341, lr=1.47e-05, step=2945]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2946/200000 [1:03:30<66:54:06,  1.22s/it, loss=0.0341, lr=1.47e-05, step=2945]Training:   1%|‚ñè         | 2946/200000 [1:03:30<66:54:06,  1.22s/it, loss=0.0522, lr=1.47e-05, step=2946]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2947/200000 [1:03:31<71:00:06,  1.30s/it, loss=0.0522, lr=1.47e-05, step=2946]Training:   1%|‚ñè         | 2947/200000 [1:03:31<71:00:06,  1.30s/it, loss=0.0390, lr=1.47e-05, step=2947]Training:   1%|‚ñè         | 2948/200000 [1:03:32<67:23:58,  1.23s/it, loss=0.0390, lr=1.47e-05, step=2947]Training:   1%|‚ñè         | 2948/200000 [1:03:32<67:23:58,  1.23s/it, loss=0.0285, lr=1.47e-05, step=2948]Training:   1%|‚ñè         | 2949/200000 [1:03:33<68:39:03,  1.25s/it, loss=0.0285, lr=1.47e-05, step=2948]Training:   1%|‚ñè         | 2949/200000 [1:03:33<68:39:03,  1.25s/it, loss=0.0288, lr=1.47e-05, step=2949]Training:   1%|‚ñè         | 2950/200000 [1:03:35<65:47:18,  1.20s/it, loss=0.0288, lr=1.47e-05, step=2949]Training:   1%|‚ñè         | 2950/200000 [1:03:35<65:47:18,  1.20s/it, loss=0.0250, lr=1.47e-05, step=2950]Training:   1%|‚ñè         | 2951/200000 [1:03:36<63:46:36,  1.17s/it, loss=0.0250, lr=1.47e-05, step=2950]Training:   1%|‚ñè         | 2951/200000 [1:03:36<63:46:36,  1.17s/it, loss=0.0325, lr=1.48e-05, step=2951]Training:   1%|‚ñè         | 2952/200000 [1:03:37<68:45:47,  1.26s/it, loss=0.0325, lr=1.48e-05, step=2951]Training:   1%|‚ñè         | 2952/200000 [1:03:37<68:45:47,  1.26s/it, loss=0.0380, lr=1.48e-05, step=2952]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2953/200000 [1:03:38<71:08:03,  1.30s/it, loss=0.0380, lr=1.48e-05, step=2952]Training:   1%|‚ñè         | 2953/200000 [1:03:38<71:08:03,  1.30s/it, loss=0.0246, lr=1.48e-05, step=2953]Training:   1%|‚ñè         | 2954/200000 [1:03:40<73:18:49,  1.34s/it, loss=0.0246, lr=1.48e-05, step=2953]Training:   1%|‚ñè         | 2954/200000 [1:03:40<73:18:49,  1.34s/it, loss=0.0312, lr=1.48e-05, step=2954]Training:   1%|‚ñè         | 2955/200000 [1:03:41<69:01:11,  1.26s/it, loss=0.0312, lr=1.48e-05, step=2954]Training:   1%|‚ñè         | 2955/200000 [1:03:41<69:01:11,  1.26s/it, loss=0.0521, lr=1.48e-05, step=2955]Training:   1%|‚ñè         | 2956/200000 [1:03:42<65:59:37,  1.21s/it, loss=0.0521, lr=1.48e-05, step=2955]Training:   1%|‚ñè         | 2956/200000 [1:03:42<65:59:37,  1.21s/it, loss=0.0344, lr=1.48e-05, step=2956]Training:   1%|‚ñè         | 2957/200000 [1:03:43<68:20:48,  1.25s/it, loss=0.0344, lr=1.48e-05, step=2956]Training:   1%|‚ñè         | 2957/200000 [1:03:43<68:20:48,  1.25s/it, loss=0.0303, lr=1.48e-05, step=2957]Training:   1%|‚ñè         | 2958/200000 [1:03:45<70:56:59,  1.30s/it, loss=0.0303, lr=1.48e-05, step=2957]Training:   1%|‚ñè         | 2958/200000 [1:03:45<70:56:59,  1.30s/it, loss=0.0828, lr=1.48e-05, step=2958]Training:   1%|‚ñè         | 2959/200000 [1:03:46<67:23:04,  1.23s/it, loss=0.0828, lr=1.48e-05, step=2958]Training:   1%|‚ñè         | 2959/200000 [1:03:46<67:23:04,  1.23s/it, loss=0.0257, lr=1.48e-05, step=2959]Training:   1%|‚ñè         | 2960/200000 [1:03:47<71:11:11,  1.30s/it, loss=0.0257, lr=1.48e-05, step=2959]Training:   1%|‚ñè         | 2960/200000 [1:03:47<71:11:11,  1.30s/it, loss=0.0411, lr=1.48e-05, step=2960]Training:   1%|‚ñè         | 2961/200000 [1:03:48<67:31:43,  1.23s/it, loss=0.0411, lr=1.48e-05, step=2960]Training:   1%|‚ñè         | 2961/200000 [1:03:48<67:31:43,  1.23s/it, loss=0.0426, lr=1.48e-05, step=2961]Training:   1%|‚ñè         | 2962/200000 [1:03:50<70:20:15,  1.29s/it, loss=0.0426, lr=1.48e-05, step=2961]Training:   1%|‚ñè         | 2962/200000 [1:03:50<70:20:15,  1.29s/it, loss=0.0911, lr=1.48e-05, step=2962]Training:   1%|‚ñè         | 2963/200000 [1:03:51<66:53:57,  1.22s/it, loss=0.0911, lr=1.48e-05, step=2962]Training:   1%|‚ñè         | 2963/200000 [1:03:51<66:53:57,  1.22s/it, loss=0.0285, lr=1.48e-05, step=2963]Training:   1%|‚ñè         | 2964/200000 [1:03:52<71:09:22,  1.30s/it, loss=0.0285, lr=1.48e-05, step=2963]Training:   1%|‚ñè         | 2964/200000 [1:03:52<71:09:22,  1.30s/it, loss=0.0265, lr=1.48e-05, step=2964]Training:   1%|‚ñè         | 2965/200000 [1:03:54<74:32:34,  1.36s/it, loss=0.0265, lr=1.48e-05, step=2964]Training:   1%|‚ñè         | 2965/200000 [1:03:54<74:32:34,  1.36s/it, loss=0.0314, lr=1.48e-05, step=2965]Training:   1%|‚ñè         | 2966/200000 [1:03:55<69:51:35,  1.28s/it, loss=0.0314, lr=1.48e-05, step=2965]Training:   1%|‚ñè         | 2966/200000 [1:03:55<69:51:35,  1.28s/it, loss=0.0553, lr=1.48e-05, step=2966]Training:   1%|‚ñè         | 2967/200000 [1:03:56<66:37:48,  1.22s/it, loss=0.0553, lr=1.48e-05, step=2966]Training:   1%|‚ñè         | 2967/200000 [1:03:56<66:37:48,  1.22s/it, loss=0.0338, lr=1.48e-05, step=2967]Training:   1%|‚ñè         | 2968/200000 [1:03:58<70:38:47,  1.29s/it, loss=0.0338, lr=1.48e-05, step=2967]Training:   1%|‚ñè         | 2968/200000 [1:03:58<70:38:47,  1.29s/it, loss=0.1507, lr=1.48e-05, step=2968]Training:   1%|‚ñè         | 2969/200000 [1:03:59<67:10:44,  1.23s/it, loss=0.1507, lr=1.48e-05, step=2968]Training:   1%|‚ñè         | 2969/200000 [1:03:59<67:10:44,  1.23s/it, loss=0.0272, lr=1.48e-05, step=2969]Training:   1%|‚ñè         | 2970/200000 [1:04:00<69:11:26,  1.26s/it, loss=0.0272, lr=1.48e-05, step=2969]Training:   1%|‚ñè         | 2970/200000 [1:04:00<69:11:26,  1.26s/it, loss=0.0217, lr=1.48e-05, step=2970]Training:   1%|‚ñè         | 2971/200000 [1:04:01<66:10:12,  1.21s/it, loss=0.0217, lr=1.48e-05, step=2970]Training:   1%|‚ñè         | 2971/200000 [1:04:01<66:10:12,  1.21s/it, loss=0.0340, lr=1.49e-05, step=2971]Training:   1%|‚ñè         | 2972/200000 [1:04:02<64:00:32,  1.17s/it, loss=0.0340, lr=1.49e-05, step=2971]Training:   1%|‚ñè         | 2972/200000 [1:04:02<64:00:32,  1.17s/it, loss=0.0215, lr=1.49e-05, step=2972]Training:   1%|‚ñè         | 2973/200000 [1:04:04<68:06:54,  1.24s/it, loss=0.0215, lr=1.49e-05, step=2972]Training:   1%|‚ñè         | 2973/200000 [1:04:04<68:06:54,  1.24s/it, loss=0.0291, lr=1.49e-05, step=2973]Training:   1%|‚ñè         | 2974/200000 [1:04:05<70:19:14,  1.28s/it, loss=0.0291, lr=1.49e-05, step=2973]Training:   1%|‚ñè         | 2974/200000 [1:04:05<70:19:14,  1.28s/it, loss=0.0386, lr=1.49e-05, step=2974]Training:   1%|‚ñè         | 2975/200000 [1:04:06<72:37:08,  1.33s/it, loss=0.0386, lr=1.49e-05, step=2974]Training:   1%|‚ñè         | 2975/200000 [1:04:06<72:37:08,  1.33s/it, loss=0.0266, lr=1.49e-05, step=2975]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2976/200000 [1:04:07<68:31:38,  1.25s/it, loss=0.0266, lr=1.49e-05, step=2975]Training:   1%|‚ñè         | 2976/200000 [1:04:07<68:31:38,  1.25s/it, loss=0.0375, lr=1.49e-05, step=2976]Training:   1%|‚ñè         | 2977/200000 [1:04:08<65:38:26,  1.20s/it, loss=0.0375, lr=1.49e-05, step=2976]Training:   1%|‚ñè         | 2977/200000 [1:04:08<65:38:26,  1.20s/it, loss=0.0404, lr=1.49e-05, step=2977]Training:   1%|‚ñè         | 2978/200000 [1:04:10<67:46:24,  1.24s/it, loss=0.0404, lr=1.49e-05, step=2977]Training:   1%|‚ñè         | 2978/200000 [1:04:10<67:46:24,  1.24s/it, loss=0.0392, lr=1.49e-05, step=2978]Training:   1%|‚ñè         | 2979/200000 [1:04:11<69:46:15,  1.27s/it, loss=0.0392, lr=1.49e-05, step=2978]Training:   1%|‚ñè         | 2979/200000 [1:04:11<69:46:15,  1.27s/it, loss=0.0299, lr=1.49e-05, step=2979]Training:   1%|‚ñè         | 2980/200000 [1:04:12<66:32:19,  1.22s/it, loss=0.0299, lr=1.49e-05, step=2979]Training:   1%|‚ñè         | 2980/200000 [1:04:12<66:32:19,  1.22s/it, loss=0.0404, lr=1.49e-05, step=2980]Training:   1%|‚ñè         | 2981/200000 [1:04:14<68:54:28,  1.26s/it, loss=0.0404, lr=1.49e-05, step=2980]Training:   1%|‚ñè         | 2981/200000 [1:04:14<68:54:28,  1.26s/it, loss=0.0460, lr=1.49e-05, step=2981]Training:   1%|‚ñè         | 2982/200000 [1:04:15<65:54:48,  1.20s/it, loss=0.0460, lr=1.49e-05, step=2981]Training:   1%|‚ñè         | 2982/200000 [1:04:15<65:54:48,  1.20s/it, loss=0.0344, lr=1.49e-05, step=2982]Training:   1%|‚ñè         | 2983/200000 [1:04:16<68:53:39,  1.26s/it, loss=0.0344, lr=1.49e-05, step=2982]Training:   1%|‚ñè         | 2983/200000 [1:04:16<68:53:39,  1.26s/it, loss=0.0314, lr=1.49e-05, step=2983]Training:   1%|‚ñè         | 2984/200000 [1:04:18<71:58:59,  1.32s/it, loss=0.0314, lr=1.49e-05, step=2983]Training:   1%|‚ñè         | 2984/200000 [1:04:18<71:58:59,  1.32s/it, loss=0.0544, lr=1.49e-05, step=2984]Training:   1%|‚ñè         | 2985/200000 [1:04:19<73:42:12,  1.35s/it, loss=0.0544, lr=1.49e-05, step=2984]Training:   1%|‚ñè         | 2985/200000 [1:04:19<73:42:12,  1.35s/it, loss=0.0350, lr=1.49e-05, step=2985]Training:   1%|‚ñè         | 2986/200000 [1:04:20<75:21:34,  1.38s/it, loss=0.0350, lr=1.49e-05, step=2985]Training:   1%|‚ñè         | 2986/200000 [1:04:20<75:21:34,  1.38s/it, loss=0.0801, lr=1.49e-05, step=2986]Training:   1%|‚ñè         | 2987/200000 [1:04:21<70:26:59,  1.29s/it, loss=0.0801, lr=1.49e-05, step=2986]Training:   1%|‚ñè         | 2987/200000 [1:04:21<70:26:59,  1.29s/it, loss=0.0325, lr=1.49e-05, step=2987]Training:   1%|‚ñè         | 2988/200000 [1:04:23<67:01:39,  1.22s/it, loss=0.0325, lr=1.49e-05, step=2987]Training:   1%|‚ñè         | 2988/200000 [1:04:23<67:01:39,  1.22s/it, loss=0.0489, lr=1.49e-05, step=2988]Training:   1%|‚ñè         | 2989/200000 [1:04:24<69:59:21,  1.28s/it, loss=0.0489, lr=1.49e-05, step=2988]Training:   1%|‚ñè         | 2989/200000 [1:04:24<69:59:21,  1.28s/it, loss=0.0245, lr=1.49e-05, step=2989]Training:   1%|‚ñè         | 2990/200000 [1:04:25<72:29:18,  1.32s/it, loss=0.0245, lr=1.49e-05, step=2989]Training:   1%|‚ñè         | 2990/200000 [1:04:25<72:29:18,  1.32s/it, loss=0.0283, lr=1.49e-05, step=2990]Training:   1%|‚ñè         | 2991/200000 [1:04:26<68:26:27,  1.25s/it, loss=0.0283, lr=1.49e-05, step=2990]Training:   1%|‚ñè         | 2991/200000 [1:04:26<68:26:27,  1.25s/it, loss=0.0342, lr=1.50e-05, step=2991]Training:   1%|‚ñè         | 2992/200000 [1:04:28<72:10:32,  1.32s/it, loss=0.0342, lr=1.50e-05, step=2991]Training:   1%|‚ñè         | 2992/200000 [1:04:28<72:10:32,  1.32s/it, loss=0.0354, lr=1.50e-05, step=2992]Training:   1%|‚ñè         | 2993/200000 [1:04:29<68:10:32,  1.25s/it, loss=0.0354, lr=1.50e-05, step=2992]Training:   1%|‚ñè         | 2993/200000 [1:04:29<68:10:32,  1.25s/it, loss=0.0249, lr=1.50e-05, step=2993]Training:   1%|‚ñè         | 2994/200000 [1:04:30<70:28:16,  1.29s/it, loss=0.0249, lr=1.50e-05, step=2993]Training:   1%|‚ñè         | 2994/200000 [1:04:30<70:28:16,  1.29s/it, loss=0.0329, lr=1.50e-05, step=2994]Training:   1%|‚ñè         | 2995/200000 [1:04:31<67:02:25,  1.23s/it, loss=0.0329, lr=1.50e-05, step=2994]Training:   1%|‚ñè         | 2995/200000 [1:04:31<67:02:25,  1.23s/it, loss=0.0268, lr=1.50e-05, step=2995]Training:   1%|‚ñè         | 2996/200000 [1:04:33<71:32:09,  1.31s/it, loss=0.0268, lr=1.50e-05, step=2995]Training:   1%|‚ñè         | 2996/200000 [1:04:33<71:32:09,  1.31s/it, loss=0.0229, lr=1.50e-05, step=2996]Training:   1%|‚ñè         | 2997/200000 [1:04:35<75:11:29,  1.37s/it, loss=0.0229, lr=1.50e-05, step=2996]Training:   1%|‚ñè         | 2997/200000 [1:04:35<75:11:29,  1.37s/it, loss=0.0368, lr=1.50e-05, step=2997]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   1%|‚ñè         | 2998/200000 [1:04:36<70:18:51,  1.28s/it, loss=0.0368, lr=1.50e-05, step=2997]Training:   1%|‚ñè         | 2998/200000 [1:04:36<70:18:51,  1.28s/it, loss=0.0314, lr=1.50e-05, step=2998]Training:   1%|‚ñè         | 2999/200000 [1:04:37<66:54:59,  1.22s/it, loss=0.0314, lr=1.50e-05, step=2998]Training:   1%|‚ñè         | 2999/200000 [1:04:37<66:54:59,  1.22s/it, loss=0.0336, lr=1.50e-05, step=2999]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3000/200000 [1:04:38<70:22:36,  1.29s/it, loss=0.0336, lr=1.50e-05, step=2999]Training:   2%|‚ñè         | 3000/200000 [1:04:38<70:22:36,  1.29s/it, loss=0.0372, lr=1.50e-05, step=3000]23:57:52.991 [I] step=3000 loss=0.0483 lr=1.48e-05 grad_norm=0.86 time=126.2s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3001/200000 [1:04:39<66:58:25,  1.22s/it, loss=0.0372, lr=1.50e-05, step=3000]Training:   2%|‚ñè         | 3001/200000 [1:04:39<66:58:25,  1.22s/it, loss=0.0456, lr=1.50e-05, step=3001]Training:   2%|‚ñè         | 3002/200000 [1:04:41<69:22:23,  1.27s/it, loss=0.0456, lr=1.50e-05, step=3001]Training:   2%|‚ñè         | 3002/200000 [1:04:41<69:22:23,  1.27s/it, loss=0.0369, lr=1.50e-05, step=3002]Training:   2%|‚ñè         | 3003/200000 [1:04:42<66:15:52,  1.21s/it, loss=0.0369, lr=1.50e-05, step=3002]Training:   2%|‚ñè         | 3003/200000 [1:04:42<66:15:52,  1.21s/it, loss=0.0391, lr=1.50e-05, step=3003]Training:   2%|‚ñè         | 3004/200000 [1:04:43<64:03:52,  1.17s/it, loss=0.0391, lr=1.50e-05, step=3003]Training:   2%|‚ñè         | 3004/200000 [1:04:43<64:03:52,  1.17s/it, loss=0.0317, lr=1.50e-05, step=3004]Training:   2%|‚ñè         | 3005/200000 [1:04:44<68:35:12,  1.25s/it, loss=0.0317, lr=1.50e-05, step=3004]Training:   2%|‚ñè         | 3005/200000 [1:04:44<68:35:12,  1.25s/it, loss=0.0272, lr=1.50e-05, step=3005]Training:   2%|‚ñè         | 3006/200000 [1:04:46<71:09:51,  1.30s/it, loss=0.0272, lr=1.50e-05, step=3005]Training:   2%|‚ñè         | 3006/200000 [1:04:46<71:09:51,  1.30s/it, loss=0.0497, lr=1.50e-05, step=3006]Training:   2%|‚ñè         | 3007/200000 [1:04:47<72:58:18,  1.33s/it, loss=0.0497, lr=1.50e-05, step=3006]Training:   2%|‚ñè         | 3007/200000 [1:04:47<72:58:18,  1.33s/it, loss=0.0342, lr=1.50e-05, step=3007]Training:   2%|‚ñè         | 3008/200000 [1:04:48<68:47:12,  1.26s/it, loss=0.0342, lr=1.50e-05, step=3007]Training:   2%|‚ñè         | 3008/200000 [1:04:48<68:47:12,  1.26s/it, loss=0.0364, lr=1.50e-05, step=3008]Training:   2%|‚ñè         | 3009/200000 [1:04:49<65:50:54,  1.20s/it, loss=0.0364, lr=1.50e-05, step=3008]Training:   2%|‚ñè         | 3009/200000 [1:04:49<65:50:54,  1.20s/it, loss=0.0194, lr=1.50e-05, step=3009]Training:   2%|‚ñè         | 3010/200000 [1:04:50<67:56:46,  1.24s/it, loss=0.0194, lr=1.50e-05, step=3009]Training:   2%|‚ñè         | 3010/200000 [1:04:50<67:56:46,  1.24s/it, loss=0.0409, lr=1.50e-05, step=3010]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3011/200000 [1:04:52<70:05:15,  1.28s/it, loss=0.0409, lr=1.50e-05, step=3010]Training:   2%|‚ñè         | 3011/200000 [1:04:52<70:05:15,  1.28s/it, loss=0.0281, lr=1.51e-05, step=3011]Training:   2%|‚ñè         | 3012/200000 [1:04:53<66:44:55,  1.22s/it, loss=0.0281, lr=1.51e-05, step=3011]Training:   2%|‚ñè         | 3012/200000 [1:04:53<66:44:55,  1.22s/it, loss=0.0229, lr=1.51e-05, step=3012]Training:   2%|‚ñè         | 3013/200000 [1:04:54<70:17:47,  1.28s/it, loss=0.0229, lr=1.51e-05, step=3012]Training:   2%|‚ñè         | 3013/200000 [1:04:54<70:17:47,  1.28s/it, loss=0.0179, lr=1.51e-05, step=3013]Training:   2%|‚ñè         | 3014/200000 [1:04:55<66:52:58,  1.22s/it, loss=0.0179, lr=1.51e-05, step=3013]Training:   2%|‚ñè         | 3014/200000 [1:04:55<66:52:58,  1.22s/it, loss=0.0351, lr=1.51e-05, step=3014]Training:   2%|‚ñè         | 3015/200000 [1:04:57<69:44:01,  1.27s/it, loss=0.0351, lr=1.51e-05, step=3014]Training:   2%|‚ñè         | 3015/200000 [1:04:57<69:44:01,  1.27s/it, loss=0.0344, lr=1.51e-05, step=3015]Training:   2%|‚ñè         | 3016/200000 [1:04:58<66:30:38,  1.22s/it, loss=0.0344, lr=1.51e-05, step=3015]Training:   2%|‚ñè         | 3016/200000 [1:04:58<66:30:38,  1.22s/it, loss=0.0315, lr=1.51e-05, step=3016]Training:   2%|‚ñè         | 3017/200000 [1:04:59<70:55:51,  1.30s/it, loss=0.0315, lr=1.51e-05, step=3016]Training:   2%|‚ñè         | 3017/200000 [1:04:59<70:55:51,  1.30s/it, loss=0.0266, lr=1.51e-05, step=3017]Training:   2%|‚ñè         | 3018/200000 [1:05:01<74:00:25,  1.35s/it, loss=0.0266, lr=1.51e-05, step=3017]Training:   2%|‚ñè         | 3018/200000 [1:05:01<74:00:25,  1.35s/it, loss=0.0406, lr=1.51e-05, step=3018]Training:   2%|‚ñè         | 3019/200000 [1:05:02<69:29:18,  1.27s/it, loss=0.0406, lr=1.51e-05, step=3018]Training:   2%|‚ñè         | 3019/200000 [1:05:02<69:29:18,  1.27s/it, loss=0.0350, lr=1.51e-05, step=3019]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3020/200000 [1:05:03<66:17:26,  1.21s/it, loss=0.0350, lr=1.51e-05, step=3019]Training:   2%|‚ñè         | 3020/200000 [1:05:03<66:17:26,  1.21s/it, loss=0.0338, lr=1.51e-05, step=3020]Training:   2%|‚ñè         | 3021/200000 [1:05:04<70:29:34,  1.29s/it, loss=0.0338, lr=1.51e-05, step=3020]Training:   2%|‚ñè         | 3021/200000 [1:05:04<70:29:34,  1.29s/it, loss=0.0232, lr=1.51e-05, step=3021]Training:   2%|‚ñè         | 3022/200000 [1:05:06<67:04:08,  1.23s/it, loss=0.0232, lr=1.51e-05, step=3021]Training:   2%|‚ñè         | 3022/200000 [1:05:06<67:04:08,  1.23s/it, loss=0.0275, lr=1.51e-05, step=3022]Training:   2%|‚ñè         | 3023/200000 [1:05:07<68:43:46,  1.26s/it, loss=0.0275, lr=1.51e-05, step=3022]Training:   2%|‚ñè         | 3023/200000 [1:05:07<68:43:46,  1.26s/it, loss=0.0322, lr=1.51e-05, step=3023]Training:   2%|‚ñè         | 3024/200000 [1:05:08<65:49:19,  1.20s/it, loss=0.0322, lr=1.51e-05, step=3023]Training:   2%|‚ñè         | 3024/200000 [1:05:08<65:49:19,  1.20s/it, loss=0.0571, lr=1.51e-05, step=3024]Training:   2%|‚ñè         | 3025/200000 [1:05:09<63:47:08,  1.17s/it, loss=0.0571, lr=1.51e-05, step=3024]Training:   2%|‚ñè         | 3025/200000 [1:05:09<63:47:08,  1.17s/it, loss=0.0375, lr=1.51e-05, step=3025]Training:   2%|‚ñè         | 3026/200000 [1:05:10<68:23:33,  1.25s/it, loss=0.0375, lr=1.51e-05, step=3025]Training:   2%|‚ñè         | 3026/200000 [1:05:10<68:23:33,  1.25s/it, loss=0.1522, lr=1.51e-05, step=3026]Training:   2%|‚ñè         | 3027/200000 [1:05:12<70:39:05,  1.29s/it, loss=0.1522, lr=1.51e-05, step=3026]Training:   2%|‚ñè         | 3027/200000 [1:05:12<70:39:05,  1.29s/it, loss=0.0256, lr=1.51e-05, step=3027]Training:   2%|‚ñè         | 3028/200000 [1:05:13<72:40:33,  1.33s/it, loss=0.0256, lr=1.51e-05, step=3027]Training:   2%|‚ñè         | 3028/200000 [1:05:13<72:40:33,  1.33s/it, loss=0.0505, lr=1.51e-05, step=3028]Training:   2%|‚ñè         | 3029/200000 [1:05:14<68:32:46,  1.25s/it, loss=0.0505, lr=1.51e-05, step=3028]Training:   2%|‚ñè         | 3029/200000 [1:05:14<68:32:46,  1.25s/it, loss=0.0285, lr=1.51e-05, step=3029]Training:   2%|‚ñè         | 3030/200000 [1:05:15<65:38:42,  1.20s/it, loss=0.0285, lr=1.51e-05, step=3029]Training:   2%|‚ñè         | 3030/200000 [1:05:15<65:38:42,  1.20s/it, loss=0.0848, lr=1.51e-05, step=3030]Training:   2%|‚ñè         | 3031/200000 [1:05:17<68:07:42,  1.25s/it, loss=0.0848, lr=1.51e-05, step=3030]Training:   2%|‚ñè         | 3031/200000 [1:05:17<68:07:42,  1.25s/it, loss=0.0414, lr=1.52e-05, step=3031]Training:   2%|‚ñè         | 3032/200000 [1:05:18<69:26:55,  1.27s/it, loss=0.0414, lr=1.52e-05, step=3031]Training:   2%|‚ñè         | 3032/200000 [1:05:18<69:26:55,  1.27s/it, loss=0.0277, lr=1.52e-05, step=3032]Training:   2%|‚ñè         | 3033/200000 [1:05:19<66:20:43,  1.21s/it, loss=0.0277, lr=1.52e-05, step=3032]Training:   2%|‚ñè         | 3033/200000 [1:05:19<66:20:43,  1.21s/it, loss=0.0355, lr=1.52e-05, step=3033]Training:   2%|‚ñè         | 3034/200000 [1:05:21<69:26:03,  1.27s/it, loss=0.0355, lr=1.52e-05, step=3033]Training:   2%|‚ñè         | 3034/200000 [1:05:21<69:26:03,  1.27s/it, loss=0.0295, lr=1.52e-05, step=3034]Training:   2%|‚ñè         | 3035/200000 [1:05:22<66:20:56,  1.21s/it, loss=0.0295, lr=1.52e-05, step=3034]Training:   2%|‚ñè         | 3035/200000 [1:05:22<66:20:56,  1.21s/it, loss=0.0217, lr=1.52e-05, step=3035]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3036/200000 [1:05:23<69:21:12,  1.27s/it, loss=0.0217, lr=1.52e-05, step=3035]Training:   2%|‚ñè         | 3036/200000 [1:05:23<69:21:12,  1.27s/it, loss=0.0318, lr=1.52e-05, step=3036]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3037/200000 [1:05:25<72:07:15,  1.32s/it, loss=0.0318, lr=1.52e-05, step=3036]Training:   2%|‚ñè         | 3037/200000 [1:05:25<72:07:15,  1.32s/it, loss=0.0402, lr=1.52e-05, step=3037]Training:   2%|‚ñè         | 3038/200000 [1:05:26<73:43:48,  1.35s/it, loss=0.0402, lr=1.52e-05, step=3037]Training:   2%|‚ñè         | 3038/200000 [1:05:26<73:43:48,  1.35s/it, loss=0.0267, lr=1.52e-05, step=3038]Training:   2%|‚ñè         | 3039/200000 [1:05:27<75:21:37,  1.38s/it, loss=0.0267, lr=1.52e-05, step=3038]Training:   2%|‚ñè         | 3039/200000 [1:05:27<75:21:37,  1.38s/it, loss=0.0387, lr=1.52e-05, step=3039]Training:   2%|‚ñè         | 3040/200000 [1:05:28<70:23:32,  1.29s/it, loss=0.0387, lr=1.52e-05, step=3039]Training:   2%|‚ñè         | 3040/200000 [1:05:28<70:23:32,  1.29s/it, loss=0.0363, lr=1.52e-05, step=3040]Training:   2%|‚ñè         | 3041/200000 [1:05:30<66:58:09,  1.22s/it, loss=0.0363, lr=1.52e-05, step=3040]Training:   2%|‚ñè         | 3041/200000 [1:05:30<66:58:09,  1.22s/it, loss=0.0376, lr=1.52e-05, step=3041]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3042/200000 [1:05:31<69:10:31,  1.26s/it, loss=0.0376, lr=1.52e-05, step=3041]Training:   2%|‚ñè         | 3042/200000 [1:05:31<69:10:31,  1.26s/it, loss=0.0229, lr=1.52e-05, step=3042]Training:   2%|‚ñè         | 3043/200000 [1:05:32<71:19:40,  1.30s/it, loss=0.0229, lr=1.52e-05, step=3042]Training:   2%|‚ñè         | 3043/200000 [1:05:32<71:19:40,  1.30s/it, loss=0.0266, lr=1.52e-05, step=3043]Training:   2%|‚ñè         | 3044/200000 [1:05:33<67:37:17,  1.24s/it, loss=0.0266, lr=1.52e-05, step=3043]Training:   2%|‚ñè         | 3044/200000 [1:05:33<67:37:17,  1.24s/it, loss=0.0134, lr=1.52e-05, step=3044]Training:   2%|‚ñè         | 3045/200000 [1:05:35<71:11:37,  1.30s/it, loss=0.0134, lr=1.52e-05, step=3044]Training:   2%|‚ñè         | 3045/200000 [1:05:35<71:11:37,  1.30s/it, loss=0.0248, lr=1.52e-05, step=3045]Training:   2%|‚ñè         | 3046/200000 [1:05:36<67:31:58,  1.23s/it, loss=0.0248, lr=1.52e-05, step=3045]Training:   2%|‚ñè         | 3046/200000 [1:05:36<67:31:58,  1.23s/it, loss=0.0337, lr=1.52e-05, step=3046]Training:   2%|‚ñè         | 3047/200000 [1:05:37<69:55:36,  1.28s/it, loss=0.0337, lr=1.52e-05, step=3046]Training:   2%|‚ñè         | 3047/200000 [1:05:37<69:55:36,  1.28s/it, loss=0.0417, lr=1.52e-05, step=3047]Training:   2%|‚ñè         | 3048/200000 [1:05:38<66:36:12,  1.22s/it, loss=0.0417, lr=1.52e-05, step=3047]Training:   2%|‚ñè         | 3048/200000 [1:05:38<66:36:12,  1.22s/it, loss=0.0486, lr=1.52e-05, step=3048]Training:   2%|‚ñè         | 3049/200000 [1:05:40<71:16:56,  1.30s/it, loss=0.0486, lr=1.52e-05, step=3048]Training:   2%|‚ñè         | 3049/200000 [1:05:40<71:16:56,  1.30s/it, loss=0.0431, lr=1.52e-05, step=3049]Training:   2%|‚ñè         | 3050/200000 [1:05:41<74:43:38,  1.37s/it, loss=0.0431, lr=1.52e-05, step=3049]Training:   2%|‚ñè         | 3050/200000 [1:05:41<74:43:38,  1.37s/it, loss=0.0762, lr=1.52e-05, step=3050]Training:   2%|‚ñè         | 3051/200000 [1:05:42<70:02:05,  1.28s/it, loss=0.0762, lr=1.52e-05, step=3050]Training:   2%|‚ñè         | 3051/200000 [1:05:42<70:02:05,  1.28s/it, loss=0.0184, lr=1.53e-05, step=3051]Training:   2%|‚ñè         | 3052/200000 [1:05:44<66:40:01,  1.22s/it, loss=0.0184, lr=1.53e-05, step=3051]Training:   2%|‚ñè         | 3052/200000 [1:05:44<66:40:01,  1.22s/it, loss=0.0321, lr=1.53e-05, step=3052]Training:   2%|‚ñè         | 3053/200000 [1:05:45<71:02:14,  1.30s/it, loss=0.0321, lr=1.53e-05, step=3052]Training:   2%|‚ñè         | 3053/200000 [1:05:45<71:02:14,  1.30s/it, loss=0.0291, lr=1.53e-05, step=3053]Training:   2%|‚ñè         | 3054/200000 [1:05:46<67:24:25,  1.23s/it, loss=0.0291, lr=1.53e-05, step=3053]Training:   2%|‚ñè         | 3054/200000 [1:05:46<67:24:25,  1.23s/it, loss=0.0285, lr=1.53e-05, step=3054]Training:   2%|‚ñè         | 3055/200000 [1:05:47<68:59:19,  1.26s/it, loss=0.0285, lr=1.53e-05, step=3054]Training:   2%|‚ñè         | 3055/200000 [1:05:47<68:59:19,  1.26s/it, loss=0.0387, lr=1.53e-05, step=3055]Training:   2%|‚ñè         | 3056/200000 [1:05:49<65:59:14,  1.21s/it, loss=0.0387, lr=1.53e-05, step=3055]Training:   2%|‚ñè         | 3056/200000 [1:05:49<65:59:14,  1.21s/it, loss=0.0220, lr=1.53e-05, step=3056]Training:   2%|‚ñè         | 3057/200000 [1:05:50<63:54:31,  1.17s/it, loss=0.0220, lr=1.53e-05, step=3056]Training:   2%|‚ñè         | 3057/200000 [1:05:50<63:54:31,  1.17s/it, loss=0.0306, lr=1.53e-05, step=3057]Training:   2%|‚ñè         | 3058/200000 [1:05:51<69:04:46,  1.26s/it, loss=0.0306, lr=1.53e-05, step=3057]Training:   2%|‚ñè         | 3058/200000 [1:05:51<69:04:46,  1.26s/it, loss=0.0239, lr=1.53e-05, step=3058]Training:   2%|‚ñè         | 3059/200000 [1:05:52<71:38:17,  1.31s/it, loss=0.0239, lr=1.53e-05, step=3058]Training:   2%|‚ñè         | 3059/200000 [1:05:52<71:38:17,  1.31s/it, loss=0.0207, lr=1.53e-05, step=3059]Training:   2%|‚ñè         | 3060/200000 [1:05:54<73:41:28,  1.35s/it, loss=0.0207, lr=1.53e-05, step=3059]Training:   2%|‚ñè         | 3060/200000 [1:05:54<73:41:28,  1.35s/it, loss=0.0361, lr=1.53e-05, step=3060]Training:   2%|‚ñè         | 3061/200000 [1:05:55<69:18:10,  1.27s/it, loss=0.0361, lr=1.53e-05, step=3060]Training:   2%|‚ñè         | 3061/200000 [1:05:55<69:18:10,  1.27s/it, loss=0.0272, lr=1.53e-05, step=3061]Training:   2%|‚ñè         | 3062/200000 [1:05:56<66:11:24,  1.21s/it, loss=0.0272, lr=1.53e-05, step=3061]Training:   2%|‚ñè         | 3062/200000 [1:05:56<66:11:24,  1.21s/it, loss=0.0323, lr=1.53e-05, step=3062]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3063/200000 [1:05:57<68:33:27,  1.25s/it, loss=0.0323, lr=1.53e-05, step=3062]Training:   2%|‚ñè         | 3063/200000 [1:05:57<68:33:27,  1.25s/it, loss=0.0276, lr=1.53e-05, step=3063]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3064/200000 [1:05:59<71:50:09,  1.31s/it, loss=0.0276, lr=1.53e-05, step=3063]Training:   2%|‚ñè         | 3064/200000 [1:05:59<71:50:09,  1.31s/it, loss=0.0268, lr=1.53e-05, step=3064]Training:   2%|‚ñè         | 3065/200000 [1:06:00<67:58:20,  1.24s/it, loss=0.0268, lr=1.53e-05, step=3064]Training:   2%|‚ñè         | 3065/200000 [1:06:00<67:58:20,  1.24s/it, loss=0.0134, lr=1.53e-05, step=3065]Training:   2%|‚ñè         | 3066/200000 [1:06:01<71:09:29,  1.30s/it, loss=0.0134, lr=1.53e-05, step=3065]Training:   2%|‚ñè         | 3066/200000 [1:06:01<71:09:29,  1.30s/it, loss=0.0366, lr=1.53e-05, step=3066]Training:   2%|‚ñè         | 3067/200000 [1:06:02<67:29:09,  1.23s/it, loss=0.0366, lr=1.53e-05, step=3066]Training:   2%|‚ñè         | 3067/200000 [1:06:02<67:29:09,  1.23s/it, loss=0.0516, lr=1.53e-05, step=3067]Training:   2%|‚ñè         | 3068/200000 [1:06:04<70:05:30,  1.28s/it, loss=0.0516, lr=1.53e-05, step=3067]Training:   2%|‚ñè         | 3068/200000 [1:06:04<70:05:30,  1.28s/it, loss=0.0384, lr=1.53e-05, step=3068]Training:   2%|‚ñè         | 3069/200000 [1:06:05<66:41:13,  1.22s/it, loss=0.0384, lr=1.53e-05, step=3068]Training:   2%|‚ñè         | 3069/200000 [1:06:05<66:41:13,  1.22s/it, loss=0.0454, lr=1.53e-05, step=3069]Training:   2%|‚ñè         | 3070/200000 [1:06:06<70:56:57,  1.30s/it, loss=0.0454, lr=1.53e-05, step=3069]Training:   2%|‚ñè         | 3070/200000 [1:06:06<70:56:57,  1.30s/it, loss=0.0573, lr=1.53e-05, step=3070]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3071/200000 [1:06:08<74:24:22,  1.36s/it, loss=0.0573, lr=1.53e-05, step=3070]Training:   2%|‚ñè         | 3071/200000 [1:06:08<74:24:22,  1.36s/it, loss=0.0358, lr=1.54e-05, step=3071]Training:   2%|‚ñè         | 3072/200000 [1:06:09<69:45:13,  1.28s/it, loss=0.0358, lr=1.54e-05, step=3071]Training:   2%|‚ñè         | 3072/200000 [1:06:09<69:45:13,  1.28s/it, loss=0.0164, lr=1.54e-05, step=3072]Training:   2%|‚ñè         | 3073/200000 [1:06:10<66:32:49,  1.22s/it, loss=0.0164, lr=1.54e-05, step=3072]Training:   2%|‚ñè         | 3073/200000 [1:06:10<66:32:49,  1.22s/it, loss=0.0349, lr=1.54e-05, step=3073]Training:   2%|‚ñè         | 3074/200000 [1:06:12<70:39:13,  1.29s/it, loss=0.0349, lr=1.54e-05, step=3073]Training:   2%|‚ñè         | 3074/200000 [1:06:12<70:39:13,  1.29s/it, loss=0.0405, lr=1.54e-05, step=3074]Training:   2%|‚ñè         | 3075/200000 [1:06:13<67:05:07,  1.23s/it, loss=0.0405, lr=1.54e-05, step=3074]Training:   2%|‚ñè         | 3075/200000 [1:06:13<67:05:07,  1.23s/it, loss=0.0393, lr=1.54e-05, step=3075]Training:   2%|‚ñè         | 3076/200000 [1:06:14<68:31:40,  1.25s/it, loss=0.0393, lr=1.54e-05, step=3075]Training:   2%|‚ñè         | 3076/200000 [1:06:14<68:31:40,  1.25s/it, loss=0.1045, lr=1.54e-05, step=3076]Training:   2%|‚ñè         | 3077/200000 [1:06:15<65:38:57,  1.20s/it, loss=0.1045, lr=1.54e-05, step=3076]Training:   2%|‚ñè         | 3077/200000 [1:06:15<65:38:57,  1.20s/it, loss=0.0679, lr=1.54e-05, step=3077]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3078/200000 [1:06:16<63:39:20,  1.16s/it, loss=0.0679, lr=1.54e-05, step=3077]Training:   2%|‚ñè         | 3078/200000 [1:06:16<63:39:20,  1.16s/it, loss=0.0359, lr=1.54e-05, step=3078]Training:   2%|‚ñè         | 3079/200000 [1:06:18<67:56:52,  1.24s/it, loss=0.0359, lr=1.54e-05, step=3078]Training:   2%|‚ñè         | 3079/200000 [1:06:18<67:56:52,  1.24s/it, loss=0.0289, lr=1.54e-05, step=3079]Training:   2%|‚ñè         | 3080/200000 [1:06:19<70:21:56,  1.29s/it, loss=0.0289, lr=1.54e-05, step=3079]Training:   2%|‚ñè         | 3080/200000 [1:06:19<70:21:56,  1.29s/it, loss=0.0277, lr=1.54e-05, step=3080]Training:   2%|‚ñè         | 3081/200000 [1:06:20<72:39:32,  1.33s/it, loss=0.0277, lr=1.54e-05, step=3080]Training:   2%|‚ñè         | 3081/200000 [1:06:20<72:39:32,  1.33s/it, loss=0.0247, lr=1.54e-05, step=3081]Training:   2%|‚ñè         | 3082/200000 [1:06:21<68:31:37,  1.25s/it, loss=0.0247, lr=1.54e-05, step=3081]Training:   2%|‚ñè         | 3082/200000 [1:06:21<68:31:37,  1.25s/it, loss=0.0173, lr=1.54e-05, step=3082]Training:   2%|‚ñè         | 3083/200000 [1:06:22<65:41:49,  1.20s/it, loss=0.0173, lr=1.54e-05, step=3082]Training:   2%|‚ñè         | 3083/200000 [1:06:22<65:41:49,  1.20s/it, loss=0.1285, lr=1.54e-05, step=3083]Training:   2%|‚ñè         | 3084/200000 [1:06:24<68:31:34,  1.25s/it, loss=0.1285, lr=1.54e-05, step=3083]Training:   2%|‚ñè         | 3084/200000 [1:06:24<68:31:34,  1.25s/it, loss=0.0447, lr=1.54e-05, step=3084]Training:   2%|‚ñè         | 3085/200000 [1:06:25<69:37:23,  1.27s/it, loss=0.0447, lr=1.54e-05, step=3084]Training:   2%|‚ñè         | 3085/200000 [1:06:25<69:37:23,  1.27s/it, loss=0.0418, lr=1.54e-05, step=3085]Training:   2%|‚ñè         | 3086/200000 [1:06:26<66:25:15,  1.21s/it, loss=0.0418, lr=1.54e-05, step=3085]Training:   2%|‚ñè         | 3086/200000 [1:06:26<66:25:15,  1.21s/it, loss=0.0433, lr=1.54e-05, step=3086]Training:   2%|‚ñè         | 3087/200000 [1:06:28<68:31:32,  1.25s/it, loss=0.0433, lr=1.54e-05, step=3086]Training:   2%|‚ñè         | 3087/200000 [1:06:28<68:31:32,  1.25s/it, loss=0.0277, lr=1.54e-05, step=3087]Training:   2%|‚ñè         | 3088/200000 [1:06:29<65:38:45,  1.20s/it, loss=0.0277, lr=1.54e-05, step=3087]Training:   2%|‚ñè         | 3088/200000 [1:06:29<65:38:45,  1.20s/it, loss=0.0312, lr=1.54e-05, step=3088]Training:   2%|‚ñè         | 3089/200000 [1:06:30<68:45:54,  1.26s/it, loss=0.0312, lr=1.54e-05, step=3088]Training:   2%|‚ñè         | 3089/200000 [1:06:30<68:45:54,  1.26s/it, loss=0.0368, lr=1.54e-05, step=3089]Training:   2%|‚ñè         | 3090/200000 [1:06:32<72:04:55,  1.32s/it, loss=0.0368, lr=1.54e-05, step=3089]Training:   2%|‚ñè         | 3090/200000 [1:06:32<72:04:55,  1.32s/it, loss=0.0322, lr=1.54e-05, step=3090]Training:   2%|‚ñè         | 3091/200000 [1:06:33<73:41:57,  1.35s/it, loss=0.0322, lr=1.54e-05, step=3090]Training:   2%|‚ñè         | 3091/200000 [1:06:33<73:41:57,  1.35s/it, loss=0.0385, lr=1.55e-05, step=3091]Training:   2%|‚ñè         | 3092/200000 [1:06:34<75:12:19,  1.37s/it, loss=0.0385, lr=1.55e-05, step=3091]Training:   2%|‚ñè         | 3092/200000 [1:06:34<75:12:19,  1.37s/it, loss=0.0294, lr=1.55e-05, step=3092]Training:   2%|‚ñè         | 3093/200000 [1:06:35<70:21:12,  1.29s/it, loss=0.0294, lr=1.55e-05, step=3092]Training:   2%|‚ñè         | 3093/200000 [1:06:35<70:21:12,  1.29s/it, loss=0.0262, lr=1.55e-05, step=3093]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3094/200000 [1:06:37<66:55:23,  1.22s/it, loss=0.0262, lr=1.55e-05, step=3093]Training:   2%|‚ñè         | 3094/200000 [1:06:37<66:55:23,  1.22s/it, loss=0.0245, lr=1.55e-05, step=3094]Training:   2%|‚ñè         | 3095/200000 [1:06:38<69:15:02,  1.27s/it, loss=0.0245, lr=1.55e-05, step=3094]Training:   2%|‚ñè         | 3095/200000 [1:06:38<69:15:02,  1.27s/it, loss=0.0418, lr=1.55e-05, step=3095]Training:   2%|‚ñè         | 3096/200000 [1:06:39<71:59:36,  1.32s/it, loss=0.0418, lr=1.55e-05, step=3095]Training:   2%|‚ñè         | 3096/200000 [1:06:39<71:59:36,  1.32s/it, loss=0.0356, lr=1.55e-05, step=3096]Training:   2%|‚ñè         | 3097/200000 [1:06:40<68:02:48,  1.24s/it, loss=0.0356, lr=1.55e-05, step=3096]Training:   2%|‚ñè         | 3097/200000 [1:06:40<68:02:48,  1.24s/it, loss=0.0339, lr=1.55e-05, step=3097]Training:   2%|‚ñè         | 3098/200000 [1:06:42<71:46:43,  1.31s/it, loss=0.0339, lr=1.55e-05, step=3097]Training:   2%|‚ñè         | 3098/200000 [1:06:42<71:46:43,  1.31s/it, loss=0.0435, lr=1.55e-05, step=3098]Training:   2%|‚ñè         | 3099/200000 [1:06:43<67:56:50,  1.24s/it, loss=0.0435, lr=1.55e-05, step=3098]Training:   2%|‚ñè         | 3099/200000 [1:06:43<67:56:50,  1.24s/it, loss=0.0558, lr=1.55e-05, step=3099]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3100/200000 [1:06:44<70:26:48,  1.29s/it, loss=0.0558, lr=1.55e-05, step=3099]Training:   2%|‚ñè         | 3100/200000 [1:06:44<70:26:48,  1.29s/it, loss=0.0318, lr=1.55e-05, step=3100]23:59:59.248 [I] step=3100 loss=0.0374 lr=1.53e-05 grad_norm=0.78 time=126.3s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3101/200000 [1:06:45<66:59:03,  1.22s/it, loss=0.0318, lr=1.55e-05, step=3100]Training:   2%|‚ñè         | 3101/200000 [1:06:45<66:59:03,  1.22s/it, loss=0.0413, lr=1.55e-05, step=3101]Training:   2%|‚ñè         | 3102/200000 [1:06:47<71:26:22,  1.31s/it, loss=0.0413, lr=1.55e-05, step=3101]Training:   2%|‚ñè         | 3102/200000 [1:06:47<71:26:22,  1.31s/it, loss=0.0275, lr=1.55e-05, step=3102]Training:   2%|‚ñè         | 3103/200000 [1:06:48<74:27:54,  1.36s/it, loss=0.0275, lr=1.55e-05, step=3102]Training:   2%|‚ñè         | 3103/200000 [1:06:48<74:27:54,  1.36s/it, loss=0.0206, lr=1.55e-05, step=3103]Training:   2%|‚ñè         | 3104/200000 [1:06:50<69:48:07,  1.28s/it, loss=0.0206, lr=1.55e-05, step=3103]Training:   2%|‚ñè         | 3104/200000 [1:06:50<69:48:07,  1.28s/it, loss=0.0420, lr=1.55e-05, step=3104]Training:   2%|‚ñè         | 3105/200000 [1:06:51<66:39:45,  1.22s/it, loss=0.0420, lr=1.55e-05, step=3104]Training:   2%|‚ñè         | 3105/200000 [1:06:51<66:39:45,  1.22s/it, loss=0.0309, lr=1.55e-05, step=3105]Training:   2%|‚ñè         | 3106/200000 [1:06:52<71:03:57,  1.30s/it, loss=0.0309, lr=1.55e-05, step=3105]Training:   2%|‚ñè         | 3106/200000 [1:06:52<71:03:57,  1.30s/it, loss=0.0239, lr=1.55e-05, step=3106]Training:   2%|‚ñè         | 3107/200000 [1:06:53<67:26:46,  1.23s/it, loss=0.0239, lr=1.55e-05, step=3106]Training:   2%|‚ñè         | 3107/200000 [1:06:53<67:26:46,  1.23s/it, loss=0.0306, lr=1.55e-05, step=3107]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3108/200000 [1:06:54<69:15:09,  1.27s/it, loss=0.0306, lr=1.55e-05, step=3107]Training:   2%|‚ñè         | 3108/200000 [1:06:54<69:15:09,  1.27s/it, loss=0.0486, lr=1.55e-05, step=3108]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3109/200000 [1:06:56<66:09:20,  1.21s/it, loss=0.0486, lr=1.55e-05, step=3108]Training:   2%|‚ñè         | 3109/200000 [1:06:56<66:09:20,  1.21s/it, loss=0.0327, lr=1.55e-05, step=3109]Training:   2%|‚ñè         | 3110/200000 [1:06:57<63:58:57,  1.17s/it, loss=0.0327, lr=1.55e-05, step=3109]Training:   2%|‚ñè         | 3110/200000 [1:06:57<63:58:57,  1.17s/it, loss=0.0261, lr=1.55e-05, step=3110]Training:   2%|‚ñè         | 3111/200000 [1:06:58<68:27:37,  1.25s/it, loss=0.0261, lr=1.55e-05, step=3110]Training:   2%|‚ñè         | 3111/200000 [1:06:58<68:27:37,  1.25s/it, loss=0.0300, lr=1.56e-05, step=3111]Training:   2%|‚ñè         | 3112/200000 [1:07:00<71:08:31,  1.30s/it, loss=0.0300, lr=1.56e-05, step=3111]Training:   2%|‚ñè         | 3112/200000 [1:07:00<71:08:31,  1.30s/it, loss=0.0269, lr=1.56e-05, step=3112]Training:   2%|‚ñè         | 3113/200000 [1:07:01<72:38:49,  1.33s/it, loss=0.0269, lr=1.56e-05, step=3112]Training:   2%|‚ñè         | 3113/200000 [1:07:01<72:38:49,  1.33s/it, loss=0.0362, lr=1.56e-05, step=3113]Training:   2%|‚ñè         | 3114/200000 [1:07:02<69:05:04,  1.26s/it, loss=0.0362, lr=1.56e-05, step=3113]Training:   2%|‚ñè         | 3114/200000 [1:07:02<69:05:04,  1.26s/it, loss=0.0198, lr=1.56e-05, step=3114]Training:   2%|‚ñè         | 3115/200000 [1:07:03<66:04:27,  1.21s/it, loss=0.0198, lr=1.56e-05, step=3114]Training:   2%|‚ñè         | 3115/200000 [1:07:03<66:04:27,  1.21s/it, loss=0.0333, lr=1.56e-05, step=3115]Training:   2%|‚ñè         | 3116/200000 [1:07:04<68:36:13,  1.25s/it, loss=0.0333, lr=1.56e-05, step=3115]Training:   2%|‚ñè         | 3116/200000 [1:07:04<68:36:13,  1.25s/it, loss=0.0506, lr=1.56e-05, step=3116]Training:   2%|‚ñè         | 3117/200000 [1:07:06<71:05:34,  1.30s/it, loss=0.0506, lr=1.56e-05, step=3116]Training:   2%|‚ñè         | 3117/200000 [1:07:06<71:05:34,  1.30s/it, loss=0.0316, lr=1.56e-05, step=3117]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3118/200000 [1:07:07<67:24:57,  1.23s/it, loss=0.0316, lr=1.56e-05, step=3117]Training:   2%|‚ñè         | 3118/200000 [1:07:07<67:24:57,  1.23s/it, loss=0.0369, lr=1.56e-05, step=3118]Training:   2%|‚ñè         | 3119/200000 [1:07:08<70:01:34,  1.28s/it, loss=0.0369, lr=1.56e-05, step=3118]Training:   2%|‚ñè         | 3119/200000 [1:07:08<70:01:34,  1.28s/it, loss=0.0238, lr=1.56e-05, step=3119]Training:   2%|‚ñè         | 3120/200000 [1:07:09<66:41:14,  1.22s/it, loss=0.0238, lr=1.56e-05, step=3119]Training:   2%|‚ñè         | 3120/200000 [1:07:09<66:41:14,  1.22s/it, loss=0.0234, lr=1.56e-05, step=3120]Training:   2%|‚ñè         | 3121/200000 [1:07:11<69:44:16,  1.28s/it, loss=0.0234, lr=1.56e-05, step=3120]Training:   2%|‚ñè         | 3121/200000 [1:07:11<69:44:16,  1.28s/it, loss=0.0391, lr=1.56e-05, step=3121]Training:   2%|‚ñè         | 3122/200000 [1:07:12<66:28:36,  1.22s/it, loss=0.0391, lr=1.56e-05, step=3121]Training:   2%|‚ñè         | 3122/200000 [1:07:12<66:28:36,  1.22s/it, loss=0.0319, lr=1.56e-05, step=3122]Training:   2%|‚ñè         | 3123/200000 [1:07:13<70:49:49,  1.30s/it, loss=0.0319, lr=1.56e-05, step=3122]Training:   2%|‚ñè         | 3123/200000 [1:07:13<70:49:49,  1.30s/it, loss=0.0368, lr=1.56e-05, step=3123]Training:   2%|‚ñè         | 3124/200000 [1:07:15<74:10:54,  1.36s/it, loss=0.0368, lr=1.56e-05, step=3123]Training:   2%|‚ñè         | 3124/200000 [1:07:15<74:10:54,  1.36s/it, loss=0.0190, lr=1.56e-05, step=3124]Training:   2%|‚ñè         | 3125/200000 [1:07:16<69:35:42,  1.27s/it, loss=0.0190, lr=1.56e-05, step=3124]Training:   2%|‚ñè         | 3125/200000 [1:07:16<69:35:42,  1.27s/it, loss=0.0254, lr=1.56e-05, step=3125]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3126/200000 [1:07:17<66:23:17,  1.21s/it, loss=0.0254, lr=1.56e-05, step=3125]Training:   2%|‚ñè         | 3126/200000 [1:07:17<66:23:17,  1.21s/it, loss=0.0308, lr=1.56e-05, step=3126]Training:   2%|‚ñè         | 3127/200000 [1:07:18<70:31:43,  1.29s/it, loss=0.0308, lr=1.56e-05, step=3126]Training:   2%|‚ñè         | 3127/200000 [1:07:18<70:31:43,  1.29s/it, loss=0.0208, lr=1.56e-05, step=3127]Training:   2%|‚ñè         | 3128/200000 [1:07:20<67:00:42,  1.23s/it, loss=0.0208, lr=1.56e-05, step=3127]Training:   2%|‚ñè         | 3128/200000 [1:07:20<67:00:42,  1.23s/it, loss=0.0333, lr=1.56e-05, step=3128]Training:   2%|‚ñè         | 3129/200000 [1:07:21<68:47:24,  1.26s/it, loss=0.0333, lr=1.56e-05, step=3128]Training:   2%|‚ñè         | 3129/200000 [1:07:21<68:47:24,  1.26s/it, loss=0.0307, lr=1.56e-05, step=3129]Training:   2%|‚ñè         | 3130/200000 [1:07:22<65:48:05,  1.20s/it, loss=0.0307, lr=1.56e-05, step=3129]Training:   2%|‚ñè         | 3130/200000 [1:07:22<65:48:05,  1.20s/it, loss=0.0263, lr=1.56e-05, step=3130]Training:   2%|‚ñè         | 3131/200000 [1:07:23<63:45:03,  1.17s/it, loss=0.0263, lr=1.56e-05, step=3130]Training:   2%|‚ñè         | 3131/200000 [1:07:23<63:45:03,  1.17s/it, loss=0.0306, lr=1.57e-05, step=3131]Training:   2%|‚ñè         | 3132/200000 [1:07:25<68:24:38,  1.25s/it, loss=0.0306, lr=1.57e-05, step=3131]Training:   2%|‚ñè         | 3132/200000 [1:07:25<68:24:38,  1.25s/it, loss=0.0254, lr=1.57e-05, step=3132]Training:   2%|‚ñè         | 3133/200000 [1:07:26<70:52:58,  1.30s/it, loss=0.0254, lr=1.57e-05, step=3132]Training:   2%|‚ñè         | 3133/200000 [1:07:26<70:52:58,  1.30s/it, loss=0.0501, lr=1.57e-05, step=3133]Training:   2%|‚ñè         | 3134/200000 [1:07:27<72:50:38,  1.33s/it, loss=0.0501, lr=1.57e-05, step=3133]Training:   2%|‚ñè         | 3134/200000 [1:07:27<72:50:38,  1.33s/it, loss=0.0274, lr=1.57e-05, step=3134]Training:   2%|‚ñè         | 3135/200000 [1:07:28<68:40:15,  1.26s/it, loss=0.0274, lr=1.57e-05, step=3134]Training:   2%|‚ñè         | 3135/200000 [1:07:28<68:40:15,  1.26s/it, loss=0.0358, lr=1.57e-05, step=3135]Training:   2%|‚ñè         | 3136/200000 [1:07:29<65:43:53,  1.20s/it, loss=0.0358, lr=1.57e-05, step=3135]Training:   2%|‚ñè         | 3136/200000 [1:07:29<65:43:53,  1.20s/it, loss=0.0370, lr=1.57e-05, step=3136]Training:   2%|‚ñè         | 3137/200000 [1:07:31<68:09:44,  1.25s/it, loss=0.0370, lr=1.57e-05, step=3136]Training:   2%|‚ñè         | 3137/200000 [1:07:31<68:09:44,  1.25s/it, loss=0.0186, lr=1.57e-05, step=3137]Training:   2%|‚ñè         | 3138/200000 [1:07:32<70:34:26,  1.29s/it, loss=0.0186, lr=1.57e-05, step=3137]Training:   2%|‚ñè         | 3138/200000 [1:07:32<70:34:26,  1.29s/it, loss=0.0547, lr=1.57e-05, step=3138]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3139/200000 [1:07:33<67:05:44,  1.23s/it, loss=0.0547, lr=1.57e-05, step=3138]Training:   2%|‚ñè         | 3139/200000 [1:07:33<67:05:44,  1.23s/it, loss=0.0259, lr=1.57e-05, step=3139]Training:   2%|‚ñè         | 3140/200000 [1:07:35<69:35:40,  1.27s/it, loss=0.0259, lr=1.57e-05, step=3139]Training:   2%|‚ñè         | 3140/200000 [1:07:35<69:35:40,  1.27s/it, loss=0.0190, lr=1.57e-05, step=3140]Training:   2%|‚ñè         | 3141/200000 [1:07:36<66:23:02,  1.21s/it, loss=0.0190, lr=1.57e-05, step=3140]Training:   2%|‚ñè         | 3141/200000 [1:07:36<66:23:02,  1.21s/it, loss=0.0223, lr=1.57e-05, step=3141]Training:   2%|‚ñè         | 3142/200000 [1:07:37<68:35:47,  1.25s/it, loss=0.0223, lr=1.57e-05, step=3141]Training:   2%|‚ñè         | 3142/200000 [1:07:37<68:35:47,  1.25s/it, loss=0.0536, lr=1.57e-05, step=3142]Training:   2%|‚ñè         | 3143/200000 [1:07:39<71:50:17,  1.31s/it, loss=0.0536, lr=1.57e-05, step=3142]Training:   2%|‚ñè         | 3143/200000 [1:07:39<71:50:17,  1.31s/it, loss=0.0315, lr=1.57e-05, step=3143]Training:   2%|‚ñè         | 3144/200000 [1:07:40<73:28:20,  1.34s/it, loss=0.0315, lr=1.57e-05, step=3143]Training:   2%|‚ñè         | 3144/200000 [1:07:40<73:28:20,  1.34s/it, loss=0.0345, lr=1.57e-05, step=3144]Training:   2%|‚ñè         | 3145/200000 [1:07:41<75:12:34,  1.38s/it, loss=0.0345, lr=1.57e-05, step=3144]Training:   2%|‚ñè         | 3145/200000 [1:07:41<75:12:34,  1.38s/it, loss=0.0217, lr=1.57e-05, step=3145]Training:   2%|‚ñè         | 3146/200000 [1:07:42<70:20:06,  1.29s/it, loss=0.0217, lr=1.57e-05, step=3145]Training:   2%|‚ñè         | 3146/200000 [1:07:42<70:20:06,  1.29s/it, loss=0.0267, lr=1.57e-05, step=3146]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3147/200000 [1:07:44<66:54:45,  1.22s/it, loss=0.0267, lr=1.57e-05, step=3146]Training:   2%|‚ñè         | 3147/200000 [1:07:44<66:54:45,  1.22s/it, loss=0.0361, lr=1.57e-05, step=3147]Training:   2%|‚ñè         | 3148/200000 [1:07:45<69:48:12,  1.28s/it, loss=0.0361, lr=1.57e-05, step=3147]Training:   2%|‚ñè         | 3148/200000 [1:07:45<69:48:12,  1.28s/it, loss=0.0464, lr=1.57e-05, step=3148]Training:   2%|‚ñè         | 3149/200000 [1:07:46<72:18:30,  1.32s/it, loss=0.0464, lr=1.57e-05, step=3148]Training:   2%|‚ñè         | 3149/200000 [1:07:46<72:18:30,  1.32s/it, loss=0.0235, lr=1.57e-05, step=3149]Training:   2%|‚ñè         | 3150/200000 [1:07:47<68:15:19,  1.25s/it, loss=0.0235, lr=1.57e-05, step=3149]Training:   2%|‚ñè         | 3150/200000 [1:07:47<68:15:19,  1.25s/it, loss=0.0622, lr=1.57e-05, step=3150]Training:   2%|‚ñè         | 3151/200000 [1:07:49<71:33:18,  1.31s/it, loss=0.0622, lr=1.57e-05, step=3150]Training:   2%|‚ñè         | 3151/200000 [1:07:49<71:33:18,  1.31s/it, loss=0.0323, lr=1.58e-05, step=3151]Training:   2%|‚ñè         | 3152/200000 [1:07:50<67:44:57,  1.24s/it, loss=0.0323, lr=1.58e-05, step=3151]Training:   2%|‚ñè         | 3152/200000 [1:07:50<67:44:57,  1.24s/it, loss=0.0329, lr=1.58e-05, step=3152]Training:   2%|‚ñè         | 3153/200000 [1:07:51<70:01:29,  1.28s/it, loss=0.0329, lr=1.58e-05, step=3152]Training:   2%|‚ñè         | 3153/200000 [1:07:51<70:01:29,  1.28s/it, loss=0.0206, lr=1.58e-05, step=3153]Training:   2%|‚ñè         | 3154/200000 [1:07:52<66:41:40,  1.22s/it, loss=0.0206, lr=1.58e-05, step=3153]Training:   2%|‚ñè         | 3154/200000 [1:07:52<66:41:40,  1.22s/it, loss=0.0246, lr=1.58e-05, step=3154]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3155/200000 [1:07:54<71:20:29,  1.30s/it, loss=0.0246, lr=1.58e-05, step=3154]Training:   2%|‚ñè         | 3155/200000 [1:07:54<71:20:29,  1.30s/it, loss=0.0368, lr=1.58e-05, step=3155]Training:   2%|‚ñè         | 3156/200000 [1:07:55<74:55:33,  1.37s/it, loss=0.0368, lr=1.58e-05, step=3155]Training:   2%|‚ñè         | 3156/200000 [1:07:55<74:55:33,  1.37s/it, loss=0.0288, lr=1.58e-05, step=3156]Training:   2%|‚ñè         | 3157/200000 [1:07:57<70:04:24,  1.28s/it, loss=0.0288, lr=1.58e-05, step=3156]Training:   2%|‚ñè         | 3157/200000 [1:07:57<70:04:24,  1.28s/it, loss=0.1401, lr=1.58e-05, step=3157]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3158/200000 [1:07:58<66:41:48,  1.22s/it, loss=0.1401, lr=1.58e-05, step=3157]Training:   2%|‚ñè         | 3158/200000 [1:07:58<66:41:48,  1.22s/it, loss=0.0178, lr=1.58e-05, step=3158]Training:   2%|‚ñè         | 3159/200000 [1:07:59<70:19:51,  1.29s/it, loss=0.0178, lr=1.58e-05, step=3158]Training:   2%|‚ñè         | 3159/200000 [1:07:59<70:19:51,  1.29s/it, loss=0.0310, lr=1.58e-05, step=3159]Training:   2%|‚ñè         | 3160/200000 [1:08:00<66:53:37,  1.22s/it, loss=0.0310, lr=1.58e-05, step=3159]Training:   2%|‚ñè         | 3160/200000 [1:08:00<66:53:37,  1.22s/it, loss=0.0351, lr=1.58e-05, step=3160]Training:   2%|‚ñè         | 3161/200000 [1:08:01<68:26:37,  1.25s/it, loss=0.0351, lr=1.58e-05, step=3160]Training:   2%|‚ñè         | 3161/200000 [1:08:01<68:26:37,  1.25s/it, loss=0.0203, lr=1.58e-05, step=3161]Training:   2%|‚ñè         | 3162/200000 [1:08:03<65:34:40,  1.20s/it, loss=0.0203, lr=1.58e-05, step=3161]Training:   2%|‚ñè         | 3162/200000 [1:08:03<65:34:40,  1.20s/it, loss=0.0456, lr=1.58e-05, step=3162]Training:   2%|‚ñè         | 3163/200000 [1:08:04<63:33:26,  1.16s/it, loss=0.0456, lr=1.58e-05, step=3162]Training:   2%|‚ñè         | 3163/200000 [1:08:04<63:33:26,  1.16s/it, loss=0.0577, lr=1.58e-05, step=3163]Training:   2%|‚ñè         | 3164/200000 [1:08:05<68:26:59,  1.25s/it, loss=0.0577, lr=1.58e-05, step=3163]Training:   2%|‚ñè         | 3164/200000 [1:08:05<68:26:59,  1.25s/it, loss=0.0261, lr=1.58e-05, step=3164]Training:   2%|‚ñè         | 3165/200000 [1:08:06<71:01:43,  1.30s/it, loss=0.0261, lr=1.58e-05, step=3164]Training:   2%|‚ñè         | 3165/200000 [1:08:06<71:01:43,  1.30s/it, loss=0.0418, lr=1.58e-05, step=3165]Training:   2%|‚ñè         | 3166/200000 [1:08:08<73:16:19,  1.34s/it, loss=0.0418, lr=1.58e-05, step=3165]Training:   2%|‚ñè         | 3166/200000 [1:08:08<73:16:19,  1.34s/it, loss=0.0286, lr=1.58e-05, step=3166]Training:   2%|‚ñè         | 3167/200000 [1:08:09<68:57:31,  1.26s/it, loss=0.0286, lr=1.58e-05, step=3166]Training:   2%|‚ñè         | 3167/200000 [1:08:09<68:57:31,  1.26s/it, loss=0.0296, lr=1.58e-05, step=3167]Training:   2%|‚ñè         | 3168/200000 [1:08:10<65:57:38,  1.21s/it, loss=0.0296, lr=1.58e-05, step=3167]Training:   2%|‚ñè         | 3168/200000 [1:08:10<65:57:38,  1.21s/it, loss=0.0454, lr=1.58e-05, step=3168]Training:   2%|‚ñè         | 3169/200000 [1:08:11<68:02:51,  1.24s/it, loss=0.0454, lr=1.58e-05, step=3168]Training:   2%|‚ñè         | 3169/200000 [1:08:11<68:02:51,  1.24s/it, loss=0.0336, lr=1.58e-05, step=3169]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3170/200000 [1:08:13<70:44:42,  1.29s/it, loss=0.0336, lr=1.58e-05, step=3169]Training:   2%|‚ñè         | 3170/200000 [1:08:13<70:44:42,  1.29s/it, loss=0.0239, lr=1.58e-05, step=3170]Training:   2%|‚ñè         | 3171/200000 [1:08:14<67:10:01,  1.23s/it, loss=0.0239, lr=1.58e-05, step=3170]Training:   2%|‚ñè         | 3171/200000 [1:08:14<67:10:01,  1.23s/it, loss=0.0338, lr=1.59e-05, step=3171]Training:   2%|‚ñè         | 3172/200000 [1:08:15<70:04:24,  1.28s/it, loss=0.0338, lr=1.59e-05, step=3171]Training:   2%|‚ñè         | 3172/200000 [1:08:15<70:04:24,  1.28s/it, loss=0.0548, lr=1.59e-05, step=3172]Training:   2%|‚ñè         | 3173/200000 [1:08:16<66:43:03,  1.22s/it, loss=0.0548, lr=1.59e-05, step=3172]Training:   2%|‚ñè         | 3173/200000 [1:08:16<66:43:03,  1.22s/it, loss=0.0259, lr=1.59e-05, step=3173]Training:   2%|‚ñè         | 3174/200000 [1:08:18<69:41:46,  1.27s/it, loss=0.0259, lr=1.59e-05, step=3173]Training:   2%|‚ñè         | 3174/200000 [1:08:18<69:41:46,  1.27s/it, loss=0.0480, lr=1.59e-05, step=3174]Training:   2%|‚ñè         | 3175/200000 [1:08:19<66:26:35,  1.22s/it, loss=0.0480, lr=1.59e-05, step=3174]Training:   2%|‚ñè         | 3175/200000 [1:08:19<66:26:35,  1.22s/it, loss=0.0237, lr=1.59e-05, step=3175]Training:   2%|‚ñè         | 3176/200000 [1:08:20<71:06:41,  1.30s/it, loss=0.0237, lr=1.59e-05, step=3175]Training:   2%|‚ñè         | 3176/200000 [1:08:20<71:06:41,  1.30s/it, loss=0.0234, lr=1.59e-05, step=3176]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3177/200000 [1:08:22<74:30:21,  1.36s/it, loss=0.0234, lr=1.59e-05, step=3176]Training:   2%|‚ñè         | 3177/200000 [1:08:22<74:30:21,  1.36s/it, loss=0.0197, lr=1.59e-05, step=3177]Training:   2%|‚ñè         | 3178/200000 [1:08:23<69:49:37,  1.28s/it, loss=0.0197, lr=1.59e-05, step=3177]Training:   2%|‚ñè         | 3178/200000 [1:08:23<69:49:37,  1.28s/it, loss=0.0226, lr=1.59e-05, step=3178]Training:   2%|‚ñè         | 3179/200000 [1:08:24<66:33:34,  1.22s/it, loss=0.0226, lr=1.59e-05, step=3178]Training:   2%|‚ñè         | 3179/200000 [1:08:24<66:33:34,  1.22s/it, loss=0.0359, lr=1.59e-05, step=3179]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3180/200000 [1:08:25<70:20:41,  1.29s/it, loss=0.0359, lr=1.59e-05, step=3179]Training:   2%|‚ñè         | 3180/200000 [1:08:25<70:20:41,  1.29s/it, loss=0.0204, lr=1.59e-05, step=3180]Training:   2%|‚ñè         | 3181/200000 [1:08:27<66:55:57,  1.22s/it, loss=0.0204, lr=1.59e-05, step=3180]Training:   2%|‚ñè         | 3181/200000 [1:08:27<66:55:57,  1.22s/it, loss=0.0309, lr=1.59e-05, step=3181]Training:   2%|‚ñè         | 3182/200000 [1:08:28<68:24:38,  1.25s/it, loss=0.0309, lr=1.59e-05, step=3181]Training:   2%|‚ñè         | 3182/200000 [1:08:28<68:24:38,  1.25s/it, loss=0.0258, lr=1.59e-05, step=3182]Training:   2%|‚ñè         | 3183/200000 [1:08:29<65:32:08,  1.20s/it, loss=0.0258, lr=1.59e-05, step=3182]Training:   2%|‚ñè         | 3183/200000 [1:08:29<65:32:08,  1.20s/it, loss=0.0232, lr=1.59e-05, step=3183]Training:   2%|‚ñè         | 3184/200000 [1:08:30<63:32:55,  1.16s/it, loss=0.0232, lr=1.59e-05, step=3183]Training:   2%|‚ñè         | 3184/200000 [1:08:30<63:32:55,  1.16s/it, loss=0.0622, lr=1.59e-05, step=3184]Training:   2%|‚ñè         | 3185/200000 [1:08:31<67:48:06,  1.24s/it, loss=0.0622, lr=1.59e-05, step=3184]Training:   2%|‚ñè         | 3185/200000 [1:08:31<67:48:06,  1.24s/it, loss=0.0713, lr=1.59e-05, step=3185]Training:   2%|‚ñè         | 3186/200000 [1:08:33<70:00:46,  1.28s/it, loss=0.0713, lr=1.59e-05, step=3185]Training:   2%|‚ñè         | 3186/200000 [1:08:33<70:00:46,  1.28s/it, loss=0.1057, lr=1.59e-05, step=3186]Training:   2%|‚ñè         | 3187/200000 [1:08:34<72:29:00,  1.33s/it, loss=0.1057, lr=1.59e-05, step=3186]Training:   2%|‚ñè         | 3187/200000 [1:08:34<72:29:00,  1.33s/it, loss=0.0284, lr=1.59e-05, step=3187]Training:   2%|‚ñè         | 3188/200000 [1:08:35<68:23:56,  1.25s/it, loss=0.0284, lr=1.59e-05, step=3187]Training:   2%|‚ñè         | 3188/200000 [1:08:35<68:23:56,  1.25s/it, loss=0.0203, lr=1.59e-05, step=3188]Training:   2%|‚ñè         | 3189/200000 [1:08:36<65:31:38,  1.20s/it, loss=0.0203, lr=1.59e-05, step=3188]Training:   2%|‚ñè         | 3189/200000 [1:08:36<65:31:38,  1.20s/it, loss=0.0416, lr=1.59e-05, step=3189]Training:   2%|‚ñè         | 3190/200000 [1:08:38<67:51:47,  1.24s/it, loss=0.0416, lr=1.59e-05, step=3189]Training:   2%|‚ñè         | 3190/200000 [1:08:38<67:51:47,  1.24s/it, loss=0.0378, lr=1.59e-05, step=3190]Training:   2%|‚ñè         | 3191/200000 [1:08:39<69:02:48,  1.26s/it, loss=0.0378, lr=1.59e-05, step=3190]Training:   2%|‚ñè         | 3191/200000 [1:08:39<69:02:48,  1.26s/it, loss=0.0318, lr=1.60e-05, step=3191]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3192/200000 [1:08:40<66:00:06,  1.21s/it, loss=0.0318, lr=1.60e-05, step=3191]Training:   2%|‚ñè         | 3192/200000 [1:08:40<66:00:06,  1.21s/it, loss=0.0214, lr=1.60e-05, step=3192]Training:   2%|‚ñè         | 3193/200000 [1:08:41<67:44:21,  1.24s/it, loss=0.0214, lr=1.60e-05, step=3192]Training:   2%|‚ñè         | 3193/200000 [1:08:41<67:44:21,  1.24s/it, loss=0.1062, lr=1.60e-05, step=3193]Training:   2%|‚ñè         | 3194/200000 [1:08:43<65:04:50,  1.19s/it, loss=0.1062, lr=1.60e-05, step=3193]Training:   2%|‚ñè         | 3194/200000 [1:08:43<65:04:50,  1.19s/it, loss=0.0275, lr=1.60e-05, step=3194]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3195/200000 [1:08:44<67:55:23,  1.24s/it, loss=0.0275, lr=1.60e-05, step=3194]Training:   2%|‚ñè         | 3195/200000 [1:08:44<67:55:23,  1.24s/it, loss=0.0254, lr=1.60e-05, step=3195]Training:   2%|‚ñè         | 3196/200000 [1:08:45<71:31:20,  1.31s/it, loss=0.0254, lr=1.60e-05, step=3195]Training:   2%|‚ñè         | 3196/200000 [1:08:45<71:31:20,  1.31s/it, loss=0.0176, lr=1.60e-05, step=3196]Training:   2%|‚ñè         | 3197/200000 [1:08:47<74:12:37,  1.36s/it, loss=0.0176, lr=1.60e-05, step=3196]Training:   2%|‚ñè         | 3197/200000 [1:08:47<74:12:37,  1.36s/it, loss=0.0307, lr=1.60e-05, step=3197]Training:   2%|‚ñè         | 3198/200000 [1:08:48<75:16:18,  1.38s/it, loss=0.0307, lr=1.60e-05, step=3197]Training:   2%|‚ñè         | 3198/200000 [1:08:48<75:16:18,  1.38s/it, loss=0.0322, lr=1.60e-05, step=3198]Training:   2%|‚ñè         | 3199/200000 [1:08:49<70:21:22,  1.29s/it, loss=0.0322, lr=1.60e-05, step=3198]Training:   2%|‚ñè         | 3199/200000 [1:08:49<70:21:22,  1.29s/it, loss=0.0602, lr=1.60e-05, step=3199]Training:   2%|‚ñè         | 3200/200000 [1:08:50<66:55:10,  1.22s/it, loss=0.0602, lr=1.60e-05, step=3199]Training:   2%|‚ñè         | 3200/200000 [1:08:50<66:55:10,  1.22s/it, loss=0.0288, lr=1.60e-05, step=3200]00:02:05.618 [I] step=3200 loss=0.0349 lr=1.58e-05 grad_norm=0.72 time=126.4s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3201/200000 [1:08:52<69:54:48,  1.28s/it, loss=0.0288, lr=1.60e-05, step=3200]Training:   2%|‚ñè         | 3201/200000 [1:08:52<69:54:48,  1.28s/it, loss=0.0300, lr=1.60e-05, step=3201]Training:   2%|‚ñè         | 3202/200000 [1:08:53<72:20:05,  1.32s/it, loss=0.0300, lr=1.60e-05, step=3201]Training:   2%|‚ñè         | 3202/200000 [1:08:53<72:20:05,  1.32s/it, loss=0.0216, lr=1.60e-05, step=3202]Training:   2%|‚ñè         | 3203/200000 [1:08:54<68:18:23,  1.25s/it, loss=0.0216, lr=1.60e-05, step=3202]Training:   2%|‚ñè         | 3203/200000 [1:08:54<68:18:23,  1.25s/it, loss=0.0178, lr=1.60e-05, step=3203]Training:   2%|‚ñè         | 3204/200000 [1:08:56<71:13:11,  1.30s/it, loss=0.0178, lr=1.60e-05, step=3203]Training:   2%|‚ñè         | 3204/200000 [1:08:56<71:13:11,  1.30s/it, loss=0.0390, lr=1.60e-05, step=3204]Training:   2%|‚ñè         | 3205/200000 [1:08:57<67:30:11,  1.23s/it, loss=0.0390, lr=1.60e-05, step=3204]Training:   2%|‚ñè         | 3205/200000 [1:08:57<67:30:11,  1.23s/it, loss=0.0253, lr=1.60e-05, step=3205]Training:   2%|‚ñè         | 3206/200000 [1:08:58<70:14:01,  1.28s/it, loss=0.0253, lr=1.60e-05, step=3205]Training:   2%|‚ñè         | 3206/200000 [1:08:58<70:14:01,  1.28s/it, loss=0.0364, lr=1.60e-05, step=3206]Training:   2%|‚ñè         | 3207/200000 [1:08:59<66:52:00,  1.22s/it, loss=0.0364, lr=1.60e-05, step=3206]Training:   2%|‚ñè         | 3207/200000 [1:08:59<66:52:00,  1.22s/it, loss=0.0359, lr=1.60e-05, step=3207]Training:   2%|‚ñè         | 3208/200000 [1:09:01<71:23:23,  1.31s/it, loss=0.0359, lr=1.60e-05, step=3207]Training:   2%|‚ñè         | 3208/200000 [1:09:01<71:23:23,  1.31s/it, loss=0.0329, lr=1.60e-05, step=3208]Training:   2%|‚ñè         | 3209/200000 [1:09:02<75:00:05,  1.37s/it, loss=0.0329, lr=1.60e-05, step=3208]Training:   2%|‚ñè         | 3209/200000 [1:09:02<75:00:05,  1.37s/it, loss=0.0824, lr=1.60e-05, step=3209]Training:   2%|‚ñè         | 3210/200000 [1:09:03<70:10:07,  1.28s/it, loss=0.0824, lr=1.60e-05, step=3209]Training:   2%|‚ñè         | 3210/200000 [1:09:03<70:10:07,  1.28s/it, loss=0.0485, lr=1.60e-05, step=3210]Training:   2%|‚ñè         | 3211/200000 [1:09:04<66:44:11,  1.22s/it, loss=0.0485, lr=1.60e-05, step=3210]Training:   2%|‚ñè         | 3211/200000 [1:09:04<66:44:11,  1.22s/it, loss=0.0162, lr=1.61e-05, step=3211]Training:   2%|‚ñè         | 3212/200000 [1:09:06<70:23:47,  1.29s/it, loss=0.0162, lr=1.61e-05, step=3211]Training:   2%|‚ñè         | 3212/200000 [1:09:06<70:23:47,  1.29s/it, loss=0.0357, lr=1.61e-05, step=3212]Training:   2%|‚ñè         | 3213/200000 [1:09:07<66:56:56,  1.22s/it, loss=0.0357, lr=1.61e-05, step=3212]Training:   2%|‚ñè         | 3213/200000 [1:09:07<66:56:56,  1.22s/it, loss=0.0354, lr=1.61e-05, step=3213]Training:   2%|‚ñè         | 3214/200000 [1:09:08<68:31:28,  1.25s/it, loss=0.0354, lr=1.61e-05, step=3213]Training:   2%|‚ñè         | 3214/200000 [1:09:08<68:31:28,  1.25s/it, loss=0.0369, lr=1.61e-05, step=3214]Training:   2%|‚ñè         | 3215/200000 [1:09:09<65:40:09,  1.20s/it, loss=0.0369, lr=1.61e-05, step=3214]Training:   2%|‚ñè         | 3215/200000 [1:09:09<65:40:09,  1.20s/it, loss=0.0391, lr=1.61e-05, step=3215]Training:   2%|‚ñè         | 3216/200000 [1:09:10<63:35:23,  1.16s/it, loss=0.0391, lr=1.61e-05, step=3215]Training:   2%|‚ñè         | 3216/200000 [1:09:10<63:35:23,  1.16s/it, loss=0.0360, lr=1.61e-05, step=3216]Training:   2%|‚ñè         | 3217/200000 [1:09:12<68:10:24,  1.25s/it, loss=0.0360, lr=1.61e-05, step=3216]Training:   2%|‚ñè         | 3217/200000 [1:09:12<68:10:24,  1.25s/it, loss=0.0422, lr=1.61e-05, step=3217]Training:   2%|‚ñè         | 3218/200000 [1:09:13<71:10:56,  1.30s/it, loss=0.0422, lr=1.61e-05, step=3217]Training:   2%|‚ñè         | 3218/200000 [1:09:13<71:10:56,  1.30s/it, loss=0.0333, lr=1.61e-05, step=3218]Training:   2%|‚ñè         | 3219/200000 [1:09:15<72:41:24,  1.33s/it, loss=0.0333, lr=1.61e-05, step=3218]Training:   2%|‚ñè         | 3219/200000 [1:09:15<72:41:24,  1.33s/it, loss=0.0294, lr=1.61e-05, step=3219]Training:   2%|‚ñè         | 3220/200000 [1:09:16<68:31:25,  1.25s/it, loss=0.0294, lr=1.61e-05, step=3219]Training:   2%|‚ñè         | 3220/200000 [1:09:16<68:31:25,  1.25s/it, loss=0.0226, lr=1.61e-05, step=3220]Training:   2%|‚ñè         | 3221/200000 [1:09:17<65:38:14,  1.20s/it, loss=0.0226, lr=1.61e-05, step=3220]Training:   2%|‚ñè         | 3221/200000 [1:09:17<65:38:14,  1.20s/it, loss=0.0271, lr=1.61e-05, step=3221]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3222/200000 [1:09:18<67:57:35,  1.24s/it, loss=0.0271, lr=1.61e-05, step=3221]Training:   2%|‚ñè         | 3222/200000 [1:09:18<67:57:35,  1.24s/it, loss=0.0297, lr=1.61e-05, step=3222]Training:   2%|‚ñè         | 3223/200000 [1:09:20<69:50:55,  1.28s/it, loss=0.0297, lr=1.61e-05, step=3222]Training:   2%|‚ñè         | 3223/200000 [1:09:20<69:50:55,  1.28s/it, loss=0.0364, lr=1.61e-05, step=3223]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3224/200000 [1:09:21<66:31:58,  1.22s/it, loss=0.0364, lr=1.61e-05, step=3223]Training:   2%|‚ñè         | 3224/200000 [1:09:21<66:31:58,  1.22s/it, loss=0.0208, lr=1.61e-05, step=3224]Training:   2%|‚ñè         | 3225/200000 [1:09:22<68:51:38,  1.26s/it, loss=0.0208, lr=1.61e-05, step=3224]Training:   2%|‚ñè         | 3225/200000 [1:09:22<68:51:38,  1.26s/it, loss=0.0324, lr=1.61e-05, step=3225]Training:   2%|‚ñè         | 3226/200000 [1:09:23<65:52:59,  1.21s/it, loss=0.0324, lr=1.61e-05, step=3225]Training:   2%|‚ñè         | 3226/200000 [1:09:23<65:52:59,  1.21s/it, loss=0.0234, lr=1.61e-05, step=3226]Training:   2%|‚ñè         | 3227/200000 [1:09:25<69:00:56,  1.26s/it, loss=0.0234, lr=1.61e-05, step=3226]Training:   2%|‚ñè         | 3227/200000 [1:09:25<69:00:56,  1.26s/it, loss=0.0280, lr=1.61e-05, step=3227]Training:   2%|‚ñè         | 3228/200000 [1:09:26<65:57:50,  1.21s/it, loss=0.0280, lr=1.61e-05, step=3227]Training:   2%|‚ñè         | 3228/200000 [1:09:26<65:57:50,  1.21s/it, loss=0.0301, lr=1.61e-05, step=3228]Training:   2%|‚ñè         | 3229/200000 [1:09:27<70:26:51,  1.29s/it, loss=0.0301, lr=1.61e-05, step=3228]Training:   2%|‚ñè         | 3229/200000 [1:09:27<70:26:51,  1.29s/it, loss=0.0354, lr=1.61e-05, step=3229]Training:   2%|‚ñè         | 3230/200000 [1:09:29<73:59:41,  1.35s/it, loss=0.0354, lr=1.61e-05, step=3229]Training:   2%|‚ñè         | 3230/200000 [1:09:29<73:59:41,  1.35s/it, loss=0.0379, lr=1.61e-05, step=3230]Training:   2%|‚ñè         | 3231/200000 [1:09:30<69:26:26,  1.27s/it, loss=0.0379, lr=1.61e-05, step=3230]Training:   2%|‚ñè         | 3231/200000 [1:09:30<69:26:26,  1.27s/it, loss=0.0337, lr=1.62e-05, step=3231]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3232/200000 [1:09:31<66:15:59,  1.21s/it, loss=0.0337, lr=1.62e-05, step=3231]Training:   2%|‚ñè         | 3232/200000 [1:09:31<66:15:59,  1.21s/it, loss=0.0312, lr=1.62e-05, step=3232]Training:   2%|‚ñè         | 3233/200000 [1:09:32<70:30:00,  1.29s/it, loss=0.0312, lr=1.62e-05, step=3232]Training:   2%|‚ñè         | 3233/200000 [1:09:32<70:30:00,  1.29s/it, loss=0.0336, lr=1.62e-05, step=3233]Training:   2%|‚ñè         | 3234/200000 [1:09:33<66:59:48,  1.23s/it, loss=0.0336, lr=1.62e-05, step=3233]Training:   2%|‚ñè         | 3234/200000 [1:09:33<66:59:48,  1.23s/it, loss=0.0216, lr=1.62e-05, step=3234]Training:   2%|‚ñè         | 3235/200000 [1:09:35<68:32:21,  1.25s/it, loss=0.0216, lr=1.62e-05, step=3234]Training:   2%|‚ñè         | 3235/200000 [1:09:35<68:32:21,  1.25s/it, loss=0.0390, lr=1.62e-05, step=3235]Training:   2%|‚ñè         | 3236/200000 [1:09:36<65:35:55,  1.20s/it, loss=0.0390, lr=1.62e-05, step=3235]Training:   2%|‚ñè         | 3236/200000 [1:09:36<65:35:55,  1.20s/it, loss=0.0560, lr=1.62e-05, step=3236]Training:   2%|‚ñè         | 3237/200000 [1:09:37<63:36:45,  1.16s/it, loss=0.0560, lr=1.62e-05, step=3236]Training:   2%|‚ñè         | 3237/200000 [1:09:37<63:36:45,  1.16s/it, loss=0.0413, lr=1.62e-05, step=3237]Training:   2%|‚ñè         | 3238/200000 [1:09:38<68:28:42,  1.25s/it, loss=0.0413, lr=1.62e-05, step=3237]Training:   2%|‚ñè         | 3238/200000 [1:09:38<68:28:42,  1.25s/it, loss=0.0693, lr=1.62e-05, step=3238]Training:   2%|‚ñè         | 3239/200000 [1:09:40<71:03:57,  1.30s/it, loss=0.0693, lr=1.62e-05, step=3238]Training:   2%|‚ñè         | 3239/200000 [1:09:40<71:03:57,  1.30s/it, loss=0.0246, lr=1.62e-05, step=3239]Training:   2%|‚ñè         | 3240/200000 [1:09:41<73:13:26,  1.34s/it, loss=0.0246, lr=1.62e-05, step=3239]Training:   2%|‚ñè         | 3240/200000 [1:09:41<73:13:26,  1.34s/it, loss=0.1183, lr=1.62e-05, step=3240]Training:   2%|‚ñè         | 3241/200000 [1:09:42<68:52:13,  1.26s/it, loss=0.1183, lr=1.62e-05, step=3240]Training:   2%|‚ñè         | 3241/200000 [1:09:42<68:52:13,  1.26s/it, loss=0.0300, lr=1.62e-05, step=3241]Training:   2%|‚ñè         | 3242/200000 [1:09:43<65:51:40,  1.21s/it, loss=0.0300, lr=1.62e-05, step=3241]Training:   2%|‚ñè         | 3242/200000 [1:09:43<65:51:40,  1.21s/it, loss=0.0235, lr=1.62e-05, step=3242]Training:   2%|‚ñè         | 3243/200000 [1:09:45<68:12:06,  1.25s/it, loss=0.0235, lr=1.62e-05, step=3242]Training:   2%|‚ñè         | 3243/200000 [1:09:45<68:12:06,  1.25s/it, loss=0.0268, lr=1.62e-05, step=3243]Training:   2%|‚ñè         | 3244/200000 [1:09:46<69:42:22,  1.28s/it, loss=0.0268, lr=1.62e-05, step=3243]Training:   2%|‚ñè         | 3244/200000 [1:09:46<69:42:22,  1.28s/it, loss=0.0251, lr=1.62e-05, step=3244]Training:   2%|‚ñè         | 3245/200000 [1:09:47<66:28:05,  1.22s/it, loss=0.0251, lr=1.62e-05, step=3244]Training:   2%|‚ñè         | 3245/200000 [1:09:47<66:28:05,  1.22s/it, loss=0.0279, lr=1.62e-05, step=3245]Training:   2%|‚ñè         | 3246/200000 [1:09:48<68:56:52,  1.26s/it, loss=0.0279, lr=1.62e-05, step=3245]Training:   2%|‚ñè         | 3246/200000 [1:09:48<68:56:52,  1.26s/it, loss=0.0267, lr=1.62e-05, step=3246]Training:   2%|‚ñè         | 3247/200000 [1:09:49<65:51:45,  1.21s/it, loss=0.0267, lr=1.62e-05, step=3246]Training:   2%|‚ñè         | 3247/200000 [1:09:49<65:51:45,  1.21s/it, loss=0.0619, lr=1.62e-05, step=3247]Training:   2%|‚ñè         | 3248/200000 [1:09:51<68:49:23,  1.26s/it, loss=0.0619, lr=1.62e-05, step=3247]Training:   2%|‚ñè         | 3248/200000 [1:09:51<68:49:23,  1.26s/it, loss=0.0290, lr=1.62e-05, step=3248]Training:   2%|‚ñè         | 3249/200000 [1:09:52<71:27:54,  1.31s/it, loss=0.0290, lr=1.62e-05, step=3248]Training:   2%|‚ñè         | 3249/200000 [1:09:52<71:27:54,  1.31s/it, loss=0.0233, lr=1.62e-05, step=3249]Training:   2%|‚ñè         | 3250/200000 [1:09:54<74:08:45,  1.36s/it, loss=0.0233, lr=1.62e-05, step=3249]Training:   2%|‚ñè         | 3250/200000 [1:09:54<74:08:45,  1.36s/it, loss=0.0471, lr=1.62e-05, step=3250]Training:   2%|‚ñè         | 3251/200000 [1:09:55<74:49:54,  1.37s/it, loss=0.0471, lr=1.62e-05, step=3250]Training:   2%|‚ñè         | 3251/200000 [1:09:55<74:49:54,  1.37s/it, loss=0.0497, lr=1.63e-05, step=3251]Training:   2%|‚ñè         | 3252/200000 [1:09:56<70:02:31,  1.28s/it, loss=0.0497, lr=1.63e-05, step=3251]Training:   2%|‚ñè         | 3252/200000 [1:09:56<70:02:31,  1.28s/it, loss=0.0202, lr=1.63e-05, step=3252]Training:   2%|‚ñè         | 3253/200000 [1:09:57<66:42:12,  1.22s/it, loss=0.0202, lr=1.63e-05, step=3252]Training:   2%|‚ñè         | 3253/200000 [1:09:57<66:42:12,  1.22s/it, loss=0.0173, lr=1.63e-05, step=3253]Training:   2%|‚ñè         | 3254/200000 [1:09:59<69:45:06,  1.28s/it, loss=0.0173, lr=1.63e-05, step=3253]Training:   2%|‚ñè         | 3254/200000 [1:09:59<69:45:06,  1.28s/it, loss=0.0329, lr=1.63e-05, step=3254]Training:   2%|‚ñè         | 3255/200000 [1:10:00<71:50:54,  1.31s/it, loss=0.0329, lr=1.63e-05, step=3254]Training:   2%|‚ñè         | 3255/200000 [1:10:00<71:50:54,  1.31s/it, loss=0.0300, lr=1.63e-05, step=3255]Training:   2%|‚ñè         | 3256/200000 [1:10:01<67:57:21,  1.24s/it, loss=0.0300, lr=1.63e-05, step=3255]Training:   2%|‚ñè         | 3256/200000 [1:10:01<67:57:21,  1.24s/it, loss=0.0225, lr=1.63e-05, step=3256]Training:   2%|‚ñè         | 3257/200000 [1:10:03<70:55:06,  1.30s/it, loss=0.0225, lr=1.63e-05, step=3256]Training:   2%|‚ñè         | 3257/200000 [1:10:03<70:55:06,  1.30s/it, loss=0.0310, lr=1.63e-05, step=3257]Training:   2%|‚ñè         | 3258/200000 [1:10:04<67:17:36,  1.23s/it, loss=0.0310, lr=1.63e-05, step=3257]Training:   2%|‚ñè         | 3258/200000 [1:10:04<67:17:36,  1.23s/it, loss=0.0179, lr=1.63e-05, step=3258]Training:   2%|‚ñè         | 3259/200000 [1:10:05<69:40:43,  1.27s/it, loss=0.0179, lr=1.63e-05, step=3258]Training:   2%|‚ñè         | 3259/200000 [1:10:05<69:40:43,  1.27s/it, loss=0.0258, lr=1.63e-05, step=3259]Training:   2%|‚ñè         | 3260/200000 [1:10:06<66:26:22,  1.22s/it, loss=0.0258, lr=1.63e-05, step=3259]Training:   2%|‚ñè         | 3260/200000 [1:10:06<66:26:22,  1.22s/it, loss=0.0206, lr=1.63e-05, step=3260]Training:   2%|‚ñè         | 3261/200000 [1:10:08<71:04:26,  1.30s/it, loss=0.0206, lr=1.63e-05, step=3260]Training:   2%|‚ñè         | 3261/200000 [1:10:08<71:04:26,  1.30s/it, loss=0.0336, lr=1.63e-05, step=3261]Training:   2%|‚ñè         | 3262/200000 [1:10:09<73:56:09,  1.35s/it, loss=0.0336, lr=1.63e-05, step=3261]Training:   2%|‚ñè         | 3262/200000 [1:10:09<73:56:09,  1.35s/it, loss=0.0142, lr=1.63e-05, step=3262]Training:   2%|‚ñè         | 3263/200000 [1:10:10<69:24:24,  1.27s/it, loss=0.0142, lr=1.63e-05, step=3262]Training:   2%|‚ñè         | 3263/200000 [1:10:10<69:24:24,  1.27s/it, loss=0.0283, lr=1.63e-05, step=3263]Training:   2%|‚ñè         | 3264/200000 [1:10:11<66:13:16,  1.21s/it, loss=0.0283, lr=1.63e-05, step=3263]Training:   2%|‚ñè         | 3264/200000 [1:10:11<66:13:16,  1.21s/it, loss=0.0230, lr=1.63e-05, step=3264]Training:   2%|‚ñè         | 3265/200000 [1:10:13<70:42:30,  1.29s/it, loss=0.0230, lr=1.63e-05, step=3264]Training:   2%|‚ñè         | 3265/200000 [1:10:13<70:42:30,  1.29s/it, loss=0.0441, lr=1.63e-05, step=3265]Training:   2%|‚ñè         | 3266/200000 [1:10:14<67:08:55,  1.23s/it, loss=0.0441, lr=1.63e-05, step=3265]Training:   2%|‚ñè         | 3266/200000 [1:10:14<67:08:55,  1.23s/it, loss=0.0189, lr=1.63e-05, step=3266]Training:   2%|‚ñè         | 3267/200000 [1:10:15<68:47:39,  1.26s/it, loss=0.0189, lr=1.63e-05, step=3266]Training:   2%|‚ñè         | 3267/200000 [1:10:15<68:47:39,  1.26s/it, loss=0.0354, lr=1.63e-05, step=3267]Training:   2%|‚ñè         | 3268/200000 [1:10:16<65:47:32,  1.20s/it, loss=0.0354, lr=1.63e-05, step=3267]Training:   2%|‚ñè         | 3268/200000 [1:10:16<65:47:32,  1.20s/it, loss=0.0293, lr=1.63e-05, step=3268]Training:   2%|‚ñè         | 3269/200000 [1:10:17<63:42:52,  1.17s/it, loss=0.0293, lr=1.63e-05, step=3268]Training:   2%|‚ñè         | 3269/200000 [1:10:17<63:42:52,  1.17s/it, loss=0.0440, lr=1.63e-05, step=3269]Training:   2%|‚ñè         | 3270/200000 [1:10:19<68:22:47,  1.25s/it, loss=0.0440, lr=1.63e-05, step=3269]Training:   2%|‚ñè         | 3270/200000 [1:10:19<68:22:47,  1.25s/it, loss=0.0443, lr=1.63e-05, step=3270]Training:   2%|‚ñè         | 3271/200000 [1:10:20<71:03:29,  1.30s/it, loss=0.0443, lr=1.63e-05, step=3270]Training:   2%|‚ñè         | 3271/200000 [1:10:20<71:03:29,  1.30s/it, loss=0.0284, lr=1.64e-05, step=3271]Training:   2%|‚ñè         | 3272/200000 [1:10:22<73:20:25,  1.34s/it, loss=0.0284, lr=1.64e-05, step=3271]Training:   2%|‚ñè         | 3272/200000 [1:10:22<73:20:25,  1.34s/it, loss=0.0243, lr=1.64e-05, step=3272]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3273/200000 [1:10:23<69:00:40,  1.26s/it, loss=0.0243, lr=1.64e-05, step=3272]Training:   2%|‚ñè         | 3273/200000 [1:10:23<69:00:40,  1.26s/it, loss=0.0229, lr=1.64e-05, step=3273]Training:   2%|‚ñè         | 3274/200000 [1:10:24<65:58:12,  1.21s/it, loss=0.0229, lr=1.64e-05, step=3273]Training:   2%|‚ñè         | 3274/200000 [1:10:24<65:58:12,  1.21s/it, loss=0.0313, lr=1.64e-05, step=3274]Training:   2%|‚ñè         | 3275/200000 [1:10:25<68:28:49,  1.25s/it, loss=0.0313, lr=1.64e-05, step=3274]Training:   2%|‚ñè         | 3275/200000 [1:10:25<68:28:49,  1.25s/it, loss=0.0311, lr=1.64e-05, step=3275]Training:   2%|‚ñè         | 3276/200000 [1:10:26<70:58:13,  1.30s/it, loss=0.0311, lr=1.64e-05, step=3275]Training:   2%|‚ñè         | 3276/200000 [1:10:26<70:58:13,  1.30s/it, loss=0.0320, lr=1.64e-05, step=3276]Training:   2%|‚ñè         | 3277/200000 [1:10:28<67:20:10,  1.23s/it, loss=0.0320, lr=1.64e-05, step=3276]Training:   2%|‚ñè         | 3277/200000 [1:10:28<67:20:10,  1.23s/it, loss=0.0278, lr=1.64e-05, step=3277]Training:   2%|‚ñè         | 3278/200000 [1:10:29<70:10:38,  1.28s/it, loss=0.0278, lr=1.64e-05, step=3277]Training:   2%|‚ñè         | 3278/200000 [1:10:29<70:10:38,  1.28s/it, loss=0.0555, lr=1.64e-05, step=3278]Training:   2%|‚ñè         | 3279/200000 [1:10:30<66:48:03,  1.22s/it, loss=0.0555, lr=1.64e-05, step=3278]Training:   2%|‚ñè         | 3279/200000 [1:10:30<66:48:03,  1.22s/it, loss=0.0198, lr=1.64e-05, step=3279]Training:   2%|‚ñè         | 3280/200000 [1:10:31<69:49:51,  1.28s/it, loss=0.0198, lr=1.64e-05, step=3279]Training:   2%|‚ñè         | 3280/200000 [1:10:31<69:49:51,  1.28s/it, loss=0.0240, lr=1.64e-05, step=3280]Training:   2%|‚ñè         | 3281/200000 [1:10:33<66:32:55,  1.22s/it, loss=0.0240, lr=1.64e-05, step=3280]Training:   2%|‚ñè         | 3281/200000 [1:10:33<66:32:55,  1.22s/it, loss=0.0251, lr=1.64e-05, step=3281]Training:   2%|‚ñè         | 3282/200000 [1:10:34<70:51:19,  1.30s/it, loss=0.0251, lr=1.64e-05, step=3281]Training:   2%|‚ñè         | 3282/200000 [1:10:34<70:51:19,  1.30s/it, loss=0.0223, lr=1.64e-05, step=3282]Training:   2%|‚ñè         | 3283/200000 [1:10:36<74:18:46,  1.36s/it, loss=0.0223, lr=1.64e-05, step=3282]Training:   2%|‚ñè         | 3283/200000 [1:10:36<74:18:46,  1.36s/it, loss=0.0338, lr=1.64e-05, step=3283]Training:   2%|‚ñè         | 3284/200000 [1:10:37<69:39:59,  1.27s/it, loss=0.0338, lr=1.64e-05, step=3283]Training:   2%|‚ñè         | 3284/200000 [1:10:37<69:39:59,  1.27s/it, loss=0.0359, lr=1.64e-05, step=3284]Training:   2%|‚ñè         | 3285/200000 [1:10:38<66:24:59,  1.22s/it, loss=0.0359, lr=1.64e-05, step=3284]Training:   2%|‚ñè         | 3285/200000 [1:10:38<66:24:59,  1.22s/it, loss=0.0252, lr=1.64e-05, step=3285]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3286/200000 [1:10:39<70:29:12,  1.29s/it, loss=0.0252, lr=1.64e-05, step=3285]Training:   2%|‚ñè         | 3286/200000 [1:10:39<70:29:12,  1.29s/it, loss=0.0291, lr=1.64e-05, step=3286]Training:   2%|‚ñè         | 3287/200000 [1:10:40<67:00:17,  1.23s/it, loss=0.0291, lr=1.64e-05, step=3286]Training:   2%|‚ñè         | 3287/200000 [1:10:40<67:00:17,  1.23s/it, loss=0.0284, lr=1.64e-05, step=3287]Training:   2%|‚ñè         | 3288/200000 [1:10:42<69:38:34,  1.27s/it, loss=0.0284, lr=1.64e-05, step=3287]Training:   2%|‚ñè         | 3288/200000 [1:10:42<69:38:34,  1.27s/it, loss=0.0216, lr=1.64e-05, step=3288]Training:   2%|‚ñè         | 3289/200000 [1:10:43<66:23:24,  1.22s/it, loss=0.0216, lr=1.64e-05, step=3288]Training:   2%|‚ñè         | 3289/200000 [1:10:43<66:23:24,  1.22s/it, loss=0.0209, lr=1.64e-05, step=3289]Training:   2%|‚ñè         | 3290/200000 [1:10:44<64:08:41,  1.17s/it, loss=0.0209, lr=1.64e-05, step=3289]Training:   2%|‚ñè         | 3290/200000 [1:10:44<64:08:41,  1.17s/it, loss=0.0352, lr=1.64e-05, step=3290]Training:   2%|‚ñè         | 3291/200000 [1:10:45<68:15:03,  1.25s/it, loss=0.0352, lr=1.64e-05, step=3290]Training:   2%|‚ñè         | 3291/200000 [1:10:45<68:15:03,  1.25s/it, loss=0.0249, lr=1.65e-05, step=3291]Training:   2%|‚ñè         | 3292/200000 [1:10:47<70:41:37,  1.29s/it, loss=0.0249, lr=1.65e-05, step=3291]Training:   2%|‚ñè         | 3292/200000 [1:10:47<70:41:37,  1.29s/it, loss=0.0335, lr=1.65e-05, step=3292]Training:   2%|‚ñè         | 3293/200000 [1:10:48<72:56:41,  1.33s/it, loss=0.0335, lr=1.65e-05, step=3292]Training:   2%|‚ñè         | 3293/200000 [1:10:48<72:56:41,  1.33s/it, loss=0.0509, lr=1.65e-05, step=3293]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3294/200000 [1:10:49<68:43:38,  1.26s/it, loss=0.0509, lr=1.65e-05, step=3293]Training:   2%|‚ñè         | 3294/200000 [1:10:49<68:43:38,  1.26s/it, loss=0.0279, lr=1.65e-05, step=3294]Training:   2%|‚ñè         | 3295/200000 [1:10:50<65:44:40,  1.20s/it, loss=0.0279, lr=1.65e-05, step=3294]Training:   2%|‚ñè         | 3295/200000 [1:10:50<65:44:40,  1.20s/it, loss=0.0301, lr=1.65e-05, step=3295]Training:   2%|‚ñè         | 3296/200000 [1:10:51<67:56:26,  1.24s/it, loss=0.0301, lr=1.65e-05, step=3295]Training:   2%|‚ñè         | 3296/200000 [1:10:51<67:56:26,  1.24s/it, loss=0.0365, lr=1.65e-05, step=3296]Training:   2%|‚ñè         | 3297/200000 [1:10:53<68:48:29,  1.26s/it, loss=0.0365, lr=1.65e-05, step=3296]Training:   2%|‚ñè         | 3297/200000 [1:10:53<68:48:29,  1.26s/it, loss=0.0287, lr=1.65e-05, step=3297]Training:   2%|‚ñè         | 3298/200000 [1:10:54<65:47:38,  1.20s/it, loss=0.0287, lr=1.65e-05, step=3297]Training:   2%|‚ñè         | 3298/200000 [1:10:54<65:47:38,  1.20s/it, loss=0.0228, lr=1.65e-05, step=3298]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3299/200000 [1:10:55<67:55:09,  1.24s/it, loss=0.0228, lr=1.65e-05, step=3298]Training:   2%|‚ñè         | 3299/200000 [1:10:55<67:55:09,  1.24s/it, loss=0.0276, lr=1.65e-05, step=3299]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3300/200000 [1:10:56<65:11:39,  1.19s/it, loss=0.0276, lr=1.65e-05, step=3299]Training:   2%|‚ñè         | 3300/200000 [1:10:56<65:11:39,  1.19s/it, loss=0.0269, lr=1.65e-05, step=3300]00:04:11.462 [I] step=3300 loss=0.0325 lr=1.63e-05 grad_norm=0.71 time=125.8s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3301/200000 [1:10:58<68:23:32,  1.25s/it, loss=0.0269, lr=1.65e-05, step=3300]Training:   2%|‚ñè         | 3301/200000 [1:10:58<68:23:32,  1.25s/it, loss=0.0202, lr=1.65e-05, step=3301]Training:   2%|‚ñè         | 3302/200000 [1:10:59<71:21:30,  1.31s/it, loss=0.0202, lr=1.65e-05, step=3301]Training:   2%|‚ñè         | 3302/200000 [1:10:59<71:21:30,  1.31s/it, loss=0.1516, lr=1.65e-05, step=3302]Training:   2%|‚ñè         | 3303/200000 [1:11:01<73:12:31,  1.34s/it, loss=0.1516, lr=1.65e-05, step=3302]Training:   2%|‚ñè         | 3303/200000 [1:11:01<73:12:31,  1.34s/it, loss=0.0266, lr=1.65e-05, step=3303]Training:   2%|‚ñè         | 3304/200000 [1:11:02<74:37:13,  1.37s/it, loss=0.0266, lr=1.65e-05, step=3303]Training:   2%|‚ñè         | 3304/200000 [1:11:02<74:37:13,  1.37s/it, loss=0.0190, lr=1.65e-05, step=3304]Training:   2%|‚ñè         | 3305/200000 [1:11:03<69:53:36,  1.28s/it, loss=0.0190, lr=1.65e-05, step=3304]Training:   2%|‚ñè         | 3305/200000 [1:11:03<69:53:36,  1.28s/it, loss=0.0185, lr=1.65e-05, step=3305]Training:   2%|‚ñè         | 3306/200000 [1:11:04<66:35:21,  1.22s/it, loss=0.0185, lr=1.65e-05, step=3305]Training:   2%|‚ñè         | 3306/200000 [1:11:04<66:35:21,  1.22s/it, loss=0.0305, lr=1.65e-05, step=3306]Training:   2%|‚ñè         | 3307/200000 [1:11:05<69:42:05,  1.28s/it, loss=0.0305, lr=1.65e-05, step=3306]Training:   2%|‚ñè         | 3307/200000 [1:11:05<69:42:05,  1.28s/it, loss=0.0251, lr=1.65e-05, step=3307]Training:   2%|‚ñè         | 3308/200000 [1:11:07<72:27:52,  1.33s/it, loss=0.0251, lr=1.65e-05, step=3307]Training:   2%|‚ñè         | 3308/200000 [1:11:07<72:27:52,  1.33s/it, loss=0.0295, lr=1.65e-05, step=3308]Training:   2%|‚ñè         | 3309/200000 [1:11:08<68:22:10,  1.25s/it, loss=0.0295, lr=1.65e-05, step=3308]Training:   2%|‚ñè         | 3309/200000 [1:11:08<68:22:10,  1.25s/it, loss=0.0180, lr=1.65e-05, step=3309]Training:   2%|‚ñè         | 3310/200000 [1:11:09<71:09:03,  1.30s/it, loss=0.0180, lr=1.65e-05, step=3309]Training:   2%|‚ñè         | 3310/200000 [1:11:09<71:09:03,  1.30s/it, loss=0.0452, lr=1.65e-05, step=3310]Training:   2%|‚ñè         | 3311/200000 [1:11:11<67:26:28,  1.23s/it, loss=0.0452, lr=1.65e-05, step=3310]Training:   2%|‚ñè         | 3311/200000 [1:11:11<67:26:28,  1.23s/it, loss=0.0336, lr=1.66e-05, step=3311]Training:   2%|‚ñè         | 3312/200000 [1:11:12<70:14:33,  1.29s/it, loss=0.0336, lr=1.66e-05, step=3311]Training:   2%|‚ñè         | 3312/200000 [1:11:12<70:14:33,  1.29s/it, loss=0.0188, lr=1.66e-05, step=3312]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3313/200000 [1:11:13<66:49:48,  1.22s/it, loss=0.0188, lr=1.66e-05, step=3312]Training:   2%|‚ñè         | 3313/200000 [1:11:13<66:49:48,  1.22s/it, loss=0.0228, lr=1.66e-05, step=3313]Training:   2%|‚ñè         | 3314/200000 [1:11:14<71:23:25,  1.31s/it, loss=0.0228, lr=1.66e-05, step=3313]Training:   2%|‚ñè         | 3314/200000 [1:11:14<71:23:25,  1.31s/it, loss=0.0244, lr=1.66e-05, step=3314]Training:   2%|‚ñè         | 3315/200000 [1:11:16<74:59:23,  1.37s/it, loss=0.0244, lr=1.66e-05, step=3314]Training:   2%|‚ñè         | 3315/200000 [1:11:16<74:59:23,  1.37s/it, loss=0.0392, lr=1.66e-05, step=3315]Training:   2%|‚ñè         | 3316/200000 [1:11:17<70:06:37,  1.28s/it, loss=0.0392, lr=1.66e-05, step=3315]Training:   2%|‚ñè         | 3316/200000 [1:11:17<70:06:37,  1.28s/it, loss=0.0322, lr=1.66e-05, step=3316]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3317/200000 [1:11:18<66:43:34,  1.22s/it, loss=0.0322, lr=1.66e-05, step=3316]Training:   2%|‚ñè         | 3317/200000 [1:11:18<66:43:34,  1.22s/it, loss=0.0283, lr=1.66e-05, step=3317]Training:   2%|‚ñè         | 3318/200000 [1:11:20<70:24:38,  1.29s/it, loss=0.0283, lr=1.66e-05, step=3317]Training:   2%|‚ñè         | 3318/200000 [1:11:20<70:24:38,  1.29s/it, loss=1.1231, lr=1.66e-05, step=3318]Training:   2%|‚ñè         | 3319/200000 [1:11:21<66:56:15,  1.23s/it, loss=1.1231, lr=1.66e-05, step=3318]Training:   2%|‚ñè         | 3319/200000 [1:11:21<66:56:15,  1.23s/it, loss=0.0355, lr=1.66e-05, step=3319]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3320/200000 [1:11:22<69:13:07,  1.27s/it, loss=0.0355, lr=1.66e-05, step=3319]Training:   2%|‚ñè         | 3320/200000 [1:11:22<69:13:07,  1.27s/it, loss=0.0303, lr=1.66e-05, step=3320]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3321/200000 [1:11:23<66:05:29,  1.21s/it, loss=0.0303, lr=1.66e-05, step=3320]Training:   2%|‚ñè         | 3321/200000 [1:11:23<66:05:29,  1.21s/it, loss=0.0386, lr=1.66e-05, step=3321]Training:   2%|‚ñè         | 3322/200000 [1:11:24<63:55:24,  1.17s/it, loss=0.0386, lr=1.66e-05, step=3321]Training:   2%|‚ñè         | 3322/200000 [1:11:24<63:55:24,  1.17s/it, loss=0.0419, lr=1.66e-05, step=3322]Training:   2%|‚ñè         | 3323/200000 [1:11:26<68:23:59,  1.25s/it, loss=0.0419, lr=1.66e-05, step=3322]Training:   2%|‚ñè         | 3323/200000 [1:11:26<68:23:59,  1.25s/it, loss=0.0488, lr=1.66e-05, step=3323]Training:   2%|‚ñè         | 3324/200000 [1:11:27<71:00:07,  1.30s/it, loss=0.0488, lr=1.66e-05, step=3323]Training:   2%|‚ñè         | 3324/200000 [1:11:27<71:00:07,  1.30s/it, loss=0.0300, lr=1.66e-05, step=3324]Training:   2%|‚ñè         | 3325/200000 [1:11:28<72:49:02,  1.33s/it, loss=0.0300, lr=1.66e-05, step=3324]Training:   2%|‚ñè         | 3325/200000 [1:11:28<72:49:02,  1.33s/it, loss=0.0216, lr=1.66e-05, step=3325]Training:   2%|‚ñè         | 3326/200000 [1:11:30<68:34:34,  1.26s/it, loss=0.0216, lr=1.66e-05, step=3325]Training:   2%|‚ñè         | 3326/200000 [1:11:30<68:34:34,  1.26s/it, loss=0.0335, lr=1.66e-05, step=3326]Training:   2%|‚ñè         | 3327/200000 [1:11:31<65:41:21,  1.20s/it, loss=0.0335, lr=1.66e-05, step=3326]Training:   2%|‚ñè         | 3327/200000 [1:11:31<65:41:21,  1.20s/it, loss=0.0425, lr=1.66e-05, step=3327]Training:   2%|‚ñè         | 3328/200000 [1:11:32<68:12:13,  1.25s/it, loss=0.0425, lr=1.66e-05, step=3327]Training:   2%|‚ñè         | 3328/200000 [1:11:32<68:12:13,  1.25s/it, loss=0.0235, lr=1.66e-05, step=3328]Training:   2%|‚ñè         | 3329/200000 [1:11:33<70:25:08,  1.29s/it, loss=0.0235, lr=1.66e-05, step=3328]Training:   2%|‚ñè         | 3329/200000 [1:11:33<70:25:08,  1.29s/it, loss=0.0275, lr=1.66e-05, step=3329]Training:   2%|‚ñè         | 3330/200000 [1:11:34<66:54:08,  1.22s/it, loss=0.0275, lr=1.66e-05, step=3329]Training:   2%|‚ñè         | 3330/200000 [1:11:34<66:54:08,  1.22s/it, loss=0.0285, lr=1.66e-05, step=3330]Training:   2%|‚ñè         | 3331/200000 [1:11:36<69:33:13,  1.27s/it, loss=0.0285, lr=1.66e-05, step=3330]Training:   2%|‚ñè         | 3331/200000 [1:11:36<69:33:13,  1.27s/it, loss=0.0750, lr=1.67e-05, step=3331]Training:   2%|‚ñè         | 3332/200000 [1:11:37<66:19:57,  1.21s/it, loss=0.0750, lr=1.67e-05, step=3331]Training:   2%|‚ñè         | 3332/200000 [1:11:37<66:19:57,  1.21s/it, loss=0.0215, lr=1.67e-05, step=3332]Training:   2%|‚ñè         | 3333/200000 [1:11:38<69:27:15,  1.27s/it, loss=0.0215, lr=1.67e-05, step=3332]Training:   2%|‚ñè         | 3333/200000 [1:11:38<69:27:15,  1.27s/it, loss=0.0162, lr=1.67e-05, step=3333]Training:   2%|‚ñè         | 3334/200000 [1:11:39<66:14:59,  1.21s/it, loss=0.0162, lr=1.67e-05, step=3333]Training:   2%|‚ñè         | 3334/200000 [1:11:39<66:14:59,  1.21s/it, loss=0.0251, lr=1.67e-05, step=3334]Training:   2%|‚ñè         | 3335/200000 [1:11:41<71:32:00,  1.31s/it, loss=0.0251, lr=1.67e-05, step=3334]Training:   2%|‚ñè         | 3335/200000 [1:11:41<71:32:00,  1.31s/it, loss=0.0411, lr=1.67e-05, step=3335]Training:   2%|‚ñè         | 3336/200000 [1:11:42<74:46:49,  1.37s/it, loss=0.0411, lr=1.67e-05, step=3335]Training:   2%|‚ñè         | 3336/200000 [1:11:42<74:46:49,  1.37s/it, loss=0.0282, lr=1.67e-05, step=3336]Training:   2%|‚ñè         | 3337/200000 [1:11:44<69:59:37,  1.28s/it, loss=0.0282, lr=1.67e-05, step=3336]Training:   2%|‚ñè         | 3337/200000 [1:11:44<69:59:37,  1.28s/it, loss=0.0273, lr=1.67e-05, step=3337]Training:   2%|‚ñè         | 3338/200000 [1:11:45<66:36:25,  1.22s/it, loss=0.0273, lr=1.67e-05, step=3337]Training:   2%|‚ñè         | 3338/200000 [1:11:45<66:36:25,  1.22s/it, loss=0.0269, lr=1.67e-05, step=3338]Training:   2%|‚ñè         | 3339/200000 [1:11:46<69:54:52,  1.28s/it, loss=0.0269, lr=1.67e-05, step=3338]Training:   2%|‚ñè         | 3339/200000 [1:11:46<69:54:52,  1.28s/it, loss=0.0259, lr=1.67e-05, step=3339]Training:   2%|‚ñè         | 3340/200000 [1:11:47<66:34:36,  1.22s/it, loss=0.0259, lr=1.67e-05, step=3339]Training:   2%|‚ñè         | 3340/200000 [1:11:47<66:34:36,  1.22s/it, loss=0.0398, lr=1.67e-05, step=3340]Training:   2%|‚ñè         | 3341/200000 [1:11:48<68:54:31,  1.26s/it, loss=0.0398, lr=1.67e-05, step=3340]Training:   2%|‚ñè         | 3341/200000 [1:11:48<68:54:31,  1.26s/it, loss=0.0523, lr=1.67e-05, step=3341]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3342/200000 [1:11:50<65:52:19,  1.21s/it, loss=0.0523, lr=1.67e-05, step=3341]Training:   2%|‚ñè         | 3342/200000 [1:11:50<65:52:19,  1.21s/it, loss=0.0531, lr=1.67e-05, step=3342]Training:   2%|‚ñè         | 3343/200000 [1:11:51<63:46:37,  1.17s/it, loss=0.0531, lr=1.67e-05, step=3342]Training:   2%|‚ñè         | 3343/200000 [1:11:51<63:46:37,  1.17s/it, loss=0.0216, lr=1.67e-05, step=3343]Training:   2%|‚ñè         | 3344/200000 [1:11:52<68:36:25,  1.26s/it, loss=0.0216, lr=1.67e-05, step=3343]Training:   2%|‚ñè         | 3344/200000 [1:11:52<68:36:25,  1.26s/it, loss=0.0489, lr=1.67e-05, step=3344]Training:   2%|‚ñè         | 3345/200000 [1:11:53<71:00:20,  1.30s/it, loss=0.0489, lr=1.67e-05, step=3344]Training:   2%|‚ñè         | 3345/200000 [1:11:53<71:00:20,  1.30s/it, loss=0.0227, lr=1.67e-05, step=3345]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3346/200000 [1:11:55<72:53:59,  1.33s/it, loss=0.0227, lr=1.67e-05, step=3345]Training:   2%|‚ñè         | 3346/200000 [1:11:55<72:53:59,  1.33s/it, loss=0.0261, lr=1.67e-05, step=3346]Training:   2%|‚ñè         | 3347/200000 [1:11:56<68:41:46,  1.26s/it, loss=0.0261, lr=1.67e-05, step=3346]Training:   2%|‚ñè         | 3347/200000 [1:11:56<68:41:46,  1.26s/it, loss=0.0211, lr=1.67e-05, step=3347]Training:   2%|‚ñè         | 3348/200000 [1:11:57<65:43:00,  1.20s/it, loss=0.0211, lr=1.67e-05, step=3347]Training:   2%|‚ñè         | 3348/200000 [1:11:57<65:43:00,  1.20s/it, loss=0.0518, lr=1.67e-05, step=3348]Training:   2%|‚ñè         | 3349/200000 [1:11:58<68:08:06,  1.25s/it, loss=0.0518, lr=1.67e-05, step=3348]Training:   2%|‚ñè         | 3349/200000 [1:11:58<68:08:06,  1.25s/it, loss=0.0423, lr=1.67e-05, step=3349]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3350/200000 [1:12:00<69:35:57,  1.27s/it, loss=0.0423, lr=1.67e-05, step=3349]Training:   2%|‚ñè         | 3350/200000 [1:12:00<69:35:57,  1.27s/it, loss=0.0275, lr=1.67e-05, step=3350]Training:   2%|‚ñè         | 3351/200000 [1:12:01<66:21:59,  1.21s/it, loss=0.0275, lr=1.67e-05, step=3350]Training:   2%|‚ñè         | 3351/200000 [1:12:01<66:21:59,  1.21s/it, loss=0.0215, lr=1.68e-05, step=3351]Training:   2%|‚ñè         | 3352/200000 [1:12:02<68:23:08,  1.25s/it, loss=0.0215, lr=1.68e-05, step=3351]Training:   2%|‚ñè         | 3352/200000 [1:12:02<68:23:08,  1.25s/it, loss=0.0341, lr=1.68e-05, step=3352]Training:   2%|‚ñè         | 3353/200000 [1:12:03<65:32:26,  1.20s/it, loss=0.0341, lr=1.68e-05, step=3352]Training:   2%|‚ñè         | 3353/200000 [1:12:03<65:32:26,  1.20s/it, loss=0.0412, lr=1.68e-05, step=3353]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3354/200000 [1:12:05<68:39:47,  1.26s/it, loss=0.0412, lr=1.68e-05, step=3353]Training:   2%|‚ñè         | 3354/200000 [1:12:05<68:39:47,  1.26s/it, loss=0.0214, lr=1.68e-05, step=3354]Training:   2%|‚ñè         | 3355/200000 [1:12:06<72:00:54,  1.32s/it, loss=0.0214, lr=1.68e-05, step=3354]Training:   2%|‚ñè         | 3355/200000 [1:12:06<72:00:54,  1.32s/it, loss=0.0217, lr=1.68e-05, step=3355]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3356/200000 [1:12:07<73:40:54,  1.35s/it, loss=0.0217, lr=1.68e-05, step=3355]Training:   2%|‚ñè         | 3356/200000 [1:12:07<73:40:54,  1.35s/it, loss=0.0471, lr=1.68e-05, step=3356]Training:   2%|‚ñè         | 3357/200000 [1:12:09<74:29:23,  1.36s/it, loss=0.0471, lr=1.68e-05, step=3356]Training:   2%|‚ñè         | 3357/200000 [1:12:09<74:29:23,  1.36s/it, loss=0.0212, lr=1.68e-05, step=3357]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3358/200000 [1:12:10<69:46:26,  1.28s/it, loss=0.0212, lr=1.68e-05, step=3357]Training:   2%|‚ñè         | 3358/200000 [1:12:10<69:46:26,  1.28s/it, loss=0.0231, lr=1.68e-05, step=3358]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3359/200000 [1:12:11<66:28:47,  1.22s/it, loss=0.0231, lr=1.68e-05, step=3358]Training:   2%|‚ñè         | 3359/200000 [1:12:11<66:28:47,  1.22s/it, loss=0.0266, lr=1.68e-05, step=3359]Training:   2%|‚ñè         | 3360/200000 [1:12:12<69:31:26,  1.27s/it, loss=0.0266, lr=1.68e-05, step=3359]Training:   2%|‚ñè         | 3360/200000 [1:12:12<69:31:26,  1.27s/it, loss=0.0174, lr=1.68e-05, step=3360]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3361/200000 [1:12:14<71:26:44,  1.31s/it, loss=0.0174, lr=1.68e-05, step=3360]Training:   2%|‚ñè         | 3361/200000 [1:12:14<71:26:44,  1.31s/it, loss=0.0232, lr=1.68e-05, step=3361]Training:   2%|‚ñè         | 3362/200000 [1:12:15<67:39:17,  1.24s/it, loss=0.0232, lr=1.68e-05, step=3361]Training:   2%|‚ñè         | 3362/200000 [1:12:15<67:39:17,  1.24s/it, loss=0.0387, lr=1.68e-05, step=3362]Training:   2%|‚ñè         | 3363/200000 [1:12:16<70:51:42,  1.30s/it, loss=0.0387, lr=1.68e-05, step=3362]Training:   2%|‚ñè         | 3363/200000 [1:12:16<70:51:42,  1.30s/it, loss=0.0208, lr=1.68e-05, step=3363]Training:   2%|‚ñè         | 3364/200000 [1:12:17<67:16:40,  1.23s/it, loss=0.0208, lr=1.68e-05, step=3363]Training:   2%|‚ñè         | 3364/200000 [1:12:17<67:16:40,  1.23s/it, loss=0.0334, lr=1.68e-05, step=3364]Training:   2%|‚ñè         | 3365/200000 [1:12:19<69:35:42,  1.27s/it, loss=0.0334, lr=1.68e-05, step=3364]Training:   2%|‚ñè         | 3365/200000 [1:12:19<69:35:42,  1.27s/it, loss=0.0289, lr=1.68e-05, step=3365]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3366/200000 [1:12:20<66:21:39,  1.21s/it, loss=0.0289, lr=1.68e-05, step=3365]Training:   2%|‚ñè         | 3366/200000 [1:12:20<66:21:39,  1.21s/it, loss=0.0331, lr=1.68e-05, step=3366]Training:   2%|‚ñè         | 3367/200000 [1:12:21<71:54:02,  1.32s/it, loss=0.0331, lr=1.68e-05, step=3366]Training:   2%|‚ñè         | 3367/200000 [1:12:21<71:54:02,  1.32s/it, loss=0.0313, lr=1.68e-05, step=3367]Training:   2%|‚ñè         | 3368/200000 [1:12:23<75:22:08,  1.38s/it, loss=0.0313, lr=1.68e-05, step=3367]Training:   2%|‚ñè         | 3368/200000 [1:12:23<75:22:08,  1.38s/it, loss=0.0316, lr=1.68e-05, step=3368]Training:   2%|‚ñè         | 3369/200000 [1:12:24<70:23:47,  1.29s/it, loss=0.0316, lr=1.68e-05, step=3368]Training:   2%|‚ñè         | 3369/200000 [1:12:24<70:23:47,  1.29s/it, loss=0.0262, lr=1.68e-05, step=3369]Training:   2%|‚ñè         | 3370/200000 [1:12:25<66:54:32,  1.23s/it, loss=0.0262, lr=1.68e-05, step=3369]Training:   2%|‚ñè         | 3370/200000 [1:12:25<66:54:32,  1.23s/it, loss=0.0474, lr=1.68e-05, step=3370]Training:   2%|‚ñè         | 3371/200000 [1:12:27<71:04:43,  1.30s/it, loss=0.0474, lr=1.68e-05, step=3370]Training:   2%|‚ñè         | 3371/200000 [1:12:27<71:04:43,  1.30s/it, loss=0.0175, lr=1.69e-05, step=3371]Training:   2%|‚ñè         | 3372/200000 [1:12:28<67:23:23,  1.23s/it, loss=0.0175, lr=1.69e-05, step=3371]Training:   2%|‚ñè         | 3372/200000 [1:12:28<67:23:23,  1.23s/it, loss=0.0210, lr=1.69e-05, step=3372]Training:   2%|‚ñè         | 3373/200000 [1:12:29<69:29:33,  1.27s/it, loss=0.0210, lr=1.69e-05, step=3372]Training:   2%|‚ñè         | 3373/200000 [1:12:29<69:29:33,  1.27s/it, loss=0.0501, lr=1.69e-05, step=3373]Training:   2%|‚ñè         | 3374/200000 [1:12:30<66:15:49,  1.21s/it, loss=0.0501, lr=1.69e-05, step=3373]Training:   2%|‚ñè         | 3374/200000 [1:12:30<66:15:49,  1.21s/it, loss=0.0459, lr=1.69e-05, step=3374]Training:   2%|‚ñè         | 3375/200000 [1:12:31<64:00:12,  1.17s/it, loss=0.0459, lr=1.69e-05, step=3374]Training:   2%|‚ñè         | 3375/200000 [1:12:31<64:00:12,  1.17s/it, loss=0.0300, lr=1.69e-05, step=3375]Training:   2%|‚ñè         | 3376/200000 [1:12:33<68:43:12,  1.26s/it, loss=0.0300, lr=1.69e-05, step=3375]Training:   2%|‚ñè         | 3376/200000 [1:12:33<68:43:12,  1.26s/it, loss=0.0292, lr=1.69e-05, step=3376]Training:   2%|‚ñè         | 3377/200000 [1:12:34<71:18:40,  1.31s/it, loss=0.0292, lr=1.69e-05, step=3376]Training:   2%|‚ñè         | 3377/200000 [1:12:34<71:18:40,  1.31s/it, loss=0.0205, lr=1.69e-05, step=3377]Training:   2%|‚ñè         | 3378/200000 [1:12:35<73:26:32,  1.34s/it, loss=0.0205, lr=1.69e-05, step=3377]Training:   2%|‚ñè         | 3378/200000 [1:12:35<73:26:32,  1.34s/it, loss=0.0398, lr=1.69e-05, step=3378]Training:   2%|‚ñè         | 3379/200000 [1:12:37<69:02:40,  1.26s/it, loss=0.0398, lr=1.69e-05, step=3378]Training:   2%|‚ñè         | 3379/200000 [1:12:37<69:02:40,  1.26s/it, loss=0.0167, lr=1.69e-05, step=3379]Training:   2%|‚ñè         | 3380/200000 [1:12:38<65:57:08,  1.21s/it, loss=0.0167, lr=1.69e-05, step=3379]Training:   2%|‚ñè         | 3380/200000 [1:12:38<65:57:08,  1.21s/it, loss=0.0518, lr=1.69e-05, step=3380]Training:   2%|‚ñè         | 3381/200000 [1:12:39<68:30:37,  1.25s/it, loss=0.0518, lr=1.69e-05, step=3380]Training:   2%|‚ñè         | 3381/200000 [1:12:39<68:30:37,  1.25s/it, loss=0.0551, lr=1.69e-05, step=3381]Training:   2%|‚ñè         | 3382/200000 [1:12:40<70:43:21,  1.29s/it, loss=0.0551, lr=1.69e-05, step=3381]Training:   2%|‚ñè         | 3382/200000 [1:12:40<70:43:21,  1.29s/it, loss=0.0314, lr=1.69e-05, step=3382]Training:   2%|‚ñè         | 3383/200000 [1:12:41<67:09:09,  1.23s/it, loss=0.0314, lr=1.69e-05, step=3382]Training:   2%|‚ñè         | 3383/200000 [1:12:41<67:09:09,  1.23s/it, loss=0.0245, lr=1.69e-05, step=3383]Training:   2%|‚ñè         | 3384/200000 [1:12:43<70:07:49,  1.28s/it, loss=0.0245, lr=1.69e-05, step=3383]Training:   2%|‚ñè         | 3384/200000 [1:12:43<70:07:49,  1.28s/it, loss=0.0263, lr=1.69e-05, step=3384]Training:   2%|‚ñè         | 3385/200000 [1:12:44<66:42:18,  1.22s/it, loss=0.0263, lr=1.69e-05, step=3384]Training:   2%|‚ñè         | 3385/200000 [1:12:44<66:42:18,  1.22s/it, loss=0.0197, lr=1.69e-05, step=3385]Training:   2%|‚ñè         | 3386/200000 [1:12:45<69:35:43,  1.27s/it, loss=0.0197, lr=1.69e-05, step=3385]Training:   2%|‚ñè         | 3386/200000 [1:12:45<69:35:43,  1.27s/it, loss=0.0489, lr=1.69e-05, step=3386]Training:   2%|‚ñè         | 3387/200000 [1:12:46<66:21:06,  1.21s/it, loss=0.0489, lr=1.69e-05, step=3386]Training:   2%|‚ñè         | 3387/200000 [1:12:46<66:21:06,  1.21s/it, loss=0.0265, lr=1.69e-05, step=3387]Training:   2%|‚ñè         | 3388/200000 [1:12:48<70:39:49,  1.29s/it, loss=0.0265, lr=1.69e-05, step=3387]Training:   2%|‚ñè         | 3388/200000 [1:12:48<70:39:49,  1.29s/it, loss=0.0830, lr=1.69e-05, step=3388]Training:   2%|‚ñè         | 3389/200000 [1:12:49<74:09:21,  1.36s/it, loss=0.0830, lr=1.69e-05, step=3388]Training:   2%|‚ñè         | 3389/200000 [1:12:49<74:09:21,  1.36s/it, loss=0.0169, lr=1.69e-05, step=3389]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3390/200000 [1:12:50<69:31:48,  1.27s/it, loss=0.0169, lr=1.69e-05, step=3389]Training:   2%|‚ñè         | 3390/200000 [1:12:50<69:31:48,  1.27s/it, loss=0.0445, lr=1.69e-05, step=3390]Training:   2%|‚ñè         | 3391/200000 [1:12:52<66:18:33,  1.21s/it, loss=0.0445, lr=1.69e-05, step=3390]Training:   2%|‚ñè         | 3391/200000 [1:12:52<66:18:33,  1.21s/it, loss=0.0346, lr=1.70e-05, step=3391]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3392/200000 [1:12:53<70:14:57,  1.29s/it, loss=0.0346, lr=1.70e-05, step=3391]Training:   2%|‚ñè         | 3392/200000 [1:12:53<70:14:57,  1.29s/it, loss=0.0292, lr=1.70e-05, step=3392]Training:   2%|‚ñè         | 3393/200000 [1:12:54<66:48:27,  1.22s/it, loss=0.0292, lr=1.70e-05, step=3392]Training:   2%|‚ñè         | 3393/200000 [1:12:54<66:48:27,  1.22s/it, loss=0.0268, lr=1.70e-05, step=3393]Training:   2%|‚ñè         | 3394/200000 [1:12:55<69:17:23,  1.27s/it, loss=0.0268, lr=1.70e-05, step=3393]Training:   2%|‚ñè         | 3394/200000 [1:12:55<69:17:23,  1.27s/it, loss=0.0212, lr=1.70e-05, step=3394]Training:   2%|‚ñè         | 3395/200000 [1:12:57<66:09:09,  1.21s/it, loss=0.0212, lr=1.70e-05, step=3394]Training:   2%|‚ñè         | 3395/200000 [1:12:57<66:09:09,  1.21s/it, loss=0.0244, lr=1.70e-05, step=3395]Training:   2%|‚ñè         | 3396/200000 [1:12:58<63:56:34,  1.17s/it, loss=0.0244, lr=1.70e-05, step=3395]Training:   2%|‚ñè         | 3396/200000 [1:12:58<63:56:34,  1.17s/it, loss=0.0878, lr=1.70e-05, step=3396]Training:   2%|‚ñè         | 3397/200000 [1:12:59<68:02:44,  1.25s/it, loss=0.0878, lr=1.70e-05, step=3396]Training:   2%|‚ñè         | 3397/200000 [1:12:59<68:02:44,  1.25s/it, loss=0.0405, lr=1.70e-05, step=3397]Training:   2%|‚ñè         | 3398/200000 [1:13:00<70:37:43,  1.29s/it, loss=0.0405, lr=1.70e-05, step=3397]Training:   2%|‚ñè         | 3398/200000 [1:13:00<70:37:43,  1.29s/it, loss=0.0473, lr=1.70e-05, step=3398]Training:   2%|‚ñè         | 3399/200000 [1:13:02<72:09:16,  1.32s/it, loss=0.0473, lr=1.70e-05, step=3398]Training:   2%|‚ñè         | 3399/200000 [1:13:02<72:09:16,  1.32s/it, loss=0.0273, lr=1.70e-05, step=3399]Training:   2%|‚ñè         | 3400/200000 [1:13:03<68:07:18,  1.25s/it, loss=0.0273, lr=1.70e-05, step=3399]Training:   2%|‚ñè         | 3400/200000 [1:13:03<68:07:18,  1.25s/it, loss=0.0317, lr=1.70e-05, step=3400]00:06:17.793 [I] step=3400 loss=0.0449 lr=1.68e-05 grad_norm=0.72 time=126.3s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3401/200000 [1:13:04<65:19:37,  1.20s/it, loss=0.0317, lr=1.70e-05, step=3400]Training:   2%|‚ñè         | 3401/200000 [1:13:04<65:19:37,  1.20s/it, loss=0.0484, lr=1.70e-05, step=3401]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3402/200000 [1:13:05<67:48:20,  1.24s/it, loss=0.0484, lr=1.70e-05, step=3401]Training:   2%|‚ñè         | 3402/200000 [1:13:05<67:48:20,  1.24s/it, loss=0.0246, lr=1.70e-05, step=3402]Training:   2%|‚ñè         | 3403/200000 [1:13:07<69:08:42,  1.27s/it, loss=0.0246, lr=1.70e-05, step=3402]Training:   2%|‚ñè         | 3403/200000 [1:13:07<69:08:42,  1.27s/it, loss=0.0256, lr=1.70e-05, step=3403]Training:   2%|‚ñè         | 3404/200000 [1:13:08<66:01:56,  1.21s/it, loss=0.0256, lr=1.70e-05, step=3403]Training:   2%|‚ñè         | 3404/200000 [1:13:08<66:01:56,  1.21s/it, loss=0.0348, lr=1.70e-05, step=3404]Training:   2%|‚ñè         | 3405/200000 [1:13:09<67:46:16,  1.24s/it, loss=0.0348, lr=1.70e-05, step=3404]Training:   2%|‚ñè         | 3405/200000 [1:13:09<67:46:16,  1.24s/it, loss=0.0169, lr=1.70e-05, step=3405]Training:   2%|‚ñè         | 3406/200000 [1:13:10<65:03:59,  1.19s/it, loss=0.0169, lr=1.70e-05, step=3405]Training:   2%|‚ñè         | 3406/200000 [1:13:10<65:03:59,  1.19s/it, loss=0.0204, lr=1.70e-05, step=3406]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3407/200000 [1:13:12<68:14:49,  1.25s/it, loss=0.0204, lr=1.70e-05, step=3406]Training:   2%|‚ñè         | 3407/200000 [1:13:12<68:14:49,  1.25s/it, loss=0.0323, lr=1.70e-05, step=3407]Training:   2%|‚ñè         | 3408/200000 [1:13:13<71:00:59,  1.30s/it, loss=0.0323, lr=1.70e-05, step=3407]Training:   2%|‚ñè         | 3408/200000 [1:13:13<71:00:59,  1.30s/it, loss=0.0204, lr=1.70e-05, step=3408]Training:   2%|‚ñè         | 3409/200000 [1:13:14<73:03:43,  1.34s/it, loss=0.0204, lr=1.70e-05, step=3408]Training:   2%|‚ñè         | 3409/200000 [1:13:14<73:03:43,  1.34s/it, loss=0.0373, lr=1.70e-05, step=3409]Training:   2%|‚ñè         | 3410/200000 [1:13:16<74:48:20,  1.37s/it, loss=0.0373, lr=1.70e-05, step=3409]Training:   2%|‚ñè         | 3410/200000 [1:13:16<74:48:20,  1.37s/it, loss=0.0178, lr=1.70e-05, step=3410]Training:   2%|‚ñè         | 3411/200000 [1:13:17<70:01:34,  1.28s/it, loss=0.0178, lr=1.70e-05, step=3410]Training:   2%|‚ñè         | 3411/200000 [1:13:17<70:01:34,  1.28s/it, loss=0.0544, lr=1.71e-05, step=3411]Training:   2%|‚ñè         | 3412/200000 [1:13:18<66:39:54,  1.22s/it, loss=0.0544, lr=1.71e-05, step=3411]Training:   2%|‚ñè         | 3412/200000 [1:13:18<66:39:54,  1.22s/it, loss=0.0329, lr=1.71e-05, step=3412]Training:   2%|‚ñè         | 3413/200000 [1:13:19<69:38:17,  1.28s/it, loss=0.0329, lr=1.71e-05, step=3412]Training:   2%|‚ñè         | 3413/200000 [1:13:19<69:38:17,  1.28s/it, loss=0.0589, lr=1.71e-05, step=3413]Training:   2%|‚ñè         | 3414/200000 [1:13:21<72:21:52,  1.33s/it, loss=0.0589, lr=1.71e-05, step=3413]Training:   2%|‚ñè         | 3414/200000 [1:13:21<72:21:52,  1.33s/it, loss=0.0384, lr=1.71e-05, step=3414]Training:   2%|‚ñè         | 3415/200000 [1:13:22<68:18:20,  1.25s/it, loss=0.0384, lr=1.71e-05, step=3414]Training:   2%|‚ñè         | 3415/200000 [1:13:22<68:18:20,  1.25s/it, loss=0.0326, lr=1.71e-05, step=3415]Training:   2%|‚ñè         | 3416/200000 [1:13:23<71:45:49,  1.31s/it, loss=0.0326, lr=1.71e-05, step=3415]Training:   2%|‚ñè         | 3416/200000 [1:13:23<71:45:49,  1.31s/it, loss=0.0610, lr=1.71e-05, step=3416]Training:   2%|‚ñè         | 3417/200000 [1:13:24<67:52:31,  1.24s/it, loss=0.0610, lr=1.71e-05, step=3416]Training:   2%|‚ñè         | 3417/200000 [1:13:24<67:52:31,  1.24s/it, loss=0.0446, lr=1.71e-05, step=3417]Training:   2%|‚ñè         | 3418/200000 [1:13:26<69:40:49,  1.28s/it, loss=0.0446, lr=1.71e-05, step=3417]Training:   2%|‚ñè         | 3418/200000 [1:13:26<69:40:49,  1.28s/it, loss=0.0211, lr=1.71e-05, step=3418]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3419/200000 [1:13:27<66:23:08,  1.22s/it, loss=0.0211, lr=1.71e-05, step=3418]Training:   2%|‚ñè         | 3419/200000 [1:13:27<66:23:08,  1.22s/it, loss=0.0367, lr=1.71e-05, step=3419]Training:   2%|‚ñè         | 3420/200000 [1:13:28<71:46:56,  1.31s/it, loss=0.0367, lr=1.71e-05, step=3419]Training:   2%|‚ñè         | 3420/200000 [1:13:28<71:46:56,  1.31s/it, loss=0.0303, lr=1.71e-05, step=3420]Training:   2%|‚ñè         | 3421/200000 [1:13:30<74:39:39,  1.37s/it, loss=0.0303, lr=1.71e-05, step=3420]Training:   2%|‚ñè         | 3421/200000 [1:13:30<74:39:39,  1.37s/it, loss=0.0247, lr=1.71e-05, step=3421]Training:   2%|‚ñè         | 3422/200000 [1:13:31<69:53:40,  1.28s/it, loss=0.0247, lr=1.71e-05, step=3421]Training:   2%|‚ñè         | 3422/200000 [1:13:31<69:53:40,  1.28s/it, loss=0.0492, lr=1.71e-05, step=3422]Training:   2%|‚ñè         | 3423/200000 [1:13:32<66:32:25,  1.22s/it, loss=0.0492, lr=1.71e-05, step=3422]Training:   2%|‚ñè         | 3423/200000 [1:13:32<66:32:25,  1.22s/it, loss=0.0244, lr=1.71e-05, step=3423]Training:   2%|‚ñè         | 3424/200000 [1:13:34<70:53:49,  1.30s/it, loss=0.0244, lr=1.71e-05, step=3423]Training:   2%|‚ñè         | 3424/200000 [1:13:34<70:53:49,  1.30s/it, loss=0.0802, lr=1.71e-05, step=3424]Training:   2%|‚ñè         | 3425/200000 [1:13:35<67:17:52,  1.23s/it, loss=0.0802, lr=1.71e-05, step=3424]Training:   2%|‚ñè         | 3425/200000 [1:13:35<67:17:52,  1.23s/it, loss=0.0263, lr=1.71e-05, step=3425]Training:   2%|‚ñè         | 3426/200000 [1:13:36<68:47:23,  1.26s/it, loss=0.0263, lr=1.71e-05, step=3425]Training:   2%|‚ñè         | 3426/200000 [1:13:36<68:47:23,  1.26s/it, loss=0.0372, lr=1.71e-05, step=3426]Training:   2%|‚ñè         | 3427/200000 [1:13:37<65:46:04,  1.20s/it, loss=0.0372, lr=1.71e-05, step=3426]Training:   2%|‚ñè         | 3427/200000 [1:13:37<65:46:04,  1.20s/it, loss=0.0273, lr=1.71e-05, step=3427]Training:   2%|‚ñè         | 3428/200000 [1:13:38<63:39:31,  1.17s/it, loss=0.0273, lr=1.71e-05, step=3427]Training:   2%|‚ñè         | 3428/200000 [1:13:38<63:39:31,  1.17s/it, loss=0.0383, lr=1.71e-05, step=3428]Training:   2%|‚ñè         | 3429/200000 [1:13:40<68:12:38,  1.25s/it, loss=0.0383, lr=1.71e-05, step=3428]Training:   2%|‚ñè         | 3429/200000 [1:13:40<68:12:38,  1.25s/it, loss=0.0399, lr=1.71e-05, step=3429]Training:   2%|‚ñè         | 3430/200000 [1:13:41<70:33:55,  1.29s/it, loss=0.0399, lr=1.71e-05, step=3429]Training:   2%|‚ñè         | 3430/200000 [1:13:41<70:33:55,  1.29s/it, loss=0.0396, lr=1.71e-05, step=3430]Training:   2%|‚ñè         | 3431/200000 [1:13:42<72:59:01,  1.34s/it, loss=0.0396, lr=1.71e-05, step=3430]Training:   2%|‚ñè         | 3431/200000 [1:13:42<72:59:01,  1.34s/it, loss=0.0225, lr=1.72e-05, step=3431]Training:   2%|‚ñè         | 3432/200000 [1:13:43<68:42:13,  1.26s/it, loss=0.0225, lr=1.72e-05, step=3431]Training:   2%|‚ñè         | 3432/200000 [1:13:43<68:42:13,  1.26s/it, loss=0.0219, lr=1.72e-05, step=3432]Training:   2%|‚ñè         | 3433/200000 [1:13:44<65:43:28,  1.20s/it, loss=0.0219, lr=1.72e-05, step=3432]Training:   2%|‚ñè         | 3433/200000 [1:13:44<65:43:28,  1.20s/it, loss=0.0241, lr=1.72e-05, step=3433]Training:   2%|‚ñè         | 3434/200000 [1:13:46<68:14:05,  1.25s/it, loss=0.0241, lr=1.72e-05, step=3433]Training:   2%|‚ñè         | 3434/200000 [1:13:46<68:14:05,  1.25s/it, loss=0.0335, lr=1.72e-05, step=3434]Training:   2%|‚ñè         | 3435/200000 [1:13:47<70:44:07,  1.30s/it, loss=0.0335, lr=1.72e-05, step=3434]Training:   2%|‚ñè         | 3435/200000 [1:13:47<70:44:07,  1.30s/it, loss=0.0223, lr=1.72e-05, step=3435]Training:   2%|‚ñè         | 3436/200000 [1:13:48<67:08:24,  1.23s/it, loss=0.0223, lr=1.72e-05, step=3435]Training:   2%|‚ñè         | 3436/200000 [1:13:48<67:08:24,  1.23s/it, loss=0.0288, lr=1.72e-05, step=3436]Training:   2%|‚ñè         | 3437/200000 [1:13:50<70:30:42,  1.29s/it, loss=0.0288, lr=1.72e-05, step=3436]Training:   2%|‚ñè         | 3437/200000 [1:13:50<70:30:42,  1.29s/it, loss=0.0355, lr=1.72e-05, step=3437]Training:   2%|‚ñè         | 3438/200000 [1:13:51<66:59:20,  1.23s/it, loss=0.0355, lr=1.72e-05, step=3437]Training:   2%|‚ñè         | 3438/200000 [1:13:51<66:59:20,  1.23s/it, loss=0.0298, lr=1.72e-05, step=3438]Training:   2%|‚ñè         | 3439/200000 [1:13:52<69:39:46,  1.28s/it, loss=0.0298, lr=1.72e-05, step=3438]Training:   2%|‚ñè         | 3439/200000 [1:13:52<69:39:46,  1.28s/it, loss=0.0494, lr=1.72e-05, step=3439]Training:   2%|‚ñè         | 3440/200000 [1:13:53<66:23:02,  1.22s/it, loss=0.0494, lr=1.72e-05, step=3439]Training:   2%|‚ñè         | 3440/200000 [1:13:53<66:23:02,  1.22s/it, loss=0.0290, lr=1.72e-05, step=3440]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3441/200000 [1:13:55<70:52:58,  1.30s/it, loss=0.0290, lr=1.72e-05, step=3440]Training:   2%|‚ñè         | 3441/200000 [1:13:55<70:52:58,  1.30s/it, loss=0.0337, lr=1.72e-05, step=3441]Training:   2%|‚ñè         | 3442/200000 [1:13:56<73:29:37,  1.35s/it, loss=0.0337, lr=1.72e-05, step=3441]Training:   2%|‚ñè         | 3442/200000 [1:13:56<73:29:37,  1.35s/it, loss=0.0294, lr=1.72e-05, step=3442]Training:   2%|‚ñè         | 3443/200000 [1:13:57<69:05:44,  1.27s/it, loss=0.0294, lr=1.72e-05, step=3442]Training:   2%|‚ñè         | 3443/200000 [1:13:57<69:05:44,  1.27s/it, loss=0.0139, lr=1.72e-05, step=3443]Training:   2%|‚ñè         | 3444/200000 [1:13:58<66:02:00,  1.21s/it, loss=0.0139, lr=1.72e-05, step=3443]Training:   2%|‚ñè         | 3444/200000 [1:13:58<66:02:00,  1.21s/it, loss=0.0243, lr=1.72e-05, step=3444]Training:   2%|‚ñè         | 3445/200000 [1:14:00<70:15:28,  1.29s/it, loss=0.0243, lr=1.72e-05, step=3444]Training:   2%|‚ñè         | 3445/200000 [1:14:00<70:15:28,  1.29s/it, loss=0.0215, lr=1.72e-05, step=3445]Training:   2%|‚ñè         | 3446/200000 [1:14:01<66:47:25,  1.22s/it, loss=0.0215, lr=1.72e-05, step=3445]Training:   2%|‚ñè         | 3446/200000 [1:14:01<66:47:25,  1.22s/it, loss=0.0200, lr=1.72e-05, step=3446]Training:   2%|‚ñè         | 3447/200000 [1:14:02<67:47:29,  1.24s/it, loss=0.0200, lr=1.72e-05, step=3446]Training:   2%|‚ñè         | 3447/200000 [1:14:02<67:47:29,  1.24s/it, loss=0.0193, lr=1.72e-05, step=3447]Training:   2%|‚ñè         | 3448/200000 [1:14:03<65:05:09,  1.19s/it, loss=0.0193, lr=1.72e-05, step=3447]Training:   2%|‚ñè         | 3448/200000 [1:14:03<65:05:09,  1.19s/it, loss=0.0412, lr=1.72e-05, step=3448]Training:   2%|‚ñè         | 3449/200000 [1:14:04<63:13:31,  1.16s/it, loss=0.0412, lr=1.72e-05, step=3448]Training:   2%|‚ñè         | 3449/200000 [1:14:04<63:13:31,  1.16s/it, loss=0.0574, lr=1.72e-05, step=3449]Training:   2%|‚ñè         | 3450/200000 [1:14:06<67:32:33,  1.24s/it, loss=0.0574, lr=1.72e-05, step=3449]Training:   2%|‚ñè         | 3450/200000 [1:14:06<67:32:33,  1.24s/it, loss=0.0418, lr=1.72e-05, step=3450]Training:   2%|‚ñè         | 3451/200000 [1:14:07<70:06:28,  1.28s/it, loss=0.0418, lr=1.72e-05, step=3450]Training:   2%|‚ñè         | 3451/200000 [1:14:07<70:06:28,  1.28s/it, loss=0.0278, lr=1.73e-05, step=3451]Training:   2%|‚ñè         | 3452/200000 [1:14:09<71:56:42,  1.32s/it, loss=0.0278, lr=1.73e-05, step=3451]Training:   2%|‚ñè         | 3452/200000 [1:14:09<71:56:42,  1.32s/it, loss=0.0220, lr=1.73e-05, step=3452]Training:   2%|‚ñè         | 3453/200000 [1:14:10<67:59:46,  1.25s/it, loss=0.0220, lr=1.73e-05, step=3452]Training:   2%|‚ñè         | 3453/200000 [1:14:10<67:59:46,  1.25s/it, loss=0.0245, lr=1.73e-05, step=3453]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3454/200000 [1:14:11<65:14:36,  1.20s/it, loss=0.0245, lr=1.73e-05, step=3453]Training:   2%|‚ñè         | 3454/200000 [1:14:11<65:14:36,  1.20s/it, loss=0.0215, lr=1.73e-05, step=3454]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3455/200000 [1:14:12<68:10:40,  1.25s/it, loss=0.0215, lr=1.73e-05, step=3454]Training:   2%|‚ñè         | 3455/200000 [1:14:12<68:10:40,  1.25s/it, loss=0.0316, lr=1.73e-05, step=3455]Training:   2%|‚ñè         | 3456/200000 [1:14:13<69:45:35,  1.28s/it, loss=0.0316, lr=1.73e-05, step=3455]Training:   2%|‚ñè         | 3456/200000 [1:14:13<69:45:35,  1.28s/it, loss=0.0211, lr=1.73e-05, step=3456]Training:   2%|‚ñè         | 3457/200000 [1:14:15<66:29:47,  1.22s/it, loss=0.0211, lr=1.73e-05, step=3456]Training:   2%|‚ñè         | 3457/200000 [1:14:15<66:29:47,  1.22s/it, loss=0.0129, lr=1.73e-05, step=3457]Training:   2%|‚ñè         | 3458/200000 [1:14:16<68:31:23,  1.26s/it, loss=0.0129, lr=1.73e-05, step=3457]Training:   2%|‚ñè         | 3458/200000 [1:14:16<68:31:23,  1.26s/it, loss=0.0338, lr=1.73e-05, step=3458]Training:   2%|‚ñè         | 3459/200000 [1:14:17<65:39:29,  1.20s/it, loss=0.0338, lr=1.73e-05, step=3458]Training:   2%|‚ñè         | 3459/200000 [1:14:17<65:39:29,  1.20s/it, loss=0.0389, lr=1.73e-05, step=3459]Training:   2%|‚ñè         | 3460/200000 [1:14:18<68:39:03,  1.26s/it, loss=0.0389, lr=1.73e-05, step=3459]Training:   2%|‚ñè         | 3460/200000 [1:14:18<68:39:03,  1.26s/it, loss=0.0279, lr=1.73e-05, step=3460]Training:   2%|‚ñè         | 3461/200000 [1:14:20<71:55:46,  1.32s/it, loss=0.0279, lr=1.73e-05, step=3460]Training:   2%|‚ñè         | 3461/200000 [1:14:20<71:55:46,  1.32s/it, loss=0.0259, lr=1.73e-05, step=3461]Training:   2%|‚ñè         | 3462/200000 [1:14:21<73:35:17,  1.35s/it, loss=0.0259, lr=1.73e-05, step=3461]Training:   2%|‚ñè         | 3462/200000 [1:14:21<73:35:17,  1.35s/it, loss=0.0351, lr=1.73e-05, step=3462]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3463/200000 [1:14:23<74:23:53,  1.36s/it, loss=0.0351, lr=1.73e-05, step=3462]Training:   2%|‚ñè         | 3463/200000 [1:14:23<74:23:53,  1.36s/it, loss=0.0283, lr=1.73e-05, step=3463]Training:   2%|‚ñè         | 3464/200000 [1:14:24<69:40:49,  1.28s/it, loss=0.0283, lr=1.73e-05, step=3463]Training:   2%|‚ñè         | 3464/200000 [1:14:24<69:40:49,  1.28s/it, loss=0.0432, lr=1.73e-05, step=3464]Training:   2%|‚ñè         | 3465/200000 [1:14:25<66:24:40,  1.22s/it, loss=0.0432, lr=1.73e-05, step=3464]Training:   2%|‚ñè         | 3465/200000 [1:14:25<66:24:40,  1.22s/it, loss=0.0264, lr=1.73e-05, step=3465]Training:   2%|‚ñè         | 3466/200000 [1:14:26<69:31:59,  1.27s/it, loss=0.0264, lr=1.73e-05, step=3465]Training:   2%|‚ñè         | 3466/200000 [1:14:26<69:31:59,  1.27s/it, loss=0.0227, lr=1.73e-05, step=3466]Training:   2%|‚ñè         | 3467/200000 [1:14:28<71:32:32,  1.31s/it, loss=0.0227, lr=1.73e-05, step=3466]Training:   2%|‚ñè         | 3467/200000 [1:14:28<71:32:32,  1.31s/it, loss=0.0162, lr=1.73e-05, step=3467]Training:   2%|‚ñè         | 3468/200000 [1:14:29<67:41:05,  1.24s/it, loss=0.0162, lr=1.73e-05, step=3467]Training:   2%|‚ñè         | 3468/200000 [1:14:29<67:41:05,  1.24s/it, loss=0.0581, lr=1.73e-05, step=3468]Training:   2%|‚ñè         | 3469/200000 [1:14:30<71:15:19,  1.31s/it, loss=0.0581, lr=1.73e-05, step=3468]Training:   2%|‚ñè         | 3469/200000 [1:14:30<71:15:19,  1.31s/it, loss=0.0263, lr=1.73e-05, step=3469]Training:   2%|‚ñè         | 3470/200000 [1:14:31<67:28:12,  1.24s/it, loss=0.0263, lr=1.73e-05, step=3469]Training:   2%|‚ñè         | 3470/200000 [1:14:31<67:28:12,  1.24s/it, loss=0.0236, lr=1.73e-05, step=3470]Training:   2%|‚ñè         | 3471/200000 [1:14:33<69:36:45,  1.28s/it, loss=0.0236, lr=1.73e-05, step=3470]Training:   2%|‚ñè         | 3471/200000 [1:14:33<69:36:45,  1.28s/it, loss=0.0316, lr=1.74e-05, step=3471]Training:   2%|‚ñè         | 3472/200000 [1:14:34<66:18:55,  1.21s/it, loss=0.0316, lr=1.74e-05, step=3471]Training:   2%|‚ñè         | 3472/200000 [1:14:34<66:18:55,  1.21s/it, loss=0.0289, lr=1.74e-05, step=3472]Training:   2%|‚ñè         | 3473/200000 [1:14:35<71:06:20,  1.30s/it, loss=0.0289, lr=1.74e-05, step=3472]Training:   2%|‚ñè         | 3473/200000 [1:14:35<71:06:20,  1.30s/it, loss=0.0817, lr=1.74e-05, step=3473]Training:   2%|‚ñè         | 3474/200000 [1:14:37<74:40:14,  1.37s/it, loss=0.0817, lr=1.74e-05, step=3473]Training:   2%|‚ñè         | 3474/200000 [1:14:37<74:40:14,  1.37s/it, loss=0.0352, lr=1.74e-05, step=3474]Training:   2%|‚ñè         | 3475/200000 [1:14:38<69:53:10,  1.28s/it, loss=0.0352, lr=1.74e-05, step=3474]Training:   2%|‚ñè         | 3475/200000 [1:14:38<69:53:10,  1.28s/it, loss=0.0266, lr=1.74e-05, step=3475]Training:   2%|‚ñè         | 3476/200000 [1:14:39<66:33:13,  1.22s/it, loss=0.0266, lr=1.74e-05, step=3475]Training:   2%|‚ñè         | 3476/200000 [1:14:39<66:33:13,  1.22s/it, loss=0.0243, lr=1.74e-05, step=3476]Training:   2%|‚ñè         | 3477/200000 [1:14:40<70:48:58,  1.30s/it, loss=0.0243, lr=1.74e-05, step=3476]Training:   2%|‚ñè         | 3477/200000 [1:14:40<70:48:58,  1.30s/it, loss=0.0359, lr=1.74e-05, step=3477]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3478/200000 [1:14:41<67:11:51,  1.23s/it, loss=0.0359, lr=1.74e-05, step=3477]Training:   2%|‚ñè         | 3478/200000 [1:14:41<67:11:51,  1.23s/it, loss=0.0569, lr=1.74e-05, step=3478]Training:   2%|‚ñè         | 3479/200000 [1:14:43<69:25:51,  1.27s/it, loss=0.0569, lr=1.74e-05, step=3478]Training:   2%|‚ñè         | 3479/200000 [1:14:43<69:25:51,  1.27s/it, loss=0.0191, lr=1.74e-05, step=3479]Training:   2%|‚ñè         | 3480/200000 [1:14:44<66:14:33,  1.21s/it, loss=0.0191, lr=1.74e-05, step=3479]Training:   2%|‚ñè         | 3480/200000 [1:14:44<66:14:33,  1.21s/it, loss=0.0361, lr=1.74e-05, step=3480]Training:   2%|‚ñè         | 3481/200000 [1:14:45<64:00:28,  1.17s/it, loss=0.0361, lr=1.74e-05, step=3480]Training:   2%|‚ñè         | 3481/200000 [1:14:45<64:00:28,  1.17s/it, loss=0.0242, lr=1.74e-05, step=3481]Training:   2%|‚ñè         | 3482/200000 [1:14:46<69:06:35,  1.27s/it, loss=0.0242, lr=1.74e-05, step=3481]Training:   2%|‚ñè         | 3482/200000 [1:14:46<69:06:35,  1.27s/it, loss=0.0204, lr=1.74e-05, step=3482]Training:   2%|‚ñè         | 3483/200000 [1:14:48<72:16:44,  1.32s/it, loss=0.0204, lr=1.74e-05, step=3482]Training:   2%|‚ñè         | 3483/200000 [1:14:48<72:16:44,  1.32s/it, loss=0.2697, lr=1.74e-05, step=3483]Training:   2%|‚ñè         | 3484/200000 [1:14:49<73:59:14,  1.36s/it, loss=0.2697, lr=1.74e-05, step=3483]Training:   2%|‚ñè         | 3484/200000 [1:14:49<73:59:14,  1.36s/it, loss=0.0305, lr=1.74e-05, step=3484]Training:   2%|‚ñè         | 3485/200000 [1:14:50<69:27:15,  1.27s/it, loss=0.0305, lr=1.74e-05, step=3484]Training:   2%|‚ñè         | 3485/200000 [1:14:50<69:27:15,  1.27s/it, loss=0.0215, lr=1.74e-05, step=3485]Training:   2%|‚ñè         | 3486/200000 [1:14:51<66:15:33,  1.21s/it, loss=0.0215, lr=1.74e-05, step=3485]Training:   2%|‚ñè         | 3486/200000 [1:14:51<66:15:33,  1.21s/it, loss=0.0402, lr=1.74e-05, step=3486]Training:   2%|‚ñè         | 3487/200000 [1:14:53<68:29:43,  1.25s/it, loss=0.0402, lr=1.74e-05, step=3486]Training:   2%|‚ñè         | 3487/200000 [1:14:53<68:29:43,  1.25s/it, loss=0.0242, lr=1.74e-05, step=3487]Training:   2%|‚ñè         | 3488/200000 [1:14:54<71:11:18,  1.30s/it, loss=0.0242, lr=1.74e-05, step=3487]Training:   2%|‚ñè         | 3488/200000 [1:14:54<71:11:18,  1.30s/it, loss=0.0275, lr=1.74e-05, step=3488]Training:   2%|‚ñè         | 3489/200000 [1:14:55<67:28:30,  1.24s/it, loss=0.0275, lr=1.74e-05, step=3488]Training:   2%|‚ñè         | 3489/200000 [1:14:55<67:28:30,  1.24s/it, loss=0.0245, lr=1.74e-05, step=3489]Training:   2%|‚ñè         | 3490/200000 [1:14:57<70:49:19,  1.30s/it, loss=0.0245, lr=1.74e-05, step=3489]Training:   2%|‚ñè         | 3490/200000 [1:14:57<70:49:19,  1.30s/it, loss=0.0243, lr=1.74e-05, step=3490]Training:   2%|‚ñè         | 3491/200000 [1:14:58<67:12:43,  1.23s/it, loss=0.0243, lr=1.74e-05, step=3490]Training:   2%|‚ñè         | 3491/200000 [1:14:58<67:12:43,  1.23s/it, loss=0.0256, lr=1.75e-05, step=3491]Training:   2%|‚ñè         | 3492/200000 [1:14:59<70:05:42,  1.28s/it, loss=0.0256, lr=1.75e-05, step=3491]Training:   2%|‚ñè         | 3492/200000 [1:14:59<70:05:42,  1.28s/it, loss=0.0230, lr=1.75e-05, step=3492]Training:   2%|‚ñè         | 3493/200000 [1:15:00<66:38:22,  1.22s/it, loss=0.0230, lr=1.75e-05, step=3492]Training:   2%|‚ñè         | 3493/200000 [1:15:00<66:38:22,  1.22s/it, loss=0.0220, lr=1.75e-05, step=3493]Training:   2%|‚ñè         | 3494/200000 [1:15:02<70:55:58,  1.30s/it, loss=0.0220, lr=1.75e-05, step=3493]Training:   2%|‚ñè         | 3494/200000 [1:15:02<70:55:58,  1.30s/it, loss=0.0319, lr=1.75e-05, step=3494]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3495/200000 [1:15:03<74:11:21,  1.36s/it, loss=0.0319, lr=1.75e-05, step=3494]Training:   2%|‚ñè         | 3495/200000 [1:15:03<74:11:21,  1.36s/it, loss=0.0296, lr=1.75e-05, step=3495]Training:   2%|‚ñè         | 3496/200000 [1:15:04<69:32:41,  1.27s/it, loss=0.0296, lr=1.75e-05, step=3495]Training:   2%|‚ñè         | 3496/200000 [1:15:04<69:32:41,  1.27s/it, loss=0.0375, lr=1.75e-05, step=3496]Training:   2%|‚ñè         | 3497/200000 [1:15:05<66:19:18,  1.22s/it, loss=0.0375, lr=1.75e-05, step=3496]Training:   2%|‚ñè         | 3497/200000 [1:15:05<66:19:18,  1.22s/it, loss=0.0325, lr=1.75e-05, step=3497]Training:   2%|‚ñè         | 3498/200000 [1:15:07<70:21:37,  1.29s/it, loss=0.0325, lr=1.75e-05, step=3497]Training:   2%|‚ñè         | 3498/200000 [1:15:07<70:21:37,  1.29s/it, loss=0.0294, lr=1.75e-05, step=3498]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3499/200000 [1:15:08<66:53:18,  1.23s/it, loss=0.0294, lr=1.75e-05, step=3498]Training:   2%|‚ñè         | 3499/200000 [1:15:08<66:53:18,  1.23s/it, loss=0.0174, lr=1.75e-05, step=3499]Training:   2%|‚ñè         | 3500/200000 [1:15:09<69:03:08,  1.27s/it, loss=0.0174, lr=1.75e-05, step=3499]Training:   2%|‚ñè         | 3500/200000 [1:15:09<69:03:08,  1.27s/it, loss=0.0375, lr=1.75e-05, step=3500]00:08:24.191 [I] step=3500 loss=0.0339 lr=1.73e-05 grad_norm=0.71 time=126.4s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3501/200000 [1:15:10<66:00:53,  1.21s/it, loss=0.0375, lr=1.75e-05, step=3500]Training:   2%|‚ñè         | 3501/200000 [1:15:10<66:00:53,  1.21s/it, loss=0.0218, lr=1.75e-05, step=3501]Training:   2%|‚ñè         | 3502/200000 [1:15:11<63:51:18,  1.17s/it, loss=0.0218, lr=1.75e-05, step=3501]Training:   2%|‚ñè         | 3502/200000 [1:15:11<63:51:18,  1.17s/it, loss=0.0293, lr=1.75e-05, step=3502]Training:   2%|‚ñè         | 3503/200000 [1:15:13<67:58:05,  1.25s/it, loss=0.0293, lr=1.75e-05, step=3502]Training:   2%|‚ñè         | 3503/200000 [1:15:13<67:58:05,  1.25s/it, loss=0.0343, lr=1.75e-05, step=3503]Training:   2%|‚ñè         | 3504/200000 [1:15:14<70:37:48,  1.29s/it, loss=0.0343, lr=1.75e-05, step=3503]Training:   2%|‚ñè         | 3504/200000 [1:15:14<70:37:48,  1.29s/it, loss=0.0300, lr=1.75e-05, step=3504]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3505/200000 [1:15:16<72:52:59,  1.34s/it, loss=0.0300, lr=1.75e-05, step=3504]Training:   2%|‚ñè         | 3505/200000 [1:15:16<72:52:59,  1.34s/it, loss=0.0271, lr=1.75e-05, step=3505]Training:   2%|‚ñè         | 3506/200000 [1:15:17<68:38:52,  1.26s/it, loss=0.0271, lr=1.75e-05, step=3505]Training:   2%|‚ñè         | 3506/200000 [1:15:17<68:38:52,  1.26s/it, loss=0.0189, lr=1.75e-05, step=3506]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3507/200000 [1:15:18<65:41:25,  1.20s/it, loss=0.0189, lr=1.75e-05, step=3506]Training:   2%|‚ñè         | 3507/200000 [1:15:18<65:41:25,  1.20s/it, loss=0.0285, lr=1.75e-05, step=3507]Training:   2%|‚ñè         | 3508/200000 [1:15:19<67:59:52,  1.25s/it, loss=0.0285, lr=1.75e-05, step=3507]Training:   2%|‚ñè         | 3508/200000 [1:15:19<67:59:52,  1.25s/it, loss=0.0333, lr=1.75e-05, step=3508]Training:   2%|‚ñè         | 3509/200000 [1:15:21<69:09:28,  1.27s/it, loss=0.0333, lr=1.75e-05, step=3508]Training:   2%|‚ñè         | 3509/200000 [1:15:21<69:09:28,  1.27s/it, loss=0.0233, lr=1.75e-05, step=3509]Training:   2%|‚ñè         | 3510/200000 [1:15:22<66:34:20,  1.22s/it, loss=0.0233, lr=1.75e-05, step=3509]Training:   2%|‚ñè         | 3510/200000 [1:15:22<66:34:20,  1.22s/it, loss=0.0347, lr=1.75e-05, step=3510]Training:   2%|‚ñè         | 3511/200000 [1:15:23<68:15:38,  1.25s/it, loss=0.0347, lr=1.75e-05, step=3510]Training:   2%|‚ñè         | 3511/200000 [1:15:23<68:15:38,  1.25s/it, loss=0.0311, lr=1.76e-05, step=3511]Training:   2%|‚ñè         | 3512/200000 [1:15:24<65:22:00,  1.20s/it, loss=0.0311, lr=1.76e-05, step=3511]Training:   2%|‚ñè         | 3512/200000 [1:15:24<65:22:00,  1.20s/it, loss=0.0267, lr=1.76e-05, step=3512]Training:   2%|‚ñè         | 3513/200000 [1:15:25<68:22:56,  1.25s/it, loss=0.0267, lr=1.76e-05, step=3512]Training:   2%|‚ñè         | 3513/200000 [1:15:25<68:22:56,  1.25s/it, loss=0.0166, lr=1.76e-05, step=3513]Training:   2%|‚ñè         | 3514/200000 [1:15:27<71:46:49,  1.32s/it, loss=0.0166, lr=1.76e-05, step=3513]Training:   2%|‚ñè         | 3514/200000 [1:15:27<71:46:49,  1.32s/it, loss=0.0190, lr=1.76e-05, step=3514]Training:   2%|‚ñè         | 3515/200000 [1:15:28<73:37:45,  1.35s/it, loss=0.0190, lr=1.76e-05, step=3514]Training:   2%|‚ñè         | 3515/200000 [1:15:28<73:37:45,  1.35s/it, loss=0.0274, lr=1.76e-05, step=3515]Training:   2%|‚ñè         | 3516/200000 [1:15:30<74:38:04,  1.37s/it, loss=0.0274, lr=1.76e-05, step=3515]Training:   2%|‚ñè         | 3516/200000 [1:15:30<74:38:04,  1.37s/it, loss=0.0276, lr=1.76e-05, step=3516]Training:   2%|‚ñè         | 3517/200000 [1:15:31<69:58:50,  1.28s/it, loss=0.0276, lr=1.76e-05, step=3516]Training:   2%|‚ñè         | 3517/200000 [1:15:31<69:58:50,  1.28s/it, loss=0.0286, lr=1.76e-05, step=3517]Training:   2%|‚ñè         | 3518/200000 [1:15:32<66:37:17,  1.22s/it, loss=0.0286, lr=1.76e-05, step=3517]Training:   2%|‚ñè         | 3518/200000 [1:15:32<66:37:17,  1.22s/it, loss=0.0252, lr=1.76e-05, step=3518]Training:   2%|‚ñè         | 3519/200000 [1:15:33<69:36:17,  1.28s/it, loss=0.0252, lr=1.76e-05, step=3518]Training:   2%|‚ñè         | 3519/200000 [1:15:33<69:36:17,  1.28s/it, loss=0.0312, lr=1.76e-05, step=3519]Training:   2%|‚ñè         | 3520/200000 [1:15:35<72:10:06,  1.32s/it, loss=0.0312, lr=1.76e-05, step=3519]Training:   2%|‚ñè         | 3520/200000 [1:15:35<72:10:06,  1.32s/it, loss=0.0841, lr=1.76e-05, step=3520]Training:   2%|‚ñè         | 3521/200000 [1:15:36<68:05:52,  1.25s/it, loss=0.0841, lr=1.76e-05, step=3520]Training:   2%|‚ñè         | 3521/200000 [1:15:36<68:05:52,  1.25s/it, loss=0.0242, lr=1.76e-05, step=3521]Training:   2%|‚ñè         | 3522/200000 [1:15:37<71:50:16,  1.32s/it, loss=0.0242, lr=1.76e-05, step=3521]Training:   2%|‚ñè         | 3522/200000 [1:15:37<71:50:16,  1.32s/it, loss=0.0253, lr=1.76e-05, step=3522]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3523/200000 [1:15:38<67:55:17,  1.24s/it, loss=0.0253, lr=1.76e-05, step=3522]Training:   2%|‚ñè         | 3523/200000 [1:15:38<67:55:17,  1.24s/it, loss=0.0318, lr=1.76e-05, step=3523]Training:   2%|‚ñè         | 3524/200000 [1:15:40<69:42:26,  1.28s/it, loss=0.0318, lr=1.76e-05, step=3523]Training:   2%|‚ñè         | 3524/200000 [1:15:40<69:42:26,  1.28s/it, loss=0.0493, lr=1.76e-05, step=3524]Training:   2%|‚ñè         | 3525/200000 [1:15:41<66:23:36,  1.22s/it, loss=0.0493, lr=1.76e-05, step=3524]Training:   2%|‚ñè         | 3525/200000 [1:15:41<66:23:36,  1.22s/it, loss=0.0252, lr=1.76e-05, step=3525]Training:   2%|‚ñè         | 3526/200000 [1:15:42<71:02:23,  1.30s/it, loss=0.0252, lr=1.76e-05, step=3525]Training:   2%|‚ñè         | 3526/200000 [1:15:42<71:02:23,  1.30s/it, loss=0.0324, lr=1.76e-05, step=3526]Training:   2%|‚ñè         | 3527/200000 [1:15:44<74:02:55,  1.36s/it, loss=0.0324, lr=1.76e-05, step=3526]Training:   2%|‚ñè         | 3527/200000 [1:15:44<74:02:55,  1.36s/it, loss=0.0333, lr=1.76e-05, step=3527]Training:   2%|‚ñè         | 3528/200000 [1:15:45<69:26:16,  1.27s/it, loss=0.0333, lr=1.76e-05, step=3527]Training:   2%|‚ñè         | 3528/200000 [1:15:45<69:26:16,  1.27s/it, loss=0.0403, lr=1.76e-05, step=3528]Training:   2%|‚ñè         | 3529/200000 [1:15:46<66:16:07,  1.21s/it, loss=0.0403, lr=1.76e-05, step=3528]Training:   2%|‚ñè         | 3529/200000 [1:15:46<66:16:07,  1.21s/it, loss=0.0262, lr=1.76e-05, step=3529]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3530/200000 [1:15:47<70:35:40,  1.29s/it, loss=0.0262, lr=1.76e-05, step=3529]Training:   2%|‚ñè         | 3530/200000 [1:15:47<70:35:40,  1.29s/it, loss=0.0302, lr=1.76e-05, step=3530]Training:   2%|‚ñè         | 3531/200000 [1:15:48<67:01:58,  1.23s/it, loss=0.0302, lr=1.76e-05, step=3530]Training:   2%|‚ñè         | 3531/200000 [1:15:48<67:01:58,  1.23s/it, loss=0.0283, lr=1.77e-05, step=3531]Training:   2%|‚ñè         | 3532/200000 [1:15:50<68:36:53,  1.26s/it, loss=0.0283, lr=1.77e-05, step=3531]Training:   2%|‚ñè         | 3532/200000 [1:15:50<68:36:53,  1.26s/it, loss=0.0223, lr=1.77e-05, step=3532]Training:   2%|‚ñè         | 3533/200000 [1:15:51<65:40:02,  1.20s/it, loss=0.0223, lr=1.77e-05, step=3532]Training:   2%|‚ñè         | 3533/200000 [1:15:51<65:40:02,  1.20s/it, loss=0.0289, lr=1.77e-05, step=3533]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3534/200000 [1:15:52<63:36:13,  1.17s/it, loss=0.0289, lr=1.77e-05, step=3533]Training:   2%|‚ñè         | 3534/200000 [1:15:52<63:36:13,  1.17s/it, loss=0.0273, lr=1.77e-05, step=3534]Training:   2%|‚ñè         | 3535/200000 [1:15:53<68:12:24,  1.25s/it, loss=0.0273, lr=1.77e-05, step=3534]Training:   2%|‚ñè         | 3535/200000 [1:15:53<68:12:24,  1.25s/it, loss=0.0387, lr=1.77e-05, step=3535]Training:   2%|‚ñè         | 3536/200000 [1:15:55<70:59:53,  1.30s/it, loss=0.0387, lr=1.77e-05, step=3535]Training:   2%|‚ñè         | 3536/200000 [1:15:55<70:59:53,  1.30s/it, loss=0.0199, lr=1.77e-05, step=3536]Training:   2%|‚ñè         | 3537/200000 [1:15:56<73:04:32,  1.34s/it, loss=0.0199, lr=1.77e-05, step=3536]Training:   2%|‚ñè         | 3537/200000 [1:15:56<73:04:32,  1.34s/it, loss=0.0238, lr=1.77e-05, step=3537]Training:   2%|‚ñè         | 3538/200000 [1:15:57<68:47:36,  1.26s/it, loss=0.0238, lr=1.77e-05, step=3537]Training:   2%|‚ñè         | 3538/200000 [1:15:57<68:47:36,  1.26s/it, loss=0.0424, lr=1.77e-05, step=3538]Training:   2%|‚ñè         | 3539/200000 [1:15:58<65:47:10,  1.21s/it, loss=0.0424, lr=1.77e-05, step=3538]Training:   2%|‚ñè         | 3539/200000 [1:15:58<65:47:10,  1.21s/it, loss=0.0209, lr=1.77e-05, step=3539]Training:   2%|‚ñè         | 3540/200000 [1:16:00<68:11:32,  1.25s/it, loss=0.0209, lr=1.77e-05, step=3539]Training:   2%|‚ñè         | 3540/200000 [1:16:00<68:11:32,  1.25s/it, loss=0.0199, lr=1.77e-05, step=3540]Training:   2%|‚ñè         | 3541/200000 [1:16:01<70:23:54,  1.29s/it, loss=0.0199, lr=1.77e-05, step=3540]Training:   2%|‚ñè         | 3541/200000 [1:16:01<70:23:54,  1.29s/it, loss=0.0217, lr=1.77e-05, step=3541]Training:   2%|‚ñè         | 3542/200000 [1:16:02<66:54:59,  1.23s/it, loss=0.0217, lr=1.77e-05, step=3541]Training:   2%|‚ñè         | 3542/200000 [1:16:02<66:54:59,  1.23s/it, loss=0.0258, lr=1.77e-05, step=3542]Training:   2%|‚ñè         | 3543/200000 [1:16:04<70:04:00,  1.28s/it, loss=0.0258, lr=1.77e-05, step=3542]Training:   2%|‚ñè         | 3543/200000 [1:16:04<70:04:00,  1.28s/it, loss=0.0149, lr=1.77e-05, step=3543]Training:   2%|‚ñè         | 3544/200000 [1:16:05<66:40:51,  1.22s/it, loss=0.0149, lr=1.77e-05, step=3543]Training:   2%|‚ñè         | 3544/200000 [1:16:05<66:40:51,  1.22s/it, loss=0.0190, lr=1.77e-05, step=3544]Training:   2%|‚ñè         | 3545/200000 [1:16:06<69:41:51,  1.28s/it, loss=0.0190, lr=1.77e-05, step=3544]Training:   2%|‚ñè         | 3545/200000 [1:16:06<69:41:51,  1.28s/it, loss=0.0365, lr=1.77e-05, step=3545]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3546/200000 [1:16:07<66:24:07,  1.22s/it, loss=0.0365, lr=1.77e-05, step=3545]Training:   2%|‚ñè         | 3546/200000 [1:16:07<66:24:07,  1.22s/it, loss=0.0408, lr=1.77e-05, step=3546]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3547/200000 [1:16:09<70:40:54,  1.30s/it, loss=0.0408, lr=1.77e-05, step=3546]Training:   2%|‚ñè         | 3547/200000 [1:16:09<70:40:54,  1.30s/it, loss=0.0461, lr=1.77e-05, step=3547]Training:   2%|‚ñè         | 3548/200000 [1:16:10<74:10:17,  1.36s/it, loss=0.0461, lr=1.77e-05, step=3547]Training:   2%|‚ñè         | 3548/200000 [1:16:10<74:10:17,  1.36s/it, loss=0.0391, lr=1.77e-05, step=3548]Training:   2%|‚ñè         | 3549/200000 [1:16:11<69:33:40,  1.27s/it, loss=0.0391, lr=1.77e-05, step=3548]Training:   2%|‚ñè         | 3549/200000 [1:16:11<69:33:40,  1.27s/it, loss=0.0528, lr=1.77e-05, step=3549]Training:   2%|‚ñè         | 3550/200000 [1:16:12<66:18:23,  1.22s/it, loss=0.0528, lr=1.77e-05, step=3549]Training:   2%|‚ñè         | 3550/200000 [1:16:12<66:18:23,  1.22s/it, loss=0.0287, lr=1.77e-05, step=3550]Training:   2%|‚ñè         | 3551/200000 [1:16:14<70:00:28,  1.28s/it, loss=0.0287, lr=1.77e-05, step=3550]Training:   2%|‚ñè         | 3551/200000 [1:16:14<70:00:28,  1.28s/it, loss=0.0248, lr=1.78e-05, step=3551]Training:   2%|‚ñè         | 3552/200000 [1:16:15<66:37:10,  1.22s/it, loss=0.0248, lr=1.78e-05, step=3551]Training:   2%|‚ñè         | 3552/200000 [1:16:15<66:37:10,  1.22s/it, loss=0.0224, lr=1.78e-05, step=3552]Training:   2%|‚ñè         | 3553/200000 [1:16:16<68:54:29,  1.26s/it, loss=0.0224, lr=1.78e-05, step=3552]Training:   2%|‚ñè         | 3553/200000 [1:16:16<68:54:29,  1.26s/it, loss=0.0440, lr=1.78e-05, step=3553]Training:   2%|‚ñè         | 3554/200000 [1:16:17<65:52:19,  1.21s/it, loss=0.0440, lr=1.78e-05, step=3553]Training:   2%|‚ñè         | 3554/200000 [1:16:17<65:52:19,  1.21s/it, loss=0.1582, lr=1.78e-05, step=3554]Training:   2%|‚ñè         | 3555/200000 [1:16:18<63:44:47,  1.17s/it, loss=0.1582, lr=1.78e-05, step=3554]Training:   2%|‚ñè         | 3555/200000 [1:16:18<63:44:47,  1.17s/it, loss=0.0233, lr=1.78e-05, step=3555]Training:   2%|‚ñè         | 3556/200000 [1:16:20<67:55:39,  1.24s/it, loss=0.0233, lr=1.78e-05, step=3555]Training:   2%|‚ñè         | 3556/200000 [1:16:20<67:55:39,  1.24s/it, loss=0.0259, lr=1.78e-05, step=3556]Training:   2%|‚ñè         | 3557/200000 [1:16:21<70:26:03,  1.29s/it, loss=0.0259, lr=1.78e-05, step=3556]Training:   2%|‚ñè         | 3557/200000 [1:16:21<70:26:03,  1.29s/it, loss=0.0270, lr=1.78e-05, step=3557]Training:   2%|‚ñè         | 3558/200000 [1:16:23<72:01:54,  1.32s/it, loss=0.0270, lr=1.78e-05, step=3557]Training:   2%|‚ñè         | 3558/200000 [1:16:23<72:01:54,  1.32s/it, loss=0.0506, lr=1.78e-05, step=3558]Training:   2%|‚ñè         | 3559/200000 [1:16:24<68:02:51,  1.25s/it, loss=0.0506, lr=1.78e-05, step=3558]Training:   2%|‚ñè         | 3559/200000 [1:16:24<68:02:51,  1.25s/it, loss=0.0277, lr=1.78e-05, step=3559]Training:   2%|‚ñè         | 3560/200000 [1:16:25<65:14:59,  1.20s/it, loss=0.0277, lr=1.78e-05, step=3559]Training:   2%|‚ñè         | 3560/200000 [1:16:25<65:14:59,  1.20s/it, loss=0.0213, lr=1.78e-05, step=3560]Training:   2%|‚ñè         | 3561/200000 [1:16:26<67:40:31,  1.24s/it, loss=0.0213, lr=1.78e-05, step=3560]Training:   2%|‚ñè         | 3561/200000 [1:16:26<67:40:31,  1.24s/it, loss=0.0232, lr=1.78e-05, step=3561]Training:   2%|‚ñè         | 3562/200000 [1:16:27<69:00:39,  1.26s/it, loss=0.0232, lr=1.78e-05, step=3561]Training:   2%|‚ñè         | 3562/200000 [1:16:27<69:00:39,  1.26s/it, loss=0.0216, lr=1.78e-05, step=3562]Training:   2%|‚ñè         | 3563/200000 [1:16:28<65:56:47,  1.21s/it, loss=0.0216, lr=1.78e-05, step=3562]Training:   2%|‚ñè         | 3563/200000 [1:16:28<65:56:47,  1.21s/it, loss=0.0291, lr=1.78e-05, step=3563]Training:   2%|‚ñè         | 3564/200000 [1:16:30<68:52:56,  1.26s/it, loss=0.0291, lr=1.78e-05, step=3563]Training:   2%|‚ñè         | 3564/200000 [1:16:30<68:52:56,  1.26s/it, loss=0.0386, lr=1.78e-05, step=3564]Training:   2%|‚ñè         | 3565/200000 [1:16:31<65:51:58,  1.21s/it, loss=0.0386, lr=1.78e-05, step=3564]Training:   2%|‚ñè         | 3565/200000 [1:16:31<65:51:58,  1.21s/it, loss=0.0145, lr=1.78e-05, step=3565]Training:   2%|‚ñè         | 3566/200000 [1:16:32<68:36:41,  1.26s/it, loss=0.0145, lr=1.78e-05, step=3565]Training:   2%|‚ñè         | 3566/200000 [1:16:32<68:36:41,  1.26s/it, loss=0.0409, lr=1.78e-05, step=3566]WARNING:root:Token length (51) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3567/200000 [1:16:34<71:53:06,  1.32s/it, loss=0.0409, lr=1.78e-05, step=3566]Training:   2%|‚ñè         | 3567/200000 [1:16:34<71:53:06,  1.32s/it, loss=0.0723, lr=1.78e-05, step=3567]Training:   2%|‚ñè         | 3568/200000 [1:16:35<73:28:31,  1.35s/it, loss=0.0723, lr=1.78e-05, step=3567]Training:   2%|‚ñè         | 3568/200000 [1:16:35<73:28:31,  1.35s/it, loss=0.0287, lr=1.78e-05, step=3568]Training:   2%|‚ñè         | 3569/200000 [1:16:37<74:49:08,  1.37s/it, loss=0.0287, lr=1.78e-05, step=3568]Training:   2%|‚ñè         | 3569/200000 [1:16:37<74:49:08,  1.37s/it, loss=0.0220, lr=1.78e-05, step=3569]Training:   2%|‚ñè         | 3570/200000 [1:16:38<69:59:30,  1.28s/it, loss=0.0220, lr=1.78e-05, step=3569]Training:   2%|‚ñè         | 3570/200000 [1:16:38<69:59:30,  1.28s/it, loss=0.0191, lr=1.78e-05, step=3570]Training:   2%|‚ñè         | 3571/200000 [1:16:39<66:37:46,  1.22s/it, loss=0.0191, lr=1.78e-05, step=3570]Training:   2%|‚ñè         | 3571/200000 [1:16:39<66:37:46,  1.22s/it, loss=0.0241, lr=1.79e-05, step=3571]Training:   2%|‚ñè         | 3572/200000 [1:16:40<69:37:01,  1.28s/it, loss=0.0241, lr=1.79e-05, step=3571]Training:   2%|‚ñè         | 3572/200000 [1:16:40<69:37:01,  1.28s/it, loss=0.0349, lr=1.79e-05, step=3572]Training:   2%|‚ñè         | 3573/200000 [1:16:42<71:45:29,  1.32s/it, loss=0.0349, lr=1.79e-05, step=3572]Training:   2%|‚ñè         | 3573/200000 [1:16:42<71:45:29,  1.32s/it, loss=0.0251, lr=1.79e-05, step=3573]Training:   2%|‚ñè         | 3574/200000 [1:16:43<67:49:21,  1.24s/it, loss=0.0251, lr=1.79e-05, step=3573]Training:   2%|‚ñè         | 3574/200000 [1:16:43<67:49:21,  1.24s/it, loss=0.0257, lr=1.79e-05, step=3574]Training:   2%|‚ñè         | 3575/200000 [1:16:44<71:11:13,  1.30s/it, loss=0.0257, lr=1.79e-05, step=3574]Training:   2%|‚ñè         | 3575/200000 [1:16:44<71:11:13,  1.30s/it, loss=0.0189, lr=1.79e-05, step=3575]Training:   2%|‚ñè         | 3576/200000 [1:16:45<67:26:38,  1.24s/it, loss=0.0189, lr=1.79e-05, step=3575]Training:   2%|‚ñè         | 3576/200000 [1:16:45<67:26:38,  1.24s/it, loss=0.0288, lr=1.79e-05, step=3576]Training:   2%|‚ñè         | 3577/200000 [1:16:47<69:42:45,  1.28s/it, loss=0.0288, lr=1.79e-05, step=3576]Training:   2%|‚ñè         | 3577/200000 [1:16:47<69:42:45,  1.28s/it, loss=0.0262, lr=1.79e-05, step=3577]Training:   2%|‚ñè         | 3578/200000 [1:16:48<66:24:32,  1.22s/it, loss=0.0262, lr=1.79e-05, step=3577]Training:   2%|‚ñè         | 3578/200000 [1:16:48<66:24:32,  1.22s/it, loss=0.0192, lr=1.79e-05, step=3578]Training:   2%|‚ñè         | 3579/200000 [1:16:49<71:02:42,  1.30s/it, loss=0.0192, lr=1.79e-05, step=3578]Training:   2%|‚ñè         | 3579/200000 [1:16:49<71:02:42,  1.30s/it, loss=0.0181, lr=1.79e-05, step=3579]Training:   2%|‚ñè         | 3580/200000 [1:16:51<74:41:53,  1.37s/it, loss=0.0181, lr=1.79e-05, step=3579]Training:   2%|‚ñè         | 3580/200000 [1:16:51<74:41:53,  1.37s/it, loss=0.0347, lr=1.79e-05, step=3580]Training:   2%|‚ñè         | 3581/200000 [1:16:52<69:56:22,  1.28s/it, loss=0.0347, lr=1.79e-05, step=3580]Training:   2%|‚ñè         | 3581/200000 [1:16:52<69:56:22,  1.28s/it, loss=0.0227, lr=1.79e-05, step=3581]Training:   2%|‚ñè         | 3582/200000 [1:16:53<66:36:52,  1.22s/it, loss=0.0227, lr=1.79e-05, step=3581]Training:   2%|‚ñè         | 3582/200000 [1:16:53<66:36:52,  1.22s/it, loss=0.0330, lr=1.79e-05, step=3582]Training:   2%|‚ñè         | 3583/200000 [1:16:54<70:55:00,  1.30s/it, loss=0.0330, lr=1.79e-05, step=3582]Training:   2%|‚ñè         | 3583/200000 [1:16:54<70:55:00,  1.30s/it, loss=0.0304, lr=1.79e-05, step=3583]Training:   2%|‚ñè         | 3584/200000 [1:16:55<67:16:12,  1.23s/it, loss=0.0304, lr=1.79e-05, step=3583]Training:   2%|‚ñè         | 3584/200000 [1:16:55<67:16:12,  1.23s/it, loss=0.0239, lr=1.79e-05, step=3584]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3585/200000 [1:16:57<69:14:34,  1.27s/it, loss=0.0239, lr=1.79e-05, step=3584]Training:   2%|‚ñè         | 3585/200000 [1:16:57<69:14:34,  1.27s/it, loss=0.0256, lr=1.79e-05, step=3585]Training:   2%|‚ñè         | 3586/200000 [1:16:58<66:06:32,  1.21s/it, loss=0.0256, lr=1.79e-05, step=3585]Training:   2%|‚ñè         | 3586/200000 [1:16:58<66:06:32,  1.21s/it, loss=0.0404, lr=1.79e-05, step=3586]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3587/200000 [1:16:59<63:56:00,  1.17s/it, loss=0.0404, lr=1.79e-05, step=3586]Training:   2%|‚ñè         | 3587/200000 [1:16:59<63:56:00,  1.17s/it, loss=0.0213, lr=1.79e-05, step=3587]Training:   2%|‚ñè         | 3588/200000 [1:17:00<69:06:49,  1.27s/it, loss=0.0213, lr=1.79e-05, step=3587]Training:   2%|‚ñè         | 3588/200000 [1:17:00<69:06:49,  1.27s/it, loss=0.0273, lr=1.79e-05, step=3588]Training:   2%|‚ñè         | 3589/200000 [1:17:02<71:23:03,  1.31s/it, loss=0.0273, lr=1.79e-05, step=3588]Training:   2%|‚ñè         | 3589/200000 [1:17:02<71:23:03,  1.31s/it, loss=0.0219, lr=1.79e-05, step=3589]Training:   2%|‚ñè         | 3590/200000 [1:17:03<72:39:53,  1.33s/it, loss=0.0219, lr=1.79e-05, step=3589]Training:   2%|‚ñè         | 3590/200000 [1:17:03<72:39:53,  1.33s/it, loss=0.0255, lr=1.79e-05, step=3590]Training:   2%|‚ñè         | 3591/200000 [1:17:04<68:29:30,  1.26s/it, loss=0.0255, lr=1.79e-05, step=3590]Training:   2%|‚ñè         | 3591/200000 [1:17:04<68:29:30,  1.26s/it, loss=0.0296, lr=1.80e-05, step=3591]Training:   2%|‚ñè         | 3592/200000 [1:17:05<65:35:49,  1.20s/it, loss=0.0296, lr=1.80e-05, step=3591]Training:   2%|‚ñè         | 3592/200000 [1:17:05<65:35:49,  1.20s/it, loss=0.0234, lr=1.80e-05, step=3592]Training:   2%|‚ñè         | 3593/200000 [1:17:07<67:56:36,  1.25s/it, loss=0.0234, lr=1.80e-05, step=3592]Training:   2%|‚ñè         | 3593/200000 [1:17:07<67:56:36,  1.25s/it, loss=0.0180, lr=1.80e-05, step=3593]Training:   2%|‚ñè         | 3594/200000 [1:17:08<70:35:21,  1.29s/it, loss=0.0180, lr=1.80e-05, step=3593]Training:   2%|‚ñè         | 3594/200000 [1:17:08<70:35:21,  1.29s/it, loss=0.0220, lr=1.80e-05, step=3594]Training:   2%|‚ñè         | 3595/200000 [1:17:09<67:02:15,  1.23s/it, loss=0.0220, lr=1.80e-05, step=3594]Training:   2%|‚ñè         | 3595/200000 [1:17:09<67:02:15,  1.23s/it, loss=0.0240, lr=1.80e-05, step=3595]Training:   2%|‚ñè         | 3596/200000 [1:17:11<70:50:53,  1.30s/it, loss=0.0240, lr=1.80e-05, step=3595]Training:   2%|‚ñè         | 3596/200000 [1:17:11<70:50:53,  1.30s/it, loss=0.0212, lr=1.80e-05, step=3596]Training:   2%|‚ñè         | 3597/200000 [1:17:12<67:13:44,  1.23s/it, loss=0.0212, lr=1.80e-05, step=3596]Training:   2%|‚ñè         | 3597/200000 [1:17:12<67:13:44,  1.23s/it, loss=0.0336, lr=1.80e-05, step=3597]Training:   2%|‚ñè         | 3598/200000 [1:17:13<70:03:57,  1.28s/it, loss=0.0336, lr=1.80e-05, step=3597]Training:   2%|‚ñè         | 3598/200000 [1:17:13<70:03:57,  1.28s/it, loss=0.0233, lr=1.80e-05, step=3598]Training:   2%|‚ñè         | 3599/200000 [1:17:14<66:40:31,  1.22s/it, loss=0.0233, lr=1.80e-05, step=3598]Training:   2%|‚ñè         | 3599/200000 [1:17:14<66:40:31,  1.22s/it, loss=0.0361, lr=1.80e-05, step=3599]Training:   2%|‚ñè         | 3600/200000 [1:17:16<70:52:33,  1.30s/it, loss=0.0361, lr=1.80e-05, step=3599]Training:   2%|‚ñè         | 3600/200000 [1:17:16<70:52:33,  1.30s/it, loss=0.0310, lr=1.80e-05, step=3600]00:10:30.964 [I] step=3600 loss=0.0304 lr=1.78e-05 grad_norm=0.59 time=126.8s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3601/200000 [1:17:17<74:22:50,  1.36s/it, loss=0.0310, lr=1.80e-05, step=3600]Training:   2%|‚ñè         | 3601/200000 [1:17:17<74:22:50,  1.36s/it, loss=0.0254, lr=1.80e-05, step=3601]Training:   2%|‚ñè         | 3602/200000 [1:17:18<69:41:39,  1.28s/it, loss=0.0254, lr=1.80e-05, step=3601]Training:   2%|‚ñè         | 3602/200000 [1:17:18<69:41:39,  1.28s/it, loss=0.0280, lr=1.80e-05, step=3602]Training:   2%|‚ñè         | 3603/200000 [1:17:19<66:25:52,  1.22s/it, loss=0.0280, lr=1.80e-05, step=3602]Training:   2%|‚ñè         | 3603/200000 [1:17:19<66:25:52,  1.22s/it, loss=0.0179, lr=1.80e-05, step=3603]Training:   2%|‚ñè         | 3604/200000 [1:17:21<70:25:25,  1.29s/it, loss=0.0179, lr=1.80e-05, step=3603]Training:   2%|‚ñè         | 3604/200000 [1:17:21<70:25:25,  1.29s/it, loss=0.0390, lr=1.80e-05, step=3604]Training:   2%|‚ñè         | 3605/200000 [1:17:22<66:53:57,  1.23s/it, loss=0.0390, lr=1.80e-05, step=3604]Training:   2%|‚ñè         | 3605/200000 [1:17:22<66:53:57,  1.23s/it, loss=0.0227, lr=1.80e-05, step=3605]Training:   2%|‚ñè         | 3606/200000 [1:17:23<68:53:32,  1.26s/it, loss=0.0227, lr=1.80e-05, step=3605]Training:   2%|‚ñè         | 3606/200000 [1:17:23<68:53:32,  1.26s/it, loss=0.0308, lr=1.80e-05, step=3606]Training:   2%|‚ñè         | 3607/200000 [1:17:24<65:50:22,  1.21s/it, loss=0.0308, lr=1.80e-05, step=3606]Training:   2%|‚ñè         | 3607/200000 [1:17:24<65:50:22,  1.21s/it, loss=0.0279, lr=1.80e-05, step=3607]Training:   2%|‚ñè         | 3608/200000 [1:17:25<63:42:05,  1.17s/it, loss=0.0279, lr=1.80e-05, step=3607]Training:   2%|‚ñè         | 3608/200000 [1:17:25<63:42:05,  1.17s/it, loss=0.0543, lr=1.80e-05, step=3608]Training:   2%|‚ñè         | 3609/200000 [1:17:27<67:44:25,  1.24s/it, loss=0.0543, lr=1.80e-05, step=3608]Training:   2%|‚ñè         | 3609/200000 [1:17:27<67:44:25,  1.24s/it, loss=0.0232, lr=1.80e-05, step=3609]Training:   2%|‚ñè         | 3610/200000 [1:17:28<70:14:10,  1.29s/it, loss=0.0232, lr=1.80e-05, step=3609]Training:   2%|‚ñè         | 3610/200000 [1:17:28<70:14:10,  1.29s/it, loss=0.0244, lr=1.80e-05, step=3610]Training:   2%|‚ñè         | 3611/200000 [1:17:30<71:55:19,  1.32s/it, loss=0.0244, lr=1.80e-05, step=3610]Training:   2%|‚ñè         | 3611/200000 [1:17:30<71:55:19,  1.32s/it, loss=0.0193, lr=1.81e-05, step=3611]Training:   2%|‚ñè         | 3612/200000 [1:17:31<67:59:09,  1.25s/it, loss=0.0193, lr=1.81e-05, step=3611]Training:   2%|‚ñè         | 3612/200000 [1:17:31<67:59:09,  1.25s/it, loss=0.0184, lr=1.81e-05, step=3612]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3613/200000 [1:17:32<65:12:08,  1.20s/it, loss=0.0184, lr=1.81e-05, step=3612]Training:   2%|‚ñè         | 3613/200000 [1:17:32<65:12:08,  1.20s/it, loss=0.0278, lr=1.81e-05, step=3613]Training:   2%|‚ñè         | 3614/200000 [1:17:33<67:41:33,  1.24s/it, loss=0.0278, lr=1.81e-05, step=3613]Training:   2%|‚ñè         | 3614/200000 [1:17:33<67:41:33,  1.24s/it, loss=0.0487, lr=1.81e-05, step=3614]Training:   2%|‚ñè         | 3615/200000 [1:17:34<69:00:32,  1.27s/it, loss=0.0487, lr=1.81e-05, step=3614]Training:   2%|‚ñè         | 3615/200000 [1:17:34<69:00:32,  1.27s/it, loss=0.0387, lr=1.81e-05, step=3615]Training:   2%|‚ñè         | 3616/200000 [1:17:35<65:54:50,  1.21s/it, loss=0.0387, lr=1.81e-05, step=3615]Training:   2%|‚ñè         | 3616/200000 [1:17:35<65:54:50,  1.21s/it, loss=0.0313, lr=1.81e-05, step=3616]Training:   2%|‚ñè         | 3617/200000 [1:17:37<68:29:55,  1.26s/it, loss=0.0313, lr=1.81e-05, step=3616]Training:   2%|‚ñè         | 3617/200000 [1:17:37<68:29:55,  1.26s/it, loss=0.0433, lr=1.81e-05, step=3617]Training:   2%|‚ñè         | 3618/200000 [1:17:38<65:35:55,  1.20s/it, loss=0.0433, lr=1.81e-05, step=3617]Training:   2%|‚ñè         | 3618/200000 [1:17:38<65:35:55,  1.20s/it, loss=0.0199, lr=1.81e-05, step=3618]Training:   2%|‚ñè         | 3619/200000 [1:17:39<67:52:55,  1.24s/it, loss=0.0199, lr=1.81e-05, step=3618]Training:   2%|‚ñè         | 3619/200000 [1:17:39<67:52:55,  1.24s/it, loss=0.0411, lr=1.81e-05, step=3619]Training:   2%|‚ñè         | 3620/200000 [1:17:41<71:00:30,  1.30s/it, loss=0.0411, lr=1.81e-05, step=3619]Training:   2%|‚ñè         | 3620/200000 [1:17:41<71:00:30,  1.30s/it, loss=0.0218, lr=1.81e-05, step=3620]Training:   2%|‚ñè         | 3621/200000 [1:17:42<72:54:35,  1.34s/it, loss=0.0218, lr=1.81e-05, step=3620]Training:   2%|‚ñè         | 3621/200000 [1:17:42<72:54:35,  1.34s/it, loss=0.0327, lr=1.81e-05, step=3621]Training:   2%|‚ñè         | 3622/200000 [1:17:44<74:44:08,  1.37s/it, loss=0.0327, lr=1.81e-05, step=3621]Training:   2%|‚ñè         | 3622/200000 [1:17:44<74:44:08,  1.37s/it, loss=0.0429, lr=1.81e-05, step=3622]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3623/200000 [1:17:45<69:56:34,  1.28s/it, loss=0.0429, lr=1.81e-05, step=3622]Training:   2%|‚ñè         | 3623/200000 [1:17:45<69:56:34,  1.28s/it, loss=0.0227, lr=1.81e-05, step=3623]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3624/200000 [1:17:46<66:35:58,  1.22s/it, loss=0.0227, lr=1.81e-05, step=3623]Training:   2%|‚ñè         | 3624/200000 [1:17:46<66:35:58,  1.22s/it, loss=0.0174, lr=1.81e-05, step=3624]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3625/200000 [1:17:47<69:36:57,  1.28s/it, loss=0.0174, lr=1.81e-05, step=3624]Training:   2%|‚ñè         | 3625/200000 [1:17:47<69:36:57,  1.28s/it, loss=0.0226, lr=1.81e-05, step=3625]Training:   2%|‚ñè         | 3626/200000 [1:17:49<72:10:14,  1.32s/it, loss=0.0226, lr=1.81e-05, step=3625]Training:   2%|‚ñè         | 3626/200000 [1:17:49<72:10:14,  1.32s/it, loss=0.0292, lr=1.81e-05, step=3626]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3627/200000 [1:17:50<68:09:33,  1.25s/it, loss=0.0292, lr=1.81e-05, step=3626]Training:   2%|‚ñè         | 3627/200000 [1:17:50<68:09:33,  1.25s/it, loss=0.0236, lr=1.81e-05, step=3627]Training:   2%|‚ñè         | 3628/200000 [1:17:51<71:11:36,  1.31s/it, loss=0.0236, lr=1.81e-05, step=3627]Training:   2%|‚ñè         | 3628/200000 [1:17:51<71:11:36,  1.31s/it, loss=0.0208, lr=1.81e-05, step=3628]Training:   2%|‚ñè         | 3629/200000 [1:17:52<67:27:38,  1.24s/it, loss=0.0208, lr=1.81e-05, step=3628]Training:   2%|‚ñè         | 3629/200000 [1:17:52<67:27:38,  1.24s/it, loss=0.0273, lr=1.81e-05, step=3629]Training:   2%|‚ñè         | 3630/200000 [1:17:53<69:46:26,  1.28s/it, loss=0.0273, lr=1.81e-05, step=3629]Training:   2%|‚ñè         | 3630/200000 [1:17:53<69:46:26,  1.28s/it, loss=0.0221, lr=1.81e-05, step=3630]Training:   2%|‚ñè         | 3631/200000 [1:17:55<66:26:25,  1.22s/it, loss=0.0221, lr=1.81e-05, step=3630]Training:   2%|‚ñè         | 3631/200000 [1:17:55<66:26:25,  1.22s/it, loss=0.0704, lr=1.82e-05, step=3631]Training:   2%|‚ñè         | 3632/200000 [1:17:56<71:15:32,  1.31s/it, loss=0.0704, lr=1.82e-05, step=3631]Training:   2%|‚ñè         | 3632/200000 [1:17:56<71:15:32,  1.31s/it, loss=0.0135, lr=1.82e-05, step=3632]Training:   2%|‚ñè         | 3633/200000 [1:17:58<74:47:57,  1.37s/it, loss=0.0135, lr=1.82e-05, step=3632]Training:   2%|‚ñè         | 3633/200000 [1:17:58<74:47:57,  1.37s/it, loss=0.0240, lr=1.82e-05, step=3633]Training:   2%|‚ñè         | 3634/200000 [1:17:59<69:57:35,  1.28s/it, loss=0.0240, lr=1.82e-05, step=3633]Training:   2%|‚ñè         | 3634/200000 [1:17:59<69:57:35,  1.28s/it, loss=0.0191, lr=1.82e-05, step=3634]Training:   2%|‚ñè         | 3635/200000 [1:18:00<66:34:47,  1.22s/it, loss=0.0191, lr=1.82e-05, step=3634]Training:   2%|‚ñè         | 3635/200000 [1:18:00<66:34:47,  1.22s/it, loss=0.0351, lr=1.82e-05, step=3635]Training:   2%|‚ñè         | 3636/200000 [1:18:01<70:31:54,  1.29s/it, loss=0.0351, lr=1.82e-05, step=3635]Training:   2%|‚ñè         | 3636/200000 [1:18:01<70:31:54,  1.29s/it, loss=0.0329, lr=1.82e-05, step=3636]Training:   2%|‚ñè         | 3637/200000 [1:18:02<66:58:45,  1.23s/it, loss=0.0329, lr=1.82e-05, step=3636]Training:   2%|‚ñè         | 3637/200000 [1:18:02<66:58:45,  1.23s/it, loss=0.0310, lr=1.82e-05, step=3637]Training:   2%|‚ñè         | 3638/200000 [1:18:04<69:12:18,  1.27s/it, loss=0.0310, lr=1.82e-05, step=3637]Training:   2%|‚ñè         | 3638/200000 [1:18:04<69:12:18,  1.27s/it, loss=0.0280, lr=1.82e-05, step=3638]Training:   2%|‚ñè         | 3639/200000 [1:18:05<66:04:03,  1.21s/it, loss=0.0280, lr=1.82e-05, step=3638]Training:   2%|‚ñè         | 3639/200000 [1:18:05<66:04:03,  1.21s/it, loss=0.0255, lr=1.82e-05, step=3639]Training:   2%|‚ñè         | 3640/200000 [1:18:06<63:50:19,  1.17s/it, loss=0.0255, lr=1.82e-05, step=3639]Training:   2%|‚ñè         | 3640/200000 [1:18:06<63:50:19,  1.17s/it, loss=0.0242, lr=1.82e-05, step=3640]Training:   2%|‚ñè         | 3641/200000 [1:18:07<69:02:55,  1.27s/it, loss=0.0242, lr=1.82e-05, step=3640]Training:   2%|‚ñè         | 3641/200000 [1:18:07<69:02:55,  1.27s/it, loss=0.0304, lr=1.82e-05, step=3641]Training:   2%|‚ñè         | 3642/200000 [1:18:09<70:54:30,  1.30s/it, loss=0.0304, lr=1.82e-05, step=3641]Training:   2%|‚ñè         | 3642/200000 [1:18:09<70:54:30,  1.30s/it, loss=0.0170, lr=1.82e-05, step=3642]Training:   2%|‚ñè         | 3643/200000 [1:18:10<73:06:09,  1.34s/it, loss=0.0170, lr=1.82e-05, step=3642]Training:   2%|‚ñè         | 3643/200000 [1:18:10<73:06:09,  1.34s/it, loss=0.0162, lr=1.82e-05, step=3643]Training:   2%|‚ñè         | 3644/200000 [1:18:11<68:47:25,  1.26s/it, loss=0.0162, lr=1.82e-05, step=3643]Training:   2%|‚ñè         | 3644/200000 [1:18:11<68:47:25,  1.26s/it, loss=0.0364, lr=1.82e-05, step=3644]Training:   2%|‚ñè         | 3645/200000 [1:18:12<65:46:04,  1.21s/it, loss=0.0364, lr=1.82e-05, step=3644]Training:   2%|‚ñè         | 3645/200000 [1:18:12<65:46:04,  1.21s/it, loss=0.0300, lr=1.82e-05, step=3645]Training:   2%|‚ñè         | 3646/200000 [1:18:14<68:04:40,  1.25s/it, loss=0.0300, lr=1.82e-05, step=3645]Training:   2%|‚ñè         | 3646/200000 [1:18:14<68:04:40,  1.25s/it, loss=0.0238, lr=1.82e-05, step=3646]Training:   2%|‚ñè         | 3647/200000 [1:18:15<70:13:25,  1.29s/it, loss=0.0238, lr=1.82e-05, step=3646]Training:   2%|‚ñè         | 3647/200000 [1:18:15<70:13:25,  1.29s/it, loss=0.0504, lr=1.82e-05, step=3647]Training:   2%|‚ñè         | 3648/200000 [1:18:16<66:46:41,  1.22s/it, loss=0.0504, lr=1.82e-05, step=3647]Training:   2%|‚ñè         | 3648/200000 [1:18:16<66:46:41,  1.22s/it, loss=0.1002, lr=1.82e-05, step=3648]Training:   2%|‚ñè         | 3649/200000 [1:18:18<70:14:24,  1.29s/it, loss=0.1002, lr=1.82e-05, step=3648]Training:   2%|‚ñè         | 3649/200000 [1:18:18<70:14:24,  1.29s/it, loss=0.0242, lr=1.82e-05, step=3649]Training:   2%|‚ñè         | 3650/200000 [1:18:19<66:44:33,  1.22s/it, loss=0.0242, lr=1.82e-05, step=3649]Training:   2%|‚ñè         | 3650/200000 [1:18:19<66:44:33,  1.22s/it, loss=0.0278, lr=1.82e-05, step=3650]Training:   2%|‚ñè         | 3651/200000 [1:18:20<69:25:53,  1.27s/it, loss=0.0278, lr=1.82e-05, step=3650]Training:   2%|‚ñè         | 3651/200000 [1:18:20<69:25:53,  1.27s/it, loss=0.0141, lr=1.83e-05, step=3651]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3652/200000 [1:18:21<66:12:35,  1.21s/it, loss=0.0141, lr=1.83e-05, step=3651]Training:   2%|‚ñè         | 3652/200000 [1:18:21<66:12:35,  1.21s/it, loss=0.0284, lr=1.83e-05, step=3652]Training:   2%|‚ñè         | 3653/200000 [1:18:23<70:36:44,  1.29s/it, loss=0.0284, lr=1.83e-05, step=3652]Training:   2%|‚ñè         | 3653/200000 [1:18:23<70:36:44,  1.29s/it, loss=0.0348, lr=1.83e-05, step=3653]Training:   2%|‚ñè         | 3654/200000 [1:18:24<74:02:37,  1.36s/it, loss=0.0348, lr=1.83e-05, step=3653]Training:   2%|‚ñè         | 3654/200000 [1:18:24<74:02:37,  1.36s/it, loss=0.0244, lr=1.83e-05, step=3654]Training:   2%|‚ñè         | 3655/200000 [1:18:25<69:24:37,  1.27s/it, loss=0.0244, lr=1.83e-05, step=3654]Training:   2%|‚ñè         | 3655/200000 [1:18:25<69:24:37,  1.27s/it, loss=0.0236, lr=1.83e-05, step=3655]Training:   2%|‚ñè         | 3656/200000 [1:18:26<66:12:08,  1.21s/it, loss=0.0236, lr=1.83e-05, step=3655]Training:   2%|‚ñè         | 3656/200000 [1:18:26<66:12:08,  1.21s/it, loss=0.0246, lr=1.83e-05, step=3656]Training:   2%|‚ñè         | 3657/200000 [1:18:28<70:20:15,  1.29s/it, loss=0.0246, lr=1.83e-05, step=3656]Training:   2%|‚ñè         | 3657/200000 [1:18:28<70:20:15,  1.29s/it, loss=0.0211, lr=1.83e-05, step=3657]Training:   2%|‚ñè         | 3658/200000 [1:18:29<66:51:26,  1.23s/it, loss=0.0211, lr=1.83e-05, step=3657]Training:   2%|‚ñè         | 3658/200000 [1:18:29<66:51:26,  1.23s/it, loss=0.0490, lr=1.83e-05, step=3658]Training:   2%|‚ñè         | 3659/200000 [1:18:30<68:18:34,  1.25s/it, loss=0.0490, lr=1.83e-05, step=3658]Training:   2%|‚ñè         | 3659/200000 [1:18:30<68:18:34,  1.25s/it, loss=0.0512, lr=1.83e-05, step=3659]Training:   2%|‚ñè         | 3660/200000 [1:18:31<65:26:01,  1.20s/it, loss=0.0512, lr=1.83e-05, step=3659]Training:   2%|‚ñè         | 3660/200000 [1:18:31<65:26:01,  1.20s/it, loss=0.0247, lr=1.83e-05, step=3660]Training:   2%|‚ñè         | 3661/200000 [1:18:32<63:27:16,  1.16s/it, loss=0.0247, lr=1.83e-05, step=3660]Training:   2%|‚ñè         | 3661/200000 [1:18:32<63:27:16,  1.16s/it, loss=0.0207, lr=1.83e-05, step=3661]Training:   2%|‚ñè         | 3662/200000 [1:18:34<67:56:59,  1.25s/it, loss=0.0207, lr=1.83e-05, step=3661]Training:   2%|‚ñè         | 3662/200000 [1:18:34<67:56:59,  1.25s/it, loss=0.0654, lr=1.83e-05, step=3662]Training:   2%|‚ñè         | 3663/200000 [1:18:35<70:28:25,  1.29s/it, loss=0.0654, lr=1.83e-05, step=3662]Training:   2%|‚ñè         | 3663/200000 [1:18:35<70:28:25,  1.29s/it, loss=0.0170, lr=1.83e-05, step=3663]Training:   2%|‚ñè         | 3664/200000 [1:18:36<72:28:17,  1.33s/it, loss=0.0170, lr=1.83e-05, step=3663]Training:   2%|‚ñè         | 3664/200000 [1:18:36<72:28:17,  1.33s/it, loss=0.0206, lr=1.83e-05, step=3664]Training:   2%|‚ñè         | 3665/200000 [1:18:38<68:23:06,  1.25s/it, loss=0.0206, lr=1.83e-05, step=3664]Training:   2%|‚ñè         | 3665/200000 [1:18:38<68:23:06,  1.25s/it, loss=0.0231, lr=1.83e-05, step=3665]Training:   2%|‚ñè         | 3666/200000 [1:18:39<65:28:53,  1.20s/it, loss=0.0231, lr=1.83e-05, step=3665]Training:   2%|‚ñè         | 3666/200000 [1:18:39<65:28:53,  1.20s/it, loss=0.0220, lr=1.83e-05, step=3666]Training:   2%|‚ñè         | 3667/200000 [1:18:40<67:41:40,  1.24s/it, loss=0.0220, lr=1.83e-05, step=3666]Training:   2%|‚ñè         | 3667/200000 [1:18:40<67:41:40,  1.24s/it, loss=0.0256, lr=1.83e-05, step=3667]Training:   2%|‚ñè         | 3668/200000 [1:18:41<69:36:54,  1.28s/it, loss=0.0256, lr=1.83e-05, step=3667]Training:   2%|‚ñè         | 3668/200000 [1:18:41<69:36:54,  1.28s/it, loss=0.0305, lr=1.83e-05, step=3668]Training:   2%|‚ñè         | 3669/200000 [1:18:42<66:20:31,  1.22s/it, loss=0.0305, lr=1.83e-05, step=3668]Training:   2%|‚ñè         | 3669/200000 [1:18:42<66:20:31,  1.22s/it, loss=0.0210, lr=1.83e-05, step=3669]Training:   2%|‚ñè         | 3670/200000 [1:18:44<68:53:10,  1.26s/it, loss=0.0210, lr=1.83e-05, step=3669]Training:   2%|‚ñè         | 3670/200000 [1:18:44<68:53:10,  1.26s/it, loss=0.0256, lr=1.83e-05, step=3670]Training:   2%|‚ñè         | 3671/200000 [1:18:45<65:49:04,  1.21s/it, loss=0.0256, lr=1.83e-05, step=3670]Training:   2%|‚ñè         | 3671/200000 [1:18:45<65:49:04,  1.21s/it, loss=0.0310, lr=1.84e-05, step=3671]Training:   2%|‚ñè         | 3672/200000 [1:18:46<68:42:05,  1.26s/it, loss=0.0310, lr=1.84e-05, step=3671]Training:   2%|‚ñè         | 3672/200000 [1:18:46<68:42:05,  1.26s/it, loss=0.0257, lr=1.84e-05, step=3672]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3673/200000 [1:18:48<71:59:35,  1.32s/it, loss=0.0257, lr=1.84e-05, step=3672]Training:   2%|‚ñè         | 3673/200000 [1:18:48<71:59:35,  1.32s/it, loss=0.0305, lr=1.84e-05, step=3673]Training:   2%|‚ñè         | 3674/200000 [1:18:49<73:35:27,  1.35s/it, loss=0.0305, lr=1.84e-05, step=3673]Training:   2%|‚ñè         | 3674/200000 [1:18:49<73:35:27,  1.35s/it, loss=0.0132, lr=1.84e-05, step=3674]Training:   2%|‚ñè         | 3675/200000 [1:18:51<74:42:57,  1.37s/it, loss=0.0132, lr=1.84e-05, step=3674]Training:   2%|‚ñè         | 3675/200000 [1:18:51<74:42:57,  1.37s/it, loss=0.1662, lr=1.84e-05, step=3675]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3676/200000 [1:18:52<69:54:52,  1.28s/it, loss=0.1662, lr=1.84e-05, step=3675]Training:   2%|‚ñè         | 3676/200000 [1:18:52<69:54:52,  1.28s/it, loss=0.0273, lr=1.84e-05, step=3676]Training:   2%|‚ñè         | 3677/200000 [1:18:53<66:34:01,  1.22s/it, loss=0.0273, lr=1.84e-05, step=3676]Training:   2%|‚ñè         | 3677/200000 [1:18:53<66:34:01,  1.22s/it, loss=0.0167, lr=1.84e-05, step=3677]Training:   2%|‚ñè         | 3678/200000 [1:18:54<68:46:00,  1.26s/it, loss=0.0167, lr=1.84e-05, step=3677]Training:   2%|‚ñè         | 3678/200000 [1:18:54<68:46:00,  1.26s/it, loss=0.0276, lr=1.84e-05, step=3678]Training:   2%|‚ñè         | 3679/200000 [1:18:55<71:03:53,  1.30s/it, loss=0.0276, lr=1.84e-05, step=3678]Training:   2%|‚ñè         | 3679/200000 [1:18:55<71:03:53,  1.30s/it, loss=0.0237, lr=1.84e-05, step=3679]Training:   2%|‚ñè         | 3680/200000 [1:18:56<67:20:43,  1.23s/it, loss=0.0237, lr=1.84e-05, step=3679]Training:   2%|‚ñè         | 3680/200000 [1:18:56<67:20:43,  1.23s/it, loss=0.0187, lr=1.84e-05, step=3680]Training:   2%|‚ñè         | 3681/200000 [1:18:58<70:23:58,  1.29s/it, loss=0.0187, lr=1.84e-05, step=3680]Training:   2%|‚ñè         | 3681/200000 [1:18:58<70:23:58,  1.29s/it, loss=0.0266, lr=1.84e-05, step=3681]Training:   2%|‚ñè         | 3682/200000 [1:18:59<66:54:06,  1.23s/it, loss=0.0266, lr=1.84e-05, step=3681]Training:   2%|‚ñè         | 3682/200000 [1:18:59<66:54:06,  1.23s/it, loss=0.0265, lr=1.84e-05, step=3682]Training:   2%|‚ñè         | 3683/200000 [1:19:00<69:10:10,  1.27s/it, loss=0.0265, lr=1.84e-05, step=3682]Training:   2%|‚ñè         | 3683/200000 [1:19:00<69:10:10,  1.27s/it, loss=0.0229, lr=1.84e-05, step=3683]Training:   2%|‚ñè         | 3684/200000 [1:19:01<66:00:45,  1.21s/it, loss=0.0229, lr=1.84e-05, step=3683]Training:   2%|‚ñè         | 3684/200000 [1:19:01<66:00:45,  1.21s/it, loss=0.0267, lr=1.84e-05, step=3684]Training:   2%|‚ñè         | 3685/200000 [1:19:03<71:36:56,  1.31s/it, loss=0.0267, lr=1.84e-05, step=3684]Training:   2%|‚ñè         | 3685/200000 [1:19:03<71:36:56,  1.31s/it, loss=0.0144, lr=1.84e-05, step=3685]Training:   2%|‚ñè         | 3686/200000 [1:19:05<75:06:05,  1.38s/it, loss=0.0144, lr=1.84e-05, step=3685]Training:   2%|‚ñè         | 3686/200000 [1:19:05<75:06:05,  1.38s/it, loss=0.0416, lr=1.84e-05, step=3686]Training:   2%|‚ñè         | 3687/200000 [1:19:06<70:11:20,  1.29s/it, loss=0.0416, lr=1.84e-05, step=3686]Training:   2%|‚ñè         | 3687/200000 [1:19:06<70:11:20,  1.29s/it, loss=0.0304, lr=1.84e-05, step=3687]Training:   2%|‚ñè         | 3688/200000 [1:19:07<66:44:13,  1.22s/it, loss=0.0304, lr=1.84e-05, step=3687]Training:   2%|‚ñè         | 3688/200000 [1:19:07<66:44:13,  1.22s/it, loss=0.0217, lr=1.84e-05, step=3688]Training:   2%|‚ñè         | 3689/200000 [1:19:08<71:00:27,  1.30s/it, loss=0.0217, lr=1.84e-05, step=3688]Training:   2%|‚ñè         | 3689/200000 [1:19:08<71:00:27,  1.30s/it, loss=0.0173, lr=1.84e-05, step=3689]Training:   2%|‚ñè         | 3690/200000 [1:19:09<67:19:04,  1.23s/it, loss=0.0173, lr=1.84e-05, step=3689]Training:   2%|‚ñè         | 3690/200000 [1:19:09<67:19:04,  1.23s/it, loss=0.0245, lr=1.84e-05, step=3690]Training:   2%|‚ñè         | 3691/200000 [1:19:11<68:43:50,  1.26s/it, loss=0.0245, lr=1.84e-05, step=3690]Training:   2%|‚ñè         | 3691/200000 [1:19:11<68:43:50,  1.26s/it, loss=0.0260, lr=1.85e-05, step=3691]Training:   2%|‚ñè         | 3692/200000 [1:19:12<65:42:37,  1.21s/it, loss=0.0260, lr=1.85e-05, step=3691]Training:   2%|‚ñè         | 3692/200000 [1:19:12<65:42:37,  1.21s/it, loss=0.0236, lr=1.85e-05, step=3692]Training:   2%|‚ñè         | 3693/200000 [1:19:13<63:36:49,  1.17s/it, loss=0.0236, lr=1.85e-05, step=3692]Training:   2%|‚ñè         | 3693/200000 [1:19:13<63:36:49,  1.17s/it, loss=0.0236, lr=1.85e-05, step=3693]Training:   2%|‚ñè         | 3694/200000 [1:19:14<68:50:46,  1.26s/it, loss=0.0236, lr=1.85e-05, step=3693]Training:   2%|‚ñè         | 3694/200000 [1:19:14<68:50:46,  1.26s/it, loss=0.0324, lr=1.85e-05, step=3694]Training:   2%|‚ñè         | 3695/200000 [1:19:16<71:22:23,  1.31s/it, loss=0.0324, lr=1.85e-05, step=3694]Training:   2%|‚ñè         | 3695/200000 [1:19:16<71:22:23,  1.31s/it, loss=0.0415, lr=1.85e-05, step=3695]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3696/200000 [1:19:17<72:54:12,  1.34s/it, loss=0.0415, lr=1.85e-05, step=3695]Training:   2%|‚ñè         | 3696/200000 [1:19:17<72:54:12,  1.34s/it, loss=0.0171, lr=1.85e-05, step=3696]Training:   2%|‚ñè         | 3697/200000 [1:19:18<68:39:40,  1.26s/it, loss=0.0171, lr=1.85e-05, step=3696]Training:   2%|‚ñè         | 3697/200000 [1:19:18<68:39:40,  1.26s/it, loss=0.0224, lr=1.85e-05, step=3697]Training:   2%|‚ñè         | 3698/200000 [1:19:19<65:41:23,  1.20s/it, loss=0.0224, lr=1.85e-05, step=3697]Training:   2%|‚ñè         | 3698/200000 [1:19:19<65:41:23,  1.20s/it, loss=0.0195, lr=1.85e-05, step=3698]Training:   2%|‚ñè         | 3699/200000 [1:19:21<68:08:27,  1.25s/it, loss=0.0195, lr=1.85e-05, step=3698]Training:   2%|‚ñè         | 3699/200000 [1:19:21<68:08:27,  1.25s/it, loss=0.0187, lr=1.85e-05, step=3699]Training:   2%|‚ñè         | 3700/200000 [1:19:22<70:36:07,  1.29s/it, loss=0.0187, lr=1.85e-05, step=3699]Training:   2%|‚ñè         | 3700/200000 [1:19:22<70:36:07,  1.29s/it, loss=0.0239, lr=1.85e-05, step=3700]00:12:36.802 [I] step=3700 loss=0.0297 lr=1.83e-05 grad_norm=0.66 time=125.8s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3701/200000 [1:19:23<67:03:31,  1.23s/it, loss=0.0239, lr=1.85e-05, step=3700]Training:   2%|‚ñè         | 3701/200000 [1:19:23<67:03:31,  1.23s/it, loss=0.0162, lr=1.85e-05, step=3701]Training:   2%|‚ñè         | 3702/200000 [1:19:24<70:41:19,  1.30s/it, loss=0.0162, lr=1.85e-05, step=3701]Training:   2%|‚ñè         | 3702/200000 [1:19:24<70:41:19,  1.30s/it, loss=0.0463, lr=1.85e-05, step=3702]Training:   2%|‚ñè         | 3703/200000 [1:19:26<67:07:33,  1.23s/it, loss=0.0463, lr=1.85e-05, step=3702]Training:   2%|‚ñè         | 3703/200000 [1:19:26<67:07:33,  1.23s/it, loss=0.0448, lr=1.85e-05, step=3703]Training:   2%|‚ñè         | 3704/200000 [1:19:27<69:58:32,  1.28s/it, loss=0.0448, lr=1.85e-05, step=3703]Training:   2%|‚ñè         | 3704/200000 [1:19:27<69:58:32,  1.28s/it, loss=0.0284, lr=1.85e-05, step=3704]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3705/200000 [1:19:28<66:35:35,  1.22s/it, loss=0.0284, lr=1.85e-05, step=3704]Training:   2%|‚ñè         | 3705/200000 [1:19:28<66:35:35,  1.22s/it, loss=0.0340, lr=1.85e-05, step=3705]Training:   2%|‚ñè         | 3706/200000 [1:19:29<70:36:31,  1.29s/it, loss=0.0340, lr=1.85e-05, step=3705]Training:   2%|‚ñè         | 3706/200000 [1:19:29<70:36:31,  1.29s/it, loss=0.1598, lr=1.85e-05, step=3706]Training:   2%|‚ñè         | 3707/200000 [1:19:31<74:07:03,  1.36s/it, loss=0.1598, lr=1.85e-05, step=3706]Training:   2%|‚ñè         | 3707/200000 [1:19:31<74:07:03,  1.36s/it, loss=0.0745, lr=1.85e-05, step=3707]Training:   2%|‚ñè         | 3708/200000 [1:19:32<69:30:06,  1.27s/it, loss=0.0745, lr=1.85e-05, step=3707]Training:   2%|‚ñè         | 3708/200000 [1:19:32<69:30:06,  1.27s/it, loss=0.0301, lr=1.85e-05, step=3708]Training:   2%|‚ñè         | 3709/200000 [1:19:33<66:17:29,  1.22s/it, loss=0.0301, lr=1.85e-05, step=3708]Training:   2%|‚ñè         | 3709/200000 [1:19:33<66:17:29,  1.22s/it, loss=0.0183, lr=1.85e-05, step=3709]Training:   2%|‚ñè         | 3710/200000 [1:19:35<70:23:13,  1.29s/it, loss=0.0183, lr=1.85e-05, step=3709]Training:   2%|‚ñè         | 3710/200000 [1:19:35<70:23:13,  1.29s/it, loss=0.0253, lr=1.85e-05, step=3710]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3711/200000 [1:19:36<66:52:43,  1.23s/it, loss=0.0253, lr=1.85e-05, step=3710]Training:   2%|‚ñè         | 3711/200000 [1:19:36<66:52:43,  1.23s/it, loss=0.0158, lr=1.86e-05, step=3711]Training:   2%|‚ñè         | 3712/200000 [1:19:37<69:22:40,  1.27s/it, loss=0.0158, lr=1.86e-05, step=3711]Training:   2%|‚ñè         | 3712/200000 [1:19:37<69:22:40,  1.27s/it, loss=0.0244, lr=1.86e-05, step=3712]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3713/200000 [1:19:38<66:10:23,  1.21s/it, loss=0.0244, lr=1.86e-05, step=3712]Training:   2%|‚ñè         | 3713/200000 [1:19:38<66:10:23,  1.21s/it, loss=0.0172, lr=1.86e-05, step=3713]Training:   2%|‚ñè         | 3714/200000 [1:19:39<63:56:33,  1.17s/it, loss=0.0172, lr=1.86e-05, step=3713]Training:   2%|‚ñè         | 3714/200000 [1:19:39<63:56:33,  1.17s/it, loss=0.0786, lr=1.86e-05, step=3714]Training:   2%|‚ñè         | 3715/200000 [1:19:41<68:00:14,  1.25s/it, loss=0.0786, lr=1.86e-05, step=3714]Training:   2%|‚ñè         | 3715/200000 [1:19:41<68:00:14,  1.25s/it, loss=0.0266, lr=1.86e-05, step=3715]Training:   2%|‚ñè         | 3716/200000 [1:19:42<70:40:56,  1.30s/it, loss=0.0266, lr=1.86e-05, step=3715]Training:   2%|‚ñè         | 3716/200000 [1:19:42<70:40:56,  1.30s/it, loss=0.0235, lr=1.86e-05, step=3716]Training:   2%|‚ñè         | 3717/200000 [1:19:43<72:10:56,  1.32s/it, loss=0.0235, lr=1.86e-05, step=3716]Training:   2%|‚ñè         | 3717/200000 [1:19:43<72:10:56,  1.32s/it, loss=0.0227, lr=1.86e-05, step=3717]Training:   2%|‚ñè         | 3718/200000 [1:19:45<68:07:01,  1.25s/it, loss=0.0227, lr=1.86e-05, step=3717]Training:   2%|‚ñè         | 3718/200000 [1:19:45<68:07:01,  1.25s/it, loss=0.0266, lr=1.86e-05, step=3718]Training:   2%|‚ñè         | 3719/200000 [1:19:46<65:18:16,  1.20s/it, loss=0.0266, lr=1.86e-05, step=3718]Training:   2%|‚ñè         | 3719/200000 [1:19:46<65:18:16,  1.20s/it, loss=0.0315, lr=1.86e-05, step=3719]Training:   2%|‚ñè         | 3720/200000 [1:19:47<67:45:18,  1.24s/it, loss=0.0315, lr=1.86e-05, step=3719]Training:   2%|‚ñè         | 3720/200000 [1:19:47<67:45:18,  1.24s/it, loss=0.0223, lr=1.86e-05, step=3720]Training:   2%|‚ñè         | 3721/200000 [1:19:48<69:01:52,  1.27s/it, loss=0.0223, lr=1.86e-05, step=3720]Training:   2%|‚ñè         | 3721/200000 [1:19:48<69:01:52,  1.27s/it, loss=0.0216, lr=1.86e-05, step=3721]Training:   2%|‚ñè         | 3722/200000 [1:19:49<65:57:08,  1.21s/it, loss=0.0216, lr=1.86e-05, step=3721]Training:   2%|‚ñè         | 3722/200000 [1:19:49<65:57:08,  1.21s/it, loss=0.0468, lr=1.86e-05, step=3722]Training:   2%|‚ñè         | 3723/200000 [1:19:51<67:56:06,  1.25s/it, loss=0.0468, lr=1.86e-05, step=3722]Training:   2%|‚ñè         | 3723/200000 [1:19:51<67:56:06,  1.25s/it, loss=0.0234, lr=1.86e-05, step=3723]Training:   2%|‚ñè         | 3724/200000 [1:19:52<65:10:21,  1.20s/it, loss=0.0234, lr=1.86e-05, step=3723]Training:   2%|‚ñè         | 3724/200000 [1:19:52<65:10:21,  1.20s/it, loss=0.0238, lr=1.86e-05, step=3724]Training:   2%|‚ñè         | 3725/200000 [1:19:53<67:15:38,  1.23s/it, loss=0.0238, lr=1.86e-05, step=3724]Training:   2%|‚ñè         | 3725/200000 [1:19:53<67:15:38,  1.23s/it, loss=0.0470, lr=1.86e-05, step=3725]Training:   2%|‚ñè         | 3726/200000 [1:19:55<70:47:31,  1.30s/it, loss=0.0470, lr=1.86e-05, step=3725]Training:   2%|‚ñè         | 3726/200000 [1:19:55<70:47:31,  1.30s/it, loss=0.0209, lr=1.86e-05, step=3726]Training:   2%|‚ñè         | 3727/200000 [1:19:56<72:47:32,  1.34s/it, loss=0.0209, lr=1.86e-05, step=3726]Training:   2%|‚ñè         | 3727/200000 [1:19:56<72:47:32,  1.34s/it, loss=0.0327, lr=1.86e-05, step=3727]Training:   2%|‚ñè         | 3728/200000 [1:19:57<74:35:37,  1.37s/it, loss=0.0327, lr=1.86e-05, step=3727]Training:   2%|‚ñè         | 3728/200000 [1:19:57<74:35:37,  1.37s/it, loss=0.0339, lr=1.86e-05, step=3728]Training:   2%|‚ñè         | 3729/200000 [1:19:58<69:51:05,  1.28s/it, loss=0.0339, lr=1.86e-05, step=3728]Training:   2%|‚ñè         | 3729/200000 [1:19:58<69:51:05,  1.28s/it, loss=0.0361, lr=1.86e-05, step=3729]Training:   2%|‚ñè         | 3730/200000 [1:20:00<66:31:01,  1.22s/it, loss=0.0361, lr=1.86e-05, step=3729]Training:   2%|‚ñè         | 3730/200000 [1:20:00<66:31:01,  1.22s/it, loss=0.0169, lr=1.86e-05, step=3730]Training:   2%|‚ñè         | 3731/200000 [1:20:01<69:35:49,  1.28s/it, loss=0.0169, lr=1.86e-05, step=3730]Training:   2%|‚ñè         | 3731/200000 [1:20:01<69:35:49,  1.28s/it, loss=0.0198, lr=1.87e-05, step=3731]Training:   2%|‚ñè         | 3732/200000 [1:20:02<71:55:21,  1.32s/it, loss=0.0198, lr=1.87e-05, step=3731]Training:   2%|‚ñè         | 3732/200000 [1:20:02<71:55:21,  1.32s/it, loss=0.0258, lr=1.87e-05, step=3732]Training:   2%|‚ñè         | 3733/200000 [1:20:03<67:57:49,  1.25s/it, loss=0.0258, lr=1.87e-05, step=3732]Training:   2%|‚ñè         | 3733/200000 [1:20:03<67:57:49,  1.25s/it, loss=0.0273, lr=1.87e-05, step=3733]Training:   2%|‚ñè         | 3734/200000 [1:20:05<71:38:43,  1.31s/it, loss=0.0273, lr=1.87e-05, step=3733]Training:   2%|‚ñè         | 3734/200000 [1:20:05<71:38:43,  1.31s/it, loss=0.0185, lr=1.87e-05, step=3734]Training:   2%|‚ñè         | 3735/200000 [1:20:06<67:43:55,  1.24s/it, loss=0.0185, lr=1.87e-05, step=3734]Training:   2%|‚ñè         | 3735/200000 [1:20:06<67:43:55,  1.24s/it, loss=0.0318, lr=1.87e-05, step=3735]Training:   2%|‚ñè         | 3736/200000 [1:20:07<70:12:39,  1.29s/it, loss=0.0318, lr=1.87e-05, step=3735]Training:   2%|‚ñè         | 3736/200000 [1:20:07<70:12:39,  1.29s/it, loss=0.0321, lr=1.87e-05, step=3736]Training:   2%|‚ñè         | 3737/200000 [1:20:08<66:43:08,  1.22s/it, loss=0.0321, lr=1.87e-05, step=3736]Training:   2%|‚ñè         | 3737/200000 [1:20:08<66:43:08,  1.22s/it, loss=0.0183, lr=1.87e-05, step=3737]Training:   2%|‚ñè         | 3738/200000 [1:20:10<71:11:35,  1.31s/it, loss=0.0183, lr=1.87e-05, step=3737]Training:   2%|‚ñè         | 3738/200000 [1:20:10<71:11:35,  1.31s/it, loss=0.1144, lr=1.87e-05, step=3738]Training:   2%|‚ñè         | 3739/200000 [1:20:11<74:42:23,  1.37s/it, loss=0.1144, lr=1.87e-05, step=3738]Training:   2%|‚ñè         | 3739/200000 [1:20:11<74:42:23,  1.37s/it, loss=0.0198, lr=1.87e-05, step=3739]Training:   2%|‚ñè         | 3740/200000 [1:20:13<69:53:25,  1.28s/it, loss=0.0198, lr=1.87e-05, step=3739]Training:   2%|‚ñè         | 3740/200000 [1:20:13<69:53:25,  1.28s/it, loss=0.0135, lr=1.87e-05, step=3740]Training:   2%|‚ñè         | 3741/200000 [1:20:14<66:32:15,  1.22s/it, loss=0.0135, lr=1.87e-05, step=3740]Training:   2%|‚ñè         | 3741/200000 [1:20:14<66:32:15,  1.22s/it, loss=0.0191, lr=1.87e-05, step=3741]Training:   2%|‚ñè         | 3742/200000 [1:20:15<70:46:07,  1.30s/it, loss=0.0191, lr=1.87e-05, step=3741]Training:   2%|‚ñè         | 3742/200000 [1:20:15<70:46:07,  1.30s/it, loss=0.0221, lr=1.87e-05, step=3742]Training:   2%|‚ñè         | 3743/200000 [1:20:16<67:09:04,  1.23s/it, loss=0.0221, lr=1.87e-05, step=3742]Training:   2%|‚ñè         | 3743/200000 [1:20:16<67:09:04,  1.23s/it, loss=0.0246, lr=1.87e-05, step=3743]Training:   2%|‚ñè         | 3744/200000 [1:20:18<68:46:36,  1.26s/it, loss=0.0246, lr=1.87e-05, step=3743]Training:   2%|‚ñè         | 3744/200000 [1:20:18<68:46:36,  1.26s/it, loss=0.0289, lr=1.87e-05, step=3744]Training:   2%|‚ñè         | 3745/200000 [1:20:19<65:47:02,  1.21s/it, loss=0.0289, lr=1.87e-05, step=3744]Training:   2%|‚ñè         | 3745/200000 [1:20:19<65:47:02,  1.21s/it, loss=0.0307, lr=1.87e-05, step=3745]Training:   2%|‚ñè         | 3746/200000 [1:20:20<63:39:31,  1.17s/it, loss=0.0307, lr=1.87e-05, step=3745]Training:   2%|‚ñè         | 3746/200000 [1:20:20<63:39:31,  1.17s/it, loss=0.0318, lr=1.87e-05, step=3746]Training:   2%|‚ñè         | 3747/200000 [1:20:21<68:52:41,  1.26s/it, loss=0.0318, lr=1.87e-05, step=3746]Training:   2%|‚ñè         | 3747/200000 [1:20:21<68:52:41,  1.26s/it, loss=0.0288, lr=1.87e-05, step=3747]Training:   2%|‚ñè         | 3748/200000 [1:20:23<71:16:37,  1.31s/it, loss=0.0288, lr=1.87e-05, step=3747]Training:   2%|‚ñè         | 3748/200000 [1:20:23<71:16:37,  1.31s/it, loss=0.0345, lr=1.87e-05, step=3748]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3749/200000 [1:20:24<73:25:58,  1.35s/it, loss=0.0345, lr=1.87e-05, step=3748]Training:   2%|‚ñè         | 3749/200000 [1:20:24<73:25:58,  1.35s/it, loss=0.0195, lr=1.87e-05, step=3749]Training:   2%|‚ñè         | 3750/200000 [1:20:25<68:59:04,  1.27s/it, loss=0.0195, lr=1.87e-05, step=3749]Training:   2%|‚ñè         | 3750/200000 [1:20:25<68:59:04,  1.27s/it, loss=0.0313, lr=1.87e-05, step=3750]Training:   2%|‚ñè         | 3751/200000 [1:20:26<65:54:08,  1.21s/it, loss=0.0313, lr=1.87e-05, step=3750]Training:   2%|‚ñè         | 3751/200000 [1:20:26<65:54:08,  1.21s/it, loss=0.0258, lr=1.88e-05, step=3751]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3752/200000 [1:20:28<68:11:11,  1.25s/it, loss=0.0258, lr=1.88e-05, step=3751]Training:   2%|‚ñè         | 3752/200000 [1:20:28<68:11:11,  1.25s/it, loss=0.0199, lr=1.88e-05, step=3752]Training:   2%|‚ñè         | 3753/200000 [1:20:29<70:18:28,  1.29s/it, loss=0.0199, lr=1.88e-05, step=3752]Training:   2%|‚ñè         | 3753/200000 [1:20:29<70:18:28,  1.29s/it, loss=0.0325, lr=1.88e-05, step=3753]Training:   2%|‚ñè         | 3754/200000 [1:20:30<66:50:17,  1.23s/it, loss=0.0325, lr=1.88e-05, step=3753]Training:   2%|‚ñè         | 3754/200000 [1:20:30<66:50:17,  1.23s/it, loss=0.0681, lr=1.88e-05, step=3754]Training:   2%|‚ñè         | 3755/200000 [1:20:31<70:23:35,  1.29s/it, loss=0.0681, lr=1.88e-05, step=3754]Training:   2%|‚ñè         | 3755/200000 [1:20:31<70:23:35,  1.29s/it, loss=0.0235, lr=1.88e-05, step=3755]Training:   2%|‚ñè         | 3756/200000 [1:20:32<66:53:14,  1.23s/it, loss=0.0235, lr=1.88e-05, step=3755]Training:   2%|‚ñè         | 3756/200000 [1:20:32<66:53:14,  1.23s/it, loss=0.0127, lr=1.88e-05, step=3756]Training:   2%|‚ñè         | 3757/200000 [1:20:34<69:46:55,  1.28s/it, loss=0.0127, lr=1.88e-05, step=3756]Training:   2%|‚ñè         | 3757/200000 [1:20:34<69:46:55,  1.28s/it, loss=0.0181, lr=1.88e-05, step=3757]Training:   2%|‚ñè         | 3758/200000 [1:20:35<66:25:41,  1.22s/it, loss=0.0181, lr=1.88e-05, step=3757]Training:   2%|‚ñè         | 3758/200000 [1:20:35<66:25:41,  1.22s/it, loss=0.0281, lr=1.88e-05, step=3758]Training:   2%|‚ñè         | 3759/200000 [1:20:36<70:50:16,  1.30s/it, loss=0.0281, lr=1.88e-05, step=3758]Training:   2%|‚ñè         | 3759/200000 [1:20:36<70:50:16,  1.30s/it, loss=0.0196, lr=1.88e-05, step=3759]Training:   2%|‚ñè         | 3760/200000 [1:20:38<74:14:48,  1.36s/it, loss=0.0196, lr=1.88e-05, step=3759]Training:   2%|‚ñè         | 3760/200000 [1:20:38<74:14:48,  1.36s/it, loss=0.0218, lr=1.88e-05, step=3760]Training:   2%|‚ñè         | 3761/200000 [1:20:39<69:34:26,  1.28s/it, loss=0.0218, lr=1.88e-05, step=3760]Training:   2%|‚ñè         | 3761/200000 [1:20:39<69:34:26,  1.28s/it, loss=0.0325, lr=1.88e-05, step=3761]Training:   2%|‚ñè         | 3762/200000 [1:20:40<66:18:59,  1.22s/it, loss=0.0325, lr=1.88e-05, step=3761]Training:   2%|‚ñè         | 3762/200000 [1:20:40<66:18:59,  1.22s/it, loss=0.0283, lr=1.88e-05, step=3762]Training:   2%|‚ñè         | 3763/200000 [1:20:42<70:19:14,  1.29s/it, loss=0.0283, lr=1.88e-05, step=3762]Training:   2%|‚ñè         | 3763/200000 [1:20:42<70:19:14,  1.29s/it, loss=0.0213, lr=1.88e-05, step=3763]Training:   2%|‚ñè         | 3764/200000 [1:20:43<66:49:23,  1.23s/it, loss=0.0213, lr=1.88e-05, step=3763]Training:   2%|‚ñè         | 3764/200000 [1:20:43<66:49:23,  1.23s/it, loss=0.0665, lr=1.88e-05, step=3764]Training:   2%|‚ñè         | 3765/200000 [1:20:44<68:54:49,  1.26s/it, loss=0.0665, lr=1.88e-05, step=3764]Training:   2%|‚ñè         | 3765/200000 [1:20:44<68:54:49,  1.26s/it, loss=0.0340, lr=1.88e-05, step=3765]Training:   2%|‚ñè         | 3766/200000 [1:20:45<65:50:19,  1.21s/it, loss=0.0340, lr=1.88e-05, step=3765]Training:   2%|‚ñè         | 3766/200000 [1:20:45<65:50:19,  1.21s/it, loss=0.0210, lr=1.88e-05, step=3766]Training:   2%|‚ñè         | 3767/200000 [1:20:46<63:43:06,  1.17s/it, loss=0.0210, lr=1.88e-05, step=3766]Training:   2%|‚ñè         | 3767/200000 [1:20:46<63:43:06,  1.17s/it, loss=0.0159, lr=1.88e-05, step=3767]Training:   2%|‚ñè         | 3768/200000 [1:20:48<68:35:18,  1.26s/it, loss=0.0159, lr=1.88e-05, step=3767]Training:   2%|‚ñè         | 3768/200000 [1:20:48<68:35:18,  1.26s/it, loss=0.0395, lr=1.88e-05, step=3768]Training:   2%|‚ñè         | 3769/200000 [1:20:49<71:04:22,  1.30s/it, loss=0.0395, lr=1.88e-05, step=3768]Training:   2%|‚ñè         | 3769/200000 [1:20:49<71:04:22,  1.30s/it, loss=0.0194, lr=1.88e-05, step=3769]Training:   2%|‚ñè         | 3770/200000 [1:20:50<73:07:50,  1.34s/it, loss=0.0194, lr=1.88e-05, step=3769]Training:   2%|‚ñè         | 3770/200000 [1:20:50<73:07:50,  1.34s/it, loss=0.0249, lr=1.88e-05, step=3770]Training:   2%|‚ñè         | 3771/200000 [1:20:52<68:47:40,  1.26s/it, loss=0.0249, lr=1.88e-05, step=3770]Training:   2%|‚ñè         | 3771/200000 [1:20:52<68:47:40,  1.26s/it, loss=0.0234, lr=1.89e-05, step=3771]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3772/200000 [1:20:53<65:43:36,  1.21s/it, loss=0.0234, lr=1.89e-05, step=3771]Training:   2%|‚ñè         | 3772/200000 [1:20:53<65:43:36,  1.21s/it, loss=0.0406, lr=1.89e-05, step=3772]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3773/200000 [1:20:54<67:47:54,  1.24s/it, loss=0.0406, lr=1.89e-05, step=3772]Training:   2%|‚ñè         | 3773/200000 [1:20:54<67:47:54,  1.24s/it, loss=0.0230, lr=1.89e-05, step=3773]Training:   2%|‚ñè         | 3774/200000 [1:20:55<69:08:11,  1.27s/it, loss=0.0230, lr=1.89e-05, step=3773]Training:   2%|‚ñè         | 3774/200000 [1:20:55<69:08:11,  1.27s/it, loss=0.0231, lr=1.89e-05, step=3774]Training:   2%|‚ñè         | 3775/200000 [1:20:56<66:00:12,  1.21s/it, loss=0.0231, lr=1.89e-05, step=3774]Training:   2%|‚ñè         | 3775/200000 [1:20:56<66:00:12,  1.21s/it, loss=0.0366, lr=1.89e-05, step=3775]Training:   2%|‚ñè         | 3776/200000 [1:20:58<68:53:42,  1.26s/it, loss=0.0366, lr=1.89e-05, step=3775]Training:   2%|‚ñè         | 3776/200000 [1:20:58<68:53:42,  1.26s/it, loss=0.0181, lr=1.89e-05, step=3776]Training:   2%|‚ñè         | 3777/200000 [1:20:59<65:50:13,  1.21s/it, loss=0.0181, lr=1.89e-05, step=3776]Training:   2%|‚ñè         | 3777/200000 [1:20:59<65:50:13,  1.21s/it, loss=0.0315, lr=1.89e-05, step=3777]Training:   2%|‚ñè         | 3778/200000 [1:21:00<68:37:56,  1.26s/it, loss=0.0315, lr=1.89e-05, step=3777]Training:   2%|‚ñè         | 3778/200000 [1:21:00<68:37:56,  1.26s/it, loss=0.0306, lr=1.89e-05, step=3778]Training:   2%|‚ñè         | 3779/200000 [1:21:02<71:55:01,  1.32s/it, loss=0.0306, lr=1.89e-05, step=3778]Training:   2%|‚ñè         | 3779/200000 [1:21:02<71:55:01,  1.32s/it, loss=0.0428, lr=1.89e-05, step=3779]Training:   2%|‚ñè         | 3780/200000 [1:21:03<73:32:07,  1.35s/it, loss=0.0428, lr=1.89e-05, step=3779]Training:   2%|‚ñè         | 3780/200000 [1:21:03<73:32:07,  1.35s/it, loss=0.0318, lr=1.89e-05, step=3780]Training:   2%|‚ñè         | 3781/200000 [1:21:05<75:02:20,  1.38s/it, loss=0.0318, lr=1.89e-05, step=3780]Training:   2%|‚ñè         | 3781/200000 [1:21:05<75:02:20,  1.38s/it, loss=0.0384, lr=1.89e-05, step=3781]Training:   2%|‚ñè         | 3782/200000 [1:21:06<70:07:44,  1.29s/it, loss=0.0384, lr=1.89e-05, step=3781]Training:   2%|‚ñè         | 3782/200000 [1:21:06<70:07:44,  1.29s/it, loss=0.0178, lr=1.89e-05, step=3782]Training:   2%|‚ñè         | 3783/200000 [1:21:07<66:42:28,  1.22s/it, loss=0.0178, lr=1.89e-05, step=3782]Training:   2%|‚ñè         | 3783/200000 [1:21:07<66:42:28,  1.22s/it, loss=0.0194, lr=1.89e-05, step=3783]Training:   2%|‚ñè         | 3784/200000 [1:21:08<69:34:54,  1.28s/it, loss=0.0194, lr=1.89e-05, step=3783]Training:   2%|‚ñè         | 3784/200000 [1:21:08<69:34:54,  1.28s/it, loss=0.0432, lr=1.89e-05, step=3784]Training:   2%|‚ñè         | 3785/200000 [1:21:09<71:53:39,  1.32s/it, loss=0.0432, lr=1.89e-05, step=3784]Training:   2%|‚ñè         | 3785/200000 [1:21:09<71:53:39,  1.32s/it, loss=0.0132, lr=1.89e-05, step=3785]Training:   2%|‚ñè         | 3786/200000 [1:21:11<67:55:35,  1.25s/it, loss=0.0132, lr=1.89e-05, step=3785]Training:   2%|‚ñè         | 3786/200000 [1:21:11<67:55:35,  1.25s/it, loss=0.0260, lr=1.89e-05, step=3786]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3787/200000 [1:21:12<70:48:04,  1.30s/it, loss=0.0260, lr=1.89e-05, step=3786]Training:   2%|‚ñè         | 3787/200000 [1:21:12<70:48:04,  1.30s/it, loss=0.0183, lr=1.89e-05, step=3787]Training:   2%|‚ñè         | 3788/200000 [1:21:13<67:10:18,  1.23s/it, loss=0.0183, lr=1.89e-05, step=3787]Training:   2%|‚ñè         | 3788/200000 [1:21:13<67:10:18,  1.23s/it, loss=0.0246, lr=1.89e-05, step=3788]Training:   2%|‚ñè         | 3789/200000 [1:21:14<69:49:49,  1.28s/it, loss=0.0246, lr=1.89e-05, step=3788]Training:   2%|‚ñè         | 3789/200000 [1:21:14<69:49:49,  1.28s/it, loss=0.0239, lr=1.89e-05, step=3789]Training:   2%|‚ñè         | 3790/200000 [1:21:16<66:27:56,  1.22s/it, loss=0.0239, lr=1.89e-05, step=3789]Training:   2%|‚ñè         | 3790/200000 [1:21:16<66:27:56,  1.22s/it, loss=0.0179, lr=1.89e-05, step=3790]Training:   2%|‚ñè         | 3791/200000 [1:21:17<71:05:59,  1.30s/it, loss=0.0179, lr=1.89e-05, step=3790]Training:   2%|‚ñè         | 3791/200000 [1:21:17<71:05:59,  1.30s/it, loss=0.0201, lr=1.90e-05, step=3791]Training:   2%|‚ñè         | 3792/200000 [1:21:19<74:40:59,  1.37s/it, loss=0.0201, lr=1.90e-05, step=3791]Training:   2%|‚ñè         | 3792/200000 [1:21:19<74:40:59,  1.37s/it, loss=0.0215, lr=1.90e-05, step=3792]Training:   2%|‚ñè         | 3793/200000 [1:21:20<69:55:15,  1.28s/it, loss=0.0215, lr=1.90e-05, step=3792]Training:   2%|‚ñè         | 3793/200000 [1:21:20<69:55:15,  1.28s/it, loss=0.0228, lr=1.90e-05, step=3793]Training:   2%|‚ñè         | 3794/200000 [1:21:21<66:32:56,  1.22s/it, loss=0.0228, lr=1.90e-05, step=3793]Training:   2%|‚ñè         | 3794/200000 [1:21:21<66:32:56,  1.22s/it, loss=0.0329, lr=1.90e-05, step=3794]Training:   2%|‚ñè         | 3795/200000 [1:21:22<70:47:22,  1.30s/it, loss=0.0329, lr=1.90e-05, step=3794]Training:   2%|‚ñè         | 3795/200000 [1:21:22<70:47:22,  1.30s/it, loss=0.0177, lr=1.90e-05, step=3795]Training:   2%|‚ñè         | 3796/200000 [1:21:23<67:09:34,  1.23s/it, loss=0.0177, lr=1.90e-05, step=3795]Training:   2%|‚ñè         | 3796/200000 [1:21:23<67:09:34,  1.23s/it, loss=0.0263, lr=1.90e-05, step=3796]Training:   2%|‚ñè         | 3797/200000 [1:21:25<69:16:48,  1.27s/it, loss=0.0263, lr=1.90e-05, step=3796]Training:   2%|‚ñè         | 3797/200000 [1:21:25<69:16:48,  1.27s/it, loss=0.2018, lr=1.90e-05, step=3797]Training:   2%|‚ñè         | 3798/200000 [1:21:26<66:05:38,  1.21s/it, loss=0.2018, lr=1.90e-05, step=3797]Training:   2%|‚ñè         | 3798/200000 [1:21:26<66:05:38,  1.21s/it, loss=0.0163, lr=1.90e-05, step=3798]Training:   2%|‚ñè         | 3799/200000 [1:21:27<63:52:21,  1.17s/it, loss=0.0163, lr=1.90e-05, step=3798]Training:   2%|‚ñè         | 3799/200000 [1:21:27<63:52:21,  1.17s/it, loss=0.0236, lr=1.90e-05, step=3799]Training:   2%|‚ñè         | 3800/200000 [1:21:28<69:00:59,  1.27s/it, loss=0.0236, lr=1.90e-05, step=3799]Training:   2%|‚ñè         | 3800/200000 [1:21:28<69:00:59,  1.27s/it, loss=0.0221, lr=1.90e-05, step=3800]00:14:43.495 [I] step=3800 loss=0.0325 lr=1.88e-05 grad_norm=0.63 time=126.7s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3801/200000 [1:21:30<71:23:58,  1.31s/it, loss=0.0221, lr=1.90e-05, step=3800]Training:   2%|‚ñè         | 3801/200000 [1:21:30<71:23:58,  1.31s/it, loss=0.0798, lr=1.90e-05, step=3801]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3802/200000 [1:21:31<73:20:09,  1.35s/it, loss=0.0798, lr=1.90e-05, step=3801]Training:   2%|‚ñè         | 3802/200000 [1:21:31<73:20:09,  1.35s/it, loss=0.0213, lr=1.90e-05, step=3802]Training:   2%|‚ñè         | 3803/200000 [1:21:32<68:57:28,  1.27s/it, loss=0.0213, lr=1.90e-05, step=3802]Training:   2%|‚ñè         | 3803/200000 [1:21:32<68:57:28,  1.27s/it, loss=0.0121, lr=1.90e-05, step=3803]Training:   2%|‚ñè         | 3804/200000 [1:21:33<65:53:37,  1.21s/it, loss=0.0121, lr=1.90e-05, step=3803]Training:   2%|‚ñè         | 3804/200000 [1:21:33<65:53:37,  1.21s/it, loss=0.0238, lr=1.90e-05, step=3804]Training:   2%|‚ñè         | 3805/200000 [1:21:35<68:15:17,  1.25s/it, loss=0.0238, lr=1.90e-05, step=3804]Training:   2%|‚ñè         | 3805/200000 [1:21:35<68:15:17,  1.25s/it, loss=0.0379, lr=1.90e-05, step=3805]Training:   2%|‚ñè         | 3806/200000 [1:21:36<70:38:34,  1.30s/it, loss=0.0379, lr=1.90e-05, step=3805]Training:   2%|‚ñè         | 3806/200000 [1:21:36<70:38:34,  1.30s/it, loss=0.0323, lr=1.90e-05, step=3806]Training:   2%|‚ñè         | 3807/200000 [1:21:37<67:04:25,  1.23s/it, loss=0.0323, lr=1.90e-05, step=3806]Training:   2%|‚ñè         | 3807/200000 [1:21:37<67:04:25,  1.23s/it, loss=0.0815, lr=1.90e-05, step=3807]Training:   2%|‚ñè         | 3808/200000 [1:21:39<70:04:10,  1.29s/it, loss=0.0815, lr=1.90e-05, step=3807]Training:   2%|‚ñè         | 3808/200000 [1:21:39<70:04:10,  1.29s/it, loss=0.0274, lr=1.90e-05, step=3808]Training:   2%|‚ñè         | 3809/200000 [1:21:40<66:38:57,  1.22s/it, loss=0.0274, lr=1.90e-05, step=3808]Training:   2%|‚ñè         | 3809/200000 [1:21:40<66:38:57,  1.22s/it, loss=0.0429, lr=1.90e-05, step=3809]Training:   2%|‚ñè         | 3810/200000 [1:21:41<69:38:10,  1.28s/it, loss=0.0429, lr=1.90e-05, step=3809]Training:   2%|‚ñè         | 3810/200000 [1:21:41<69:38:10,  1.28s/it, loss=0.0293, lr=1.90e-05, step=3810]Training:   2%|‚ñè         | 3811/200000 [1:21:42<66:21:08,  1.22s/it, loss=0.0293, lr=1.90e-05, step=3810]Training:   2%|‚ñè         | 3811/200000 [1:21:42<66:21:08,  1.22s/it, loss=0.0220, lr=1.91e-05, step=3811]Training:   2%|‚ñè         | 3812/200000 [1:21:44<70:39:10,  1.30s/it, loss=0.0220, lr=1.91e-05, step=3811]Training:   2%|‚ñè         | 3812/200000 [1:21:44<70:39:10,  1.30s/it, loss=0.0214, lr=1.91e-05, step=3812]Training:   2%|‚ñè         | 3813/200000 [1:21:45<73:17:14,  1.34s/it, loss=0.0214, lr=1.91e-05, step=3812]Training:   2%|‚ñè         | 3813/200000 [1:21:45<73:17:14,  1.34s/it, loss=0.0231, lr=1.91e-05, step=3813]Training:   2%|‚ñè         | 3814/200000 [1:21:46<68:55:03,  1.26s/it, loss=0.0231, lr=1.91e-05, step=3813]Training:   2%|‚ñè         | 3814/200000 [1:21:46<68:55:03,  1.26s/it, loss=0.0359, lr=1.91e-05, step=3814]Training:   2%|‚ñè         | 3815/200000 [1:21:47<65:51:44,  1.21s/it, loss=0.0359, lr=1.91e-05, step=3814]Training:   2%|‚ñè         | 3815/200000 [1:21:47<65:51:44,  1.21s/it, loss=0.0174, lr=1.91e-05, step=3815]Training:   2%|‚ñè         | 3816/200000 [1:21:49<69:52:10,  1.28s/it, loss=0.0174, lr=1.91e-05, step=3815]Training:   2%|‚ñè         | 3816/200000 [1:21:49<69:52:10,  1.28s/it, loss=0.0204, lr=1.91e-05, step=3816]Training:   2%|‚ñè         | 3817/200000 [1:21:50<66:31:40,  1.22s/it, loss=0.0204, lr=1.91e-05, step=3816]Training:   2%|‚ñè         | 3817/200000 [1:21:50<66:31:40,  1.22s/it, loss=0.0270, lr=1.91e-05, step=3817]Training:   2%|‚ñè         | 3818/200000 [1:21:51<68:04:04,  1.25s/it, loss=0.0270, lr=1.91e-05, step=3817]Training:   2%|‚ñè         | 3818/200000 [1:21:51<68:04:04,  1.25s/it, loss=0.1595, lr=1.91e-05, step=3818]Training:   2%|‚ñè         | 3819/200000 [1:21:52<65:15:15,  1.20s/it, loss=0.1595, lr=1.91e-05, step=3818]Training:   2%|‚ñè         | 3819/200000 [1:21:52<65:15:15,  1.20s/it, loss=0.0160, lr=1.91e-05, step=3819]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3820/200000 [1:21:53<63:16:19,  1.16s/it, loss=0.0160, lr=1.91e-05, step=3819]Training:   2%|‚ñè         | 3820/200000 [1:21:53<63:16:19,  1.16s/it, loss=0.0269, lr=1.91e-05, step=3820]Training:   2%|‚ñè         | 3821/200000 [1:21:55<67:54:08,  1.25s/it, loss=0.0269, lr=1.91e-05, step=3820]Training:   2%|‚ñè         | 3821/200000 [1:21:55<67:54:08,  1.25s/it, loss=0.0310, lr=1.91e-05, step=3821]Training:   2%|‚ñè         | 3822/200000 [1:21:56<69:55:03,  1.28s/it, loss=0.0310, lr=1.91e-05, step=3821]Training:   2%|‚ñè         | 3822/200000 [1:21:56<69:55:03,  1.28s/it, loss=0.1133, lr=1.91e-05, step=3822]Training:   2%|‚ñè         | 3823/200000 [1:21:57<72:19:51,  1.33s/it, loss=0.1133, lr=1.91e-05, step=3822]Training:   2%|‚ñè         | 3823/200000 [1:21:57<72:19:51,  1.33s/it, loss=0.0284, lr=1.91e-05, step=3823]Training:   2%|‚ñè         | 3824/200000 [1:21:58<68:13:07,  1.25s/it, loss=0.0284, lr=1.91e-05, step=3823]Training:   2%|‚ñè         | 3824/200000 [1:21:58<68:13:07,  1.25s/it, loss=0.0318, lr=1.91e-05, step=3824]Training:   2%|‚ñè         | 3825/200000 [1:22:00<65:21:51,  1.20s/it, loss=0.0318, lr=1.91e-05, step=3824]Training:   2%|‚ñè         | 3825/200000 [1:22:00<65:21:51,  1.20s/it, loss=0.0358, lr=1.91e-05, step=3825]Training:   2%|‚ñè         | 3826/200000 [1:22:01<67:20:26,  1.24s/it, loss=0.0358, lr=1.91e-05, step=3825]Training:   2%|‚ñè         | 3826/200000 [1:22:01<67:20:26,  1.24s/it, loss=0.0311, lr=1.91e-05, step=3826]Training:   2%|‚ñè         | 3827/200000 [1:22:02<68:42:31,  1.26s/it, loss=0.0311, lr=1.91e-05, step=3826]Training:   2%|‚ñè         | 3827/200000 [1:22:02<68:42:31,  1.26s/it, loss=0.0491, lr=1.91e-05, step=3827]Training:   2%|‚ñè         | 3828/200000 [1:22:03<65:42:38,  1.21s/it, loss=0.0491, lr=1.91e-05, step=3827]Training:   2%|‚ñè         | 3828/200000 [1:22:03<65:42:38,  1.21s/it, loss=0.0256, lr=1.91e-05, step=3828]Training:   2%|‚ñè         | 3829/200000 [1:22:05<68:29:09,  1.26s/it, loss=0.0256, lr=1.91e-05, step=3828]Training:   2%|‚ñè         | 3829/200000 [1:22:05<68:29:09,  1.26s/it, loss=0.0189, lr=1.91e-05, step=3829]Training:   2%|‚ñè         | 3830/200000 [1:22:06<65:33:42,  1.20s/it, loss=0.0189, lr=1.91e-05, step=3829]Training:   2%|‚ñè         | 3830/200000 [1:22:06<65:33:42,  1.20s/it, loss=0.0206, lr=1.91e-05, step=3830]Training:   2%|‚ñè         | 3831/200000 [1:22:07<68:10:56,  1.25s/it, loss=0.0206, lr=1.91e-05, step=3830]Training:   2%|‚ñè         | 3831/200000 [1:22:07<68:10:56,  1.25s/it, loss=0.0268, lr=1.92e-05, step=3831]Training:   2%|‚ñè         | 3832/200000 [1:22:09<71:26:20,  1.31s/it, loss=0.0268, lr=1.92e-05, step=3831]Training:   2%|‚ñè         | 3832/200000 [1:22:09<71:26:20,  1.31s/it, loss=0.0194, lr=1.92e-05, step=3832]Training:   2%|‚ñè         | 3833/200000 [1:22:10<73:09:06,  1.34s/it, loss=0.0194, lr=1.92e-05, step=3832]Training:   2%|‚ñè         | 3833/200000 [1:22:10<73:09:06,  1.34s/it, loss=0.0317, lr=1.92e-05, step=3833]Training:   2%|‚ñè         | 3834/200000 [1:22:11<74:50:29,  1.37s/it, loss=0.0317, lr=1.92e-05, step=3833]Training:   2%|‚ñè         | 3834/200000 [1:22:11<74:50:29,  1.37s/it, loss=0.0153, lr=1.92e-05, step=3834]Training:   2%|‚ñè         | 3835/200000 [1:22:12<69:58:03,  1.28s/it, loss=0.0153, lr=1.92e-05, step=3834]Training:   2%|‚ñè         | 3835/200000 [1:22:12<69:58:03,  1.28s/it, loss=0.0225, lr=1.92e-05, step=3835]Training:   2%|‚ñè         | 3836/200000 [1:22:14<66:34:54,  1.22s/it, loss=0.0225, lr=1.92e-05, step=3835]Training:   2%|‚ñè         | 3836/200000 [1:22:14<66:34:54,  1.22s/it, loss=0.0253, lr=1.92e-05, step=3836]Training:   2%|‚ñè         | 3837/200000 [1:22:15<69:25:28,  1.27s/it, loss=0.0253, lr=1.92e-05, step=3836]Training:   2%|‚ñè         | 3837/200000 [1:22:15<69:25:28,  1.27s/it, loss=0.0806, lr=1.92e-05, step=3837]Training:   2%|‚ñè         | 3838/200000 [1:22:16<72:11:12,  1.32s/it, loss=0.0806, lr=1.92e-05, step=3837]Training:   2%|‚ñè         | 3838/200000 [1:22:16<72:11:12,  1.32s/it, loss=0.0211, lr=1.92e-05, step=3838]Training:   2%|‚ñè         | 3839/200000 [1:22:17<68:08:59,  1.25s/it, loss=0.0211, lr=1.92e-05, step=3838]Training:   2%|‚ñè         | 3839/200000 [1:22:17<68:08:59,  1.25s/it, loss=0.0165, lr=1.92e-05, step=3839]Training:   2%|‚ñè         | 3840/200000 [1:22:19<70:58:02,  1.30s/it, loss=0.0165, lr=1.92e-05, step=3839]Training:   2%|‚ñè         | 3840/200000 [1:22:19<70:58:02,  1.30s/it, loss=0.0222, lr=1.92e-05, step=3840]Training:   2%|‚ñè         | 3841/200000 [1:22:20<67:17:30,  1.23s/it, loss=0.0222, lr=1.92e-05, step=3840]Training:   2%|‚ñè         | 3841/200000 [1:22:20<67:17:30,  1.23s/it, loss=0.0318, lr=1.92e-05, step=3841]Training:   2%|‚ñè         | 3842/200000 [1:22:21<70:02:07,  1.29s/it, loss=0.0318, lr=1.92e-05, step=3841]Training:   2%|‚ñè         | 3842/200000 [1:22:21<70:02:07,  1.29s/it, loss=0.0249, lr=1.92e-05, step=3842]Training:   2%|‚ñè         | 3843/200000 [1:22:22<66:37:57,  1.22s/it, loss=0.0249, lr=1.92e-05, step=3842]Training:   2%|‚ñè         | 3843/200000 [1:22:22<66:37:57,  1.22s/it, loss=0.0323, lr=1.92e-05, step=3843]Training:   2%|‚ñè         | 3844/200000 [1:22:24<71:06:48,  1.31s/it, loss=0.0323, lr=1.92e-05, step=3843]Training:   2%|‚ñè         | 3844/200000 [1:22:24<71:06:48,  1.31s/it, loss=0.0289, lr=1.92e-05, step=3844]Training:   2%|‚ñè         | 3845/200000 [1:22:25<74:42:24,  1.37s/it, loss=0.0289, lr=1.92e-05, step=3844]Training:   2%|‚ñè         | 3845/200000 [1:22:25<74:42:24,  1.37s/it, loss=0.1310, lr=1.92e-05, step=3845]Training:   2%|‚ñè         | 3846/200000 [1:22:27<69:52:57,  1.28s/it, loss=0.1310, lr=1.92e-05, step=3845]Training:   2%|‚ñè         | 3846/200000 [1:22:27<69:52:57,  1.28s/it, loss=0.0243, lr=1.92e-05, step=3846]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3847/200000 [1:22:28<66:30:07,  1.22s/it, loss=0.0243, lr=1.92e-05, step=3846]Training:   2%|‚ñè         | 3847/200000 [1:22:28<66:30:07,  1.22s/it, loss=0.0184, lr=1.92e-05, step=3847]Training:   2%|‚ñè         | 3848/200000 [1:22:29<70:32:27,  1.29s/it, loss=0.0184, lr=1.92e-05, step=3847]Training:   2%|‚ñè         | 3848/200000 [1:22:29<70:32:27,  1.29s/it, loss=0.0264, lr=1.92e-05, step=3848]Training:   2%|‚ñè         | 3849/200000 [1:22:30<66:59:32,  1.23s/it, loss=0.0264, lr=1.92e-05, step=3848]Training:   2%|‚ñè         | 3849/200000 [1:22:30<66:59:32,  1.23s/it, loss=0.0337, lr=1.92e-05, step=3849]Training:   2%|‚ñè         | 3850/200000 [1:22:32<69:29:21,  1.28s/it, loss=0.0337, lr=1.92e-05, step=3849]Training:   2%|‚ñè         | 3850/200000 [1:22:32<69:29:21,  1.28s/it, loss=0.0436, lr=1.92e-05, step=3850]Training:   2%|‚ñè         | 3851/200000 [1:22:33<66:11:56,  1.21s/it, loss=0.0436, lr=1.92e-05, step=3850]Training:   2%|‚ñè         | 3851/200000 [1:22:33<66:11:56,  1.21s/it, loss=0.0275, lr=1.93e-05, step=3851]Training:   2%|‚ñè         | 3852/200000 [1:22:34<63:56:04,  1.17s/it, loss=0.0275, lr=1.93e-05, step=3851]Training:   2%|‚ñè         | 3852/200000 [1:22:34<63:56:04,  1.17s/it, loss=0.0368, lr=1.93e-05, step=3852]Training:   2%|‚ñè         | 3853/200000 [1:22:35<68:57:50,  1.27s/it, loss=0.0368, lr=1.93e-05, step=3852]Training:   2%|‚ñè         | 3853/200000 [1:22:35<68:57:50,  1.27s/it, loss=0.0175, lr=1.93e-05, step=3853]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3854/200000 [1:22:37<71:13:21,  1.31s/it, loss=0.0175, lr=1.93e-05, step=3853]Training:   2%|‚ñè         | 3854/200000 [1:22:37<71:13:21,  1.31s/it, loss=0.0234, lr=1.93e-05, step=3854]Training:   2%|‚ñè         | 3855/200000 [1:22:38<73:25:46,  1.35s/it, loss=0.0234, lr=1.93e-05, step=3854]Training:   2%|‚ñè         | 3855/200000 [1:22:38<73:25:46,  1.35s/it, loss=0.0378, lr=1.93e-05, step=3855]Training:   2%|‚ñè         | 3856/200000 [1:22:39<68:59:07,  1.27s/it, loss=0.0378, lr=1.93e-05, step=3855]Training:   2%|‚ñè         | 3856/200000 [1:22:39<68:59:07,  1.27s/it, loss=0.0259, lr=1.93e-05, step=3856]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3857/200000 [1:22:40<65:53:44,  1.21s/it, loss=0.0259, lr=1.93e-05, step=3856]Training:   2%|‚ñè         | 3857/200000 [1:22:40<65:53:44,  1.21s/it, loss=0.0210, lr=1.93e-05, step=3857]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3858/200000 [1:22:42<68:12:23,  1.25s/it, loss=0.0210, lr=1.93e-05, step=3857]Training:   2%|‚ñè         | 3858/200000 [1:22:42<68:12:23,  1.25s/it, loss=0.0190, lr=1.93e-05, step=3858]Training:   2%|‚ñè         | 3859/200000 [1:22:43<70:36:02,  1.30s/it, loss=0.0190, lr=1.93e-05, step=3858]Training:   2%|‚ñè         | 3859/200000 [1:22:43<70:36:02,  1.30s/it, loss=0.0246, lr=1.93e-05, step=3859]Training:   2%|‚ñè         | 3860/200000 [1:22:44<67:00:47,  1.23s/it, loss=0.0246, lr=1.93e-05, step=3859]Training:   2%|‚ñè         | 3860/200000 [1:22:44<67:00:47,  1.23s/it, loss=0.0243, lr=1.93e-05, step=3860]Training:   2%|‚ñè         | 3861/200000 [1:22:45<70:08:47,  1.29s/it, loss=0.0243, lr=1.93e-05, step=3860]Training:   2%|‚ñè         | 3861/200000 [1:22:45<70:08:47,  1.29s/it, loss=0.0272, lr=1.93e-05, step=3861]Training:   2%|‚ñè         | 3862/200000 [1:22:47<66:42:00,  1.22s/it, loss=0.0272, lr=1.93e-05, step=3861]Training:   2%|‚ñè         | 3862/200000 [1:22:47<66:42:00,  1.22s/it, loss=0.0269, lr=1.93e-05, step=3862]Training:   2%|‚ñè         | 3863/200000 [1:22:48<69:27:09,  1.27s/it, loss=0.0269, lr=1.93e-05, step=3862]Training:   2%|‚ñè         | 3863/200000 [1:22:48<69:27:09,  1.27s/it, loss=0.0318, lr=1.93e-05, step=3863]Training:   2%|‚ñè         | 3864/200000 [1:22:49<66:12:17,  1.22s/it, loss=0.0318, lr=1.93e-05, step=3863]Training:   2%|‚ñè         | 3864/200000 [1:22:49<66:12:17,  1.22s/it, loss=0.0249, lr=1.93e-05, step=3864]Training:   2%|‚ñè         | 3865/200000 [1:22:50<70:32:51,  1.29s/it, loss=0.0249, lr=1.93e-05, step=3864]Training:   2%|‚ñè         | 3865/200000 [1:22:50<70:32:51,  1.29s/it, loss=0.0184, lr=1.93e-05, step=3865]Training:   2%|‚ñè         | 3866/200000 [1:22:52<74:00:37,  1.36s/it, loss=0.0184, lr=1.93e-05, step=3865]Training:   2%|‚ñè         | 3866/200000 [1:22:52<74:00:37,  1.36s/it, loss=0.0203, lr=1.93e-05, step=3866]Training:   2%|‚ñè         | 3867/200000 [1:22:53<69:24:35,  1.27s/it, loss=0.0203, lr=1.93e-05, step=3866]Training:   2%|‚ñè         | 3867/200000 [1:22:53<69:24:35,  1.27s/it, loss=0.0220, lr=1.93e-05, step=3867]Training:   2%|‚ñè         | 3868/200000 [1:22:54<66:10:40,  1.21s/it, loss=0.0220, lr=1.93e-05, step=3867]Training:   2%|‚ñè         | 3868/200000 [1:22:54<66:10:40,  1.21s/it, loss=0.0385, lr=1.93e-05, step=3868]Training:   2%|‚ñè         | 3869/200000 [1:22:56<70:10:12,  1.29s/it, loss=0.0385, lr=1.93e-05, step=3868]Training:   2%|‚ñè         | 3869/200000 [1:22:56<70:10:12,  1.29s/it, loss=0.0325, lr=1.93e-05, step=3869]Training:   2%|‚ñè         | 3870/200000 [1:22:57<66:42:04,  1.22s/it, loss=0.0325, lr=1.93e-05, step=3869]Training:   2%|‚ñè         | 3870/200000 [1:22:57<66:42:04,  1.22s/it, loss=0.0161, lr=1.93e-05, step=3870]Training:   2%|‚ñè         | 3871/200000 [1:22:58<68:59:41,  1.27s/it, loss=0.0161, lr=1.93e-05, step=3870]Training:   2%|‚ñè         | 3871/200000 [1:22:58<68:59:41,  1.27s/it, loss=0.0149, lr=1.94e-05, step=3871]Training:   2%|‚ñè         | 3872/200000 [1:22:59<65:51:41,  1.21s/it, loss=0.0149, lr=1.94e-05, step=3871]Training:   2%|‚ñè         | 3872/200000 [1:22:59<65:51:41,  1.21s/it, loss=0.0295, lr=1.94e-05, step=3872]Training:   2%|‚ñè         | 3873/200000 [1:23:00<63:44:13,  1.17s/it, loss=0.0295, lr=1.94e-05, step=3872]Training:   2%|‚ñè         | 3873/200000 [1:23:00<63:44:13,  1.17s/it, loss=0.0174, lr=1.94e-05, step=3873]Training:   2%|‚ñè         | 3874/200000 [1:23:02<68:12:28,  1.25s/it, loss=0.0174, lr=1.94e-05, step=3873]Training:   2%|‚ñè         | 3874/200000 [1:23:02<68:12:28,  1.25s/it, loss=0.0245, lr=1.94e-05, step=3874]Training:   2%|‚ñè         | 3875/200000 [1:23:03<70:42:29,  1.30s/it, loss=0.0245, lr=1.94e-05, step=3874]Training:   2%|‚ñè         | 3875/200000 [1:23:03<70:42:29,  1.30s/it, loss=0.0127, lr=1.94e-05, step=3875]Training:   2%|‚ñè         | 3876/200000 [1:23:04<72:46:37,  1.34s/it, loss=0.0127, lr=1.94e-05, step=3875]Training:   2%|‚ñè         | 3876/200000 [1:23:04<72:46:37,  1.34s/it, loss=0.0341, lr=1.94e-05, step=3876]Training:   2%|‚ñè         | 3877/200000 [1:23:06<68:32:43,  1.26s/it, loss=0.0341, lr=1.94e-05, step=3876]Training:   2%|‚ñè         | 3877/200000 [1:23:06<68:32:43,  1.26s/it, loss=0.0215, lr=1.94e-05, step=3877]Training:   2%|‚ñè         | 3878/200000 [1:23:07<65:35:58,  1.20s/it, loss=0.0215, lr=1.94e-05, step=3877]Training:   2%|‚ñè         | 3878/200000 [1:23:07<65:35:58,  1.20s/it, loss=0.0207, lr=1.94e-05, step=3878]Training:   2%|‚ñè         | 3879/200000 [1:23:08<67:54:42,  1.25s/it, loss=0.0207, lr=1.94e-05, step=3878]Training:   2%|‚ñè         | 3879/200000 [1:23:08<67:54:42,  1.25s/it, loss=0.0219, lr=1.94e-05, step=3879]Training:   2%|‚ñè         | 3880/200000 [1:23:09<69:51:20,  1.28s/it, loss=0.0219, lr=1.94e-05, step=3879]Training:   2%|‚ñè         | 3880/200000 [1:23:09<69:51:20,  1.28s/it, loss=0.0234, lr=1.94e-05, step=3880]Training:   2%|‚ñè         | 3881/200000 [1:23:10<66:25:30,  1.22s/it, loss=0.0234, lr=1.94e-05, step=3880]Training:   2%|‚ñè         | 3881/200000 [1:23:10<66:25:30,  1.22s/it, loss=0.0177, lr=1.94e-05, step=3881]Training:   2%|‚ñè         | 3882/200000 [1:23:12<69:14:16,  1.27s/it, loss=0.0177, lr=1.94e-05, step=3881]Training:   2%|‚ñè         | 3882/200000 [1:23:12<69:14:16,  1.27s/it, loss=0.0320, lr=1.94e-05, step=3882]Training:   2%|‚ñè         | 3883/200000 [1:23:13<66:05:11,  1.21s/it, loss=0.0320, lr=1.94e-05, step=3882]Training:   2%|‚ñè         | 3883/200000 [1:23:13<66:05:11,  1.21s/it, loss=0.0227, lr=1.94e-05, step=3883]Training:   2%|‚ñè         | 3884/200000 [1:23:14<68:15:49,  1.25s/it, loss=0.0227, lr=1.94e-05, step=3883]Training:   2%|‚ñè         | 3884/200000 [1:23:14<68:15:49,  1.25s/it, loss=0.0851, lr=1.94e-05, step=3884]Training:   2%|‚ñè         | 3885/200000 [1:23:16<71:40:36,  1.32s/it, loss=0.0851, lr=1.94e-05, step=3884]Training:   2%|‚ñè         | 3885/200000 [1:23:16<71:40:36,  1.32s/it, loss=0.0232, lr=1.94e-05, step=3885]Training:   2%|‚ñè         | 3886/200000 [1:23:17<73:13:38,  1.34s/it, loss=0.0232, lr=1.94e-05, step=3885]Training:   2%|‚ñè         | 3886/200000 [1:23:17<73:13:38,  1.34s/it, loss=0.0156, lr=1.94e-05, step=3886]Training:   2%|‚ñè         | 3887/200000 [1:23:19<74:59:17,  1.38s/it, loss=0.0156, lr=1.94e-05, step=3886]Training:   2%|‚ñè         | 3887/200000 [1:23:19<74:59:17,  1.38s/it, loss=0.0208, lr=1.94e-05, step=3887]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3888/200000 [1:23:20<70:05:33,  1.29s/it, loss=0.0208, lr=1.94e-05, step=3887]Training:   2%|‚ñè         | 3888/200000 [1:23:20<70:05:33,  1.29s/it, loss=0.0213, lr=1.94e-05, step=3888]Training:   2%|‚ñè         | 3889/200000 [1:23:21<66:39:18,  1.22s/it, loss=0.0213, lr=1.94e-05, step=3888]Training:   2%|‚ñè         | 3889/200000 [1:23:21<66:39:18,  1.22s/it, loss=0.0697, lr=1.94e-05, step=3889]Training:   2%|‚ñè         | 3890/200000 [1:23:22<68:49:21,  1.26s/it, loss=0.0697, lr=1.94e-05, step=3889]Training:   2%|‚ñè         | 3890/200000 [1:23:22<68:49:21,  1.26s/it, loss=0.0193, lr=1.94e-05, step=3890]Training:   2%|‚ñè         | 3891/200000 [1:23:23<71:20:31,  1.31s/it, loss=0.0193, lr=1.94e-05, step=3890]Training:   2%|‚ñè         | 3891/200000 [1:23:23<71:20:31,  1.31s/it, loss=0.0137, lr=1.95e-05, step=3891]Training:   2%|‚ñè         | 3892/200000 [1:23:25<67:31:52,  1.24s/it, loss=0.0137, lr=1.95e-05, step=3891]Training:   2%|‚ñè         | 3892/200000 [1:23:25<67:31:52,  1.24s/it, loss=0.0204, lr=1.95e-05, step=3892]Training:   2%|‚ñè         | 3893/200000 [1:23:26<70:45:40,  1.30s/it, loss=0.0204, lr=1.95e-05, step=3892]Training:   2%|‚ñè         | 3893/200000 [1:23:26<70:45:40,  1.30s/it, loss=0.0189, lr=1.95e-05, step=3893]Training:   2%|‚ñè         | 3894/200000 [1:23:27<67:07:28,  1.23s/it, loss=0.0189, lr=1.95e-05, step=3893]Training:   2%|‚ñè         | 3894/200000 [1:23:27<67:07:28,  1.23s/it, loss=0.0204, lr=1.95e-05, step=3894]Training:   2%|‚ñè         | 3895/200000 [1:23:28<69:49:24,  1.28s/it, loss=0.0204, lr=1.95e-05, step=3894]Training:   2%|‚ñè         | 3895/200000 [1:23:28<69:49:24,  1.28s/it, loss=0.0278, lr=1.95e-05, step=3895]Training:   2%|‚ñè         | 3896/200000 [1:23:30<66:29:41,  1.22s/it, loss=0.0278, lr=1.95e-05, step=3895]Training:   2%|‚ñè         | 3896/200000 [1:23:30<66:29:41,  1.22s/it, loss=0.0290, lr=1.95e-05, step=3896]Training:   2%|‚ñè         | 3897/200000 [1:23:31<71:06:50,  1.31s/it, loss=0.0290, lr=1.95e-05, step=3896]Training:   2%|‚ñè         | 3897/200000 [1:23:31<71:06:50,  1.31s/it, loss=0.0211, lr=1.95e-05, step=3897]Training:   2%|‚ñè         | 3898/200000 [1:23:33<74:40:33,  1.37s/it, loss=0.0211, lr=1.95e-05, step=3897]Training:   2%|‚ñè         | 3898/200000 [1:23:33<74:40:33,  1.37s/it, loss=0.0249, lr=1.95e-05, step=3898]Training:   2%|‚ñè         | 3899/200000 [1:23:34<69:52:32,  1.28s/it, loss=0.0249, lr=1.95e-05, step=3898]Training:   2%|‚ñè         | 3899/200000 [1:23:34<69:52:32,  1.28s/it, loss=0.0335, lr=1.95e-05, step=3899]Training:   2%|‚ñè         | 3900/200000 [1:23:35<66:30:57,  1.22s/it, loss=0.0335, lr=1.95e-05, step=3899]Training:   2%|‚ñè         | 3900/200000 [1:23:35<66:30:57,  1.22s/it, loss=0.1085, lr=1.95e-05, step=3900]00:16:49.997 [I] step=3900 loss=0.0314 lr=1.93e-05 grad_norm=0.58 time=126.5s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 3901/200000 [1:23:36<70:46:47,  1.30s/it, loss=0.1085, lr=1.95e-05, step=3900]Training:   2%|‚ñè         | 3901/200000 [1:23:36<70:46:47,  1.30s/it, loss=0.0212, lr=1.95e-05, step=3901]Training:   2%|‚ñè         | 3902/200000 [1:23:37<67:09:48,  1.23s/it, loss=0.0212, lr=1.95e-05, step=3901]Training:   2%|‚ñè         | 3902/200000 [1:23:37<67:09:48,  1.23s/it, loss=0.0294, lr=1.95e-05, step=3902]Training:   2%|‚ñè         | 3903/200000 [1:23:39<68:08:39,  1.25s/it, loss=0.0294, lr=1.95e-05, step=3902]Training:   2%|‚ñè         | 3903/200000 [1:23:39<68:08:39,  1.25s/it, loss=0.0352, lr=1.95e-05, step=3903]Training:   2%|‚ñè         | 3904/200000 [1:23:40<65:17:25,  1.20s/it, loss=0.0352, lr=1.95e-05, step=3903]Training:   2%|‚ñè         | 3904/200000 [1:23:40<65:17:25,  1.20s/it, loss=0.0262, lr=1.95e-05, step=3904]Training:   2%|‚ñè         | 3905/200000 [1:23:41<63:17:49,  1.16s/it, loss=0.0262, lr=1.95e-05, step=3904]Training:   2%|‚ñè         | 3905/200000 [1:23:41<63:17:49,  1.16s/it, loss=0.0207, lr=1.95e-05, step=3905]Training:   2%|‚ñè         | 3906/200000 [1:23:42<69:05:03,  1.27s/it, loss=0.0207, lr=1.95e-05, step=3905]Training:   2%|‚ñè         | 3906/200000 [1:23:42<69:05:03,  1.27s/it, loss=0.0227, lr=1.95e-05, step=3906]Training:   2%|‚ñè         | 3907/200000 [1:23:44<71:29:48,  1.31s/it, loss=0.0227, lr=1.95e-05, step=3906]Training:   2%|‚ñè         | 3907/200000 [1:23:44<71:29:48,  1.31s/it, loss=0.0218, lr=1.95e-05, step=3907]Training:   2%|‚ñè         | 3908/200000 [1:23:45<73:33:18,  1.35s/it, loss=0.0218, lr=1.95e-05, step=3907]Training:   2%|‚ñè         | 3908/200000 [1:23:45<73:33:18,  1.35s/it, loss=0.0246, lr=1.95e-05, step=3908]Training:   2%|‚ñè         | 3909/200000 [1:23:46<69:05:36,  1.27s/it, loss=0.0246, lr=1.95e-05, step=3908]Training:   2%|‚ñè         | 3909/200000 [1:23:46<69:05:36,  1.27s/it, loss=0.0223, lr=1.95e-05, step=3909]Training:   2%|‚ñè         | 3910/200000 [1:23:47<65:58:38,  1.21s/it, loss=0.0223, lr=1.95e-05, step=3909]Training:   2%|‚ñè         | 3910/200000 [1:23:47<65:58:38,  1.21s/it, loss=0.0235, lr=1.95e-05, step=3910]Training:   2%|‚ñè         | 3911/200000 [1:23:49<68:19:01,  1.25s/it, loss=0.0235, lr=1.95e-05, step=3910]Training:   2%|‚ñè         | 3911/200000 [1:23:49<68:19:01,  1.25s/it, loss=0.0251, lr=1.96e-05, step=3911]Training:   2%|‚ñè         | 3912/200000 [1:23:50<70:44:36,  1.30s/it, loss=0.0251, lr=1.96e-05, step=3911]Training:   2%|‚ñè         | 3912/200000 [1:23:50<70:44:36,  1.30s/it, loss=0.0302, lr=1.96e-05, step=3912]Training:   2%|‚ñè         | 3913/200000 [1:23:51<67:06:08,  1.23s/it, loss=0.0302, lr=1.96e-05, step=3912]Training:   2%|‚ñè         | 3913/200000 [1:23:51<67:06:08,  1.23s/it, loss=0.0251, lr=1.96e-05, step=3913]Training:   2%|‚ñè         | 3914/200000 [1:23:53<70:42:53,  1.30s/it, loss=0.0251, lr=1.96e-05, step=3913]Training:   2%|‚ñè         | 3914/200000 [1:23:53<70:42:53,  1.30s/it, loss=0.0221, lr=1.96e-05, step=3914]Training:   2%|‚ñè         | 3915/200000 [1:23:54<67:05:34,  1.23s/it, loss=0.0221, lr=1.96e-05, step=3914]Training:   2%|‚ñè         | 3915/200000 [1:23:54<67:05:34,  1.23s/it, loss=0.0349, lr=1.96e-05, step=3915]Training:   2%|‚ñè         | 3916/200000 [1:23:55<69:52:34,  1.28s/it, loss=0.0349, lr=1.96e-05, step=3915]Training:   2%|‚ñè         | 3916/200000 [1:23:55<69:52:34,  1.28s/it, loss=0.0303, lr=1.96e-05, step=3916]Training:   2%|‚ñè         | 3917/200000 [1:23:56<66:30:26,  1.22s/it, loss=0.0303, lr=1.96e-05, step=3916]Training:   2%|‚ñè         | 3917/200000 [1:23:56<66:30:26,  1.22s/it, loss=0.0448, lr=1.96e-05, step=3917]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3918/200000 [1:23:58<70:46:10,  1.30s/it, loss=0.0448, lr=1.96e-05, step=3917]Training:   2%|‚ñè         | 3918/200000 [1:23:58<70:46:10,  1.30s/it, loss=0.0208, lr=1.96e-05, step=3918]Training:   2%|‚ñè         | 3919/200000 [1:23:59<73:58:50,  1.36s/it, loss=0.0208, lr=1.96e-05, step=3918]Training:   2%|‚ñè         | 3919/200000 [1:23:59<73:58:50,  1.36s/it, loss=0.0174, lr=1.96e-05, step=3919]Training:   2%|‚ñè         | 3920/200000 [1:24:00<69:22:50,  1.27s/it, loss=0.0174, lr=1.96e-05, step=3919]Training:   2%|‚ñè         | 3920/200000 [1:24:00<69:22:50,  1.27s/it, loss=0.0199, lr=1.96e-05, step=3920]Training:   2%|‚ñè         | 3921/200000 [1:24:01<66:10:25,  1.21s/it, loss=0.0199, lr=1.96e-05, step=3920]Training:   2%|‚ñè         | 3921/200000 [1:24:01<66:10:25,  1.21s/it, loss=0.0345, lr=1.96e-05, step=3921]Training:   2%|‚ñè         | 3922/200000 [1:24:03<70:16:05,  1.29s/it, loss=0.0345, lr=1.96e-05, step=3921]Training:   2%|‚ñè         | 3922/200000 [1:24:03<70:16:05,  1.29s/it, loss=0.0231, lr=1.96e-05, step=3922]Training:   2%|‚ñè         | 3923/200000 [1:24:04<66:45:21,  1.23s/it, loss=0.0231, lr=1.96e-05, step=3922]Training:   2%|‚ñè         | 3923/200000 [1:24:04<66:45:21,  1.23s/it, loss=0.0227, lr=1.96e-05, step=3923]Training:   2%|‚ñè         | 3924/200000 [1:24:05<68:10:50,  1.25s/it, loss=0.0227, lr=1.96e-05, step=3923]Training:   2%|‚ñè         | 3924/200000 [1:24:05<68:10:50,  1.25s/it, loss=0.0272, lr=1.96e-05, step=3924]Training:   2%|‚ñè         | 3925/200000 [1:24:06<65:19:46,  1.20s/it, loss=0.0272, lr=1.96e-05, step=3924]Training:   2%|‚ñè         | 3925/200000 [1:24:06<65:19:46,  1.20s/it, loss=0.0236, lr=1.96e-05, step=3925]Training:   2%|‚ñè         | 3926/200000 [1:24:07<63:17:49,  1.16s/it, loss=0.0236, lr=1.96e-05, step=3925]Training:   2%|‚ñè         | 3926/200000 [1:24:07<63:17:49,  1.16s/it, loss=0.0188, lr=1.96e-05, step=3926]Training:   2%|‚ñè         | 3927/200000 [1:24:09<67:50:19,  1.25s/it, loss=0.0188, lr=1.96e-05, step=3926]Training:   2%|‚ñè         | 3927/200000 [1:24:09<67:50:19,  1.25s/it, loss=0.0235, lr=1.96e-05, step=3927]Training:   2%|‚ñè         | 3928/200000 [1:24:10<70:28:31,  1.29s/it, loss=0.0235, lr=1.96e-05, step=3927]Training:   2%|‚ñè         | 3928/200000 [1:24:10<70:28:31,  1.29s/it, loss=0.0359, lr=1.96e-05, step=3928]Training:   2%|‚ñè         | 3929/200000 [1:24:11<72:29:17,  1.33s/it, loss=0.0359, lr=1.96e-05, step=3928]Training:   2%|‚ñè         | 3929/200000 [1:24:11<72:29:17,  1.33s/it, loss=0.0228, lr=1.96e-05, step=3929]Training:   2%|‚ñè         | 3930/200000 [1:24:13<68:19:04,  1.25s/it, loss=0.0228, lr=1.96e-05, step=3929]Training:   2%|‚ñè         | 3930/200000 [1:24:13<68:19:04,  1.25s/it, loss=0.0259, lr=1.96e-05, step=3930]Training:   2%|‚ñè         | 3931/200000 [1:24:14<65:27:31,  1.20s/it, loss=0.0259, lr=1.96e-05, step=3930]Training:   2%|‚ñè         | 3931/200000 [1:24:14<65:27:31,  1.20s/it, loss=0.0267, lr=1.97e-05, step=3931]Training:   2%|‚ñè         | 3932/200000 [1:24:15<67:53:05,  1.25s/it, loss=0.0267, lr=1.97e-05, step=3931]Training:   2%|‚ñè         | 3932/200000 [1:24:15<67:53:05,  1.25s/it, loss=0.0277, lr=1.97e-05, step=3932]Training:   2%|‚ñè         | 3933/200000 [1:24:16<69:15:03,  1.27s/it, loss=0.0277, lr=1.97e-05, step=3932]Training:   2%|‚ñè         | 3933/200000 [1:24:16<69:15:03,  1.27s/it, loss=0.0222, lr=1.97e-05, step=3933]Training:   2%|‚ñè         | 3934/200000 [1:24:17<66:04:57,  1.21s/it, loss=0.0222, lr=1.97e-05, step=3933]Training:   2%|‚ñè         | 3934/200000 [1:24:17<66:04:57,  1.21s/it, loss=0.0334, lr=1.97e-05, step=3934]Training:   2%|‚ñè         | 3935/200000 [1:24:19<68:06:30,  1.25s/it, loss=0.0334, lr=1.97e-05, step=3934]Training:   2%|‚ñè         | 3935/200000 [1:24:19<68:06:30,  1.25s/it, loss=0.0265, lr=1.97e-05, step=3935]Training:   2%|‚ñè         | 3936/200000 [1:24:20<65:16:11,  1.20s/it, loss=0.0265, lr=1.97e-05, step=3935]Training:   2%|‚ñè         | 3936/200000 [1:24:20<65:16:11,  1.20s/it, loss=0.0309, lr=1.97e-05, step=3936]Training:   2%|‚ñè         | 3937/200000 [1:24:21<67:45:02,  1.24s/it, loss=0.0309, lr=1.97e-05, step=3936]Training:   2%|‚ñè         | 3937/200000 [1:24:21<67:45:02,  1.24s/it, loss=0.0216, lr=1.97e-05, step=3937]Training:   2%|‚ñè         | 3938/200000 [1:24:23<71:12:41,  1.31s/it, loss=0.0216, lr=1.97e-05, step=3937]Training:   2%|‚ñè         | 3938/200000 [1:24:23<71:12:41,  1.31s/it, loss=0.0928, lr=1.97e-05, step=3938]Training:   2%|‚ñè         | 3939/200000 [1:24:24<72:58:08,  1.34s/it, loss=0.0928, lr=1.97e-05, step=3938]Training:   2%|‚ñè         | 3939/200000 [1:24:24<72:58:08,  1.34s/it, loss=0.0246, lr=1.97e-05, step=3939]Training:   2%|‚ñè         | 3940/200000 [1:24:25<74:06:39,  1.36s/it, loss=0.0246, lr=1.97e-05, step=3939]Training:   2%|‚ñè         | 3940/200000 [1:24:25<74:06:39,  1.36s/it, loss=0.0143, lr=1.97e-05, step=3940]Training:   2%|‚ñè         | 3941/200000 [1:24:27<69:29:12,  1.28s/it, loss=0.0143, lr=1.97e-05, step=3940]Training:   2%|‚ñè         | 3941/200000 [1:24:27<69:29:12,  1.28s/it, loss=0.0249, lr=1.97e-05, step=3941]Training:   2%|‚ñè         | 3942/200000 [1:24:28<66:11:26,  1.22s/it, loss=0.0249, lr=1.97e-05, step=3941]Training:   2%|‚ñè         | 3942/200000 [1:24:28<66:11:26,  1.22s/it, loss=0.0332, lr=1.97e-05, step=3942]Training:   2%|‚ñè         | 3943/200000 [1:24:29<69:17:24,  1.27s/it, loss=0.0332, lr=1.97e-05, step=3942]Training:   2%|‚ñè         | 3943/200000 [1:24:29<69:17:24,  1.27s/it, loss=0.0201, lr=1.97e-05, step=3943]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3944/200000 [1:24:30<71:50:52,  1.32s/it, loss=0.0201, lr=1.97e-05, step=3943]Training:   2%|‚ñè         | 3944/200000 [1:24:30<71:50:52,  1.32s/it, loss=0.0257, lr=1.97e-05, step=3944]Training:   2%|‚ñè         | 3945/200000 [1:24:32<67:53:44,  1.25s/it, loss=0.0257, lr=1.97e-05, step=3944]Training:   2%|‚ñè         | 3945/200000 [1:24:32<67:53:44,  1.25s/it, loss=0.0260, lr=1.97e-05, step=3945]Training:   2%|‚ñè         | 3946/200000 [1:24:33<71:40:36,  1.32s/it, loss=0.0260, lr=1.97e-05, step=3945]Training:   2%|‚ñè         | 3946/200000 [1:24:33<71:40:36,  1.32s/it, loss=0.0205, lr=1.97e-05, step=3946]Training:   2%|‚ñè         | 3947/200000 [1:24:34<67:45:33,  1.24s/it, loss=0.0205, lr=1.97e-05, step=3946]Training:   2%|‚ñè         | 3947/200000 [1:24:34<67:45:33,  1.24s/it, loss=0.0296, lr=1.97e-05, step=3947]Training:   2%|‚ñè         | 3948/200000 [1:24:35<70:11:46,  1.29s/it, loss=0.0296, lr=1.97e-05, step=3947]Training:   2%|‚ñè         | 3948/200000 [1:24:35<70:11:46,  1.29s/it, loss=0.0396, lr=1.97e-05, step=3948]Training:   2%|‚ñè         | 3949/200000 [1:24:37<66:45:30,  1.23s/it, loss=0.0396, lr=1.97e-05, step=3948]Training:   2%|‚ñè         | 3949/200000 [1:24:37<66:45:30,  1.23s/it, loss=0.0340, lr=1.97e-05, step=3949]Training:   2%|‚ñè         | 3950/200000 [1:24:38<71:58:58,  1.32s/it, loss=0.0340, lr=1.97e-05, step=3949]Training:   2%|‚ñè         | 3950/200000 [1:24:38<71:58:58,  1.32s/it, loss=0.0190, lr=1.97e-05, step=3950]Training:   2%|‚ñè         | 3951/200000 [1:24:40<75:10:56,  1.38s/it, loss=0.0190, lr=1.97e-05, step=3950]Training:   2%|‚ñè         | 3951/200000 [1:24:40<75:10:56,  1.38s/it, loss=0.0187, lr=1.98e-05, step=3951]Training:   2%|‚ñè         | 3952/200000 [1:24:41<70:12:44,  1.29s/it, loss=0.0187, lr=1.98e-05, step=3951]Training:   2%|‚ñè         | 3952/200000 [1:24:41<70:12:44,  1.29s/it, loss=0.0351, lr=1.98e-05, step=3952]Training:   2%|‚ñè         | 3953/200000 [1:24:42<66:44:01,  1.23s/it, loss=0.0351, lr=1.98e-05, step=3952]Training:   2%|‚ñè         | 3953/200000 [1:24:42<66:44:01,  1.23s/it, loss=0.0204, lr=1.98e-05, step=3953]Training:   2%|‚ñè         | 3954/200000 [1:24:43<70:58:47,  1.30s/it, loss=0.0204, lr=1.98e-05, step=3953]Training:   2%|‚ñè         | 3954/200000 [1:24:43<70:58:47,  1.30s/it, loss=0.0323, lr=1.98e-05, step=3954]Training:   2%|‚ñè         | 3955/200000 [1:24:44<67:16:38,  1.24s/it, loss=0.0323, lr=1.98e-05, step=3954]Training:   2%|‚ñè         | 3955/200000 [1:24:44<67:16:38,  1.24s/it, loss=0.0218, lr=1.98e-05, step=3955]Training:   2%|‚ñè         | 3956/200000 [1:24:46<68:56:35,  1.27s/it, loss=0.0218, lr=1.98e-05, step=3955]Training:   2%|‚ñè         | 3956/200000 [1:24:46<68:56:35,  1.27s/it, loss=0.0337, lr=1.98e-05, step=3956]Training:   2%|‚ñè         | 3957/200000 [1:24:47<65:52:43,  1.21s/it, loss=0.0337, lr=1.98e-05, step=3956]Training:   2%|‚ñè         | 3957/200000 [1:24:47<65:52:43,  1.21s/it, loss=0.0459, lr=1.98e-05, step=3957]Training:   2%|‚ñè         | 3958/200000 [1:24:48<63:42:34,  1.17s/it, loss=0.0459, lr=1.98e-05, step=3957]Training:   2%|‚ñè         | 3958/200000 [1:24:48<63:42:34,  1.17s/it, loss=0.0183, lr=1.98e-05, step=3958]Training:   2%|‚ñè         | 3959/200000 [1:24:49<68:15:01,  1.25s/it, loss=0.0183, lr=1.98e-05, step=3958]Training:   2%|‚ñè         | 3959/200000 [1:24:49<68:15:01,  1.25s/it, loss=0.0245, lr=1.98e-05, step=3959]Training:   2%|‚ñè         | 3960/200000 [1:24:51<70:05:17,  1.29s/it, loss=0.0245, lr=1.98e-05, step=3959]Training:   2%|‚ñè         | 3960/200000 [1:24:51<70:05:17,  1.29s/it, loss=0.0225, lr=1.98e-05, step=3960]Training:   2%|‚ñè         | 3961/200000 [1:24:52<71:40:00,  1.32s/it, loss=0.0225, lr=1.98e-05, step=3960]Training:   2%|‚ñè         | 3961/200000 [1:24:52<71:40:00,  1.32s/it, loss=0.0218, lr=1.98e-05, step=3961]Training:   2%|‚ñè         | 3962/200000 [1:24:53<67:44:47,  1.24s/it, loss=0.0218, lr=1.98e-05, step=3961]Training:   2%|‚ñè         | 3962/200000 [1:24:53<67:44:47,  1.24s/it, loss=0.0196, lr=1.98e-05, step=3962]Training:   2%|‚ñè         | 3963/200000 [1:24:54<65:02:12,  1.19s/it, loss=0.0196, lr=1.98e-05, step=3962]Training:   2%|‚ñè         | 3963/200000 [1:24:54<65:02:12,  1.19s/it, loss=0.0170, lr=1.98e-05, step=3963]Training:   2%|‚ñè         | 3964/200000 [1:24:55<67:25:06,  1.24s/it, loss=0.0170, lr=1.98e-05, step=3963]Training:   2%|‚ñè         | 3964/200000 [1:24:55<67:25:06,  1.24s/it, loss=0.0272, lr=1.98e-05, step=3964]Training:   2%|‚ñè         | 3965/200000 [1:24:57<69:51:19,  1.28s/it, loss=0.0272, lr=1.98e-05, step=3964]Training:   2%|‚ñè         | 3965/200000 [1:24:57<69:51:19,  1.28s/it, loss=0.0256, lr=1.98e-05, step=3965]Training:   2%|‚ñè         | 3966/200000 [1:24:58<66:28:13,  1.22s/it, loss=0.0256, lr=1.98e-05, step=3965]Training:   2%|‚ñè         | 3966/200000 [1:24:58<66:28:13,  1.22s/it, loss=0.0327, lr=1.98e-05, step=3966]Training:   2%|‚ñè         | 3967/200000 [1:24:59<70:09:13,  1.29s/it, loss=0.0327, lr=1.98e-05, step=3966]Training:   2%|‚ñè         | 3967/200000 [1:24:59<70:09:13,  1.29s/it, loss=0.1139, lr=1.98e-05, step=3967]Training:   2%|‚ñè         | 3968/200000 [1:25:00<66:40:39,  1.22s/it, loss=0.1139, lr=1.98e-05, step=3967]Training:   2%|‚ñè         | 3968/200000 [1:25:00<66:40:39,  1.22s/it, loss=0.0168, lr=1.98e-05, step=3968]Training:   2%|‚ñè         | 3969/200000 [1:25:02<69:39:13,  1.28s/it, loss=0.0168, lr=1.98e-05, step=3968]Training:   2%|‚ñè         | 3969/200000 [1:25:02<69:39:13,  1.28s/it, loss=0.0263, lr=1.98e-05, step=3969]Training:   2%|‚ñè         | 3970/200000 [1:25:03<66:21:21,  1.22s/it, loss=0.0263, lr=1.98e-05, step=3969]Training:   2%|‚ñè         | 3970/200000 [1:25:03<66:21:21,  1.22s/it, loss=0.0213, lr=1.98e-05, step=3970]Training:   2%|‚ñè         | 3971/200000 [1:25:04<70:40:34,  1.30s/it, loss=0.0213, lr=1.98e-05, step=3970]Training:   2%|‚ñè         | 3971/200000 [1:25:04<70:40:34,  1.30s/it, loss=0.0334, lr=1.99e-05, step=3971]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3972/200000 [1:25:06<74:03:42,  1.36s/it, loss=0.0334, lr=1.99e-05, step=3971]Training:   2%|‚ñè         | 3972/200000 [1:25:06<74:03:42,  1.36s/it, loss=0.0200, lr=1.99e-05, step=3972]Training:   2%|‚ñè         | 3973/200000 [1:25:07<69:26:38,  1.28s/it, loss=0.0200, lr=1.99e-05, step=3972]Training:   2%|‚ñè         | 3973/200000 [1:25:07<69:26:38,  1.28s/it, loss=0.0291, lr=1.99e-05, step=3973]Training:   2%|‚ñè         | 3974/200000 [1:25:08<66:09:57,  1.22s/it, loss=0.0291, lr=1.99e-05, step=3973]Training:   2%|‚ñè         | 3974/200000 [1:25:08<66:09:57,  1.22s/it, loss=0.0261, lr=1.99e-05, step=3974]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3975/200000 [1:25:10<70:16:54,  1.29s/it, loss=0.0261, lr=1.99e-05, step=3974]Training:   2%|‚ñè         | 3975/200000 [1:25:10<70:16:54,  1.29s/it, loss=0.0194, lr=1.99e-05, step=3975]Training:   2%|‚ñè         | 3976/200000 [1:25:11<66:46:58,  1.23s/it, loss=0.0194, lr=1.99e-05, step=3975]Training:   2%|‚ñè         | 3976/200000 [1:25:11<66:46:58,  1.23s/it, loss=0.0222, lr=1.99e-05, step=3976]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3977/200000 [1:25:12<68:56:38,  1.27s/it, loss=0.0222, lr=1.99e-05, step=3976]Training:   2%|‚ñè         | 3977/200000 [1:25:12<68:56:38,  1.27s/it, loss=0.0192, lr=1.99e-05, step=3977]Training:   2%|‚ñè         | 3978/200000 [1:25:13<65:48:59,  1.21s/it, loss=0.0192, lr=1.99e-05, step=3977]Training:   2%|‚ñè         | 3978/200000 [1:25:13<65:48:59,  1.21s/it, loss=0.0189, lr=1.99e-05, step=3978]Training:   2%|‚ñè         | 3979/200000 [1:25:14<63:41:28,  1.17s/it, loss=0.0189, lr=1.99e-05, step=3978]Training:   2%|‚ñè         | 3979/200000 [1:25:14<63:41:28,  1.17s/it, loss=0.0158, lr=1.99e-05, step=3979]Training:   2%|‚ñè         | 3980/200000 [1:25:16<68:12:19,  1.25s/it, loss=0.0158, lr=1.99e-05, step=3979]Training:   2%|‚ñè         | 3980/200000 [1:25:16<68:12:19,  1.25s/it, loss=0.0163, lr=1.99e-05, step=3980]Training:   2%|‚ñè         | 3981/200000 [1:25:17<69:29:41,  1.28s/it, loss=0.0163, lr=1.99e-05, step=3980]Training:   2%|‚ñè         | 3981/200000 [1:25:17<69:29:41,  1.28s/it, loss=0.0171, lr=1.99e-05, step=3981]Training:   2%|‚ñè         | 3982/200000 [1:25:18<71:44:06,  1.32s/it, loss=0.0171, lr=1.99e-05, step=3981]Training:   2%|‚ñè         | 3982/200000 [1:25:18<71:44:06,  1.32s/it, loss=0.0234, lr=1.99e-05, step=3982]Training:   2%|‚ñè         | 3983/200000 [1:25:19<67:46:29,  1.24s/it, loss=0.0234, lr=1.99e-05, step=3982]Training:   2%|‚ñè         | 3983/200000 [1:25:19<67:46:29,  1.24s/it, loss=0.0301, lr=1.99e-05, step=3983]Training:   2%|‚ñè         | 3984/200000 [1:25:21<65:01:21,  1.19s/it, loss=0.0301, lr=1.99e-05, step=3983]Training:   2%|‚ñè         | 3984/200000 [1:25:21<65:01:21,  1.19s/it, loss=0.0241, lr=1.99e-05, step=3984]Training:   2%|‚ñè         | 3985/200000 [1:25:22<67:23:31,  1.24s/it, loss=0.0241, lr=1.99e-05, step=3984]Training:   2%|‚ñè         | 3985/200000 [1:25:22<67:23:31,  1.24s/it, loss=0.0366, lr=1.99e-05, step=3985]Training:   2%|‚ñè         | 3986/200000 [1:25:23<68:37:25,  1.26s/it, loss=0.0366, lr=1.99e-05, step=3985]Training:   2%|‚ñè         | 3986/200000 [1:25:23<68:37:25,  1.26s/it, loss=0.0188, lr=1.99e-05, step=3986]Training:   2%|‚ñè         | 3987/200000 [1:25:24<65:37:30,  1.21s/it, loss=0.0188, lr=1.99e-05, step=3986]Training:   2%|‚ñè         | 3987/200000 [1:25:24<65:37:30,  1.21s/it, loss=0.0134, lr=1.99e-05, step=3987]Training:   2%|‚ñè         | 3988/200000 [1:25:26<68:36:01,  1.26s/it, loss=0.0134, lr=1.99e-05, step=3987]Training:   2%|‚ñè         | 3988/200000 [1:25:26<68:36:01,  1.26s/it, loss=0.0193, lr=1.99e-05, step=3988]Training:   2%|‚ñè         | 3989/200000 [1:25:27<65:36:58,  1.21s/it, loss=0.0193, lr=1.99e-05, step=3988]Training:   2%|‚ñè         | 3989/200000 [1:25:27<65:36:58,  1.21s/it, loss=0.0362, lr=1.99e-05, step=3989]Training:   2%|‚ñè         | 3990/200000 [1:25:28<67:53:37,  1.25s/it, loss=0.0362, lr=1.99e-05, step=3989]Training:   2%|‚ñè         | 3990/200000 [1:25:28<67:53:37,  1.25s/it, loss=0.0186, lr=1.99e-05, step=3990]Training:   2%|‚ñè         | 3991/200000 [1:25:30<71:25:31,  1.31s/it, loss=0.0186, lr=1.99e-05, step=3990]Training:   2%|‚ñè         | 3991/200000 [1:25:30<71:25:31,  1.31s/it, loss=0.0334, lr=2.00e-05, step=3991]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3992/200000 [1:25:31<73:07:31,  1.34s/it, loss=0.0334, lr=2.00e-05, step=3991]Training:   2%|‚ñè         | 3992/200000 [1:25:31<73:07:31,  1.34s/it, loss=0.0158, lr=2.00e-05, step=3992]Training:   2%|‚ñè         | 3993/200000 [1:25:32<74:49:17,  1.37s/it, loss=0.0158, lr=2.00e-05, step=3992]Training:   2%|‚ñè         | 3993/200000 [1:25:32<74:49:17,  1.37s/it, loss=0.0200, lr=2.00e-05, step=3993]Training:   2%|‚ñè         | 3994/200000 [1:25:33<69:56:32,  1.28s/it, loss=0.0200, lr=2.00e-05, step=3993]Training:   2%|‚ñè         | 3994/200000 [1:25:33<69:56:32,  1.28s/it, loss=0.0225, lr=2.00e-05, step=3994]Training:   2%|‚ñè         | 3995/200000 [1:25:35<66:33:55,  1.22s/it, loss=0.0225, lr=2.00e-05, step=3994]Training:   2%|‚ñè         | 3995/200000 [1:25:35<66:33:55,  1.22s/it, loss=0.0254, lr=2.00e-05, step=3995]Training:   2%|‚ñè         | 3996/200000 [1:25:36<69:32:50,  1.28s/it, loss=0.0254, lr=2.00e-05, step=3995]Training:   2%|‚ñè         | 3996/200000 [1:25:36<69:32:50,  1.28s/it, loss=0.0254, lr=2.00e-05, step=3996]Training:   2%|‚ñè         | 3997/200000 [1:25:37<72:01:10,  1.32s/it, loss=0.0254, lr=2.00e-05, step=3996]Training:   2%|‚ñè         | 3997/200000 [1:25:37<72:01:10,  1.32s/it, loss=0.0224, lr=2.00e-05, step=3997]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 3998/200000 [1:25:38<67:58:52,  1.25s/it, loss=0.0224, lr=2.00e-05, step=3997]Training:   2%|‚ñè         | 3998/200000 [1:25:38<67:58:52,  1.25s/it, loss=0.0344, lr=2.00e-05, step=3998]Training:   2%|‚ñè         | 3999/200000 [1:25:40<70:53:50,  1.30s/it, loss=0.0344, lr=2.00e-05, step=3998]Training:   2%|‚ñè         | 3999/200000 [1:25:40<70:53:50,  1.30s/it, loss=0.0204, lr=2.00e-05, step=3999]Training:   2%|‚ñè         | 4000/200000 [1:25:41<67:12:57,  1.23s/it, loss=0.0204, lr=2.00e-05, step=3999]Training:   2%|‚ñè         | 4000/200000 [1:25:41<67:12:57,  1.23s/it, loss=0.0186, lr=2.00e-05, step=4000]00:18:56.145 [I] step=4000 loss=0.0267 lr=1.98e-05 grad_norm=0.55 time=126.1s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4001/200000 [1:25:42<69:58:18,  1.29s/it, loss=0.0186, lr=2.00e-05, step=4000]Training:   2%|‚ñè         | 4001/200000 [1:25:42<69:58:18,  1.29s/it, loss=0.0350, lr=2.00e-05, step=4001]Training:   2%|‚ñè         | 4002/200000 [1:25:43<66:34:12,  1.22s/it, loss=0.0350, lr=2.00e-05, step=4001]Training:   2%|‚ñè         | 4002/200000 [1:25:43<66:34:12,  1.22s/it, loss=0.0178, lr=2.00e-05, step=4002]Training:   2%|‚ñè         | 4003/200000 [1:25:45<71:06:23,  1.31s/it, loss=0.0178, lr=2.00e-05, step=4002]Training:   2%|‚ñè         | 4003/200000 [1:25:45<71:06:23,  1.31s/it, loss=0.0339, lr=2.00e-05, step=4003]Training:   2%|‚ñè         | 4004/200000 [1:25:46<74:38:49,  1.37s/it, loss=0.0339, lr=2.00e-05, step=4003]Training:   2%|‚ñè         | 4004/200000 [1:25:46<74:38:49,  1.37s/it, loss=0.0229, lr=2.00e-05, step=4004]Training:   2%|‚ñè         | 4005/200000 [1:25:48<69:49:17,  1.28s/it, loss=0.0229, lr=2.00e-05, step=4004]Training:   2%|‚ñè         | 4005/200000 [1:25:48<69:49:17,  1.28s/it, loss=0.0299, lr=2.00e-05, step=4005]Training:   2%|‚ñè         | 4006/200000 [1:25:49<66:27:38,  1.22s/it, loss=0.0299, lr=2.00e-05, step=4005]Training:   2%|‚ñè         | 4006/200000 [1:25:49<66:27:38,  1.22s/it, loss=0.0239, lr=2.00e-05, step=4006]Training:   2%|‚ñè         | 4007/200000 [1:25:50<70:02:03,  1.29s/it, loss=0.0239, lr=2.00e-05, step=4006]Training:   2%|‚ñè         | 4007/200000 [1:25:50<70:02:03,  1.29s/it, loss=0.0233, lr=2.00e-05, step=4007]Training:   2%|‚ñè         | 4008/200000 [1:25:51<66:35:45,  1.22s/it, loss=0.0233, lr=2.00e-05, step=4007]Training:   2%|‚ñè         | 4008/200000 [1:25:51<66:35:45,  1.22s/it, loss=0.0185, lr=2.00e-05, step=4008]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4009/200000 [1:25:52<67:32:41,  1.24s/it, loss=0.0185, lr=2.00e-05, step=4008]Training:   2%|‚ñè         | 4009/200000 [1:25:52<67:32:41,  1.24s/it, loss=0.0218, lr=2.00e-05, step=4009]Training:   2%|‚ñè         | 4010/200000 [1:25:53<64:52:21,  1.19s/it, loss=0.0218, lr=2.00e-05, step=4009]Training:   2%|‚ñè         | 4010/200000 [1:25:53<64:52:21,  1.19s/it, loss=0.0355, lr=2.00e-05, step=4010]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4011/200000 [1:25:55<63:01:18,  1.16s/it, loss=0.0355, lr=2.00e-05, step=4010]Training:   2%|‚ñè         | 4011/200000 [1:25:55<63:01:18,  1.16s/it, loss=0.0226, lr=2.01e-05, step=4011]Training:   2%|‚ñè         | 4012/200000 [1:25:56<68:13:35,  1.25s/it, loss=0.0226, lr=2.01e-05, step=4011]Training:   2%|‚ñè         | 4012/200000 [1:25:56<68:13:35,  1.25s/it, loss=0.0247, lr=2.01e-05, step=4012]Training:   2%|‚ñè         | 4013/200000 [1:25:57<71:20:20,  1.31s/it, loss=0.0247, lr=2.01e-05, step=4012]Training:   2%|‚ñè         | 4013/200000 [1:25:57<71:20:20,  1.31s/it, loss=0.0362, lr=2.01e-05, step=4013]Training:   2%|‚ñè         | 4014/200000 [1:25:59<73:25:51,  1.35s/it, loss=0.0362, lr=2.01e-05, step=4013]Training:   2%|‚ñè         | 4014/200000 [1:25:59<73:25:51,  1.35s/it, loss=0.0513, lr=2.01e-05, step=4014]Training:   2%|‚ñè         | 4015/200000 [1:26:00<68:59:06,  1.27s/it, loss=0.0513, lr=2.01e-05, step=4014]Training:   2%|‚ñè         | 4015/200000 [1:26:00<68:59:06,  1.27s/it, loss=0.0179, lr=2.01e-05, step=4015]Training:   2%|‚ñè         | 4016/200000 [1:26:01<65:51:40,  1.21s/it, loss=0.0179, lr=2.01e-05, step=4015]Training:   2%|‚ñè         | 4016/200000 [1:26:01<65:51:40,  1.21s/it, loss=0.0233, lr=2.01e-05, step=4016]Training:   2%|‚ñè         | 4017/200000 [1:26:02<68:16:19,  1.25s/it, loss=0.0233, lr=2.01e-05, step=4016]Training:   2%|‚ñè         | 4017/200000 [1:26:02<68:16:19,  1.25s/it, loss=0.0231, lr=2.01e-05, step=4017]Training:   2%|‚ñè         | 4018/200000 [1:26:04<70:44:45,  1.30s/it, loss=0.0231, lr=2.01e-05, step=4017]Training:   2%|‚ñè         | 4018/200000 [1:26:04<70:44:45,  1.30s/it, loss=0.0354, lr=2.01e-05, step=4018]Training:   2%|‚ñè         | 4019/200000 [1:26:05<67:08:05,  1.23s/it, loss=0.0354, lr=2.01e-05, step=4018]Training:   2%|‚ñè         | 4019/200000 [1:26:05<67:08:05,  1.23s/it, loss=0.0232, lr=2.01e-05, step=4019]Training:   2%|‚ñè         | 4020/200000 [1:26:06<70:49:44,  1.30s/it, loss=0.0232, lr=2.01e-05, step=4019]Training:   2%|‚ñè         | 4020/200000 [1:26:06<70:49:44,  1.30s/it, loss=0.0242, lr=2.01e-05, step=4020]Training:   2%|‚ñè         | 4021/200000 [1:26:07<67:08:21,  1.23s/it, loss=0.0242, lr=2.01e-05, step=4020]Training:   2%|‚ñè         | 4021/200000 [1:26:07<67:08:21,  1.23s/it, loss=0.0167, lr=2.01e-05, step=4021]Training:   2%|‚ñè         | 4022/200000 [1:26:09<69:39:42,  1.28s/it, loss=0.0167, lr=2.01e-05, step=4021]Training:   2%|‚ñè         | 4022/200000 [1:26:09<69:39:42,  1.28s/it, loss=0.0240, lr=2.01e-05, step=4022]Training:   2%|‚ñè         | 4023/200000 [1:26:10<66:22:56,  1.22s/it, loss=0.0240, lr=2.01e-05, step=4022]Training:   2%|‚ñè         | 4023/200000 [1:26:10<66:22:56,  1.22s/it, loss=0.0418, lr=2.01e-05, step=4023]Training:   2%|‚ñè         | 4024/200000 [1:26:11<71:14:03,  1.31s/it, loss=0.0418, lr=2.01e-05, step=4023]Training:   2%|‚ñè         | 4024/200000 [1:26:11<71:14:03,  1.31s/it, loss=0.0293, lr=2.01e-05, step=4024]Training:   2%|‚ñè         | 4025/200000 [1:26:13<73:49:11,  1.36s/it, loss=0.0293, lr=2.01e-05, step=4024]Training:   2%|‚ñè         | 4025/200000 [1:26:13<73:49:11,  1.36s/it, loss=0.0332, lr=2.01e-05, step=4025]Training:   2%|‚ñè         | 4026/200000 [1:26:14<69:14:24,  1.27s/it, loss=0.0332, lr=2.01e-05, step=4025]Training:   2%|‚ñè         | 4026/200000 [1:26:14<69:14:24,  1.27s/it, loss=0.0667, lr=2.01e-05, step=4026]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4027/200000 [1:26:15<66:04:03,  1.21s/it, loss=0.0667, lr=2.01e-05, step=4026]Training:   2%|‚ñè         | 4027/200000 [1:26:15<66:04:03,  1.21s/it, loss=0.0161, lr=2.01e-05, step=4027]Training:   2%|‚ñè         | 4028/200000 [1:26:16<69:23:23,  1.27s/it, loss=0.0161, lr=2.01e-05, step=4027]Training:   2%|‚ñè         | 4028/200000 [1:26:16<69:23:23,  1.27s/it, loss=0.0373, lr=2.01e-05, step=4028]Training:   2%|‚ñè         | 4029/200000 [1:26:18<66:10:45,  1.22s/it, loss=0.0373, lr=2.01e-05, step=4028]Training:   2%|‚ñè         | 4029/200000 [1:26:18<66:10:45,  1.22s/it, loss=0.0144, lr=2.01e-05, step=4029]Training:   2%|‚ñè         | 4030/200000 [1:26:19<67:55:33,  1.25s/it, loss=0.0144, lr=2.01e-05, step=4029]Training:   2%|‚ñè         | 4030/200000 [1:26:19<67:55:33,  1.25s/it, loss=0.0244, lr=2.01e-05, step=4030]Training:   2%|‚ñè         | 4031/200000 [1:26:20<65:06:39,  1.20s/it, loss=0.0244, lr=2.01e-05, step=4030]Training:   2%|‚ñè         | 4031/200000 [1:26:20<65:06:39,  1.20s/it, loss=0.1433, lr=2.02e-05, step=4031]Training:   2%|‚ñè         | 4032/200000 [1:26:21<63:09:52,  1.16s/it, loss=0.1433, lr=2.02e-05, step=4031]Training:   2%|‚ñè         | 4032/200000 [1:26:21<63:09:52,  1.16s/it, loss=0.0187, lr=2.02e-05, step=4032]Training:   2%|‚ñè         | 4033/200000 [1:26:22<67:59:45,  1.25s/it, loss=0.0187, lr=2.02e-05, step=4032]Training:   2%|‚ñè         | 4033/200000 [1:26:22<67:59:45,  1.25s/it, loss=0.0358, lr=2.02e-05, step=4033]Training:   2%|‚ñè         | 4034/200000 [1:26:24<71:07:31,  1.31s/it, loss=0.0358, lr=2.02e-05, step=4033]Training:   2%|‚ñè         | 4034/200000 [1:26:24<71:07:31,  1.31s/it, loss=0.0475, lr=2.02e-05, step=4034]Training:   2%|‚ñè         | 4035/200000 [1:26:25<73:12:40,  1.34s/it, loss=0.0475, lr=2.02e-05, step=4034]Training:   2%|‚ñè         | 4035/200000 [1:26:25<73:12:40,  1.34s/it, loss=0.0260, lr=2.02e-05, step=4035]Training:   2%|‚ñè         | 4036/200000 [1:26:26<68:50:06,  1.26s/it, loss=0.0260, lr=2.02e-05, step=4035]Training:   2%|‚ñè         | 4036/200000 [1:26:26<68:50:06,  1.26s/it, loss=0.0236, lr=2.02e-05, step=4036]Training:   2%|‚ñè         | 4037/200000 [1:26:27<65:46:15,  1.21s/it, loss=0.0236, lr=2.02e-05, step=4036]Training:   2%|‚ñè         | 4037/200000 [1:26:27<65:46:15,  1.21s/it, loss=0.0317, lr=2.02e-05, step=4037]Training:   2%|‚ñè         | 4038/200000 [1:26:29<67:29:12,  1.24s/it, loss=0.0317, lr=2.02e-05, step=4037]Training:   2%|‚ñè         | 4038/200000 [1:26:29<67:29:12,  1.24s/it, loss=0.0279, lr=2.02e-05, step=4038]Training:   2%|‚ñè         | 4039/200000 [1:26:30<68:59:16,  1.27s/it, loss=0.0279, lr=2.02e-05, step=4038]Training:   2%|‚ñè         | 4039/200000 [1:26:30<68:59:16,  1.27s/it, loss=0.0309, lr=2.02e-05, step=4039]Training:   2%|‚ñè         | 4040/200000 [1:26:31<65:52:31,  1.21s/it, loss=0.0309, lr=2.02e-05, step=4039]Training:   2%|‚ñè         | 4040/200000 [1:26:31<65:52:31,  1.21s/it, loss=0.0188, lr=2.02e-05, step=4040]Training:   2%|‚ñè         | 4041/200000 [1:26:33<68:02:53,  1.25s/it, loss=0.0188, lr=2.02e-05, step=4040]Training:   2%|‚ñè         | 4041/200000 [1:26:33<68:02:53,  1.25s/it, loss=0.0775, lr=2.02e-05, step=4041]Training:   2%|‚ñè         | 4042/200000 [1:26:34<65:13:31,  1.20s/it, loss=0.0775, lr=2.02e-05, step=4041]Training:   2%|‚ñè         | 4042/200000 [1:26:34<65:13:31,  1.20s/it, loss=0.0347, lr=2.02e-05, step=4042]Training:   2%|‚ñè         | 4043/200000 [1:26:35<68:22:23,  1.26s/it, loss=0.0347, lr=2.02e-05, step=4042]Training:   2%|‚ñè         | 4043/200000 [1:26:35<68:22:23,  1.26s/it, loss=0.0334, lr=2.02e-05, step=4043]Training:   2%|‚ñè         | 4044/200000 [1:26:36<71:37:03,  1.32s/it, loss=0.0334, lr=2.02e-05, step=4043]Training:   2%|‚ñè         | 4044/200000 [1:26:36<71:37:03,  1.32s/it, loss=0.0212, lr=2.02e-05, step=4044]Training:   2%|‚ñè         | 4045/200000 [1:26:38<73:19:32,  1.35s/it, loss=0.0212, lr=2.02e-05, step=4044]Training:   2%|‚ñè         | 4045/200000 [1:26:38<73:19:32,  1.35s/it, loss=0.0200, lr=2.02e-05, step=4045]Training:   2%|‚ñè         | 4046/200000 [1:26:39<74:52:04,  1.38s/it, loss=0.0200, lr=2.02e-05, step=4045]Training:   2%|‚ñè         | 4046/200000 [1:26:39<74:52:04,  1.38s/it, loss=0.0191, lr=2.02e-05, step=4046]Training:   2%|‚ñè         | 4047/200000 [1:26:40<69:58:23,  1.29s/it, loss=0.0191, lr=2.02e-05, step=4046]Training:   2%|‚ñè         | 4047/200000 [1:26:40<69:58:23,  1.29s/it, loss=0.0297, lr=2.02e-05, step=4047]Training:   2%|‚ñè         | 4048/200000 [1:26:41<66:33:56,  1.22s/it, loss=0.0297, lr=2.02e-05, step=4047]Training:   2%|‚ñè         | 4048/200000 [1:26:41<66:33:56,  1.22s/it, loss=0.0256, lr=2.02e-05, step=4048]Training:   2%|‚ñè         | 4049/200000 [1:26:43<68:50:13,  1.26s/it, loss=0.0256, lr=2.02e-05, step=4048]Training:   2%|‚ñè         | 4049/200000 [1:26:43<68:50:13,  1.26s/it, loss=0.0303, lr=2.02e-05, step=4049]Training:   2%|‚ñè         | 4050/200000 [1:26:44<71:31:18,  1.31s/it, loss=0.0303, lr=2.02e-05, step=4049]Training:   2%|‚ñè         | 4050/200000 [1:26:44<71:31:18,  1.31s/it, loss=0.0138, lr=2.02e-05, step=4050]Training:   2%|‚ñè         | 4051/200000 [1:26:45<67:39:18,  1.24s/it, loss=0.0138, lr=2.02e-05, step=4050]Training:   2%|‚ñè         | 4051/200000 [1:26:45<67:39:18,  1.24s/it, loss=0.0163, lr=2.03e-05, step=4051]Training:   2%|‚ñè         | 4052/200000 [1:26:47<71:19:58,  1.31s/it, loss=0.0163, lr=2.03e-05, step=4051]Training:   2%|‚ñè         | 4052/200000 [1:26:47<71:19:58,  1.31s/it, loss=0.0213, lr=2.03e-05, step=4052]Training:   2%|‚ñè         | 4053/200000 [1:26:48<67:32:44,  1.24s/it, loss=0.0213, lr=2.03e-05, step=4052]Training:   2%|‚ñè         | 4053/200000 [1:26:48<67:32:44,  1.24s/it, loss=0.0238, lr=2.03e-05, step=4053]Training:   2%|‚ñè         | 4054/200000 [1:26:49<69:33:30,  1.28s/it, loss=0.0238, lr=2.03e-05, step=4053]Training:   2%|‚ñè         | 4054/200000 [1:26:49<69:33:30,  1.28s/it, loss=0.0112, lr=2.03e-05, step=4054]Training:   2%|‚ñè         | 4055/200000 [1:26:50<66:14:02,  1.22s/it, loss=0.0112, lr=2.03e-05, step=4054]Training:   2%|‚ñè         | 4055/200000 [1:26:50<66:14:02,  1.22s/it, loss=0.0109, lr=2.03e-05, step=4055]Training:   2%|‚ñè         | 4056/200000 [1:26:52<70:52:02,  1.30s/it, loss=0.0109, lr=2.03e-05, step=4055]Training:   2%|‚ñè         | 4056/200000 [1:26:52<70:52:02,  1.30s/it, loss=0.0225, lr=2.03e-05, step=4056]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4057/200000 [1:26:53<74:33:18,  1.37s/it, loss=0.0225, lr=2.03e-05, step=4056]Training:   2%|‚ñè         | 4057/200000 [1:26:53<74:33:18,  1.37s/it, loss=0.0359, lr=2.03e-05, step=4057]Training:   2%|‚ñè         | 4058/200000 [1:26:54<69:46:56,  1.28s/it, loss=0.0359, lr=2.03e-05, step=4057]Training:   2%|‚ñè         | 4058/200000 [1:26:54<69:46:56,  1.28s/it, loss=0.0528, lr=2.03e-05, step=4058]Training:   2%|‚ñè         | 4059/200000 [1:26:56<66:26:27,  1.22s/it, loss=0.0528, lr=2.03e-05, step=4058]Training:   2%|‚ñè         | 4059/200000 [1:26:56<66:26:27,  1.22s/it, loss=0.0259, lr=2.03e-05, step=4059]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4060/200000 [1:26:57<70:00:36,  1.29s/it, loss=0.0259, lr=2.03e-05, step=4059]Training:   2%|‚ñè         | 4060/200000 [1:26:57<70:00:36,  1.29s/it, loss=0.0275, lr=2.03e-05, step=4060]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4061/200000 [1:26:58<66:34:25,  1.22s/it, loss=0.0275, lr=2.03e-05, step=4060]Training:   2%|‚ñè         | 4061/200000 [1:26:58<66:34:25,  1.22s/it, loss=0.0160, lr=2.03e-05, step=4061]Training:   2%|‚ñè         | 4062/200000 [1:26:59<69:16:44,  1.27s/it, loss=0.0160, lr=2.03e-05, step=4061]Training:   2%|‚ñè         | 4062/200000 [1:26:59<69:16:44,  1.27s/it, loss=0.0195, lr=2.03e-05, step=4062]Training:   2%|‚ñè         | 4063/200000 [1:27:01<66:03:27,  1.21s/it, loss=0.0195, lr=2.03e-05, step=4062]Training:   2%|‚ñè         | 4063/200000 [1:27:01<66:03:27,  1.21s/it, loss=0.0212, lr=2.03e-05, step=4063]Training:   2%|‚ñè         | 4064/200000 [1:27:02<63:49:45,  1.17s/it, loss=0.0212, lr=2.03e-05, step=4063]Training:   2%|‚ñè         | 4064/200000 [1:27:02<63:49:45,  1.17s/it, loss=0.0200, lr=2.03e-05, step=4064]Training:   2%|‚ñè         | 4065/200000 [1:27:03<68:58:34,  1.27s/it, loss=0.0200, lr=2.03e-05, step=4064]Training:   2%|‚ñè         | 4065/200000 [1:27:03<68:58:34,  1.27s/it, loss=0.0259, lr=2.03e-05, step=4065]Training:   2%|‚ñè         | 4066/200000 [1:27:04<71:22:34,  1.31s/it, loss=0.0259, lr=2.03e-05, step=4065]Training:   2%|‚ñè         | 4066/200000 [1:27:04<71:22:34,  1.31s/it, loss=0.0177, lr=2.03e-05, step=4066]Training:   2%|‚ñè         | 4067/200000 [1:27:06<73:31:34,  1.35s/it, loss=0.0177, lr=2.03e-05, step=4066]Training:   2%|‚ñè         | 4067/200000 [1:27:06<73:31:34,  1.35s/it, loss=0.0315, lr=2.03e-05, step=4067]Training:   2%|‚ñè         | 4068/200000 [1:27:07<69:04:42,  1.27s/it, loss=0.0315, lr=2.03e-05, step=4067]Training:   2%|‚ñè         | 4068/200000 [1:27:07<69:04:42,  1.27s/it, loss=0.0215, lr=2.03e-05, step=4068]Training:   2%|‚ñè         | 4069/200000 [1:27:08<65:56:06,  1.21s/it, loss=0.0215, lr=2.03e-05, step=4068]Training:   2%|‚ñè         | 4069/200000 [1:27:08<65:56:06,  1.21s/it, loss=0.0204, lr=2.03e-05, step=4069]Training:   2%|‚ñè         | 4070/200000 [1:27:09<68:03:29,  1.25s/it, loss=0.0204, lr=2.03e-05, step=4069]Training:   2%|‚ñè         | 4070/200000 [1:27:09<68:03:29,  1.25s/it, loss=0.0218, lr=2.03e-05, step=4070]Training:   2%|‚ñè         | 4071/200000 [1:27:11<70:18:46,  1.29s/it, loss=0.0218, lr=2.03e-05, step=4070]Training:   2%|‚ñè         | 4071/200000 [1:27:11<70:18:46,  1.29s/it, loss=0.0174, lr=2.04e-05, step=4071]Training:   2%|‚ñè         | 4072/200000 [1:27:12<66:47:08,  1.23s/it, loss=0.0174, lr=2.04e-05, step=4071]Training:   2%|‚ñè         | 4072/200000 [1:27:12<66:47:08,  1.23s/it, loss=0.0242, lr=2.04e-05, step=4072]Training:   2%|‚ñè         | 4073/200000 [1:27:13<70:08:36,  1.29s/it, loss=0.0242, lr=2.04e-05, step=4072]Training:   2%|‚ñè         | 4073/200000 [1:27:13<70:08:36,  1.29s/it, loss=0.0392, lr=2.04e-05, step=4073]Training:   2%|‚ñè         | 4074/200000 [1:27:14<66:40:30,  1.23s/it, loss=0.0392, lr=2.04e-05, step=4073]Training:   2%|‚ñè         | 4074/200000 [1:27:14<66:40:30,  1.23s/it, loss=0.0227, lr=2.04e-05, step=4074]Training:   2%|‚ñè         | 4075/200000 [1:27:16<69:34:13,  1.28s/it, loss=0.0227, lr=2.04e-05, step=4074]Training:   2%|‚ñè         | 4075/200000 [1:27:16<69:34:13,  1.28s/it, loss=0.0158, lr=2.04e-05, step=4075]Training:   2%|‚ñè         | 4076/200000 [1:27:17<66:16:01,  1.22s/it, loss=0.0158, lr=2.04e-05, step=4075]Training:   2%|‚ñè         | 4076/200000 [1:27:17<66:16:01,  1.22s/it, loss=0.0428, lr=2.04e-05, step=4076]Training:   2%|‚ñè         | 4077/200000 [1:27:18<70:26:09,  1.29s/it, loss=0.0428, lr=2.04e-05, step=4076]Training:   2%|‚ñè         | 4077/200000 [1:27:18<70:26:09,  1.29s/it, loss=0.0311, lr=2.04e-05, step=4077]Training:   2%|‚ñè         | 4078/200000 [1:27:20<73:53:35,  1.36s/it, loss=0.0311, lr=2.04e-05, step=4077]Training:   2%|‚ñè         | 4078/200000 [1:27:20<73:53:35,  1.36s/it, loss=0.0482, lr=2.04e-05, step=4078]Training:   2%|‚ñè         | 4079/200000 [1:27:21<69:18:53,  1.27s/it, loss=0.0482, lr=2.04e-05, step=4078]Training:   2%|‚ñè         | 4079/200000 [1:27:21<69:18:53,  1.27s/it, loss=0.0189, lr=2.04e-05, step=4079]Training:   2%|‚ñè         | 4080/200000 [1:27:22<66:05:04,  1.21s/it, loss=0.0189, lr=2.04e-05, step=4079]Training:   2%|‚ñè         | 4080/200000 [1:27:22<66:05:04,  1.21s/it, loss=0.0675, lr=2.04e-05, step=4080]Training:   2%|‚ñè         | 4081/200000 [1:27:23<70:09:57,  1.29s/it, loss=0.0675, lr=2.04e-05, step=4080]Training:   2%|‚ñè         | 4081/200000 [1:27:23<70:09:57,  1.29s/it, loss=0.0208, lr=2.04e-05, step=4081]Training:   2%|‚ñè         | 4082/200000 [1:27:25<66:40:23,  1.23s/it, loss=0.0208, lr=2.04e-05, step=4081]Training:   2%|‚ñè         | 4082/200000 [1:27:25<66:40:23,  1.23s/it, loss=0.0243, lr=2.04e-05, step=4082]Training:   2%|‚ñè         | 4083/200000 [1:27:26<68:03:27,  1.25s/it, loss=0.0243, lr=2.04e-05, step=4082]Training:   2%|‚ñè         | 4083/200000 [1:27:26<68:03:27,  1.25s/it, loss=0.0190, lr=2.04e-05, step=4083]Training:   2%|‚ñè         | 4084/200000 [1:27:27<65:13:58,  1.20s/it, loss=0.0190, lr=2.04e-05, step=4083]Training:   2%|‚ñè         | 4084/200000 [1:27:27<65:13:58,  1.20s/it, loss=0.0202, lr=2.04e-05, step=4084]Training:   2%|‚ñè         | 4085/200000 [1:27:28<63:15:14,  1.16s/it, loss=0.0202, lr=2.04e-05, step=4084]Training:   2%|‚ñè         | 4085/200000 [1:27:28<63:15:14,  1.16s/it, loss=0.0320, lr=2.04e-05, step=4085]Training:   2%|‚ñè         | 4086/200000 [1:27:29<68:09:57,  1.25s/it, loss=0.0320, lr=2.04e-05, step=4085]Training:   2%|‚ñè         | 4086/200000 [1:27:29<68:09:57,  1.25s/it, loss=0.0457, lr=2.04e-05, step=4086]Training:   2%|‚ñè         | 4087/200000 [1:27:31<70:20:02,  1.29s/it, loss=0.0457, lr=2.04e-05, step=4086]Training:   2%|‚ñè         | 4087/200000 [1:27:31<70:20:02,  1.29s/it, loss=0.0286, lr=2.04e-05, step=4087]Training:   2%|‚ñè         | 4088/200000 [1:27:32<72:32:56,  1.33s/it, loss=0.0286, lr=2.04e-05, step=4087]Training:   2%|‚ñè         | 4088/200000 [1:27:32<72:32:56,  1.33s/it, loss=0.0125, lr=2.04e-05, step=4088]Training:   2%|‚ñè         | 4089/200000 [1:27:33<68:23:05,  1.26s/it, loss=0.0125, lr=2.04e-05, step=4088]Training:   2%|‚ñè         | 4089/200000 [1:27:33<68:23:05,  1.26s/it, loss=0.0180, lr=2.04e-05, step=4089]Training:   2%|‚ñè         | 4090/200000 [1:27:34<65:28:11,  1.20s/it, loss=0.0180, lr=2.04e-05, step=4089]Training:   2%|‚ñè         | 4090/200000 [1:27:34<65:28:11,  1.20s/it, loss=0.0177, lr=2.04e-05, step=4090]Training:   2%|‚ñè         | 4091/200000 [1:27:36<67:47:13,  1.25s/it, loss=0.0177, lr=2.04e-05, step=4090]Training:   2%|‚ñè         | 4091/200000 [1:27:36<67:47:13,  1.25s/it, loss=0.0213, lr=2.05e-05, step=4091]Training:   2%|‚ñè         | 4092/200000 [1:27:37<69:14:04,  1.27s/it, loss=0.0213, lr=2.05e-05, step=4091]Training:   2%|‚ñè         | 4092/200000 [1:27:37<69:14:04,  1.27s/it, loss=0.0215, lr=2.05e-05, step=4092]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4093/200000 [1:27:38<66:02:00,  1.21s/it, loss=0.0215, lr=2.05e-05, step=4092]Training:   2%|‚ñè         | 4093/200000 [1:27:38<66:02:00,  1.21s/it, loss=0.0202, lr=2.05e-05, step=4093]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4094/200000 [1:27:40<68:38:11,  1.26s/it, loss=0.0202, lr=2.05e-05, step=4093]Training:   2%|‚ñè         | 4094/200000 [1:27:40<68:38:11,  1.26s/it, loss=0.0121, lr=2.05e-05, step=4094]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4095/200000 [1:27:41<65:36:20,  1.21s/it, loss=0.0121, lr=2.05e-05, step=4094]Training:   2%|‚ñè         | 4095/200000 [1:27:41<65:36:20,  1.21s/it, loss=0.0249, lr=2.05e-05, step=4095]Training:   2%|‚ñè         | 4096/200000 [1:27:42<67:41:07,  1.24s/it, loss=0.0249, lr=2.05e-05, step=4095]Training:   2%|‚ñè         | 4096/200000 [1:27:42<67:41:07,  1.24s/it, loss=0.0205, lr=2.05e-05, step=4096]Training:   2%|‚ñè         | 4097/200000 [1:27:43<71:14:34,  1.31s/it, loss=0.0205, lr=2.05e-05, step=4096]Training:   2%|‚ñè         | 4097/200000 [1:27:43<71:14:34,  1.31s/it, loss=0.0167, lr=2.05e-05, step=4097]Training:   2%|‚ñè         | 4098/200000 [1:27:45<72:49:39,  1.34s/it, loss=0.0167, lr=2.05e-05, step=4097]Training:   2%|‚ñè         | 4098/200000 [1:27:45<72:49:39,  1.34s/it, loss=0.0343, lr=2.05e-05, step=4098]Training:   2%|‚ñè         | 4099/200000 [1:27:46<74:13:20,  1.36s/it, loss=0.0343, lr=2.05e-05, step=4098]Training:   2%|‚ñè         | 4099/200000 [1:27:46<74:13:20,  1.36s/it, loss=0.0563, lr=2.05e-05, step=4099]Training:   2%|‚ñè         | 4100/200000 [1:27:47<69:33:49,  1.28s/it, loss=0.0563, lr=2.05e-05, step=4099]Training:   2%|‚ñè         | 4100/200000 [1:27:47<69:33:49,  1.28s/it, loss=0.0348, lr=2.05e-05, step=4100]00:21:02.244 [I] step=4100 loss=0.0283 lr=2.03e-05 grad_norm=0.58 time=126.1s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4101/200000 [1:27:48<66:18:11,  1.22s/it, loss=0.0348, lr=2.05e-05, step=4100]Training:   2%|‚ñè         | 4101/200000 [1:27:48<66:18:11,  1.22s/it, loss=0.0161, lr=2.05e-05, step=4101]Training:   2%|‚ñè         | 4102/200000 [1:27:50<68:46:23,  1.26s/it, loss=0.0161, lr=2.05e-05, step=4101]Training:   2%|‚ñè         | 4102/200000 [1:27:50<68:46:23,  1.26s/it, loss=0.0141, lr=2.05e-05, step=4102]Training:   2%|‚ñè         | 4103/200000 [1:27:51<71:06:20,  1.31s/it, loss=0.0141, lr=2.05e-05, step=4102]Training:   2%|‚ñè         | 4103/200000 [1:27:51<71:06:20,  1.31s/it, loss=0.0292, lr=2.05e-05, step=4103]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4104/200000 [1:27:52<67:19:04,  1.24s/it, loss=0.0292, lr=2.05e-05, step=4103]Training:   2%|‚ñè         | 4104/200000 [1:27:52<67:19:04,  1.24s/it, loss=0.0154, lr=2.05e-05, step=4104]Training:   2%|‚ñè         | 4105/200000 [1:27:54<70:40:00,  1.30s/it, loss=0.0154, lr=2.05e-05, step=4104]Training:   2%|‚ñè         | 4105/200000 [1:27:54<70:40:00,  1.30s/it, loss=0.0409, lr=2.05e-05, step=4105]Training:   2%|‚ñè         | 4106/200000 [1:27:55<67:03:17,  1.23s/it, loss=0.0409, lr=2.05e-05, step=4105]Training:   2%|‚ñè         | 4106/200000 [1:27:55<67:03:17,  1.23s/it, loss=0.0347, lr=2.05e-05, step=4106]Training:   2%|‚ñè         | 4107/200000 [1:27:56<69:52:43,  1.28s/it, loss=0.0347, lr=2.05e-05, step=4106]Training:   2%|‚ñè         | 4107/200000 [1:27:56<69:52:43,  1.28s/it, loss=0.0197, lr=2.05e-05, step=4107]Training:   2%|‚ñè         | 4108/200000 [1:27:57<66:30:42,  1.22s/it, loss=0.0197, lr=2.05e-05, step=4107]Training:   2%|‚ñè         | 4108/200000 [1:27:57<66:30:42,  1.22s/it, loss=0.0800, lr=2.05e-05, step=4108]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4109/200000 [1:27:59<71:02:11,  1.31s/it, loss=0.0800, lr=2.05e-05, step=4108]Training:   2%|‚ñè         | 4109/200000 [1:27:59<71:02:11,  1.31s/it, loss=0.0303, lr=2.05e-05, step=4109]Training:   2%|‚ñè         | 4110/200000 [1:28:00<74:34:22,  1.37s/it, loss=0.0303, lr=2.05e-05, step=4109]Training:   2%|‚ñè         | 4110/200000 [1:28:00<74:34:22,  1.37s/it, loss=0.0132, lr=2.05e-05, step=4110]Training:   2%|‚ñè         | 4111/200000 [1:28:01<69:48:22,  1.28s/it, loss=0.0132, lr=2.05e-05, step=4110]Training:   2%|‚ñè         | 4111/200000 [1:28:01<69:48:22,  1.28s/it, loss=0.0209, lr=2.06e-05, step=4111]Training:   2%|‚ñè         | 4112/200000 [1:28:02<66:25:13,  1.22s/it, loss=0.0209, lr=2.06e-05, step=4111]Training:   2%|‚ñè         | 4112/200000 [1:28:02<66:25:13,  1.22s/it, loss=0.0224, lr=2.06e-05, step=4112]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4113/200000 [1:28:04<70:43:24,  1.30s/it, loss=0.0224, lr=2.06e-05, step=4112]Training:   2%|‚ñè         | 4113/200000 [1:28:04<70:43:24,  1.30s/it, loss=0.0166, lr=2.06e-05, step=4113]Training:   2%|‚ñè         | 4114/200000 [1:28:05<67:04:34,  1.23s/it, loss=0.0166, lr=2.06e-05, step=4113]Training:   2%|‚ñè         | 4114/200000 [1:28:05<67:04:34,  1.23s/it, loss=0.0286, lr=2.06e-05, step=4114]Training:   2%|‚ñè         | 4115/200000 [1:28:06<68:03:08,  1.25s/it, loss=0.0286, lr=2.06e-05, step=4114]Training:   2%|‚ñè         | 4115/200000 [1:28:06<68:03:08,  1.25s/it, loss=0.0245, lr=2.06e-05, step=4115]Training:   2%|‚ñè         | 4116/200000 [1:28:07<65:10:31,  1.20s/it, loss=0.0245, lr=2.06e-05, step=4115]Training:   2%|‚ñè         | 4116/200000 [1:28:07<65:10:31,  1.20s/it, loss=0.0184, lr=2.06e-05, step=4116]Training:   2%|‚ñè         | 4117/200000 [1:28:08<63:08:41,  1.16s/it, loss=0.0184, lr=2.06e-05, step=4116]Training:   2%|‚ñè         | 4117/200000 [1:28:08<63:08:41,  1.16s/it, loss=0.0147, lr=2.06e-05, step=4117]Training:   2%|‚ñè         | 4118/200000 [1:28:10<67:43:04,  1.24s/it, loss=0.0147, lr=2.06e-05, step=4117]Training:   2%|‚ñè         | 4118/200000 [1:28:10<67:43:04,  1.24s/it, loss=0.0219, lr=2.06e-05, step=4118]Training:   2%|‚ñè         | 4119/200000 [1:28:11<70:22:58,  1.29s/it, loss=0.0219, lr=2.06e-05, step=4118]Training:   2%|‚ñè         | 4119/200000 [1:28:11<70:22:58,  1.29s/it, loss=0.0349, lr=2.06e-05, step=4119]Training:   2%|‚ñè         | 4120/200000 [1:28:13<72:22:16,  1.33s/it, loss=0.0349, lr=2.06e-05, step=4119]Training:   2%|‚ñè         | 4120/200000 [1:28:13<72:22:16,  1.33s/it, loss=0.0200, lr=2.06e-05, step=4120]Training:   2%|‚ñè         | 4121/200000 [1:28:14<68:13:54,  1.25s/it, loss=0.0200, lr=2.06e-05, step=4120]Training:   2%|‚ñè         | 4121/200000 [1:28:14<68:13:54,  1.25s/it, loss=0.0194, lr=2.06e-05, step=4121]Training:   2%|‚ñè         | 4122/200000 [1:28:15<65:20:59,  1.20s/it, loss=0.0194, lr=2.06e-05, step=4121]Training:   2%|‚ñè         | 4122/200000 [1:28:15<65:20:59,  1.20s/it, loss=0.0220, lr=2.06e-05, step=4122]Training:   2%|‚ñè         | 4123/200000 [1:28:16<67:52:29,  1.25s/it, loss=0.0220, lr=2.06e-05, step=4122]Training:   2%|‚ñè         | 4123/200000 [1:28:16<67:52:29,  1.25s/it, loss=0.0160, lr=2.06e-05, step=4123]Training:   2%|‚ñè         | 4124/200000 [1:28:18<70:26:02,  1.29s/it, loss=0.0160, lr=2.06e-05, step=4123]Training:   2%|‚ñè         | 4124/200000 [1:28:18<70:26:02,  1.29s/it, loss=0.0147, lr=2.06e-05, step=4124]Training:   2%|‚ñè         | 4125/200000 [1:28:19<66:52:39,  1.23s/it, loss=0.0147, lr=2.06e-05, step=4124]Training:   2%|‚ñè         | 4125/200000 [1:28:19<66:52:39,  1.23s/it, loss=0.0372, lr=2.06e-05, step=4125]Training:   2%|‚ñè         | 4126/200000 [1:28:20<70:19:24,  1.29s/it, loss=0.0372, lr=2.06e-05, step=4125]Training:   2%|‚ñè         | 4126/200000 [1:28:20<70:19:24,  1.29s/it, loss=0.0191, lr=2.06e-05, step=4126]Training:   2%|‚ñè         | 4127/200000 [1:28:21<66:47:48,  1.23s/it, loss=0.0191, lr=2.06e-05, step=4126]Training:   2%|‚ñè         | 4127/200000 [1:28:21<66:47:48,  1.23s/it, loss=0.0244, lr=2.06e-05, step=4127]Training:   2%|‚ñè         | 4128/200000 [1:28:23<69:40:15,  1.28s/it, loss=0.0244, lr=2.06e-05, step=4127]Training:   2%|‚ñè         | 4128/200000 [1:28:23<69:40:15,  1.28s/it, loss=0.0340, lr=2.06e-05, step=4128]Training:   2%|‚ñè         | 4129/200000 [1:28:24<66:21:03,  1.22s/it, loss=0.0340, lr=2.06e-05, step=4128]Training:   2%|‚ñè         | 4129/200000 [1:28:24<66:21:03,  1.22s/it, loss=0.0267, lr=2.06e-05, step=4129]Training:   2%|‚ñè         | 4130/200000 [1:28:25<70:32:05,  1.30s/it, loss=0.0267, lr=2.06e-05, step=4129]Training:   2%|‚ñè         | 4130/200000 [1:28:25<70:32:05,  1.30s/it, loss=0.0247, lr=2.06e-05, step=4130]Training:   2%|‚ñè         | 4131/200000 [1:28:27<73:20:40,  1.35s/it, loss=0.0247, lr=2.06e-05, step=4130]Training:   2%|‚ñè         | 4131/200000 [1:28:27<73:20:40,  1.35s/it, loss=0.0394, lr=2.07e-05, step=4131]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4132/200000 [1:28:28<68:54:34,  1.27s/it, loss=0.0394, lr=2.07e-05, step=4131]Training:   2%|‚ñè         | 4132/200000 [1:28:28<68:54:34,  1.27s/it, loss=0.0324, lr=2.07e-05, step=4132]Training:   2%|‚ñè         | 4133/200000 [1:28:29<65:49:00,  1.21s/it, loss=0.0324, lr=2.07e-05, step=4132]Training:   2%|‚ñè         | 4133/200000 [1:28:29<65:49:00,  1.21s/it, loss=0.0357, lr=2.07e-05, step=4133]Training:   2%|‚ñè         | 4134/200000 [1:28:30<69:57:28,  1.29s/it, loss=0.0357, lr=2.07e-05, step=4133]Training:   2%|‚ñè         | 4134/200000 [1:28:30<69:57:28,  1.29s/it, loss=0.0259, lr=2.07e-05, step=4134]Training:   2%|‚ñè         | 4135/200000 [1:28:31<66:32:15,  1.22s/it, loss=0.0259, lr=2.07e-05, step=4134]Training:   2%|‚ñè         | 4135/200000 [1:28:31<66:32:15,  1.22s/it, loss=0.0188, lr=2.07e-05, step=4135]Training:   2%|‚ñè         | 4136/200000 [1:28:33<68:02:21,  1.25s/it, loss=0.0188, lr=2.07e-05, step=4135]Training:   2%|‚ñè         | 4136/200000 [1:28:33<68:02:21,  1.25s/it, loss=0.0244, lr=2.07e-05, step=4136]Training:   2%|‚ñè         | 4137/200000 [1:28:34<65:12:35,  1.20s/it, loss=0.0244, lr=2.07e-05, step=4136]Training:   2%|‚ñè         | 4137/200000 [1:28:34<65:12:35,  1.20s/it, loss=0.0188, lr=2.07e-05, step=4137]Training:   2%|‚ñè         | 4138/200000 [1:28:35<63:12:00,  1.16s/it, loss=0.0188, lr=2.07e-05, step=4137]Training:   2%|‚ñè         | 4138/200000 [1:28:35<63:12:00,  1.16s/it, loss=0.0284, lr=2.07e-05, step=4138]Training:   2%|‚ñè         | 4139/200000 [1:28:36<68:10:11,  1.25s/it, loss=0.0284, lr=2.07e-05, step=4138]Training:   2%|‚ñè         | 4139/200000 [1:28:36<68:10:11,  1.25s/it, loss=0.0380, lr=2.07e-05, step=4139]Training:   2%|‚ñè         | 4140/200000 [1:28:38<70:00:57,  1.29s/it, loss=0.0380, lr=2.07e-05, step=4139]Training:   2%|‚ñè         | 4140/200000 [1:28:38<70:00:57,  1.29s/it, loss=0.0247, lr=2.07e-05, step=4140]Training:   2%|‚ñè         | 4141/200000 [1:28:39<72:21:43,  1.33s/it, loss=0.0247, lr=2.07e-05, step=4140]Training:   2%|‚ñè         | 4141/200000 [1:28:39<72:21:43,  1.33s/it, loss=0.0367, lr=2.07e-05, step=4141]Training:   2%|‚ñè         | 4142/200000 [1:28:40<68:14:36,  1.25s/it, loss=0.0367, lr=2.07e-05, step=4141]Training:   2%|‚ñè         | 4142/200000 [1:28:40<68:14:36,  1.25s/it, loss=0.0147, lr=2.07e-05, step=4142]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4143/200000 [1:28:41<65:21:11,  1.20s/it, loss=0.0147, lr=2.07e-05, step=4142]Training:   2%|‚ñè         | 4143/200000 [1:28:41<65:21:11,  1.20s/it, loss=0.0370, lr=2.07e-05, step=4143]Training:   2%|‚ñè         | 4144/200000 [1:28:43<67:07:31,  1.23s/it, loss=0.0370, lr=2.07e-05, step=4143]Training:   2%|‚ñè         | 4144/200000 [1:28:43<67:07:31,  1.23s/it, loss=0.0281, lr=2.07e-05, step=4144]Training:   2%|‚ñè         | 4145/200000 [1:28:44<68:51:50,  1.27s/it, loss=0.0281, lr=2.07e-05, step=4144]Training:   2%|‚ñè         | 4145/200000 [1:28:44<68:51:50,  1.27s/it, loss=0.0209, lr=2.07e-05, step=4145]Training:   2%|‚ñè         | 4146/200000 [1:28:45<65:45:38,  1.21s/it, loss=0.0209, lr=2.07e-05, step=4145]Training:   2%|‚ñè         | 4146/200000 [1:28:45<65:45:38,  1.21s/it, loss=0.0118, lr=2.07e-05, step=4146]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4147/200000 [1:28:46<67:59:05,  1.25s/it, loss=0.0118, lr=2.07e-05, step=4146]Training:   2%|‚ñè         | 4147/200000 [1:28:46<67:59:05,  1.25s/it, loss=0.0197, lr=2.07e-05, step=4147]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4148/200000 [1:28:47<65:14:46,  1.20s/it, loss=0.0197, lr=2.07e-05, step=4147]Training:   2%|‚ñè         | 4148/200000 [1:28:47<65:14:46,  1.20s/it, loss=0.0171, lr=2.07e-05, step=4148]Training:   2%|‚ñè         | 4149/200000 [1:28:49<68:03:35,  1.25s/it, loss=0.0171, lr=2.07e-05, step=4148]Training:   2%|‚ñè         | 4149/200000 [1:28:49<68:03:35,  1.25s/it, loss=0.0213, lr=2.07e-05, step=4149]Training:   2%|‚ñè         | 4150/200000 [1:28:50<71:27:38,  1.31s/it, loss=0.0213, lr=2.07e-05, step=4149]Training:   2%|‚ñè         | 4150/200000 [1:28:50<71:27:38,  1.31s/it, loss=0.0325, lr=2.07e-05, step=4150]Training:   2%|‚ñè         | 4151/200000 [1:28:52<73:10:04,  1.34s/it, loss=0.0325, lr=2.07e-05, step=4150]Training:   2%|‚ñè         | 4151/200000 [1:28:52<73:10:04,  1.34s/it, loss=0.0272, lr=2.08e-05, step=4151]Training:   2%|‚ñè         | 4152/200000 [1:28:53<74:49:29,  1.38s/it, loss=0.0272, lr=2.08e-05, step=4151]Training:   2%|‚ñè         | 4152/200000 [1:28:53<74:49:29,  1.38s/it, loss=0.0248, lr=2.08e-05, step=4152]Training:   2%|‚ñè         | 4153/200000 [1:28:54<69:59:03,  1.29s/it, loss=0.0248, lr=2.08e-05, step=4152]Training:   2%|‚ñè         | 4153/200000 [1:28:54<69:59:03,  1.29s/it, loss=0.0336, lr=2.08e-05, step=4153]Training:   2%|‚ñè         | 4154/200000 [1:28:55<66:32:14,  1.22s/it, loss=0.0336, lr=2.08e-05, step=4153]Training:   2%|‚ñè         | 4154/200000 [1:28:55<66:32:14,  1.22s/it, loss=0.0309, lr=2.08e-05, step=4154]Training:   2%|‚ñè         | 4155/200000 [1:28:57<69:32:07,  1.28s/it, loss=0.0309, lr=2.08e-05, step=4154]Training:   2%|‚ñè         | 4155/200000 [1:28:57<69:32:07,  1.28s/it, loss=0.0185, lr=2.08e-05, step=4155]Training:   2%|‚ñè         | 4156/200000 [1:28:58<71:59:35,  1.32s/it, loss=0.0185, lr=2.08e-05, step=4155]Training:   2%|‚ñè         | 4156/200000 [1:28:58<71:59:35,  1.32s/it, loss=0.0451, lr=2.08e-05, step=4156]Training:   2%|‚ñè         | 4157/200000 [1:28:59<67:58:48,  1.25s/it, loss=0.0451, lr=2.08e-05, step=4156]Training:   2%|‚ñè         | 4157/200000 [1:28:59<67:58:48,  1.25s/it, loss=0.0168, lr=2.08e-05, step=4157]Training:   2%|‚ñè         | 4158/200000 [1:29:01<71:29:59,  1.31s/it, loss=0.0168, lr=2.08e-05, step=4157]Training:   2%|‚ñè         | 4158/200000 [1:29:01<71:29:59,  1.31s/it, loss=0.0312, lr=2.08e-05, step=4158]Training:   2%|‚ñè         | 4159/200000 [1:29:02<67:36:58,  1.24s/it, loss=0.0312, lr=2.08e-05, step=4158]Training:   2%|‚ñè         | 4159/200000 [1:29:02<67:36:58,  1.24s/it, loss=0.0184, lr=2.08e-05, step=4159]Training:   2%|‚ñè         | 4160/200000 [1:29:03<70:15:38,  1.29s/it, loss=0.0184, lr=2.08e-05, step=4159]Training:   2%|‚ñè         | 4160/200000 [1:29:03<70:15:38,  1.29s/it, loss=0.0320, lr=2.08e-05, step=4160]Training:   2%|‚ñè         | 4161/200000 [1:29:04<66:44:35,  1.23s/it, loss=0.0320, lr=2.08e-05, step=4160]Training:   2%|‚ñè         | 4161/200000 [1:29:04<66:44:35,  1.23s/it, loss=0.0179, lr=2.08e-05, step=4161]Training:   2%|‚ñè         | 4162/200000 [1:29:06<71:05:21,  1.31s/it, loss=0.0179, lr=2.08e-05, step=4161]Training:   2%|‚ñè         | 4162/200000 [1:29:06<71:05:21,  1.31s/it, loss=0.0257, lr=2.08e-05, step=4162]Training:   2%|‚ñè         | 4163/200000 [1:29:07<74:17:32,  1.37s/it, loss=0.0257, lr=2.08e-05, step=4162]Training:   2%|‚ñè         | 4163/200000 [1:29:07<74:17:32,  1.37s/it, loss=0.0278, lr=2.08e-05, step=4163]Training:   2%|‚ñè         | 4164/200000 [1:29:08<69:34:44,  1.28s/it, loss=0.0278, lr=2.08e-05, step=4163]Training:   2%|‚ñè         | 4164/200000 [1:29:08<69:34:44,  1.28s/it, loss=0.0219, lr=2.08e-05, step=4164]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4165/200000 [1:29:09<66:16:08,  1.22s/it, loss=0.0219, lr=2.08e-05, step=4164]Training:   2%|‚ñè         | 4165/200000 [1:29:09<66:16:08,  1.22s/it, loss=0.0142, lr=2.08e-05, step=4165]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4166/200000 [1:29:11<70:33:28,  1.30s/it, loss=0.0142, lr=2.08e-05, step=4165]Training:   2%|‚ñè         | 4166/200000 [1:29:11<70:33:28,  1.30s/it, loss=0.0283, lr=2.08e-05, step=4166]Training:   2%|‚ñè         | 4167/200000 [1:29:12<66:59:01,  1.23s/it, loss=0.0283, lr=2.08e-05, step=4166]Training:   2%|‚ñè         | 4167/200000 [1:29:12<66:59:01,  1.23s/it, loss=0.0325, lr=2.08e-05, step=4167]Training:   2%|‚ñè         | 4168/200000 [1:29:13<69:27:03,  1.28s/it, loss=0.0325, lr=2.08e-05, step=4167]Training:   2%|‚ñè         | 4168/200000 [1:29:13<69:27:03,  1.28s/it, loss=0.0128, lr=2.08e-05, step=4168]Training:   2%|‚ñè         | 4169/200000 [1:29:14<66:09:52,  1.22s/it, loss=0.0128, lr=2.08e-05, step=4168]Training:   2%|‚ñè         | 4169/200000 [1:29:14<66:09:52,  1.22s/it, loss=0.0273, lr=2.08e-05, step=4169]Training:   2%|‚ñè         | 4170/200000 [1:29:15<63:52:37,  1.17s/it, loss=0.0273, lr=2.08e-05, step=4169]Training:   2%|‚ñè         | 4170/200000 [1:29:15<63:52:37,  1.17s/it, loss=0.0273, lr=2.08e-05, step=4170]Training:   2%|‚ñè         | 4171/200000 [1:29:17<68:10:12,  1.25s/it, loss=0.0273, lr=2.08e-05, step=4170]Training:   2%|‚ñè         | 4171/200000 [1:29:17<68:10:12,  1.25s/it, loss=0.0267, lr=2.09e-05, step=4171]Training:   2%|‚ñè         | 4172/200000 [1:29:18<70:46:05,  1.30s/it, loss=0.0267, lr=2.09e-05, step=4171]Training:   2%|‚ñè         | 4172/200000 [1:29:18<70:46:05,  1.30s/it, loss=0.0163, lr=2.09e-05, step=4172]Training:   2%|‚ñè         | 4173/200000 [1:29:20<72:26:47,  1.33s/it, loss=0.0163, lr=2.09e-05, step=4172]Training:   2%|‚ñè         | 4173/200000 [1:29:20<72:26:47,  1.33s/it, loss=0.0380, lr=2.09e-05, step=4173]Training:   2%|‚ñè         | 4174/200000 [1:29:21<68:15:37,  1.25s/it, loss=0.0380, lr=2.09e-05, step=4173]Training:   2%|‚ñè         | 4174/200000 [1:29:21<68:15:37,  1.25s/it, loss=0.0242, lr=2.09e-05, step=4174]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4175/200000 [1:29:22<65:22:40,  1.20s/it, loss=0.0242, lr=2.09e-05, step=4174]Training:   2%|‚ñè         | 4175/200000 [1:29:22<65:22:40,  1.20s/it, loss=0.0280, lr=2.09e-05, step=4175]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4176/200000 [1:29:23<67:48:31,  1.25s/it, loss=0.0280, lr=2.09e-05, step=4175]Training:   2%|‚ñè         | 4176/200000 [1:29:23<67:48:31,  1.25s/it, loss=0.0180, lr=2.09e-05, step=4176]Training:   2%|‚ñè         | 4177/200000 [1:29:25<70:18:30,  1.29s/it, loss=0.0180, lr=2.09e-05, step=4176]Training:   2%|‚ñè         | 4177/200000 [1:29:25<70:18:30,  1.29s/it, loss=0.0173, lr=2.09e-05, step=4177]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4178/200000 [1:29:26<66:42:17,  1.23s/it, loss=0.0173, lr=2.09e-05, step=4177]Training:   2%|‚ñè         | 4178/200000 [1:29:26<66:42:17,  1.23s/it, loss=0.0273, lr=2.09e-05, step=4178]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4179/200000 [1:29:27<69:23:58,  1.28s/it, loss=0.0273, lr=2.09e-05, step=4178]Training:   2%|‚ñè         | 4179/200000 [1:29:27<69:23:58,  1.28s/it, loss=0.0230, lr=2.09e-05, step=4179]Training:   2%|‚ñè         | 4180/200000 [1:29:28<66:08:18,  1.22s/it, loss=0.0230, lr=2.09e-05, step=4179]Training:   2%|‚ñè         | 4180/200000 [1:29:28<66:08:18,  1.22s/it, loss=0.0220, lr=2.09e-05, step=4180]Training:   2%|‚ñè         | 4181/200000 [1:29:30<69:14:54,  1.27s/it, loss=0.0220, lr=2.09e-05, step=4180]Training:   2%|‚ñè         | 4181/200000 [1:29:30<69:14:54,  1.27s/it, loss=0.0209, lr=2.09e-05, step=4181]Training:   2%|‚ñè         | 4182/200000 [1:29:31<65:59:45,  1.21s/it, loss=0.0209, lr=2.09e-05, step=4181]Training:   2%|‚ñè         | 4182/200000 [1:29:31<65:59:45,  1.21s/it, loss=0.0201, lr=2.09e-05, step=4182]Training:   2%|‚ñè         | 4183/200000 [1:29:32<70:22:09,  1.29s/it, loss=0.0201, lr=2.09e-05, step=4182]Training:   2%|‚ñè         | 4183/200000 [1:29:32<70:22:09,  1.29s/it, loss=0.0535, lr=2.09e-05, step=4183]Training:   2%|‚ñè         | 4184/200000 [1:29:34<73:12:03,  1.35s/it, loss=0.0535, lr=2.09e-05, step=4183]Training:   2%|‚ñè         | 4184/200000 [1:29:34<73:12:03,  1.35s/it, loss=0.0294, lr=2.09e-05, step=4184]Training:   2%|‚ñè         | 4185/200000 [1:29:35<68:47:11,  1.26s/it, loss=0.0294, lr=2.09e-05, step=4184]Training:   2%|‚ñè         | 4185/200000 [1:29:35<68:47:11,  1.26s/it, loss=0.0236, lr=2.09e-05, step=4185]Training:   2%|‚ñè         | 4186/200000 [1:29:36<65:42:54,  1.21s/it, loss=0.0236, lr=2.09e-05, step=4185]Training:   2%|‚ñè         | 4186/200000 [1:29:36<65:42:54,  1.21s/it, loss=0.0153, lr=2.09e-05, step=4186]Training:   2%|‚ñè         | 4187/200000 [1:29:37<69:12:07,  1.27s/it, loss=0.0153, lr=2.09e-05, step=4186]Training:   2%|‚ñè         | 4187/200000 [1:29:37<69:12:07,  1.27s/it, loss=0.0163, lr=2.09e-05, step=4187]Training:   2%|‚ñè         | 4188/200000 [1:29:38<65:58:37,  1.21s/it, loss=0.0163, lr=2.09e-05, step=4187]Training:   2%|‚ñè         | 4188/200000 [1:29:38<65:58:37,  1.21s/it, loss=0.0194, lr=2.09e-05, step=4188]Training:   2%|‚ñè         | 4189/200000 [1:29:40<68:24:38,  1.26s/it, loss=0.0194, lr=2.09e-05, step=4188]Training:   2%|‚ñè         | 4189/200000 [1:29:40<68:24:38,  1.26s/it, loss=0.0148, lr=2.09e-05, step=4189]Training:   2%|‚ñè         | 4190/200000 [1:29:41<65:23:40,  1.20s/it, loss=0.0148, lr=2.09e-05, step=4189]Training:   2%|‚ñè         | 4190/200000 [1:29:41<65:23:40,  1.20s/it, loss=0.0308, lr=2.09e-05, step=4190]Training:   2%|‚ñè         | 4191/200000 [1:29:42<63:20:32,  1.16s/it, loss=0.0308, lr=2.09e-05, step=4190]Training:   2%|‚ñè         | 4191/200000 [1:29:42<63:20:32,  1.16s/it, loss=0.0226, lr=2.10e-05, step=4191]Training:   2%|‚ñè         | 4192/200000 [1:29:43<68:09:52,  1.25s/it, loss=0.0226, lr=2.10e-05, step=4191]Training:   2%|‚ñè         | 4192/200000 [1:29:43<68:09:52,  1.25s/it, loss=0.0132, lr=2.10e-05, step=4192]Training:   2%|‚ñè         | 4193/200000 [1:29:45<70:49:28,  1.30s/it, loss=0.0132, lr=2.10e-05, step=4192]Training:   2%|‚ñè         | 4193/200000 [1:29:45<70:49:28,  1.30s/it, loss=0.0385, lr=2.10e-05, step=4193]Training:   2%|‚ñè         | 4194/200000 [1:29:46<72:57:58,  1.34s/it, loss=0.0385, lr=2.10e-05, step=4193]Training:   2%|‚ñè         | 4194/200000 [1:29:46<72:57:58,  1.34s/it, loss=0.0192, lr=2.10e-05, step=4194]Training:   2%|‚ñè         | 4195/200000 [1:29:47<68:39:24,  1.26s/it, loss=0.0192, lr=2.10e-05, step=4194]Training:   2%|‚ñè         | 4195/200000 [1:29:47<68:39:24,  1.26s/it, loss=0.0170, lr=2.10e-05, step=4195]Training:   2%|‚ñè         | 4196/200000 [1:29:48<65:33:41,  1.21s/it, loss=0.0170, lr=2.10e-05, step=4195]Training:   2%|‚ñè         | 4196/200000 [1:29:48<65:33:41,  1.21s/it, loss=0.0336, lr=2.10e-05, step=4196]Training:   2%|‚ñè         | 4197/200000 [1:29:50<67:56:54,  1.25s/it, loss=0.0336, lr=2.10e-05, step=4196]Training:   2%|‚ñè         | 4197/200000 [1:29:50<67:56:54,  1.25s/it, loss=0.0159, lr=2.10e-05, step=4197]Training:   2%|‚ñè         | 4198/200000 [1:29:51<69:28:06,  1.28s/it, loss=0.0159, lr=2.10e-05, step=4197]Training:   2%|‚ñè         | 4198/200000 [1:29:51<69:28:06,  1.28s/it, loss=0.0139, lr=2.10e-05, step=4198]Training:   2%|‚ñè         | 4199/200000 [1:29:52<66:09:22,  1.22s/it, loss=0.0139, lr=2.10e-05, step=4198]Training:   2%|‚ñè         | 4199/200000 [1:29:52<66:09:22,  1.22s/it, loss=0.0200, lr=2.10e-05, step=4199]Training:   2%|‚ñè         | 4200/200000 [1:29:53<69:06:04,  1.27s/it, loss=0.0200, lr=2.10e-05, step=4199]Training:   2%|‚ñè         | 4200/200000 [1:29:53<69:06:04,  1.27s/it, loss=0.0187, lr=2.10e-05, step=4200]00:23:08.217 [I] step=4200 loss=0.0250 lr=2.08e-05 grad_norm=0.62 time=126.0s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4201/200000 [1:29:54<65:54:42,  1.21s/it, loss=0.0187, lr=2.10e-05, step=4200]Training:   2%|‚ñè         | 4201/200000 [1:29:54<65:54:42,  1.21s/it, loss=0.0193, lr=2.10e-05, step=4201]Training:   2%|‚ñè         | 4202/200000 [1:29:56<68:43:22,  1.26s/it, loss=0.0193, lr=2.10e-05, step=4201]Training:   2%|‚ñè         | 4202/200000 [1:29:56<68:43:22,  1.26s/it, loss=0.0316, lr=2.10e-05, step=4202]Training:   2%|‚ñè         | 4203/200000 [1:29:57<71:54:41,  1.32s/it, loss=0.0316, lr=2.10e-05, step=4202]Training:   2%|‚ñè         | 4203/200000 [1:29:57<71:54:41,  1.32s/it, loss=0.0175, lr=2.10e-05, step=4203]Training:   2%|‚ñè         | 4204/200000 [1:29:59<73:24:04,  1.35s/it, loss=0.0175, lr=2.10e-05, step=4203]Training:   2%|‚ñè         | 4204/200000 [1:29:59<73:24:04,  1.35s/it, loss=0.0369, lr=2.10e-05, step=4204]Training:   2%|‚ñè         | 4205/200000 [1:30:00<74:24:25,  1.37s/it, loss=0.0369, lr=2.10e-05, step=4204]Training:   2%|‚ñè         | 4205/200000 [1:30:00<74:24:25,  1.37s/it, loss=0.0207, lr=2.10e-05, step=4205]Training:   2%|‚ñè         | 4206/200000 [1:30:01<69:35:19,  1.28s/it, loss=0.0207, lr=2.10e-05, step=4205]Training:   2%|‚ñè         | 4206/200000 [1:30:01<69:35:19,  1.28s/it, loss=0.0261, lr=2.10e-05, step=4206]Training:   2%|‚ñè         | 4207/200000 [1:30:02<66:15:00,  1.22s/it, loss=0.0261, lr=2.10e-05, step=4206]Training:   2%|‚ñè         | 4207/200000 [1:30:02<66:15:00,  1.22s/it, loss=0.0395, lr=2.10e-05, step=4207]Training:   2%|‚ñè         | 4208/200000 [1:30:04<69:17:34,  1.27s/it, loss=0.0395, lr=2.10e-05, step=4207]Training:   2%|‚ñè         | 4208/200000 [1:30:04<69:17:34,  1.27s/it, loss=0.0257, lr=2.10e-05, step=4208]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4209/200000 [1:30:05<71:37:13,  1.32s/it, loss=0.0257, lr=2.10e-05, step=4208]Training:   2%|‚ñè         | 4209/200000 [1:30:05<71:37:13,  1.32s/it, loss=0.0244, lr=2.10e-05, step=4209]Training:   2%|‚ñè         | 4210/200000 [1:30:06<67:38:31,  1.24s/it, loss=0.0244, lr=2.10e-05, step=4209]Training:   2%|‚ñè         | 4210/200000 [1:30:06<67:38:31,  1.24s/it, loss=0.0157, lr=2.10e-05, step=4210]Training:   2%|‚ñè         | 4211/200000 [1:30:08<70:31:18,  1.30s/it, loss=0.0157, lr=2.10e-05, step=4210]Training:   2%|‚ñè         | 4211/200000 [1:30:08<70:31:18,  1.30s/it, loss=0.0265, lr=2.11e-05, step=4211]Training:   2%|‚ñè         | 4212/200000 [1:30:09<66:52:15,  1.23s/it, loss=0.0265, lr=2.11e-05, step=4211]Training:   2%|‚ñè         | 4212/200000 [1:30:09<66:52:15,  1.23s/it, loss=0.0353, lr=2.11e-05, step=4212]Training:   2%|‚ñè         | 4213/200000 [1:30:10<69:13:27,  1.27s/it, loss=0.0353, lr=2.11e-05, step=4212]Training:   2%|‚ñè         | 4213/200000 [1:30:10<69:13:27,  1.27s/it, loss=0.0175, lr=2.11e-05, step=4213]Training:   2%|‚ñè         | 4214/200000 [1:30:11<66:01:31,  1.21s/it, loss=0.0175, lr=2.11e-05, step=4213]Training:   2%|‚ñè         | 4214/200000 [1:30:11<66:01:31,  1.21s/it, loss=0.0438, lr=2.11e-05, step=4214]Training:   2%|‚ñè         | 4215/200000 [1:30:13<70:46:13,  1.30s/it, loss=0.0438, lr=2.11e-05, step=4214]Training:   2%|‚ñè         | 4215/200000 [1:30:13<70:46:13,  1.30s/it, loss=0.0185, lr=2.11e-05, step=4215]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4216/200000 [1:30:14<74:22:05,  1.37s/it, loss=0.0185, lr=2.11e-05, step=4215]Training:   2%|‚ñè         | 4216/200000 [1:30:14<74:22:05,  1.37s/it, loss=0.0273, lr=2.11e-05, step=4216]Training:   2%|‚ñè         | 4217/200000 [1:30:15<69:39:47,  1.28s/it, loss=0.0273, lr=2.11e-05, step=4216]Training:   2%|‚ñè         | 4217/200000 [1:30:15<69:39:47,  1.28s/it, loss=0.0293, lr=2.11e-05, step=4217]Training:   2%|‚ñè         | 4218/200000 [1:30:16<66:20:29,  1.22s/it, loss=0.0293, lr=2.11e-05, step=4217]Training:   2%|‚ñè         | 4218/200000 [1:30:16<66:20:29,  1.22s/it, loss=0.0223, lr=2.11e-05, step=4218]Training:   2%|‚ñè         | 4219/200000 [1:30:18<70:23:36,  1.29s/it, loss=0.0223, lr=2.11e-05, step=4218]Training:   2%|‚ñè         | 4219/200000 [1:30:18<70:23:36,  1.29s/it, loss=0.0219, lr=2.11e-05, step=4219]Training:   2%|‚ñè         | 4220/200000 [1:30:19<66:49:45,  1.23s/it, loss=0.0219, lr=2.11e-05, step=4219]Training:   2%|‚ñè         | 4220/200000 [1:30:19<66:49:45,  1.23s/it, loss=0.0223, lr=2.11e-05, step=4220]Training:   2%|‚ñè         | 4221/200000 [1:30:20<69:06:37,  1.27s/it, loss=0.0223, lr=2.11e-05, step=4220]Training:   2%|‚ñè         | 4221/200000 [1:30:20<69:06:37,  1.27s/it, loss=0.0518, lr=2.11e-05, step=4221]Training:   2%|‚ñè         | 4222/200000 [1:30:21<65:57:31,  1.21s/it, loss=0.0518, lr=2.11e-05, step=4221]Training:   2%|‚ñè         | 4222/200000 [1:30:21<65:57:31,  1.21s/it, loss=0.0268, lr=2.11e-05, step=4222]Training:   2%|‚ñè         | 4223/200000 [1:30:22<63:44:06,  1.17s/it, loss=0.0268, lr=2.11e-05, step=4222]Training:   2%|‚ñè         | 4223/200000 [1:30:22<63:44:06,  1.17s/it, loss=0.0360, lr=2.11e-05, step=4223]Training:   2%|‚ñè         | 4224/200000 [1:30:24<68:22:25,  1.26s/it, loss=0.0360, lr=2.11e-05, step=4223]Training:   2%|‚ñè         | 4224/200000 [1:30:24<68:22:25,  1.26s/it, loss=0.0303, lr=2.11e-05, step=4224]Training:   2%|‚ñè         | 4225/200000 [1:30:25<71:25:04,  1.31s/it, loss=0.0303, lr=2.11e-05, step=4224]Training:   2%|‚ñè         | 4225/200000 [1:30:25<71:25:04,  1.31s/it, loss=0.0249, lr=2.11e-05, step=4225]Training:   2%|‚ñè         | 4226/200000 [1:30:27<73:26:10,  1.35s/it, loss=0.0249, lr=2.11e-05, step=4225]Training:   2%|‚ñè         | 4226/200000 [1:30:27<73:26:10,  1.35s/it, loss=0.0446, lr=2.11e-05, step=4226]Training:   2%|‚ñè         | 4227/200000 [1:30:28<68:58:12,  1.27s/it, loss=0.0446, lr=2.11e-05, step=4226]Training:   2%|‚ñè         | 4227/200000 [1:30:28<68:58:12,  1.27s/it, loss=0.0175, lr=2.11e-05, step=4227]Training:   2%|‚ñè         | 4228/200000 [1:30:29<65:50:45,  1.21s/it, loss=0.0175, lr=2.11e-05, step=4227]Training:   2%|‚ñè         | 4228/200000 [1:30:29<65:50:45,  1.21s/it, loss=0.0237, lr=2.11e-05, step=4228]Training:   2%|‚ñè         | 4229/200000 [1:30:30<68:04:50,  1.25s/it, loss=0.0237, lr=2.11e-05, step=4228]Training:   2%|‚ñè         | 4229/200000 [1:30:30<68:04:50,  1.25s/it, loss=0.0666, lr=2.11e-05, step=4229]Training:   2%|‚ñè         | 4230/200000 [1:30:32<70:42:25,  1.30s/it, loss=0.0666, lr=2.11e-05, step=4229]Training:   2%|‚ñè         | 4230/200000 [1:30:32<70:42:25,  1.30s/it, loss=0.0140, lr=2.11e-05, step=4230]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4231/200000 [1:30:33<67:02:16,  1.23s/it, loss=0.0140, lr=2.11e-05, step=4230]Training:   2%|‚ñè         | 4231/200000 [1:30:33<67:02:16,  1.23s/it, loss=0.0535, lr=2.12e-05, step=4231]Training:   2%|‚ñè         | 4232/200000 [1:30:34<70:32:59,  1.30s/it, loss=0.0535, lr=2.12e-05, step=4231]Training:   2%|‚ñè         | 4232/200000 [1:30:34<70:32:59,  1.30s/it, loss=0.0163, lr=2.12e-05, step=4232]Training:   2%|‚ñè         | 4233/200000 [1:30:35<66:57:58,  1.23s/it, loss=0.0163, lr=2.12e-05, step=4232]Training:   2%|‚ñè         | 4233/200000 [1:30:35<66:57:58,  1.23s/it, loss=0.0179, lr=2.12e-05, step=4233]Training:   2%|‚ñè         | 4234/200000 [1:30:37<69:22:02,  1.28s/it, loss=0.0179, lr=2.12e-05, step=4233]Training:   2%|‚ñè         | 4234/200000 [1:30:37<69:22:02,  1.28s/it, loss=0.0224, lr=2.12e-05, step=4234]Training:   2%|‚ñè         | 4235/200000 [1:30:38<66:07:31,  1.22s/it, loss=0.0224, lr=2.12e-05, step=4234]Training:   2%|‚ñè         | 4235/200000 [1:30:38<66:07:31,  1.22s/it, loss=0.0313, lr=2.12e-05, step=4235]Training:   2%|‚ñè         | 4236/200000 [1:30:39<70:19:13,  1.29s/it, loss=0.0313, lr=2.12e-05, step=4235]Training:   2%|‚ñè         | 4236/200000 [1:30:39<70:19:13,  1.29s/it, loss=0.0176, lr=2.12e-05, step=4236]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4237/200000 [1:30:41<73:16:40,  1.35s/it, loss=0.0176, lr=2.12e-05, step=4236]Training:   2%|‚ñè         | 4237/200000 [1:30:41<73:16:40,  1.35s/it, loss=0.0310, lr=2.12e-05, step=4237]Training:   2%|‚ñè         | 4238/200000 [1:30:42<68:50:24,  1.27s/it, loss=0.0310, lr=2.12e-05, step=4237]Training:   2%|‚ñè         | 4238/200000 [1:30:42<68:50:24,  1.27s/it, loss=0.0820, lr=2.12e-05, step=4238]Training:   2%|‚ñè         | 4239/200000 [1:30:43<65:45:04,  1.21s/it, loss=0.0820, lr=2.12e-05, step=4238]Training:   2%|‚ñè         | 4239/200000 [1:30:43<65:45:04,  1.21s/it, loss=0.0254, lr=2.12e-05, step=4239]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4240/200000 [1:30:44<69:53:24,  1.29s/it, loss=0.0254, lr=2.12e-05, step=4239]Training:   2%|‚ñè         | 4240/200000 [1:30:44<69:53:24,  1.29s/it, loss=0.0194, lr=2.12e-05, step=4240]Training:   2%|‚ñè         | 4241/200000 [1:30:45<66:29:25,  1.22s/it, loss=0.0194, lr=2.12e-05, step=4240]Training:   2%|‚ñè         | 4241/200000 [1:30:45<66:29:25,  1.22s/it, loss=0.0305, lr=2.12e-05, step=4241]Training:   2%|‚ñè         | 4242/200000 [1:30:47<68:38:29,  1.26s/it, loss=0.0305, lr=2.12e-05, step=4241]Training:   2%|‚ñè         | 4242/200000 [1:30:47<68:38:29,  1.26s/it, loss=0.0152, lr=2.12e-05, step=4242]Training:   2%|‚ñè         | 4243/200000 [1:30:48<65:34:44,  1.21s/it, loss=0.0152, lr=2.12e-05, step=4242]Training:   2%|‚ñè         | 4243/200000 [1:30:48<65:34:44,  1.21s/it, loss=0.0172, lr=2.12e-05, step=4243]Training:   2%|‚ñè         | 4244/200000 [1:30:49<63:29:09,  1.17s/it, loss=0.0172, lr=2.12e-05, step=4243]Training:   2%|‚ñè         | 4244/200000 [1:30:49<63:29:09,  1.17s/it, loss=0.0272, lr=2.12e-05, step=4244]Training:   2%|‚ñè         | 4245/200000 [1:30:50<67:38:15,  1.24s/it, loss=0.0272, lr=2.12e-05, step=4244]Training:   2%|‚ñè         | 4245/200000 [1:30:50<67:38:15,  1.24s/it, loss=0.0200, lr=2.12e-05, step=4245]Training:   2%|‚ñè         | 4246/200000 [1:30:52<71:10:23,  1.31s/it, loss=0.0200, lr=2.12e-05, step=4245]Training:   2%|‚ñè         | 4246/200000 [1:30:52<71:10:23,  1.31s/it, loss=0.0269, lr=2.12e-05, step=4246]Training:   2%|‚ñè         | 4247/200000 [1:30:53<72:43:24,  1.34s/it, loss=0.0269, lr=2.12e-05, step=4246]Training:   2%|‚ñè         | 4247/200000 [1:30:53<72:43:24,  1.34s/it, loss=0.0258, lr=2.12e-05, step=4247]Training:   2%|‚ñè         | 4248/200000 [1:30:54<68:28:49,  1.26s/it, loss=0.0258, lr=2.12e-05, step=4247]Training:   2%|‚ñè         | 4248/200000 [1:30:54<68:28:49,  1.26s/it, loss=0.0237, lr=2.12e-05, step=4248]Training:   2%|‚ñè         | 4249/200000 [1:30:55<65:30:48,  1.20s/it, loss=0.0237, lr=2.12e-05, step=4248]Training:   2%|‚ñè         | 4249/200000 [1:30:55<65:30:48,  1.20s/it, loss=0.0275, lr=2.12e-05, step=4249]Training:   2%|‚ñè         | 4250/200000 [1:30:57<67:15:10,  1.24s/it, loss=0.0275, lr=2.12e-05, step=4249]Training:   2%|‚ñè         | 4250/200000 [1:30:57<67:15:10,  1.24s/it, loss=0.0448, lr=2.12e-05, step=4250]Training:   2%|‚ñè         | 4251/200000 [1:30:58<68:58:49,  1.27s/it, loss=0.0448, lr=2.12e-05, step=4250]Training:   2%|‚ñè         | 4251/200000 [1:30:58<68:58:49,  1.27s/it, loss=0.0157, lr=2.13e-05, step=4251]Training:   2%|‚ñè         | 4252/200000 [1:30:59<65:49:40,  1.21s/it, loss=0.0157, lr=2.13e-05, step=4251]Training:   2%|‚ñè         | 4252/200000 [1:30:59<65:49:40,  1.21s/it, loss=0.0276, lr=2.13e-05, step=4252]Training:   2%|‚ñè         | 4253/200000 [1:31:00<68:31:41,  1.26s/it, loss=0.0276, lr=2.13e-05, step=4252]Training:   2%|‚ñè         | 4253/200000 [1:31:00<68:31:41,  1.26s/it, loss=0.0202, lr=2.13e-05, step=4253]Training:   2%|‚ñè         | 4254/200000 [1:31:01<65:30:51,  1.20s/it, loss=0.0202, lr=2.13e-05, step=4253]Training:   2%|‚ñè         | 4254/200000 [1:31:01<65:30:51,  1.20s/it, loss=0.0173, lr=2.13e-05, step=4254]Training:   2%|‚ñè         | 4255/200000 [1:31:03<68:32:14,  1.26s/it, loss=0.0173, lr=2.13e-05, step=4254]Training:   2%|‚ñè         | 4255/200000 [1:31:03<68:32:14,  1.26s/it, loss=0.0293, lr=2.13e-05, step=4255]Training:   2%|‚ñè         | 4256/200000 [1:31:04<71:43:13,  1.32s/it, loss=0.0293, lr=2.13e-05, step=4255]Training:   2%|‚ñè         | 4256/200000 [1:31:04<71:43:13,  1.32s/it, loss=0.0165, lr=2.13e-05, step=4256]Training:   2%|‚ñè         | 4257/200000 [1:31:06<73:17:05,  1.35s/it, loss=0.0165, lr=2.13e-05, step=4256]Training:   2%|‚ñè         | 4257/200000 [1:31:06<73:17:05,  1.35s/it, loss=0.0174, lr=2.13e-05, step=4257]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4258/200000 [1:31:07<74:49:50,  1.38s/it, loss=0.0174, lr=2.13e-05, step=4257]Training:   2%|‚ñè         | 4258/200000 [1:31:07<74:49:50,  1.38s/it, loss=0.0280, lr=2.13e-05, step=4258]Training:   2%|‚ñè         | 4259/200000 [1:31:08<69:56:43,  1.29s/it, loss=0.0280, lr=2.13e-05, step=4258]Training:   2%|‚ñè         | 4259/200000 [1:31:08<69:56:43,  1.29s/it, loss=0.0342, lr=2.13e-05, step=4259]Training:   2%|‚ñè         | 4260/200000 [1:31:09<66:33:08,  1.22s/it, loss=0.0342, lr=2.13e-05, step=4259]Training:   2%|‚ñè         | 4260/200000 [1:31:09<66:33:08,  1.22s/it, loss=0.0127, lr=2.13e-05, step=4260]Training:   2%|‚ñè         | 4261/200000 [1:31:11<69:29:06,  1.28s/it, loss=0.0127, lr=2.13e-05, step=4260]Training:   2%|‚ñè         | 4261/200000 [1:31:11<69:29:06,  1.28s/it, loss=0.0183, lr=2.13e-05, step=4261]Training:   2%|‚ñè         | 4262/200000 [1:31:12<72:17:07,  1.33s/it, loss=0.0183, lr=2.13e-05, step=4261]Training:   2%|‚ñè         | 4262/200000 [1:31:12<72:17:07,  1.33s/it, loss=0.0271, lr=2.13e-05, step=4262]Training:   2%|‚ñè         | 4263/200000 [1:31:13<68:10:27,  1.25s/it, loss=0.0271, lr=2.13e-05, step=4262]Training:   2%|‚ñè         | 4263/200000 [1:31:13<68:10:27,  1.25s/it, loss=0.0212, lr=2.13e-05, step=4263]Training:   2%|‚ñè         | 4264/200000 [1:31:15<71:51:11,  1.32s/it, loss=0.0212, lr=2.13e-05, step=4263]Training:   2%|‚ñè         | 4264/200000 [1:31:15<71:51:11,  1.32s/it, loss=0.0348, lr=2.13e-05, step=4264]Training:   2%|‚ñè         | 4265/200000 [1:31:16<67:52:40,  1.25s/it, loss=0.0348, lr=2.13e-05, step=4264]Training:   2%|‚ñè         | 4265/200000 [1:31:16<67:52:40,  1.25s/it, loss=0.0232, lr=2.13e-05, step=4265]Training:   2%|‚ñè         | 4266/200000 [1:31:17<69:58:06,  1.29s/it, loss=0.0232, lr=2.13e-05, step=4265]Training:   2%|‚ñè         | 4266/200000 [1:31:17<69:58:06,  1.29s/it, loss=0.0126, lr=2.13e-05, step=4266]Training:   2%|‚ñè         | 4267/200000 [1:31:18<66:34:40,  1.22s/it, loss=0.0126, lr=2.13e-05, step=4266]Training:   2%|‚ñè         | 4267/200000 [1:31:18<66:34:40,  1.22s/it, loss=0.0240, lr=2.13e-05, step=4267]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4268/200000 [1:31:20<70:58:47,  1.31s/it, loss=0.0240, lr=2.13e-05, step=4267]Training:   2%|‚ñè         | 4268/200000 [1:31:20<70:58:47,  1.31s/it, loss=0.0290, lr=2.13e-05, step=4268]Training:   2%|‚ñè         | 4269/200000 [1:31:21<74:30:59,  1.37s/it, loss=0.0290, lr=2.13e-05, step=4268]Training:   2%|‚ñè         | 4269/200000 [1:31:21<74:30:59,  1.37s/it, loss=0.0242, lr=2.13e-05, step=4269]Training:   2%|‚ñè         | 4270/200000 [1:31:22<69:45:01,  1.28s/it, loss=0.0242, lr=2.13e-05, step=4269]Training:   2%|‚ñè         | 4270/200000 [1:31:22<69:45:01,  1.28s/it, loss=0.0209, lr=2.13e-05, step=4270]Training:   2%|‚ñè         | 4271/200000 [1:31:23<66:22:14,  1.22s/it, loss=0.0209, lr=2.13e-05, step=4270]Training:   2%|‚ñè         | 4271/200000 [1:31:23<66:22:14,  1.22s/it, loss=0.0197, lr=2.14e-05, step=4271]Training:   2%|‚ñè         | 4272/200000 [1:31:25<70:36:51,  1.30s/it, loss=0.0197, lr=2.14e-05, step=4271]Training:   2%|‚ñè         | 4272/200000 [1:31:25<70:36:51,  1.30s/it, loss=0.0267, lr=2.14e-05, step=4272]Training:   2%|‚ñè         | 4273/200000 [1:31:26<66:59:25,  1.23s/it, loss=0.0267, lr=2.14e-05, step=4272]Training:   2%|‚ñè         | 4273/200000 [1:31:26<66:59:25,  1.23s/it, loss=0.0171, lr=2.14e-05, step=4273]Training:   2%|‚ñè         | 4274/200000 [1:31:27<68:22:25,  1.26s/it, loss=0.0171, lr=2.14e-05, step=4273]Training:   2%|‚ñè         | 4274/200000 [1:31:27<68:22:25,  1.26s/it, loss=0.0205, lr=2.14e-05, step=4274]Training:   2%|‚ñè         | 4275/200000 [1:31:28<65:25:28,  1.20s/it, loss=0.0205, lr=2.14e-05, step=4274]Training:   2%|‚ñè         | 4275/200000 [1:31:28<65:25:28,  1.20s/it, loss=0.0105, lr=2.14e-05, step=4275]Training:   2%|‚ñè         | 4276/200000 [1:31:29<63:24:11,  1.17s/it, loss=0.0105, lr=2.14e-05, step=4275]Training:   2%|‚ñè         | 4276/200000 [1:31:29<63:24:11,  1.17s/it, loss=0.0400, lr=2.14e-05, step=4276]Training:   2%|‚ñè         | 4277/200000 [1:31:31<67:54:16,  1.25s/it, loss=0.0400, lr=2.14e-05, step=4276]Training:   2%|‚ñè         | 4277/200000 [1:31:31<67:54:16,  1.25s/it, loss=0.2025, lr=2.14e-05, step=4277]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4278/200000 [1:31:32<70:58:50,  1.31s/it, loss=0.2025, lr=2.14e-05, step=4277]Training:   2%|‚ñè         | 4278/200000 [1:31:32<70:58:50,  1.31s/it, loss=0.0203, lr=2.14e-05, step=4278]Training:   2%|‚ñè         | 4279/200000 [1:31:34<72:23:12,  1.33s/it, loss=0.0203, lr=2.14e-05, step=4278]Training:   2%|‚ñè         | 4279/200000 [1:31:34<72:23:12,  1.33s/it, loss=0.0270, lr=2.14e-05, step=4279]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4280/200000 [1:31:35<68:14:06,  1.26s/it, loss=0.0270, lr=2.14e-05, step=4279]Training:   2%|‚ñè         | 4280/200000 [1:31:35<68:14:06,  1.26s/it, loss=0.0212, lr=2.14e-05, step=4280]Training:   2%|‚ñè         | 4281/200000 [1:31:36<65:20:27,  1.20s/it, loss=0.0212, lr=2.14e-05, step=4280]Training:   2%|‚ñè         | 4281/200000 [1:31:36<65:20:27,  1.20s/it, loss=0.0165, lr=2.14e-05, step=4281]Training:   2%|‚ñè         | 4282/200000 [1:31:37<67:45:56,  1.25s/it, loss=0.0165, lr=2.14e-05, step=4281]Training:   2%|‚ñè         | 4282/200000 [1:31:37<67:45:56,  1.25s/it, loss=0.0182, lr=2.14e-05, step=4282]Training:   2%|‚ñè         | 4283/200000 [1:31:39<70:09:41,  1.29s/it, loss=0.0182, lr=2.14e-05, step=4282]Training:   2%|‚ñè         | 4283/200000 [1:31:39<70:09:41,  1.29s/it, loss=0.1108, lr=2.14e-05, step=4283]Training:   2%|‚ñè         | 4284/200000 [1:31:40<66:41:01,  1.23s/it, loss=0.1108, lr=2.14e-05, step=4283]Training:   2%|‚ñè         | 4284/200000 [1:31:40<66:41:01,  1.23s/it, loss=0.0210, lr=2.14e-05, step=4284]Training:   2%|‚ñè         | 4285/200000 [1:31:41<69:37:21,  1.28s/it, loss=0.0210, lr=2.14e-05, step=4284]Training:   2%|‚ñè         | 4285/200000 [1:31:41<69:37:21,  1.28s/it, loss=0.0944, lr=2.14e-05, step=4285]Training:   2%|‚ñè         | 4286/200000 [1:31:42<66:19:10,  1.22s/it, loss=0.0944, lr=2.14e-05, step=4285]Training:   2%|‚ñè         | 4286/200000 [1:31:42<66:19:10,  1.22s/it, loss=0.0217, lr=2.14e-05, step=4286]Training:   2%|‚ñè         | 4287/200000 [1:31:44<69:19:51,  1.28s/it, loss=0.0217, lr=2.14e-05, step=4286]Training:   2%|‚ñè         | 4287/200000 [1:31:44<69:19:51,  1.28s/it, loss=0.0262, lr=2.14e-05, step=4287]Training:   2%|‚ñè         | 4288/200000 [1:31:45<66:01:17,  1.21s/it, loss=0.0262, lr=2.14e-05, step=4287]Training:   2%|‚ñè         | 4288/200000 [1:31:45<66:01:17,  1.21s/it, loss=0.0229, lr=2.14e-05, step=4288]Training:   2%|‚ñè         | 4289/200000 [1:31:46<70:19:16,  1.29s/it, loss=0.0229, lr=2.14e-05, step=4288]Training:   2%|‚ñè         | 4289/200000 [1:31:46<70:19:16,  1.29s/it, loss=0.0157, lr=2.14e-05, step=4289]Training:   2%|‚ñè         | 4290/200000 [1:31:48<73:50:40,  1.36s/it, loss=0.0157, lr=2.14e-05, step=4289]Training:   2%|‚ñè         | 4290/200000 [1:31:48<73:50:40,  1.36s/it, loss=0.0299, lr=2.14e-05, step=4290]Training:   2%|‚ñè         | 4291/200000 [1:31:49<69:13:25,  1.27s/it, loss=0.0299, lr=2.14e-05, step=4290]Training:   2%|‚ñè         | 4291/200000 [1:31:49<69:13:25,  1.27s/it, loss=0.0265, lr=2.15e-05, step=4291]Training:   2%|‚ñè         | 4292/200000 [1:31:50<66:00:58,  1.21s/it, loss=0.0265, lr=2.15e-05, step=4291]Training:   2%|‚ñè         | 4292/200000 [1:31:50<66:00:58,  1.21s/it, loss=0.0273, lr=2.15e-05, step=4292]Training:   2%|‚ñè         | 4293/200000 [1:31:51<68:33:46,  1.26s/it, loss=0.0273, lr=2.15e-05, step=4292]Training:   2%|‚ñè         | 4293/200000 [1:31:51<68:33:46,  1.26s/it, loss=0.0198, lr=2.15e-05, step=4293]Training:   2%|‚ñè         | 4294/200000 [1:31:52<65:31:09,  1.21s/it, loss=0.0198, lr=2.15e-05, step=4293]Training:   2%|‚ñè         | 4294/200000 [1:31:52<65:31:09,  1.21s/it, loss=0.0297, lr=2.15e-05, step=4294]Training:   2%|‚ñè         | 4295/200000 [1:31:53<66:58:09,  1.23s/it, loss=0.0297, lr=2.15e-05, step=4294]Training:   2%|‚ñè         | 4295/200000 [1:31:53<66:58:09,  1.23s/it, loss=0.0137, lr=2.15e-05, step=4295]Training:   2%|‚ñè         | 4296/200000 [1:31:55<77:41:23,  1.43s/it, loss=0.0137, lr=2.15e-05, step=4295]Training:   2%|‚ñè         | 4296/200000 [1:31:55<77:41:23,  1.43s/it, loss=0.0226, lr=2.15e-05, step=4296]Training:   2%|‚ñè         | 4297/200000 [1:31:56<71:57:50,  1.32s/it, loss=0.0226, lr=2.15e-05, step=4296]Training:   2%|‚ñè         | 4297/200000 [1:31:56<71:57:50,  1.32s/it, loss=0.0195, lr=2.15e-05, step=4297]Training:   2%|‚ñè         | 4298/200000 [1:31:58<73:33:38,  1.35s/it, loss=0.0195, lr=2.15e-05, step=4297]Training:   2%|‚ñè         | 4298/200000 [1:31:58<73:33:38,  1.35s/it, loss=0.0304, lr=2.15e-05, step=4298]Training:   2%|‚ñè         | 4299/200000 [1:31:59<75:23:48,  1.39s/it, loss=0.0304, lr=2.15e-05, step=4298]Training:   2%|‚ñè         | 4299/200000 [1:31:59<75:23:48,  1.39s/it, loss=0.0402, lr=2.15e-05, step=4299]Training:   2%|‚ñè         | 4300/200000 [1:32:01<75:31:21,  1.39s/it, loss=0.0402, lr=2.15e-05, step=4299]Training:   2%|‚ñè         | 4300/200000 [1:32:01<75:31:21,  1.39s/it, loss=0.0272, lr=2.15e-05, step=4300]00:25:15.619 [I] step=4300 loss=0.0293 lr=2.13e-05 grad_norm=0.60 time=127.4s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4301/200000 [1:32:02<70:36:41,  1.30s/it, loss=0.0272, lr=2.15e-05, step=4300]Training:   2%|‚ñè         | 4301/200000 [1:32:02<70:36:41,  1.30s/it, loss=0.0184, lr=2.15e-05, step=4301]Training:   2%|‚ñè         | 4302/200000 [1:32:03<67:25:24,  1.24s/it, loss=0.0184, lr=2.15e-05, step=4301]Training:   2%|‚ñè         | 4302/200000 [1:32:03<67:25:24,  1.24s/it, loss=0.0283, lr=2.15e-05, step=4302]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4303/200000 [1:32:04<68:46:46,  1.27s/it, loss=0.0283, lr=2.15e-05, step=4302]Training:   2%|‚ñè         | 4303/200000 [1:32:04<68:46:46,  1.27s/it, loss=0.0147, lr=2.15e-05, step=4303]Training:   2%|‚ñè         | 4304/200000 [1:32:06<69:52:28,  1.29s/it, loss=0.0147, lr=2.15e-05, step=4303]Training:   2%|‚ñè         | 4304/200000 [1:32:06<69:52:28,  1.29s/it, loss=0.0303, lr=2.15e-05, step=4304]Training:   2%|‚ñè         | 4305/200000 [1:32:07<66:28:18,  1.22s/it, loss=0.0303, lr=2.15e-05, step=4304]Training:   2%|‚ñè         | 4305/200000 [1:32:07<66:28:18,  1.22s/it, loss=0.0194, lr=2.15e-05, step=4305]Training:   2%|‚ñè         | 4306/200000 [1:32:08<68:39:15,  1.26s/it, loss=0.0194, lr=2.15e-05, step=4305]Training:   2%|‚ñè         | 4306/200000 [1:32:08<68:39:15,  1.26s/it, loss=0.0166, lr=2.15e-05, step=4306]Training:   2%|‚ñè         | 4307/200000 [1:32:09<65:38:28,  1.21s/it, loss=0.0166, lr=2.15e-05, step=4306]Training:   2%|‚ñè         | 4307/200000 [1:32:09<65:38:28,  1.21s/it, loss=0.0219, lr=2.15e-05, step=4307]Training:   2%|‚ñè         | 4308/200000 [1:32:10<68:08:34,  1.25s/it, loss=0.0219, lr=2.15e-05, step=4307]Training:   2%|‚ñè         | 4308/200000 [1:32:10<68:08:34,  1.25s/it, loss=0.0201, lr=2.15e-05, step=4308]Training:   2%|‚ñè         | 4309/200000 [1:32:12<71:31:08,  1.32s/it, loss=0.0201, lr=2.15e-05, step=4308]Training:   2%|‚ñè         | 4309/200000 [1:32:12<71:31:08,  1.32s/it, loss=0.0311, lr=2.15e-05, step=4309]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4310/200000 [1:32:13<74:07:32,  1.36s/it, loss=0.0311, lr=2.15e-05, step=4309]Training:   2%|‚ñè         | 4310/200000 [1:32:13<74:07:32,  1.36s/it, loss=0.0290, lr=2.15e-05, step=4310]Training:   2%|‚ñè         | 4311/200000 [1:32:15<75:26:21,  1.39s/it, loss=0.0290, lr=2.15e-05, step=4310]Training:   2%|‚ñè         | 4311/200000 [1:32:15<75:26:21,  1.39s/it, loss=0.0251, lr=2.16e-05, step=4311]Training:   2%|‚ñè         | 4312/200000 [1:32:16<70:21:41,  1.29s/it, loss=0.0251, lr=2.16e-05, step=4311]Training:   2%|‚ñè         | 4312/200000 [1:32:16<70:21:41,  1.29s/it, loss=0.0243, lr=2.16e-05, step=4312]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4313/200000 [1:32:17<66:46:56,  1.23s/it, loss=0.0243, lr=2.16e-05, step=4312]Training:   2%|‚ñè         | 4313/200000 [1:32:17<66:46:56,  1.23s/it, loss=0.0146, lr=2.16e-05, step=4313]Training:   2%|‚ñè         | 4314/200000 [1:32:18<69:39:31,  1.28s/it, loss=0.0146, lr=2.16e-05, step=4313]Training:   2%|‚ñè         | 4314/200000 [1:32:18<69:39:31,  1.28s/it, loss=0.0253, lr=2.16e-05, step=4314]Training:   2%|‚ñè         | 4315/200000 [1:32:20<71:54:33,  1.32s/it, loss=0.0253, lr=2.16e-05, step=4314]Training:   2%|‚ñè         | 4315/200000 [1:32:20<71:54:33,  1.32s/it, loss=0.0228, lr=2.16e-05, step=4315]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4316/200000 [1:32:21<67:53:29,  1.25s/it, loss=0.0228, lr=2.16e-05, step=4315]Training:   2%|‚ñè         | 4316/200000 [1:32:21<67:53:29,  1.25s/it, loss=0.0215, lr=2.16e-05, step=4316]Training:   2%|‚ñè         | 4317/200000 [1:32:22<71:30:03,  1.32s/it, loss=0.0215, lr=2.16e-05, step=4316]Training:   2%|‚ñè         | 4317/200000 [1:32:22<71:30:03,  1.32s/it, loss=0.0229, lr=2.16e-05, step=4317]Training:   2%|‚ñè         | 4318/200000 [1:32:23<67:36:57,  1.24s/it, loss=0.0229, lr=2.16e-05, step=4317]Training:   2%|‚ñè         | 4318/200000 [1:32:23<67:36:57,  1.24s/it, loss=0.0208, lr=2.16e-05, step=4318]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4319/200000 [1:32:25<69:32:59,  1.28s/it, loss=0.0208, lr=2.16e-05, step=4318]Training:   2%|‚ñè         | 4319/200000 [1:32:25<69:32:59,  1.28s/it, loss=0.0367, lr=2.16e-05, step=4319]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4320/200000 [1:32:26<66:13:05,  1.22s/it, loss=0.0367, lr=2.16e-05, step=4319]Training:   2%|‚ñè         | 4320/200000 [1:32:26<66:13:05,  1.22s/it, loss=0.0200, lr=2.16e-05, step=4320]Training:   2%|‚ñè         | 4321/200000 [1:32:27<70:45:53,  1.30s/it, loss=0.0200, lr=2.16e-05, step=4320]Training:   2%|‚ñè         | 4321/200000 [1:32:27<70:45:53,  1.30s/it, loss=0.0205, lr=2.16e-05, step=4321]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4322/200000 [1:32:29<73:45:41,  1.36s/it, loss=0.0205, lr=2.16e-05, step=4321]Training:   2%|‚ñè         | 4322/200000 [1:32:29<73:45:41,  1.36s/it, loss=0.0230, lr=2.16e-05, step=4322]Training:   2%|‚ñè         | 4323/200000 [1:32:30<69:12:33,  1.27s/it, loss=0.0230, lr=2.16e-05, step=4322]Training:   2%|‚ñè         | 4323/200000 [1:32:30<69:12:33,  1.27s/it, loss=0.0346, lr=2.16e-05, step=4323]Training:   2%|‚ñè         | 4324/200000 [1:32:31<66:00:09,  1.21s/it, loss=0.0346, lr=2.16e-05, step=4323]Training:   2%|‚ñè         | 4324/200000 [1:32:31<66:00:09,  1.21s/it, loss=0.0249, lr=2.16e-05, step=4324]Training:   2%|‚ñè         | 4325/200000 [1:32:32<70:25:17,  1.30s/it, loss=0.0249, lr=2.16e-05, step=4324]Training:   2%|‚ñè         | 4325/200000 [1:32:32<70:25:17,  1.30s/it, loss=0.0780, lr=2.16e-05, step=4325]Training:   2%|‚ñè         | 4326/200000 [1:32:34<66:52:16,  1.23s/it, loss=0.0780, lr=2.16e-05, step=4325]Training:   2%|‚ñè         | 4326/200000 [1:32:34<66:52:16,  1.23s/it, loss=0.0266, lr=2.16e-05, step=4326]Training:   2%|‚ñè         | 4327/200000 [1:32:35<68:36:05,  1.26s/it, loss=0.0266, lr=2.16e-05, step=4326]Training:   2%|‚ñè         | 4327/200000 [1:32:35<68:36:05,  1.26s/it, loss=0.0504, lr=2.16e-05, step=4327]Training:   2%|‚ñè         | 4328/200000 [1:32:36<65:37:50,  1.21s/it, loss=0.0504, lr=2.16e-05, step=4327]Training:   2%|‚ñè         | 4328/200000 [1:32:36<65:37:50,  1.21s/it, loss=0.0152, lr=2.16e-05, step=4328]Training:   2%|‚ñè         | 4329/200000 [1:32:37<63:28:01,  1.17s/it, loss=0.0152, lr=2.16e-05, step=4328]Training:   2%|‚ñè         | 4329/200000 [1:32:37<63:28:01,  1.17s/it, loss=0.0243, lr=2.16e-05, step=4329]Training:   2%|‚ñè         | 4330/200000 [1:32:39<67:56:18,  1.25s/it, loss=0.0243, lr=2.16e-05, step=4329]Training:   2%|‚ñè         | 4330/200000 [1:32:39<67:56:18,  1.25s/it, loss=0.0128, lr=2.16e-05, step=4330]Training:   2%|‚ñè         | 4331/200000 [1:32:40<71:29:14,  1.32s/it, loss=0.0128, lr=2.16e-05, step=4330]Training:   2%|‚ñè         | 4331/200000 [1:32:40<71:29:14,  1.32s/it, loss=0.0351, lr=2.17e-05, step=4331]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4332/200000 [1:32:41<73:28:25,  1.35s/it, loss=0.0351, lr=2.17e-05, step=4331]Training:   2%|‚ñè         | 4332/200000 [1:32:41<73:28:25,  1.35s/it, loss=0.0152, lr=2.17e-05, step=4332]Training:   2%|‚ñè         | 4333/200000 [1:32:42<69:00:29,  1.27s/it, loss=0.0152, lr=2.17e-05, step=4332]Training:   2%|‚ñè         | 4333/200000 [1:32:42<69:00:29,  1.27s/it, loss=0.0189, lr=2.17e-05, step=4333]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4334/200000 [1:32:44<65:55:00,  1.21s/it, loss=0.0189, lr=2.17e-05, step=4333]Training:   2%|‚ñè         | 4334/200000 [1:32:44<65:55:00,  1.21s/it, loss=0.0245, lr=2.17e-05, step=4334]Training:   2%|‚ñè         | 4335/200000 [1:32:45<68:05:55,  1.25s/it, loss=0.0245, lr=2.17e-05, step=4334]Training:   2%|‚ñè         | 4335/200000 [1:32:45<68:05:55,  1.25s/it, loss=0.0511, lr=2.17e-05, step=4335]Training:   2%|‚ñè         | 4336/200000 [1:32:46<70:24:46,  1.30s/it, loss=0.0511, lr=2.17e-05, step=4335]Training:   2%|‚ñè         | 4336/200000 [1:32:46<70:24:46,  1.30s/it, loss=0.0198, lr=2.17e-05, step=4336]Training:   2%|‚ñè         | 4337/200000 [1:32:47<66:59:00,  1.23s/it, loss=0.0198, lr=2.17e-05, step=4336]Training:   2%|‚ñè         | 4337/200000 [1:32:47<66:59:00,  1.23s/it, loss=0.0217, lr=2.17e-05, step=4337]Training:   2%|‚ñè         | 4338/200000 [1:32:49<69:36:32,  1.28s/it, loss=0.0217, lr=2.17e-05, step=4337]Training:   2%|‚ñè         | 4338/200000 [1:32:49<69:36:32,  1.28s/it, loss=0.0133, lr=2.17e-05, step=4338]Training:   2%|‚ñè         | 4339/200000 [1:32:50<66:16:04,  1.22s/it, loss=0.0133, lr=2.17e-05, step=4338]Training:   2%|‚ñè         | 4339/200000 [1:32:50<66:16:04,  1.22s/it, loss=0.0150, lr=2.17e-05, step=4339]Training:   2%|‚ñè         | 4340/200000 [1:32:51<69:16:19,  1.27s/it, loss=0.0150, lr=2.17e-05, step=4339]Training:   2%|‚ñè         | 4340/200000 [1:32:51<69:16:19,  1.27s/it, loss=0.0201, lr=2.17e-05, step=4340]Training:   2%|‚ñè         | 4341/200000 [1:32:52<66:03:58,  1.22s/it, loss=0.0201, lr=2.17e-05, step=4340]Training:   2%|‚ñè         | 4341/200000 [1:32:52<66:03:58,  1.22s/it, loss=0.0248, lr=2.17e-05, step=4341]Training:   2%|‚ñè         | 4342/200000 [1:32:54<70:18:44,  1.29s/it, loss=0.0248, lr=2.17e-05, step=4341]Training:   2%|‚ñè         | 4342/200000 [1:32:54<70:18:44,  1.29s/it, loss=0.0289, lr=2.17e-05, step=4342]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4343/200000 [1:32:55<73:37:32,  1.35s/it, loss=0.0289, lr=2.17e-05, step=4342]Training:   2%|‚ñè         | 4343/200000 [1:32:55<73:37:32,  1.35s/it, loss=0.0265, lr=2.17e-05, step=4343]Training:   2%|‚ñè         | 4344/200000 [1:32:56<69:07:30,  1.27s/it, loss=0.0265, lr=2.17e-05, step=4343]Training:   2%|‚ñè         | 4344/200000 [1:32:56<69:07:30,  1.27s/it, loss=0.0284, lr=2.17e-05, step=4344]Training:   2%|‚ñè         | 4345/200000 [1:32:57<65:56:45,  1.21s/it, loss=0.0284, lr=2.17e-05, step=4344]Training:   2%|‚ñè         | 4345/200000 [1:32:57<65:56:45,  1.21s/it, loss=0.0200, lr=2.17e-05, step=4345]Training:   2%|‚ñè         | 4346/200000 [1:32:59<70:01:22,  1.29s/it, loss=0.0200, lr=2.17e-05, step=4345]Training:   2%|‚ñè         | 4346/200000 [1:32:59<70:01:22,  1.29s/it, loss=0.0139, lr=2.17e-05, step=4346]Training:   2%|‚ñè         | 4347/200000 [1:33:00<66:36:39,  1.23s/it, loss=0.0139, lr=2.17e-05, step=4346]Training:   2%|‚ñè         | 4347/200000 [1:33:00<66:36:39,  1.23s/it, loss=0.0206, lr=2.17e-05, step=4347]Training:   2%|‚ñè         | 4348/200000 [1:33:01<68:42:05,  1.26s/it, loss=0.0206, lr=2.17e-05, step=4347]Training:   2%|‚ñè         | 4348/200000 [1:33:01<68:42:05,  1.26s/it, loss=0.0192, lr=2.17e-05, step=4348]Training:   2%|‚ñè         | 4349/200000 [1:33:02<65:39:59,  1.21s/it, loss=0.0192, lr=2.17e-05, step=4348]Training:   2%|‚ñè         | 4349/200000 [1:33:02<65:39:59,  1.21s/it, loss=0.0182, lr=2.17e-05, step=4349]Training:   2%|‚ñè         | 4350/200000 [1:33:04<63:32:00,  1.17s/it, loss=0.0182, lr=2.17e-05, step=4349]Training:   2%|‚ñè         | 4350/200000 [1:33:04<63:32:00,  1.17s/it, loss=0.0097, lr=2.17e-05, step=4350]Training:   2%|‚ñè         | 4351/200000 [1:33:05<67:38:24,  1.24s/it, loss=0.0097, lr=2.17e-05, step=4350]Training:   2%|‚ñè         | 4351/200000 [1:33:05<67:38:24,  1.24s/it, loss=0.0266, lr=2.18e-05, step=4351]Training:   2%|‚ñè         | 4352/200000 [1:33:06<71:18:40,  1.31s/it, loss=0.0266, lr=2.18e-05, step=4351]Training:   2%|‚ñè         | 4352/200000 [1:33:06<71:18:40,  1.31s/it, loss=0.0500, lr=2.18e-05, step=4352]Training:   2%|‚ñè         | 4353/200000 [1:33:08<73:03:34,  1.34s/it, loss=0.0500, lr=2.18e-05, step=4352]Training:   2%|‚ñè         | 4353/200000 [1:33:08<73:03:34,  1.34s/it, loss=0.0204, lr=2.18e-05, step=4353]Training:   2%|‚ñè         | 4354/200000 [1:33:09<68:41:25,  1.26s/it, loss=0.0204, lr=2.18e-05, step=4353]Training:   2%|‚ñè         | 4354/200000 [1:33:09<68:41:25,  1.26s/it, loss=0.0392, lr=2.18e-05, step=4354]Training:   2%|‚ñè         | 4355/200000 [1:33:10<65:38:45,  1.21s/it, loss=0.0392, lr=2.18e-05, step=4354]Training:   2%|‚ñè         | 4355/200000 [1:33:10<65:38:45,  1.21s/it, loss=0.0285, lr=2.18e-05, step=4355]Training:   2%|‚ñè         | 4356/200000 [1:33:11<67:51:50,  1.25s/it, loss=0.0285, lr=2.18e-05, step=4355]Training:   2%|‚ñè         | 4356/200000 [1:33:11<67:51:50,  1.25s/it, loss=0.0147, lr=2.18e-05, step=4356]Training:   2%|‚ñè         | 4357/200000 [1:33:13<69:35:12,  1.28s/it, loss=0.0147, lr=2.18e-05, step=4356]Training:   2%|‚ñè         | 4357/200000 [1:33:13<69:35:12,  1.28s/it, loss=0.0586, lr=2.18e-05, step=4357]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4358/200000 [1:33:14<66:14:09,  1.22s/it, loss=0.0586, lr=2.18e-05, step=4357]Training:   2%|‚ñè         | 4358/200000 [1:33:14<66:14:09,  1.22s/it, loss=0.0173, lr=2.18e-05, step=4358]Training:   2%|‚ñè         | 4359/200000 [1:33:15<68:09:56,  1.25s/it, loss=0.0173, lr=2.18e-05, step=4358]Training:   2%|‚ñè         | 4359/200000 [1:33:15<68:09:56,  1.25s/it, loss=0.0265, lr=2.18e-05, step=4359]Training:   2%|‚ñè         | 4360/200000 [1:33:16<65:15:59,  1.20s/it, loss=0.0265, lr=2.18e-05, step=4359]Training:   2%|‚ñè         | 4360/200000 [1:33:16<65:15:59,  1.20s/it, loss=0.0186, lr=2.18e-05, step=4360]Training:   2%|‚ñè         | 4361/200000 [1:33:18<68:02:36,  1.25s/it, loss=0.0186, lr=2.18e-05, step=4360]Training:   2%|‚ñè         | 4361/200000 [1:33:18<68:02:36,  1.25s/it, loss=0.0204, lr=2.18e-05, step=4361]Training:   2%|‚ñè         | 4362/200000 [1:33:19<71:25:59,  1.31s/it, loss=0.0204, lr=2.18e-05, step=4361]Training:   2%|‚ñè         | 4362/200000 [1:33:19<71:25:59,  1.31s/it, loss=0.0234, lr=2.18e-05, step=4362]Training:   2%|‚ñè         | 4363/200000 [1:33:20<73:00:41,  1.34s/it, loss=0.0234, lr=2.18e-05, step=4362]Training:   2%|‚ñè         | 4363/200000 [1:33:20<73:00:41,  1.34s/it, loss=0.0167, lr=2.18e-05, step=4363]Training:   2%|‚ñè         | 4364/200000 [1:33:22<74:45:02,  1.38s/it, loss=0.0167, lr=2.18e-05, step=4363]Training:   2%|‚ñè         | 4364/200000 [1:33:22<74:45:02,  1.38s/it, loss=0.0246, lr=2.18e-05, step=4364]Training:   2%|‚ñè         | 4365/200000 [1:33:23<69:53:58,  1.29s/it, loss=0.0246, lr=2.18e-05, step=4364]Training:   2%|‚ñè         | 4365/200000 [1:33:23<69:53:58,  1.29s/it, loss=0.0232, lr=2.18e-05, step=4365]Training:   2%|‚ñè         | 4366/200000 [1:33:24<66:32:24,  1.22s/it, loss=0.0232, lr=2.18e-05, step=4365]Training:   2%|‚ñè         | 4366/200000 [1:33:24<66:32:24,  1.22s/it, loss=0.0274, lr=2.18e-05, step=4366]Training:   2%|‚ñè         | 4367/200000 [1:33:25<68:19:53,  1.26s/it, loss=0.0274, lr=2.18e-05, step=4366]Training:   2%|‚ñè         | 4367/200000 [1:33:25<68:19:53,  1.26s/it, loss=0.0349, lr=2.18e-05, step=4367]Training:   2%|‚ñè         | 4368/200000 [1:33:27<70:48:26,  1.30s/it, loss=0.0349, lr=2.18e-05, step=4367]Training:   2%|‚ñè         | 4368/200000 [1:33:27<70:48:26,  1.30s/it, loss=0.0240, lr=2.18e-05, step=4368]Training:   2%|‚ñè         | 4369/200000 [1:33:28<67:05:19,  1.23s/it, loss=0.0240, lr=2.18e-05, step=4368]Training:   2%|‚ñè         | 4369/200000 [1:33:28<67:05:19,  1.23s/it, loss=0.0188, lr=2.18e-05, step=4369]Training:   2%|‚ñè         | 4370/200000 [1:33:29<70:46:28,  1.30s/it, loss=0.0188, lr=2.18e-05, step=4369]Training:   2%|‚ñè         | 4370/200000 [1:33:29<70:46:28,  1.30s/it, loss=0.0283, lr=2.18e-05, step=4370]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4371/200000 [1:33:30<67:04:47,  1.23s/it, loss=0.0283, lr=2.18e-05, step=4370]Training:   2%|‚ñè         | 4371/200000 [1:33:30<67:04:47,  1.23s/it, loss=0.0231, lr=2.19e-05, step=4371]Training:   2%|‚ñè         | 4372/200000 [1:33:32<69:21:21,  1.28s/it, loss=0.0231, lr=2.19e-05, step=4371]Training:   2%|‚ñè         | 4372/200000 [1:33:32<69:21:21,  1.28s/it, loss=0.0233, lr=2.19e-05, step=4372]Training:   2%|‚ñè         | 4373/200000 [1:33:33<66:07:51,  1.22s/it, loss=0.0233, lr=2.19e-05, step=4372]Training:   2%|‚ñè         | 4373/200000 [1:33:33<66:07:51,  1.22s/it, loss=0.0474, lr=2.19e-05, step=4373]Training:   2%|‚ñè         | 4374/200000 [1:33:34<70:41:22,  1.30s/it, loss=0.0474, lr=2.19e-05, step=4373]Training:   2%|‚ñè         | 4374/200000 [1:33:34<70:41:22,  1.30s/it, loss=0.0313, lr=2.19e-05, step=4374]Training:   2%|‚ñè         | 4375/200000 [1:33:36<74:18:17,  1.37s/it, loss=0.0313, lr=2.19e-05, step=4374]Training:   2%|‚ñè         | 4375/200000 [1:33:36<74:18:17,  1.37s/it, loss=0.0193, lr=2.19e-05, step=4375]Training:   2%|‚ñè         | 4376/200000 [1:33:37<69:34:08,  1.28s/it, loss=0.0193, lr=2.19e-05, step=4375]Training:   2%|‚ñè         | 4376/200000 [1:33:37<69:34:08,  1.28s/it, loss=0.0240, lr=2.19e-05, step=4376]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4377/200000 [1:33:38<66:15:53,  1.22s/it, loss=0.0240, lr=2.19e-05, step=4376]Training:   2%|‚ñè         | 4377/200000 [1:33:38<66:15:53,  1.22s/it, loss=0.0231, lr=2.19e-05, step=4377]Training:   2%|‚ñè         | 4378/200000 [1:33:39<70:29:40,  1.30s/it, loss=0.0231, lr=2.19e-05, step=4377]Training:   2%|‚ñè         | 4378/200000 [1:33:39<70:29:40,  1.30s/it, loss=0.0262, lr=2.19e-05, step=4378]Training:   2%|‚ñè         | 4379/200000 [1:33:41<66:53:37,  1.23s/it, loss=0.0262, lr=2.19e-05, step=4378]Training:   2%|‚ñè         | 4379/200000 [1:33:41<66:53:37,  1.23s/it, loss=0.0216, lr=2.19e-05, step=4379]Training:   2%|‚ñè         | 4380/200000 [1:33:42<68:36:53,  1.26s/it, loss=0.0216, lr=2.19e-05, step=4379]Training:   2%|‚ñè         | 4380/200000 [1:33:42<68:36:53,  1.26s/it, loss=0.0328, lr=2.19e-05, step=4380]Training:   2%|‚ñè         | 4381/200000 [1:33:43<65:34:17,  1.21s/it, loss=0.0328, lr=2.19e-05, step=4380]Training:   2%|‚ñè         | 4381/200000 [1:33:43<65:34:17,  1.21s/it, loss=0.0325, lr=2.19e-05, step=4381]Training:   2%|‚ñè         | 4382/200000 [1:33:44<63:27:04,  1.17s/it, loss=0.0325, lr=2.19e-05, step=4381]Training:   2%|‚ñè         | 4382/200000 [1:33:44<63:27:04,  1.17s/it, loss=0.0537, lr=2.19e-05, step=4382]Training:   2%|‚ñè         | 4383/200000 [1:33:46<68:28:48,  1.26s/it, loss=0.0537, lr=2.19e-05, step=4382]Training:   2%|‚ñè         | 4383/200000 [1:33:46<68:28:48,  1.26s/it, loss=0.0142, lr=2.19e-05, step=4383]Training:   2%|‚ñè         | 4384/200000 [1:33:47<71:16:02,  1.31s/it, loss=0.0142, lr=2.19e-05, step=4383]Training:   2%|‚ñè         | 4384/200000 [1:33:47<71:16:02,  1.31s/it, loss=0.0245, lr=2.19e-05, step=4384]Training:   2%|‚ñè         | 4385/200000 [1:33:48<73:20:32,  1.35s/it, loss=0.0245, lr=2.19e-05, step=4384]Training:   2%|‚ñè         | 4385/200000 [1:33:48<73:20:32,  1.35s/it, loss=0.0250, lr=2.19e-05, step=4385]Training:   2%|‚ñè         | 4386/200000 [1:33:49<68:52:58,  1.27s/it, loss=0.0250, lr=2.19e-05, step=4385]Training:   2%|‚ñè         | 4386/200000 [1:33:49<68:52:58,  1.27s/it, loss=0.0139, lr=2.19e-05, step=4386]Training:   2%|‚ñè         | 4387/200000 [1:33:51<65:51:05,  1.21s/it, loss=0.0139, lr=2.19e-05, step=4386]Training:   2%|‚ñè         | 4387/200000 [1:33:51<65:51:05,  1.21s/it, loss=0.0196, lr=2.19e-05, step=4387]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4388/200000 [1:33:52<68:16:09,  1.26s/it, loss=0.0196, lr=2.19e-05, step=4387]Training:   2%|‚ñè         | 4388/200000 [1:33:52<68:16:09,  1.26s/it, loss=0.0235, lr=2.19e-05, step=4388]Training:   2%|‚ñè         | 4389/200000 [1:33:53<70:42:24,  1.30s/it, loss=0.0235, lr=2.19e-05, step=4388]Training:   2%|‚ñè         | 4389/200000 [1:33:53<70:42:24,  1.30s/it, loss=0.0124, lr=2.19e-05, step=4389]Training:   2%|‚ñè         | 4390/200000 [1:33:54<67:04:38,  1.23s/it, loss=0.0124, lr=2.19e-05, step=4389]Training:   2%|‚ñè         | 4390/200000 [1:33:54<67:04:38,  1.23s/it, loss=0.0317, lr=2.19e-05, step=4390]Training:   2%|‚ñè         | 4391/200000 [1:33:56<70:21:38,  1.29s/it, loss=0.0317, lr=2.19e-05, step=4390]Training:   2%|‚ñè         | 4391/200000 [1:33:56<70:21:38,  1.29s/it, loss=0.0374, lr=2.20e-05, step=4391]Training:   2%|‚ñè         | 4392/200000 [1:33:57<66:47:33,  1.23s/it, loss=0.0374, lr=2.20e-05, step=4391]Training:   2%|‚ñè         | 4392/200000 [1:33:57<66:47:33,  1.23s/it, loss=0.0167, lr=2.20e-05, step=4392]Training:   2%|‚ñè         | 4393/200000 [1:33:58<69:40:09,  1.28s/it, loss=0.0167, lr=2.20e-05, step=4392]Training:   2%|‚ñè         | 4393/200000 [1:33:58<69:40:09,  1.28s/it, loss=0.0251, lr=2.20e-05, step=4393]Training:   2%|‚ñè         | 4394/200000 [1:33:59<66:18:33,  1.22s/it, loss=0.0251, lr=2.20e-05, step=4393]Training:   2%|‚ñè         | 4394/200000 [1:33:59<66:18:33,  1.22s/it, loss=0.0159, lr=2.20e-05, step=4394]Training:   2%|‚ñè         | 4395/200000 [1:34:01<70:26:40,  1.30s/it, loss=0.0159, lr=2.20e-05, step=4394]Training:   2%|‚ñè         | 4395/200000 [1:34:01<70:26:40,  1.30s/it, loss=0.0534, lr=2.20e-05, step=4395]Training:   2%|‚ñè         | 4396/200000 [1:34:02<73:50:29,  1.36s/it, loss=0.0534, lr=2.20e-05, step=4395]Training:   2%|‚ñè         | 4396/200000 [1:34:02<73:50:29,  1.36s/it, loss=0.0315, lr=2.20e-05, step=4396]Training:   2%|‚ñè         | 4397/200000 [1:34:03<69:14:46,  1.27s/it, loss=0.0315, lr=2.20e-05, step=4396]Training:   2%|‚ñè         | 4397/200000 [1:34:03<69:14:46,  1.27s/it, loss=0.0455, lr=2.20e-05, step=4397]Training:   2%|‚ñè         | 4398/200000 [1:34:05<66:05:26,  1.22s/it, loss=0.0455, lr=2.20e-05, step=4397]Training:   2%|‚ñè         | 4398/200000 [1:34:05<66:05:26,  1.22s/it, loss=0.0254, lr=2.20e-05, step=4398]Training:   2%|‚ñè         | 4399/200000 [1:34:06<69:53:06,  1.29s/it, loss=0.0254, lr=2.20e-05, step=4398]Training:   2%|‚ñè         | 4399/200000 [1:34:06<69:53:06,  1.29s/it, loss=0.0243, lr=2.20e-05, step=4399]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4400/200000 [1:34:07<66:28:47,  1.22s/it, loss=0.0243, lr=2.20e-05, step=4399]Training:   2%|‚ñè         | 4400/200000 [1:34:07<66:28:47,  1.22s/it, loss=0.0206, lr=2.20e-05, step=4400]00:27:22.227 [I] step=4400 loss=0.0260 lr=2.18e-05 grad_norm=0.56 time=126.6s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4401/200000 [1:34:08<68:40:07,  1.26s/it, loss=0.0206, lr=2.20e-05, step=4400]Training:   2%|‚ñè         | 4401/200000 [1:34:08<68:40:07,  1.26s/it, loss=0.0355, lr=2.20e-05, step=4401]Training:   2%|‚ñè         | 4402/200000 [1:34:09<65:38:41,  1.21s/it, loss=0.0355, lr=2.20e-05, step=4401]Training:   2%|‚ñè         | 4402/200000 [1:34:09<65:38:41,  1.21s/it, loss=0.0233, lr=2.20e-05, step=4402]Training:   2%|‚ñè         | 4403/200000 [1:34:11<63:31:06,  1.17s/it, loss=0.0233, lr=2.20e-05, step=4402]Training:   2%|‚ñè         | 4403/200000 [1:34:11<63:31:06,  1.17s/it, loss=0.0213, lr=2.20e-05, step=4403]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4404/200000 [1:34:12<67:38:08,  1.24s/it, loss=0.0213, lr=2.20e-05, step=4403]Training:   2%|‚ñè         | 4404/200000 [1:34:12<67:38:08,  1.24s/it, loss=0.0177, lr=2.20e-05, step=4404]Training:   2%|‚ñè         | 4405/200000 [1:34:13<70:19:08,  1.29s/it, loss=0.0177, lr=2.20e-05, step=4404]Training:   2%|‚ñè         | 4405/200000 [1:34:13<70:19:08,  1.29s/it, loss=0.0301, lr=2.20e-05, step=4405]Training:   2%|‚ñè         | 4406/200000 [1:34:15<72:39:02,  1.34s/it, loss=0.0301, lr=2.20e-05, step=4405]Training:   2%|‚ñè         | 4406/200000 [1:34:15<72:39:02,  1.34s/it, loss=0.0254, lr=2.20e-05, step=4406]Training:   2%|‚ñè         | 4407/200000 [1:34:16<68:25:23,  1.26s/it, loss=0.0254, lr=2.20e-05, step=4406]Training:   2%|‚ñè         | 4407/200000 [1:34:16<68:25:23,  1.26s/it, loss=0.0179, lr=2.20e-05, step=4407]Training:   2%|‚ñè         | 4408/200000 [1:34:17<65:26:30,  1.20s/it, loss=0.0179, lr=2.20e-05, step=4407]Training:   2%|‚ñè         | 4408/200000 [1:34:17<65:26:30,  1.20s/it, loss=0.0202, lr=2.20e-05, step=4408]Training:   2%|‚ñè         | 4409/200000 [1:34:18<67:36:32,  1.24s/it, loss=0.0202, lr=2.20e-05, step=4408]Training:   2%|‚ñè         | 4409/200000 [1:34:18<67:36:32,  1.24s/it, loss=0.0191, lr=2.20e-05, step=4409]Training:   2%|‚ñè         | 4410/200000 [1:34:20<68:33:40,  1.26s/it, loss=0.0191, lr=2.20e-05, step=4409]Training:   2%|‚ñè         | 4410/200000 [1:34:20<68:33:40,  1.26s/it, loss=0.0218, lr=2.20e-05, step=4410]Training:   2%|‚ñè         | 4411/200000 [1:34:21<65:34:14,  1.21s/it, loss=0.0218, lr=2.20e-05, step=4410]Training:   2%|‚ñè         | 4411/200000 [1:34:21<65:34:14,  1.21s/it, loss=0.0164, lr=2.21e-05, step=4411]Training:   2%|‚ñè         | 4412/200000 [1:34:22<68:21:26,  1.26s/it, loss=0.0164, lr=2.21e-05, step=4411]Training:   2%|‚ñè         | 4412/200000 [1:34:22<68:21:26,  1.26s/it, loss=0.0351, lr=2.21e-05, step=4412]Training:   2%|‚ñè         | 4413/200000 [1:34:23<65:24:50,  1.20s/it, loss=0.0351, lr=2.21e-05, step=4412]Training:   2%|‚ñè         | 4413/200000 [1:34:23<65:24:50,  1.20s/it, loss=0.0128, lr=2.21e-05, step=4413]Training:   2%|‚ñè         | 4414/200000 [1:34:25<68:19:55,  1.26s/it, loss=0.0128, lr=2.21e-05, step=4413]Training:   2%|‚ñè         | 4414/200000 [1:34:25<68:19:55,  1.26s/it, loss=0.0138, lr=2.21e-05, step=4414]Training:   2%|‚ñè         | 4415/200000 [1:34:26<71:38:45,  1.32s/it, loss=0.0138, lr=2.21e-05, step=4414]Training:   2%|‚ñè         | 4415/200000 [1:34:26<71:38:45,  1.32s/it, loss=0.0330, lr=2.21e-05, step=4415]Training:   2%|‚ñè         | 4416/200000 [1:34:27<74:06:11,  1.36s/it, loss=0.0330, lr=2.21e-05, step=4415]Training:   2%|‚ñè         | 4416/200000 [1:34:27<74:06:11,  1.36s/it, loss=0.0405, lr=2.21e-05, step=4416]Training:   2%|‚ñè         | 4417/200000 [1:34:29<74:37:53,  1.37s/it, loss=0.0405, lr=2.21e-05, step=4416]Training:   2%|‚ñè         | 4417/200000 [1:34:29<74:37:53,  1.37s/it, loss=0.0180, lr=2.21e-05, step=4417]Training:   2%|‚ñè         | 4418/200000 [1:34:30<69:48:06,  1.28s/it, loss=0.0180, lr=2.21e-05, step=4417]Training:   2%|‚ñè         | 4418/200000 [1:34:30<69:48:06,  1.28s/it, loss=0.0555, lr=2.21e-05, step=4418]Training:   2%|‚ñè         | 4419/200000 [1:34:31<66:26:17,  1.22s/it, loss=0.0555, lr=2.21e-05, step=4418]Training:   2%|‚ñè         | 4419/200000 [1:34:31<66:26:17,  1.22s/it, loss=0.0146, lr=2.21e-05, step=4419]Training:   2%|‚ñè         | 4420/200000 [1:34:32<69:22:57,  1.28s/it, loss=0.0146, lr=2.21e-05, step=4419]Training:   2%|‚ñè         | 4420/200000 [1:34:32<69:22:57,  1.28s/it, loss=0.0223, lr=2.21e-05, step=4420]Training:   2%|‚ñè         | 4421/200000 [1:34:34<71:46:55,  1.32s/it, loss=0.0223, lr=2.21e-05, step=4420]Training:   2%|‚ñè         | 4421/200000 [1:34:34<71:46:55,  1.32s/it, loss=0.0174, lr=2.21e-05, step=4421]Training:   2%|‚ñè         | 4422/200000 [1:34:35<67:47:31,  1.25s/it, loss=0.0174, lr=2.21e-05, step=4421]Training:   2%|‚ñè         | 4422/200000 [1:34:35<67:47:31,  1.25s/it, loss=0.0262, lr=2.21e-05, step=4422]Training:   2%|‚ñè         | 4423/200000 [1:34:36<71:30:14,  1.32s/it, loss=0.0262, lr=2.21e-05, step=4422]Training:   2%|‚ñè         | 4423/200000 [1:34:36<71:30:14,  1.32s/it, loss=0.0232, lr=2.21e-05, step=4423]Training:   2%|‚ñè         | 4424/200000 [1:34:37<67:36:47,  1.24s/it, loss=0.0232, lr=2.21e-05, step=4423]Training:   2%|‚ñè         | 4424/200000 [1:34:37<67:36:47,  1.24s/it, loss=0.0402, lr=2.21e-05, step=4424]Training:   2%|‚ñè         | 4425/200000 [1:34:39<71:11:21,  1.31s/it, loss=0.0402, lr=2.21e-05, step=4424]Training:   2%|‚ñè         | 4425/200000 [1:34:39<71:11:21,  1.31s/it, loss=0.0295, lr=2.21e-05, step=4425]Training:   2%|‚ñè         | 4426/200000 [1:34:40<67:22:50,  1.24s/it, loss=0.0295, lr=2.21e-05, step=4425]Training:   2%|‚ñè         | 4426/200000 [1:34:40<67:22:50,  1.24s/it, loss=0.0149, lr=2.21e-05, step=4426]Training:   2%|‚ñè         | 4427/200000 [1:34:42<71:34:15,  1.32s/it, loss=0.0149, lr=2.21e-05, step=4426]Training:   2%|‚ñè         | 4427/200000 [1:34:42<71:34:15,  1.32s/it, loss=0.0197, lr=2.21e-05, step=4427]Training:   2%|‚ñè         | 4428/200000 [1:34:43<74:59:24,  1.38s/it, loss=0.0197, lr=2.21e-05, step=4427]Training:   2%|‚ñè         | 4428/200000 [1:34:43<74:59:24,  1.38s/it, loss=0.0228, lr=2.21e-05, step=4428]Training:   2%|‚ñè         | 4429/200000 [1:34:44<70:01:04,  1.29s/it, loss=0.0228, lr=2.21e-05, step=4428]Training:   2%|‚ñè         | 4429/200000 [1:34:44<70:01:04,  1.29s/it, loss=0.0154, lr=2.21e-05, step=4429]Training:   2%|‚ñè         | 4430/200000 [1:34:45<66:35:20,  1.23s/it, loss=0.0154, lr=2.21e-05, step=4429]Training:   2%|‚ñè         | 4430/200000 [1:34:45<66:35:20,  1.23s/it, loss=0.0376, lr=2.21e-05, step=4430]Training:   2%|‚ñè         | 4431/200000 [1:34:47<70:43:12,  1.30s/it, loss=0.0376, lr=2.21e-05, step=4430]Training:   2%|‚ñè         | 4431/200000 [1:34:47<70:43:12,  1.30s/it, loss=0.0198, lr=2.22e-05, step=4431]Training:   2%|‚ñè         | 4432/200000 [1:34:48<67:02:14,  1.23s/it, loss=0.0198, lr=2.22e-05, step=4431]Training:   2%|‚ñè         | 4432/200000 [1:34:48<67:02:14,  1.23s/it, loss=0.0152, lr=2.22e-05, step=4432]Training:   2%|‚ñè         | 4433/200000 [1:34:49<68:38:44,  1.26s/it, loss=0.0152, lr=2.22e-05, step=4432]Training:   2%|‚ñè         | 4433/200000 [1:34:49<68:38:44,  1.26s/it, loss=0.0266, lr=2.22e-05, step=4433]Training:   2%|‚ñè         | 4434/200000 [1:34:50<65:33:04,  1.21s/it, loss=0.0266, lr=2.22e-05, step=4433]Training:   2%|‚ñè         | 4434/200000 [1:34:50<65:33:04,  1.21s/it, loss=0.0234, lr=2.22e-05, step=4434]Training:   2%|‚ñè         | 4435/200000 [1:34:52<68:14:27,  1.26s/it, loss=0.0234, lr=2.22e-05, step=4434]Training:   2%|‚ñè         | 4435/200000 [1:34:52<68:14:27,  1.26s/it, loss=0.0299, lr=2.22e-05, step=4435]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4436/200000 [1:34:53<71:10:48,  1.31s/it, loss=0.0299, lr=2.22e-05, step=4435]Training:   2%|‚ñè         | 4436/200000 [1:34:53<71:10:48,  1.31s/it, loss=0.0184, lr=2.22e-05, step=4436]Training:   2%|‚ñè         | 4437/200000 [1:34:54<72:35:14,  1.34s/it, loss=0.0184, lr=2.22e-05, step=4436]Training:   2%|‚ñè         | 4437/200000 [1:34:54<72:35:14,  1.34s/it, loss=0.0467, lr=2.22e-05, step=4437]Training:   2%|‚ñè         | 4438/200000 [1:34:56<74:20:45,  1.37s/it, loss=0.0467, lr=2.22e-05, step=4437]Training:   2%|‚ñè         | 4438/200000 [1:34:56<74:20:45,  1.37s/it, loss=0.0263, lr=2.22e-05, step=4438]Training:   2%|‚ñè         | 4439/200000 [1:34:57<69:35:33,  1.28s/it, loss=0.0263, lr=2.22e-05, step=4438]Training:   2%|‚ñè         | 4439/200000 [1:34:57<69:35:33,  1.28s/it, loss=0.0152, lr=2.22e-05, step=4439]Training:   2%|‚ñè         | 4440/200000 [1:34:58<66:15:21,  1.22s/it, loss=0.0152, lr=2.22e-05, step=4439]Training:   2%|‚ñè         | 4440/200000 [1:34:58<66:15:21,  1.22s/it, loss=0.0234, lr=2.22e-05, step=4440]Training:   2%|‚ñè         | 4441/200000 [1:34:59<68:29:16,  1.26s/it, loss=0.0234, lr=2.22e-05, step=4440]Training:   2%|‚ñè         | 4441/200000 [1:34:59<68:29:16,  1.26s/it, loss=0.0171, lr=2.22e-05, step=4441]Training:   2%|‚ñè         | 4442/200000 [1:35:01<70:36:13,  1.30s/it, loss=0.0171, lr=2.22e-05, step=4441]Training:   2%|‚ñè         | 4442/200000 [1:35:01<70:36:13,  1.30s/it, loss=0.0150, lr=2.22e-05, step=4442]Training:   2%|‚ñè         | 4443/200000 [1:35:02<66:56:53,  1.23s/it, loss=0.0150, lr=2.22e-05, step=4442]Training:   2%|‚ñè         | 4443/200000 [1:35:02<66:56:53,  1.23s/it, loss=0.0159, lr=2.22e-05, step=4443]Training:   2%|‚ñè         | 4444/200000 [1:35:03<69:28:36,  1.28s/it, loss=0.0159, lr=2.22e-05, step=4443]Training:   2%|‚ñè         | 4444/200000 [1:35:03<69:28:36,  1.28s/it, loss=0.0188, lr=2.22e-05, step=4444]Training:   2%|‚ñè         | 4445/200000 [1:35:04<66:12:15,  1.22s/it, loss=0.0188, lr=2.22e-05, step=4444]Training:   2%|‚ñè         | 4445/200000 [1:35:04<66:12:15,  1.22s/it, loss=0.0199, lr=2.22e-05, step=4445]Training:   2%|‚ñè         | 4446/200000 [1:35:06<69:21:53,  1.28s/it, loss=0.0199, lr=2.22e-05, step=4445]Training:   2%|‚ñè         | 4446/200000 [1:35:06<69:21:53,  1.28s/it, loss=0.0246, lr=2.22e-05, step=4446]Training:   2%|‚ñè         | 4447/200000 [1:35:07<66:06:03,  1.22s/it, loss=0.0246, lr=2.22e-05, step=4446]Training:   2%|‚ñè         | 4447/200000 [1:35:07<66:06:03,  1.22s/it, loss=0.0414, lr=2.22e-05, step=4447]Training:   2%|‚ñè         | 4448/200000 [1:35:08<70:22:49,  1.30s/it, loss=0.0414, lr=2.22e-05, step=4447]Training:   2%|‚ñè         | 4448/200000 [1:35:08<70:22:49,  1.30s/it, loss=0.0236, lr=2.22e-05, step=4448]Training:   2%|‚ñè         | 4449/200000 [1:35:10<73:48:49,  1.36s/it, loss=0.0236, lr=2.22e-05, step=4448]Training:   2%|‚ñè         | 4449/200000 [1:35:10<73:48:49,  1.36s/it, loss=0.0203, lr=2.22e-05, step=4449]Training:   2%|‚ñè         | 4450/200000 [1:35:11<69:11:58,  1.27s/it, loss=0.0203, lr=2.22e-05, step=4449]Training:   2%|‚ñè         | 4450/200000 [1:35:11<69:11:58,  1.27s/it, loss=0.0275, lr=2.22e-05, step=4450]Training:   2%|‚ñè         | 4451/200000 [1:35:12<66:00:18,  1.22s/it, loss=0.0275, lr=2.22e-05, step=4450]Training:   2%|‚ñè         | 4451/200000 [1:35:12<66:00:18,  1.22s/it, loss=0.0197, lr=2.23e-05, step=4451]Training:   2%|‚ñè         | 4452/200000 [1:35:13<69:17:08,  1.28s/it, loss=0.0197, lr=2.23e-05, step=4451]Training:   2%|‚ñè         | 4452/200000 [1:35:13<69:17:08,  1.28s/it, loss=0.0362, lr=2.23e-05, step=4452]Training:   2%|‚ñè         | 4453/200000 [1:35:14<66:03:42,  1.22s/it, loss=0.0362, lr=2.23e-05, step=4452]Training:   2%|‚ñè         | 4453/200000 [1:35:14<66:03:42,  1.22s/it, loss=0.0262, lr=2.23e-05, step=4453]Training:   2%|‚ñè         | 4454/200000 [1:35:16<67:52:36,  1.25s/it, loss=0.0262, lr=2.23e-05, step=4453]Training:   2%|‚ñè         | 4454/200000 [1:35:16<67:52:36,  1.25s/it, loss=0.0183, lr=2.23e-05, step=4454]Training:   2%|‚ñè         | 4455/200000 [1:35:17<65:03:09,  1.20s/it, loss=0.0183, lr=2.23e-05, step=4454]Training:   2%|‚ñè         | 4455/200000 [1:35:17<65:03:09,  1.20s/it, loss=0.0169, lr=2.23e-05, step=4455]Training:   2%|‚ñè         | 4456/200000 [1:35:18<63:06:38,  1.16s/it, loss=0.0169, lr=2.23e-05, step=4455]Training:   2%|‚ñè         | 4456/200000 [1:35:18<63:06:38,  1.16s/it, loss=0.0297, lr=2.23e-05, step=4456]Training:   2%|‚ñè         | 4457/200000 [1:35:19<68:02:56,  1.25s/it, loss=0.0297, lr=2.23e-05, step=4456]Training:   2%|‚ñè         | 4457/200000 [1:35:19<68:02:56,  1.25s/it, loss=0.0305, lr=2.23e-05, step=4457]Training:   2%|‚ñè         | 4458/200000 [1:35:21<71:05:55,  1.31s/it, loss=0.0305, lr=2.23e-05, step=4457]Training:   2%|‚ñè         | 4458/200000 [1:35:21<71:05:55,  1.31s/it, loss=0.0183, lr=2.23e-05, step=4458]Training:   2%|‚ñè         | 4459/200000 [1:35:22<72:58:02,  1.34s/it, loss=0.0183, lr=2.23e-05, step=4458]Training:   2%|‚ñè         | 4459/200000 [1:35:22<72:58:02,  1.34s/it, loss=0.0209, lr=2.23e-05, step=4459]Training:   2%|‚ñè         | 4460/200000 [1:35:23<68:39:06,  1.26s/it, loss=0.0209, lr=2.23e-05, step=4459]Training:   2%|‚ñè         | 4460/200000 [1:35:23<68:39:06,  1.26s/it, loss=0.0300, lr=2.23e-05, step=4460]Training:   2%|‚ñè         | 4461/200000 [1:35:24<65:38:14,  1.21s/it, loss=0.0300, lr=2.23e-05, step=4460]Training:   2%|‚ñè         | 4461/200000 [1:35:24<65:38:14,  1.21s/it, loss=0.0256, lr=2.23e-05, step=4461]Training:   2%|‚ñè         | 4462/200000 [1:35:26<67:53:19,  1.25s/it, loss=0.0256, lr=2.23e-05, step=4461]Training:   2%|‚ñè         | 4462/200000 [1:35:26<67:53:19,  1.25s/it, loss=0.0184, lr=2.23e-05, step=4462]Training:   2%|‚ñè         | 4463/200000 [1:35:27<69:08:23,  1.27s/it, loss=0.0184, lr=2.23e-05, step=4462]Training:   2%|‚ñè         | 4463/200000 [1:35:27<69:08:23,  1.27s/it, loss=0.0158, lr=2.23e-05, step=4463]Training:   2%|‚ñè         | 4464/200000 [1:35:28<65:57:23,  1.21s/it, loss=0.0158, lr=2.23e-05, step=4463]Training:   2%|‚ñè         | 4464/200000 [1:35:28<65:57:23,  1.21s/it, loss=0.0467, lr=2.23e-05, step=4464]Training:   2%|‚ñè         | 4465/200000 [1:35:30<68:50:18,  1.27s/it, loss=0.0467, lr=2.23e-05, step=4464]Training:   2%|‚ñè         | 4465/200000 [1:35:30<68:50:18,  1.27s/it, loss=0.0230, lr=2.23e-05, step=4465]Training:   2%|‚ñè         | 4466/200000 [1:35:31<65:43:34,  1.21s/it, loss=0.0230, lr=2.23e-05, step=4465]Training:   2%|‚ñè         | 4466/200000 [1:35:31<65:43:34,  1.21s/it, loss=0.0268, lr=2.23e-05, step=4466]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4467/200000 [1:35:32<68:15:36,  1.26s/it, loss=0.0268, lr=2.23e-05, step=4466]Training:   2%|‚ñè         | 4467/200000 [1:35:32<68:15:36,  1.26s/it, loss=0.0613, lr=2.23e-05, step=4467]Training:   2%|‚ñè         | 4468/200000 [1:35:33<65:21:13,  1.20s/it, loss=0.0613, lr=2.23e-05, step=4467]Training:   2%|‚ñè         | 4468/200000 [1:35:33<65:21:13,  1.20s/it, loss=0.0230, lr=2.23e-05, step=4468]Training:   2%|‚ñè         | 4469/200000 [1:35:34<68:48:17,  1.27s/it, loss=0.0230, lr=2.23e-05, step=4468]Training:   2%|‚ñè         | 4469/200000 [1:35:34<68:48:17,  1.27s/it, loss=0.0154, lr=2.23e-05, step=4469]Training:   2%|‚ñè         | 4470/200000 [1:35:36<71:43:41,  1.32s/it, loss=0.0154, lr=2.23e-05, step=4469]Training:   2%|‚ñè         | 4470/200000 [1:35:36<71:43:41,  1.32s/it, loss=0.0170, lr=2.23e-05, step=4470]Training:   2%|‚ñè         | 4471/200000 [1:35:37<67:43:46,  1.25s/it, loss=0.0170, lr=2.23e-05, step=4470]Training:   2%|‚ñè         | 4471/200000 [1:35:37<67:43:46,  1.25s/it, loss=0.0213, lr=2.24e-05, step=4471]Training:   2%|‚ñè         | 4472/200000 [1:35:38<64:58:12,  1.20s/it, loss=0.0213, lr=2.24e-05, step=4471]Training:   2%|‚ñè         | 4472/200000 [1:35:38<64:58:12,  1.20s/it, loss=0.0207, lr=2.24e-05, step=4472]Training:   2%|‚ñè         | 4473/200000 [1:35:39<68:14:56,  1.26s/it, loss=0.0207, lr=2.24e-05, step=4472]Training:   2%|‚ñè         | 4473/200000 [1:35:39<68:14:56,  1.26s/it, loss=0.0215, lr=2.24e-05, step=4473]Training:   2%|‚ñè         | 4474/200000 [1:35:41<70:33:34,  1.30s/it, loss=0.0215, lr=2.24e-05, step=4473]Training:   2%|‚ñè         | 4474/200000 [1:35:41<70:33:34,  1.30s/it, loss=0.0187, lr=2.24e-05, step=4474]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4475/200000 [1:35:42<66:57:13,  1.23s/it, loss=0.0187, lr=2.24e-05, step=4474]Training:   2%|‚ñè         | 4475/200000 [1:35:42<66:57:13,  1.23s/it, loss=0.0149, lr=2.24e-05, step=4475]Training:   2%|‚ñè         | 4476/200000 [1:35:43<70:34:55,  1.30s/it, loss=0.0149, lr=2.24e-05, step=4475]Training:   2%|‚ñè         | 4476/200000 [1:35:43<70:34:55,  1.30s/it, loss=0.0107, lr=2.24e-05, step=4476]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4477/200000 [1:35:44<66:57:33,  1.23s/it, loss=0.0107, lr=2.24e-05, step=4476]Training:   2%|‚ñè         | 4477/200000 [1:35:44<66:57:33,  1.23s/it, loss=0.0327, lr=2.24e-05, step=4477]Training:   2%|‚ñè         | 4478/200000 [1:35:46<70:43:21,  1.30s/it, loss=0.0327, lr=2.24e-05, step=4477]Training:   2%|‚ñè         | 4478/200000 [1:35:46<70:43:21,  1.30s/it, loss=0.0443, lr=2.24e-05, step=4478]Training:   2%|‚ñè         | 4479/200000 [1:35:47<67:02:39,  1.23s/it, loss=0.0443, lr=2.24e-05, step=4478]Training:   2%|‚ñè         | 4479/200000 [1:35:47<67:02:39,  1.23s/it, loss=0.0243, lr=2.24e-05, step=4479]Training:   2%|‚ñè         | 4480/200000 [1:35:48<71:25:05,  1.31s/it, loss=0.0243, lr=2.24e-05, step=4479]Training:   2%|‚ñè         | 4480/200000 [1:35:48<71:25:05,  1.31s/it, loss=0.0276, lr=2.24e-05, step=4480]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4481/200000 [1:35:50<74:41:23,  1.38s/it, loss=0.0276, lr=2.24e-05, step=4480]Training:   2%|‚ñè         | 4481/200000 [1:35:50<74:41:23,  1.38s/it, loss=0.0149, lr=2.24e-05, step=4481]Training:   2%|‚ñè         | 4482/200000 [1:35:51<69:49:51,  1.29s/it, loss=0.0149, lr=2.24e-05, step=4481]Training:   2%|‚ñè         | 4482/200000 [1:35:51<69:49:51,  1.29s/it, loss=0.0166, lr=2.24e-05, step=4482]Training:   2%|‚ñè         | 4483/200000 [1:35:52<66:26:40,  1.22s/it, loss=0.0166, lr=2.24e-05, step=4482]Training:   2%|‚ñè         | 4483/200000 [1:35:52<66:26:40,  1.22s/it, loss=0.0205, lr=2.24e-05, step=4483]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4484/200000 [1:35:54<70:40:53,  1.30s/it, loss=0.0205, lr=2.24e-05, step=4483]Training:   2%|‚ñè         | 4484/200000 [1:35:54<70:40:53,  1.30s/it, loss=0.0174, lr=2.24e-05, step=4484]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4485/200000 [1:35:55<67:00:30,  1.23s/it, loss=0.0174, lr=2.24e-05, step=4484]Training:   2%|‚ñè         | 4485/200000 [1:35:55<67:00:30,  1.23s/it, loss=0.0224, lr=2.24e-05, step=4485]Training:   2%|‚ñè         | 4486/200000 [1:35:56<68:57:42,  1.27s/it, loss=0.0224, lr=2.24e-05, step=4485]Training:   2%|‚ñè         | 4486/200000 [1:35:56<68:57:42,  1.27s/it, loss=0.0209, lr=2.24e-05, step=4486]Training:   2%|‚ñè         | 4487/200000 [1:35:57<65:48:04,  1.21s/it, loss=0.0209, lr=2.24e-05, step=4486]Training:   2%|‚ñè         | 4487/200000 [1:35:57<65:48:04,  1.21s/it, loss=0.0190, lr=2.24e-05, step=4487]Training:   2%|‚ñè         | 4488/200000 [1:35:59<69:33:09,  1.28s/it, loss=0.0190, lr=2.24e-05, step=4487]Training:   2%|‚ñè         | 4488/200000 [1:35:59<69:33:09,  1.28s/it, loss=0.0179, lr=2.24e-05, step=4488]Training:   2%|‚ñè         | 4489/200000 [1:36:00<72:06:44,  1.33s/it, loss=0.0179, lr=2.24e-05, step=4488]Training:   2%|‚ñè         | 4489/200000 [1:36:00<72:06:44,  1.33s/it, loss=0.0193, lr=2.24e-05, step=4489]Training:   2%|‚ñè         | 4490/200000 [1:36:01<73:24:55,  1.35s/it, loss=0.0193, lr=2.24e-05, step=4489]Training:   2%|‚ñè         | 4490/200000 [1:36:01<73:24:55,  1.35s/it, loss=0.0324, lr=2.24e-05, step=4490]Training:   2%|‚ñè         | 4491/200000 [1:36:03<74:54:01,  1.38s/it, loss=0.0324, lr=2.24e-05, step=4490]Training:   2%|‚ñè         | 4491/200000 [1:36:03<74:54:01,  1.38s/it, loss=0.0239, lr=2.25e-05, step=4491]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4492/200000 [1:36:04<69:59:26,  1.29s/it, loss=0.0239, lr=2.25e-05, step=4491]Training:   2%|‚ñè         | 4492/200000 [1:36:04<69:59:26,  1.29s/it, loss=0.0127, lr=2.25e-05, step=4492]Training:   2%|‚ñè         | 4493/200000 [1:36:05<66:34:11,  1.23s/it, loss=0.0127, lr=2.25e-05, step=4492]Training:   2%|‚ñè         | 4493/200000 [1:36:05<66:34:11,  1.23s/it, loss=0.0261, lr=2.25e-05, step=4493]Training:   2%|‚ñè         | 4494/200000 [1:36:06<68:21:40,  1.26s/it, loss=0.0261, lr=2.25e-05, step=4493]Training:   2%|‚ñè         | 4494/200000 [1:36:06<68:21:40,  1.26s/it, loss=0.0156, lr=2.25e-05, step=4494]Training:   2%|‚ñè         | 4495/200000 [1:36:08<70:43:31,  1.30s/it, loss=0.0156, lr=2.25e-05, step=4494]Training:   2%|‚ñè         | 4495/200000 [1:36:08<70:43:31,  1.30s/it, loss=0.0238, lr=2.25e-05, step=4495]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4496/200000 [1:36:09<67:04:17,  1.24s/it, loss=0.0238, lr=2.25e-05, step=4495]Training:   2%|‚ñè         | 4496/200000 [1:36:09<67:04:17,  1.24s/it, loss=0.0216, lr=2.25e-05, step=4496]Training:   2%|‚ñè         | 4497/200000 [1:36:10<70:43:54,  1.30s/it, loss=0.0216, lr=2.25e-05, step=4496]Training:   2%|‚ñè         | 4497/200000 [1:36:10<70:43:54,  1.30s/it, loss=0.0187, lr=2.25e-05, step=4497]Training:   2%|‚ñè         | 4498/200000 [1:36:12<71:37:04,  1.32s/it, loss=0.0187, lr=2.25e-05, step=4497]Training:   2%|‚ñè         | 4498/200000 [1:36:12<71:37:04,  1.32s/it, loss=0.0153, lr=2.25e-05, step=4498]Training:   2%|‚ñè         | 4499/200000 [1:36:13<72:15:09,  1.33s/it, loss=0.0153, lr=2.25e-05, step=4498]Training:   2%|‚ñè         | 4499/200000 [1:36:13<72:15:09,  1.33s/it, loss=0.0327, lr=2.25e-05, step=4499]Training:   2%|‚ñè         | 4500/200000 [1:36:14<68:08:35,  1.25s/it, loss=0.0327, lr=2.25e-05, step=4499]Training:   2%|‚ñè         | 4500/200000 [1:36:14<68:08:35,  1.25s/it, loss=0.0177, lr=2.25e-05, step=4500]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
00:29:29.395 [I] step=4500 loss=0.0236 lr=2.23e-05 grad_norm=0.52 time=127.2s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4501/200000 [1:36:16<71:53:10,  1.32s/it, loss=0.0177, lr=2.25e-05, step=4500]Training:   2%|‚ñè         | 4501/200000 [1:36:16<71:53:10,  1.32s/it, loss=0.0173, lr=2.25e-05, step=4501]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4502/200000 [1:36:17<74:50:09,  1.38s/it, loss=0.0173, lr=2.25e-05, step=4501]Training:   2%|‚ñè         | 4502/200000 [1:36:17<74:50:09,  1.38s/it, loss=0.0356, lr=2.25e-05, step=4502]Training:   2%|‚ñè         | 4503/200000 [1:36:18<69:56:45,  1.29s/it, loss=0.0356, lr=2.25e-05, step=4502]Training:   2%|‚ñè         | 4503/200000 [1:36:18<69:56:45,  1.29s/it, loss=0.0388, lr=2.25e-05, step=4503]Training:   2%|‚ñè         | 4504/200000 [1:36:19<66:31:34,  1.23s/it, loss=0.0388, lr=2.25e-05, step=4503]Training:   2%|‚ñè         | 4504/200000 [1:36:19<66:31:34,  1.23s/it, loss=0.0126, lr=2.25e-05, step=4504]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4505/200000 [1:36:21<70:23:31,  1.30s/it, loss=0.0126, lr=2.25e-05, step=4504]Training:   2%|‚ñè         | 4505/200000 [1:36:21<70:23:31,  1.30s/it, loss=0.0123, lr=2.25e-05, step=4505]Training:   2%|‚ñè         | 4506/200000 [1:36:22<66:48:29,  1.23s/it, loss=0.0123, lr=2.25e-05, step=4505]Training:   2%|‚ñè         | 4506/200000 [1:36:22<66:48:29,  1.23s/it, loss=0.0270, lr=2.25e-05, step=4506]Training:   2%|‚ñè         | 4507/200000 [1:36:23<68:47:23,  1.27s/it, loss=0.0270, lr=2.25e-05, step=4506]Training:   2%|‚ñè         | 4507/200000 [1:36:23<68:47:23,  1.27s/it, loss=0.0339, lr=2.25e-05, step=4507]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4508/200000 [1:36:24<65:42:15,  1.21s/it, loss=0.0339, lr=2.25e-05, step=4507]Training:   2%|‚ñè         | 4508/200000 [1:36:24<65:42:15,  1.21s/it, loss=0.0204, lr=2.25e-05, step=4508]Training:   2%|‚ñè         | 4509/200000 [1:36:26<68:25:31,  1.26s/it, loss=0.0204, lr=2.25e-05, step=4508]Training:   2%|‚ñè         | 4509/200000 [1:36:26<68:25:31,  1.26s/it, loss=0.0698, lr=2.25e-05, step=4509]Training:   2%|‚ñè         | 4510/200000 [1:36:27<70:56:21,  1.31s/it, loss=0.0698, lr=2.25e-05, step=4509]Training:   2%|‚ñè         | 4510/200000 [1:36:27<70:56:21,  1.31s/it, loss=0.0141, lr=2.25e-05, step=4510]Training:   2%|‚ñè         | 4511/200000 [1:36:28<73:25:15,  1.35s/it, loss=0.0141, lr=2.25e-05, step=4510]Training:   2%|‚ñè         | 4511/200000 [1:36:28<73:25:15,  1.35s/it, loss=0.0158, lr=2.26e-05, step=4511]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4512/200000 [1:36:30<74:47:19,  1.38s/it, loss=0.0158, lr=2.26e-05, step=4511]Training:   2%|‚ñè         | 4512/200000 [1:36:30<74:47:19,  1.38s/it, loss=0.0163, lr=2.26e-05, step=4512]Training:   2%|‚ñè         | 4513/200000 [1:36:31<69:53:42,  1.29s/it, loss=0.0163, lr=2.26e-05, step=4512]Training:   2%|‚ñè         | 4513/200000 [1:36:31<69:53:42,  1.29s/it, loss=0.0219, lr=2.26e-05, step=4513]Training:   2%|‚ñè         | 4514/200000 [1:36:32<66:29:19,  1.22s/it, loss=0.0219, lr=2.26e-05, step=4513]Training:   2%|‚ñè         | 4514/200000 [1:36:32<66:29:19,  1.22s/it, loss=0.0152, lr=2.26e-05, step=4514]Training:   2%|‚ñè         | 4515/200000 [1:36:33<68:06:05,  1.25s/it, loss=0.0152, lr=2.26e-05, step=4514]Training:   2%|‚ñè         | 4515/200000 [1:36:33<68:06:05,  1.25s/it, loss=0.0136, lr=2.26e-05, step=4515]Training:   2%|‚ñè         | 4516/200000 [1:36:35<69:16:31,  1.28s/it, loss=0.0136, lr=2.26e-05, step=4515]Training:   2%|‚ñè         | 4516/200000 [1:36:35<69:16:31,  1.28s/it, loss=0.1881, lr=2.26e-05, step=4516]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4517/200000 [1:36:36<66:03:22,  1.22s/it, loss=0.1881, lr=2.26e-05, step=4516]Training:   2%|‚ñè         | 4517/200000 [1:36:36<66:03:22,  1.22s/it, loss=0.0151, lr=2.26e-05, step=4517]Training:   2%|‚ñè         | 4518/200000 [1:36:37<68:02:55,  1.25s/it, loss=0.0151, lr=2.26e-05, step=4517]Training:   2%|‚ñè         | 4518/200000 [1:36:37<68:02:55,  1.25s/it, loss=0.0177, lr=2.26e-05, step=4518]Training:   2%|‚ñè         | 4519/200000 [1:36:38<65:08:10,  1.20s/it, loss=0.0177, lr=2.26e-05, step=4518]Training:   2%|‚ñè         | 4519/200000 [1:36:38<65:08:10,  1.20s/it, loss=0.0259, lr=2.26e-05, step=4519]Training:   2%|‚ñè         | 4520/200000 [1:36:40<69:24:31,  1.28s/it, loss=0.0259, lr=2.26e-05, step=4519]Training:   2%|‚ñè         | 4520/200000 [1:36:40<69:24:31,  1.28s/it, loss=0.0200, lr=2.26e-05, step=4520]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4521/200000 [1:36:41<66:09:37,  1.22s/it, loss=0.0200, lr=2.26e-05, step=4520]Training:   2%|‚ñè         | 4521/200000 [1:36:41<66:09:37,  1.22s/it, loss=0.0259, lr=2.26e-05, step=4521]Training:   2%|‚ñè         | 4522/200000 [1:36:42<69:03:04,  1.27s/it, loss=0.0259, lr=2.26e-05, step=4521]Training:   2%|‚ñè         | 4522/200000 [1:36:42<69:03:04,  1.27s/it, loss=0.0198, lr=2.26e-05, step=4522]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4523/200000 [1:36:44<71:35:10,  1.32s/it, loss=0.0198, lr=2.26e-05, step=4522]Training:   2%|‚ñè         | 4523/200000 [1:36:44<71:35:10,  1.32s/it, loss=0.0286, lr=2.26e-05, step=4523]Training:   2%|‚ñè         | 4524/200000 [1:36:45<67:36:38,  1.25s/it, loss=0.0286, lr=2.26e-05, step=4523]Training:   2%|‚ñè         | 4524/200000 [1:36:45<67:36:38,  1.25s/it, loss=0.0184, lr=2.26e-05, step=4524]Training:   2%|‚ñè         | 4525/200000 [1:36:46<64:50:00,  1.19s/it, loss=0.0184, lr=2.26e-05, step=4524]Training:   2%|‚ñè         | 4525/200000 [1:36:46<64:50:00,  1.19s/it, loss=0.0233, lr=2.26e-05, step=4525]Training:   2%|‚ñè         | 4526/200000 [1:36:47<68:15:33,  1.26s/it, loss=0.0233, lr=2.26e-05, step=4525]Training:   2%|‚ñè         | 4526/200000 [1:36:47<68:15:33,  1.26s/it, loss=0.0180, lr=2.26e-05, step=4526]Training:   2%|‚ñè         | 4527/200000 [1:36:49<71:15:03,  1.31s/it, loss=0.0180, lr=2.26e-05, step=4526]Training:   2%|‚ñè         | 4527/200000 [1:36:49<71:15:03,  1.31s/it, loss=0.0250, lr=2.26e-05, step=4527]Training:   2%|‚ñè         | 4528/200000 [1:36:50<67:24:08,  1.24s/it, loss=0.0250, lr=2.26e-05, step=4527]Training:   2%|‚ñè         | 4528/200000 [1:36:50<67:24:08,  1.24s/it, loss=0.0327, lr=2.26e-05, step=4528]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4529/200000 [1:36:51<71:24:18,  1.32s/it, loss=0.0327, lr=2.26e-05, step=4528]Training:   2%|‚ñè         | 4529/200000 [1:36:51<71:24:18,  1.32s/it, loss=0.0409, lr=2.26e-05, step=4529]Training:   2%|‚ñè         | 4530/200000 [1:36:52<72:09:59,  1.33s/it, loss=0.0409, lr=2.26e-05, step=4529]Training:   2%|‚ñè         | 4530/200000 [1:36:52<72:09:59,  1.33s/it, loss=0.0214, lr=2.26e-05, step=4530]Training:   2%|‚ñè         | 4531/200000 [1:36:54<72:31:28,  1.34s/it, loss=0.0214, lr=2.26e-05, step=4530]Training:   2%|‚ñè         | 4531/200000 [1:36:54<72:31:28,  1.34s/it, loss=0.0194, lr=2.27e-05, step=4531]Training:   2%|‚ñè         | 4532/200000 [1:36:55<68:18:16,  1.26s/it, loss=0.0194, lr=2.27e-05, step=4531]Training:   2%|‚ñè         | 4532/200000 [1:36:55<68:18:16,  1.26s/it, loss=0.0322, lr=2.27e-05, step=4532]Training:   2%|‚ñè         | 4533/200000 [1:36:56<72:49:54,  1.34s/it, loss=0.0322, lr=2.27e-05, step=4532]Training:   2%|‚ñè         | 4533/200000 [1:36:56<72:49:54,  1.34s/it, loss=0.0179, lr=2.27e-05, step=4533]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4534/200000 [1:36:58<75:13:59,  1.39s/it, loss=0.0179, lr=2.27e-05, step=4533]Training:   2%|‚ñè         | 4534/200000 [1:36:58<75:13:59,  1.39s/it, loss=0.0181, lr=2.27e-05, step=4534]Training:   2%|‚ñè         | 4535/200000 [1:36:59<70:12:08,  1.29s/it, loss=0.0181, lr=2.27e-05, step=4534]Training:   2%|‚ñè         | 4535/200000 [1:36:59<70:12:08,  1.29s/it, loss=0.0292, lr=2.27e-05, step=4535]Training:   2%|‚ñè         | 4536/200000 [1:37:00<66:40:47,  1.23s/it, loss=0.0292, lr=2.27e-05, step=4535]Training:   2%|‚ñè         | 4536/200000 [1:37:00<66:40:47,  1.23s/it, loss=0.0229, lr=2.27e-05, step=4536]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4537/200000 [1:37:02<70:48:07,  1.30s/it, loss=0.0229, lr=2.27e-05, step=4536]Training:   2%|‚ñè         | 4537/200000 [1:37:02<70:48:07,  1.30s/it, loss=0.0147, lr=2.27e-05, step=4537]Training:   2%|‚ñè         | 4538/200000 [1:37:03<67:02:18,  1.23s/it, loss=0.0147, lr=2.27e-05, step=4537]Training:   2%|‚ñè         | 4538/200000 [1:37:03<67:02:18,  1.23s/it, loss=0.0320, lr=2.27e-05, step=4538]Training:   2%|‚ñè         | 4539/200000 [1:37:04<69:08:10,  1.27s/it, loss=0.0320, lr=2.27e-05, step=4538]Training:   2%|‚ñè         | 4539/200000 [1:37:04<69:08:10,  1.27s/it, loss=0.0302, lr=2.27e-05, step=4539]Training:   2%|‚ñè         | 4540/200000 [1:37:05<65:56:39,  1.21s/it, loss=0.0302, lr=2.27e-05, step=4539]Training:   2%|‚ñè         | 4540/200000 [1:37:05<65:56:39,  1.21s/it, loss=0.0212, lr=2.27e-05, step=4540]Training:   2%|‚ñè         | 4541/200000 [1:37:07<69:36:25,  1.28s/it, loss=0.0212, lr=2.27e-05, step=4540]Training:   2%|‚ñè         | 4541/200000 [1:37:07<69:36:25,  1.28s/it, loss=0.0174, lr=2.27e-05, step=4541]Training:   2%|‚ñè         | 4542/200000 [1:37:08<72:11:38,  1.33s/it, loss=0.0174, lr=2.27e-05, step=4541]Training:   2%|‚ñè         | 4542/200000 [1:37:08<72:11:38,  1.33s/it, loss=0.0180, lr=2.27e-05, step=4542]Training:   2%|‚ñè         | 4543/200000 [1:37:09<73:24:29,  1.35s/it, loss=0.0180, lr=2.27e-05, step=4542]Training:   2%|‚ñè         | 4543/200000 [1:37:09<73:24:29,  1.35s/it, loss=0.0250, lr=2.27e-05, step=4543]Training:   2%|‚ñè         | 4544/200000 [1:37:11<74:51:07,  1.38s/it, loss=0.0250, lr=2.27e-05, step=4543]Training:   2%|‚ñè         | 4544/200000 [1:37:11<74:51:07,  1.38s/it, loss=0.0286, lr=2.27e-05, step=4544]Training:   2%|‚ñè         | 4545/200000 [1:37:12<69:57:42,  1.29s/it, loss=0.0286, lr=2.27e-05, step=4544]Training:   2%|‚ñè         | 4545/200000 [1:37:12<69:57:42,  1.29s/it, loss=0.0200, lr=2.27e-05, step=4545]Training:   2%|‚ñè         | 4546/200000 [1:37:13<66:30:21,  1.22s/it, loss=0.0200, lr=2.27e-05, step=4545]Training:   2%|‚ñè         | 4546/200000 [1:37:13<66:30:21,  1.22s/it, loss=0.0201, lr=2.27e-05, step=4546]Training:   2%|‚ñè         | 4547/200000 [1:37:14<68:32:57,  1.26s/it, loss=0.0201, lr=2.27e-05, step=4546]Training:   2%|‚ñè         | 4547/200000 [1:37:14<68:32:57,  1.26s/it, loss=0.0162, lr=2.27e-05, step=4547]Training:   2%|‚ñè         | 4548/200000 [1:37:16<70:48:49,  1.30s/it, loss=0.0162, lr=2.27e-05, step=4547]Training:   2%|‚ñè         | 4548/200000 [1:37:16<70:48:49,  1.30s/it, loss=0.0171, lr=2.27e-05, step=4548]Training:   2%|‚ñè         | 4549/200000 [1:37:17<67:07:01,  1.24s/it, loss=0.0171, lr=2.27e-05, step=4548]Training:   2%|‚ñè         | 4549/200000 [1:37:17<67:07:01,  1.24s/it, loss=0.0179, lr=2.27e-05, step=4549]Training:   2%|‚ñè         | 4550/200000 [1:37:18<69:40:59,  1.28s/it, loss=0.0179, lr=2.27e-05, step=4549]Training:   2%|‚ñè         | 4550/200000 [1:37:18<69:40:59,  1.28s/it, loss=0.0191, lr=2.27e-05, step=4550]Training:   2%|‚ñè         | 4551/200000 [1:37:20<70:49:32,  1.30s/it, loss=0.0191, lr=2.27e-05, step=4550]Training:   2%|‚ñè         | 4551/200000 [1:37:20<70:49:32,  1.30s/it, loss=0.0200, lr=2.28e-05, step=4551]Training:   2%|‚ñè         | 4552/200000 [1:37:21<71:42:26,  1.32s/it, loss=0.0200, lr=2.28e-05, step=4551]Training:   2%|‚ñè         | 4552/200000 [1:37:21<71:42:26,  1.32s/it, loss=0.0241, lr=2.28e-05, step=4552]Training:   2%|‚ñè         | 4553/200000 [1:37:22<67:40:30,  1.25s/it, loss=0.0241, lr=2.28e-05, step=4552]Training:   2%|‚ñè         | 4553/200000 [1:37:22<67:40:30,  1.25s/it, loss=0.0157, lr=2.28e-05, step=4553]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4554/200000 [1:37:23<71:27:17,  1.32s/it, loss=0.0157, lr=2.28e-05, step=4553]Training:   2%|‚ñè         | 4554/200000 [1:37:23<71:27:17,  1.32s/it, loss=0.0191, lr=2.28e-05, step=4554]Training:   2%|‚ñè         | 4555/200000 [1:37:25<73:39:18,  1.36s/it, loss=0.0191, lr=2.28e-05, step=4554]Training:   2%|‚ñè         | 4555/200000 [1:37:25<73:39:18,  1.36s/it, loss=0.0279, lr=2.28e-05, step=4555]Training:   2%|‚ñè         | 4556/200000 [1:37:26<69:03:37,  1.27s/it, loss=0.0279, lr=2.28e-05, step=4555]Training:   2%|‚ñè         | 4556/200000 [1:37:26<69:03:37,  1.27s/it, loss=0.0151, lr=2.28e-05, step=4556]Training:   2%|‚ñè         | 4557/200000 [1:37:27<65:52:51,  1.21s/it, loss=0.0151, lr=2.28e-05, step=4556]Training:   2%|‚ñè         | 4557/200000 [1:37:27<65:52:51,  1.21s/it, loss=0.0197, lr=2.28e-05, step=4557]Training:   2%|‚ñè         | 4558/200000 [1:37:29<69:58:17,  1.29s/it, loss=0.0197, lr=2.28e-05, step=4557]Training:   2%|‚ñè         | 4558/200000 [1:37:29<69:58:17,  1.29s/it, loss=0.0171, lr=2.28e-05, step=4558]Training:   2%|‚ñè         | 4559/200000 [1:37:30<66:31:32,  1.23s/it, loss=0.0171, lr=2.28e-05, step=4558]Training:   2%|‚ñè         | 4559/200000 [1:37:30<66:31:32,  1.23s/it, loss=0.0223, lr=2.28e-05, step=4559]Training:   2%|‚ñè         | 4560/200000 [1:37:31<67:29:34,  1.24s/it, loss=0.0223, lr=2.28e-05, step=4559]Training:   2%|‚ñè         | 4560/200000 [1:37:31<67:29:34,  1.24s/it, loss=0.0160, lr=2.28e-05, step=4560]Training:   2%|‚ñè         | 4561/200000 [1:37:32<64:47:00,  1.19s/it, loss=0.0160, lr=2.28e-05, step=4560]Training:   2%|‚ñè         | 4561/200000 [1:37:32<64:47:00,  1.19s/it, loss=0.0289, lr=2.28e-05, step=4561]Training:   2%|‚ñè         | 4562/200000 [1:37:33<67:38:57,  1.25s/it, loss=0.0289, lr=2.28e-05, step=4561]Training:   2%|‚ñè         | 4562/200000 [1:37:33<67:38:57,  1.25s/it, loss=0.0212, lr=2.28e-05, step=4562]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4563/200000 [1:37:35<70:33:29,  1.30s/it, loss=0.0212, lr=2.28e-05, step=4562]Training:   2%|‚ñè         | 4563/200000 [1:37:35<70:33:29,  1.30s/it, loss=0.0210, lr=2.28e-05, step=4563]Training:   2%|‚ñè         | 4564/200000 [1:37:36<73:05:54,  1.35s/it, loss=0.0210, lr=2.28e-05, step=4563]Training:   2%|‚ñè         | 4564/200000 [1:37:36<73:05:54,  1.35s/it, loss=0.0148, lr=2.28e-05, step=4564]Training:   2%|‚ñè         | 4565/200000 [1:37:38<74:17:48,  1.37s/it, loss=0.0148, lr=2.28e-05, step=4564]Training:   2%|‚ñè         | 4565/200000 [1:37:38<74:17:48,  1.37s/it, loss=0.0157, lr=2.28e-05, step=4565]Training:   2%|‚ñè         | 4566/200000 [1:37:39<69:31:23,  1.28s/it, loss=0.0157, lr=2.28e-05, step=4565]Training:   2%|‚ñè         | 4566/200000 [1:37:39<69:31:23,  1.28s/it, loss=0.2152, lr=2.28e-05, step=4566]Training:   2%|‚ñè         | 4567/200000 [1:37:40<66:11:48,  1.22s/it, loss=0.2152, lr=2.28e-05, step=4566]Training:   2%|‚ñè         | 4567/200000 [1:37:40<66:11:48,  1.22s/it, loss=0.0154, lr=2.28e-05, step=4567]Training:   2%|‚ñè         | 4568/200000 [1:37:41<68:21:58,  1.26s/it, loss=0.0154, lr=2.28e-05, step=4567]Training:   2%|‚ñè         | 4568/200000 [1:37:41<68:21:58,  1.26s/it, loss=0.0224, lr=2.28e-05, step=4568]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4569/200000 [1:37:42<69:46:28,  1.29s/it, loss=0.0224, lr=2.28e-05, step=4568]Training:   2%|‚ñè         | 4569/200000 [1:37:42<69:46:28,  1.29s/it, loss=0.0136, lr=2.28e-05, step=4569]Training:   2%|‚ñè         | 4570/200000 [1:37:44<66:21:26,  1.22s/it, loss=0.0136, lr=2.28e-05, step=4569]Training:   2%|‚ñè         | 4570/200000 [1:37:44<66:21:26,  1.22s/it, loss=0.0248, lr=2.28e-05, step=4570]Training:   2%|‚ñè         | 4571/200000 [1:37:45<69:01:08,  1.27s/it, loss=0.0248, lr=2.28e-05, step=4570]Training:   2%|‚ñè         | 4571/200000 [1:37:45<69:01:08,  1.27s/it, loss=0.0318, lr=2.29e-05, step=4571]Training:   2%|‚ñè         | 4572/200000 [1:37:46<65:51:30,  1.21s/it, loss=0.0318, lr=2.29e-05, step=4571]Training:   2%|‚ñè         | 4572/200000 [1:37:46<65:51:30,  1.21s/it, loss=0.0136, lr=2.29e-05, step=4572]Training:   2%|‚ñè         | 4573/200000 [1:37:47<69:51:45,  1.29s/it, loss=0.0136, lr=2.29e-05, step=4572]Training:   2%|‚ñè         | 4573/200000 [1:37:47<69:51:45,  1.29s/it, loss=0.0109, lr=2.29e-05, step=4573]Training:   2%|‚ñè         | 4574/200000 [1:37:49<66:24:53,  1.22s/it, loss=0.0109, lr=2.29e-05, step=4573]Training:   2%|‚ñè         | 4574/200000 [1:37:49<66:24:53,  1.22s/it, loss=0.0162, lr=2.29e-05, step=4574]Training:   2%|‚ñè         | 4575/200000 [1:37:50<69:52:34,  1.29s/it, loss=0.0162, lr=2.29e-05, step=4574]Training:   2%|‚ñè         | 4575/200000 [1:37:50<69:52:34,  1.29s/it, loss=0.0428, lr=2.29e-05, step=4575]Training:   2%|‚ñè         | 4576/200000 [1:37:51<72:25:47,  1.33s/it, loss=0.0428, lr=2.29e-05, step=4575]Training:   2%|‚ñè         | 4576/200000 [1:37:51<72:25:47,  1.33s/it, loss=0.0178, lr=2.29e-05, step=4576]Training:   2%|‚ñè         | 4577/200000 [1:37:53<68:12:30,  1.26s/it, loss=0.0178, lr=2.29e-05, step=4576]Training:   2%|‚ñè         | 4577/200000 [1:37:53<68:12:30,  1.26s/it, loss=0.0210, lr=2.29e-05, step=4577]Training:   2%|‚ñè         | 4578/200000 [1:37:54<65:15:44,  1.20s/it, loss=0.0210, lr=2.29e-05, step=4577]Training:   2%|‚ñè         | 4578/200000 [1:37:54<65:15:44,  1.20s/it, loss=0.0237, lr=2.29e-05, step=4578]Training:   2%|‚ñè         | 4579/200000 [1:37:55<68:31:07,  1.26s/it, loss=0.0237, lr=2.29e-05, step=4578]Training:   2%|‚ñè         | 4579/200000 [1:37:55<68:31:07,  1.26s/it, loss=0.0150, lr=2.29e-05, step=4579]Training:   2%|‚ñè         | 4580/200000 [1:37:56<70:33:55,  1.30s/it, loss=0.0150, lr=2.29e-05, step=4579]Training:   2%|‚ñè         | 4580/200000 [1:37:56<70:33:55,  1.30s/it, loss=0.0215, lr=2.29e-05, step=4580]Training:   2%|‚ñè         | 4581/200000 [1:37:57<66:55:33,  1.23s/it, loss=0.0215, lr=2.29e-05, step=4580]Training:   2%|‚ñè         | 4581/200000 [1:37:57<66:55:33,  1.23s/it, loss=0.0218, lr=2.29e-05, step=4581]Training:   2%|‚ñè         | 4582/200000 [1:37:59<70:42:37,  1.30s/it, loss=0.0218, lr=2.29e-05, step=4581]Training:   2%|‚ñè         | 4582/200000 [1:37:59<70:42:37,  1.30s/it, loss=0.0154, lr=2.29e-05, step=4582]Training:   2%|‚ñè         | 4583/200000 [1:38:00<71:48:08,  1.32s/it, loss=0.0154, lr=2.29e-05, step=4582]Training:   2%|‚ñè         | 4583/200000 [1:38:00<71:48:08,  1.32s/it, loss=0.0201, lr=2.29e-05, step=4583]Training:   2%|‚ñè         | 4584/200000 [1:38:02<72:22:31,  1.33s/it, loss=0.0201, lr=2.29e-05, step=4583]Training:   2%|‚ñè         | 4584/200000 [1:38:02<72:22:31,  1.33s/it, loss=0.0242, lr=2.29e-05, step=4584]Training:   2%|‚ñè         | 4585/200000 [1:38:03<68:12:35,  1.26s/it, loss=0.0242, lr=2.29e-05, step=4584]Training:   2%|‚ñè         | 4585/200000 [1:38:03<68:12:35,  1.26s/it, loss=0.0317, lr=2.29e-05, step=4585]Training:   2%|‚ñè         | 4586/200000 [1:38:04<72:07:48,  1.33s/it, loss=0.0317, lr=2.29e-05, step=4585]Training:   2%|‚ñè         | 4586/200000 [1:38:04<72:07:48,  1.33s/it, loss=0.0212, lr=2.29e-05, step=4586]Training:   2%|‚ñè         | 4587/200000 [1:38:06<75:17:48,  1.39s/it, loss=0.0212, lr=2.29e-05, step=4586]Training:   2%|‚ñè         | 4587/200000 [1:38:06<75:17:48,  1.39s/it, loss=0.0219, lr=2.29e-05, step=4587]Training:   2%|‚ñè         | 4588/200000 [1:38:07<70:15:29,  1.29s/it, loss=0.0219, lr=2.29e-05, step=4587]Training:   2%|‚ñè         | 4588/200000 [1:38:07<70:15:29,  1.29s/it, loss=0.0172, lr=2.29e-05, step=4588]Training:   2%|‚ñè         | 4589/200000 [1:38:08<66:39:27,  1.23s/it, loss=0.0172, lr=2.29e-05, step=4588]Training:   2%|‚ñè         | 4589/200000 [1:38:08<66:39:27,  1.23s/it, loss=0.0213, lr=2.29e-05, step=4589]Training:   2%|‚ñè         | 4590/200000 [1:38:09<70:05:44,  1.29s/it, loss=0.0213, lr=2.29e-05, step=4589]Training:   2%|‚ñè         | 4590/200000 [1:38:09<70:05:44,  1.29s/it, loss=0.0276, lr=2.29e-05, step=4590]Training:   2%|‚ñè         | 4591/200000 [1:38:10<66:35:48,  1.23s/it, loss=0.0276, lr=2.29e-05, step=4590]Training:   2%|‚ñè         | 4591/200000 [1:38:10<66:35:48,  1.23s/it, loss=0.0130, lr=2.30e-05, step=4591]Training:   2%|‚ñè         | 4592/200000 [1:38:12<67:55:14,  1.25s/it, loss=0.0130, lr=2.30e-05, step=4591]Training:   2%|‚ñè         | 4592/200000 [1:38:12<67:55:14,  1.25s/it, loss=0.0123, lr=2.30e-05, step=4592]Training:   2%|‚ñè         | 4593/200000 [1:38:13<65:05:42,  1.20s/it, loss=0.0123, lr=2.30e-05, step=4592]Training:   2%|‚ñè         | 4593/200000 [1:38:13<65:05:42,  1.20s/it, loss=0.0200, lr=2.30e-05, step=4593]Training:   2%|‚ñè         | 4594/200000 [1:38:14<68:57:06,  1.27s/it, loss=0.0200, lr=2.30e-05, step=4593]Training:   2%|‚ñè         | 4594/200000 [1:38:14<68:57:06,  1.27s/it, loss=0.0192, lr=2.30e-05, step=4594]Training:   2%|‚ñè         | 4595/200000 [1:38:16<72:03:46,  1.33s/it, loss=0.0192, lr=2.30e-05, step=4594]Training:   2%|‚ñè         | 4595/200000 [1:38:16<72:03:46,  1.33s/it, loss=0.0205, lr=2.30e-05, step=4595]Training:   2%|‚ñè         | 4596/200000 [1:38:17<73:36:53,  1.36s/it, loss=0.0205, lr=2.30e-05, step=4595]Training:   2%|‚ñè         | 4596/200000 [1:38:17<73:36:53,  1.36s/it, loss=0.0193, lr=2.30e-05, step=4596]Training:   2%|‚ñè         | 4597/200000 [1:38:19<74:49:44,  1.38s/it, loss=0.0193, lr=2.30e-05, step=4596]Training:   2%|‚ñè         | 4597/200000 [1:38:19<74:49:44,  1.38s/it, loss=0.0214, lr=2.30e-05, step=4597]Training:   2%|‚ñè         | 4598/200000 [1:38:20<69:53:20,  1.29s/it, loss=0.0214, lr=2.30e-05, step=4597]Training:   2%|‚ñè         | 4598/200000 [1:38:20<69:53:20,  1.29s/it, loss=0.0360, lr=2.30e-05, step=4598]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4599/200000 [1:38:21<66:27:38,  1.22s/it, loss=0.0360, lr=2.30e-05, step=4598]Training:   2%|‚ñè         | 4599/200000 [1:38:21<66:27:38,  1.22s/it, loss=0.0168, lr=2.30e-05, step=4599]Training:   2%|‚ñè         | 4600/200000 [1:38:22<68:31:43,  1.26s/it, loss=0.0168, lr=2.30e-05, step=4599]Training:   2%|‚ñè         | 4600/200000 [1:38:22<68:31:43,  1.26s/it, loss=0.0310, lr=2.30e-05, step=4600]00:31:37.254 [I] step=4600 loss=0.0255 lr=2.28e-05 grad_norm=0.56 time=127.9s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4601/200000 [1:38:23<70:45:01,  1.30s/it, loss=0.0310, lr=2.30e-05, step=4600]Training:   2%|‚ñè         | 4601/200000 [1:38:23<70:45:01,  1.30s/it, loss=0.0133, lr=2.30e-05, step=4601]Training:   2%|‚ñè         | 4602/200000 [1:38:25<67:03:15,  1.24s/it, loss=0.0133, lr=2.30e-05, step=4601]Training:   2%|‚ñè         | 4602/200000 [1:38:25<67:03:15,  1.24s/it, loss=0.0170, lr=2.30e-05, step=4602]Training:   2%|‚ñè         | 4603/200000 [1:38:26<69:52:45,  1.29s/it, loss=0.0170, lr=2.30e-05, step=4602]Training:   2%|‚ñè         | 4603/200000 [1:38:26<69:52:45,  1.29s/it, loss=0.0157, lr=2.30e-05, step=4603]Training:   2%|‚ñè         | 4604/200000 [1:38:27<70:57:43,  1.31s/it, loss=0.0157, lr=2.30e-05, step=4603]Training:   2%|‚ñè         | 4604/200000 [1:38:27<70:57:43,  1.31s/it, loss=0.0162, lr=2.30e-05, step=4604]Training:   2%|‚ñè         | 4605/200000 [1:38:29<72:15:31,  1.33s/it, loss=0.0162, lr=2.30e-05, step=4604]Training:   2%|‚ñè         | 4605/200000 [1:38:29<72:15:31,  1.33s/it, loss=0.0213, lr=2.30e-05, step=4605]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4606/200000 [1:38:30<68:05:40,  1.25s/it, loss=0.0213, lr=2.30e-05, step=4605]Training:   2%|‚ñè         | 4606/200000 [1:38:30<68:05:40,  1.25s/it, loss=0.0190, lr=2.30e-05, step=4606]Training:   2%|‚ñè         | 4607/200000 [1:38:31<72:24:41,  1.33s/it, loss=0.0190, lr=2.30e-05, step=4606]Training:   2%|‚ñè         | 4607/200000 [1:38:31<72:24:41,  1.33s/it, loss=0.0507, lr=2.30e-05, step=4607]Training:   2%|‚ñè         | 4608/200000 [1:38:33<74:36:49,  1.37s/it, loss=0.0507, lr=2.30e-05, step=4607]Training:   2%|‚ñè         | 4608/200000 [1:38:33<74:36:49,  1.37s/it, loss=0.0279, lr=2.30e-05, step=4608]Training:   2%|‚ñè         | 4609/200000 [1:38:34<69:45:01,  1.29s/it, loss=0.0279, lr=2.30e-05, step=4608]Training:   2%|‚ñè         | 4609/200000 [1:38:34<69:45:01,  1.29s/it, loss=0.0228, lr=2.30e-05, step=4609]Training:   2%|‚ñè         | 4610/200000 [1:38:35<66:21:30,  1.22s/it, loss=0.0228, lr=2.30e-05, step=4609]Training:   2%|‚ñè         | 4610/200000 [1:38:35<66:21:30,  1.22s/it, loss=0.0197, lr=2.30e-05, step=4610]Training:   2%|‚ñè         | 4611/200000 [1:38:36<70:17:09,  1.30s/it, loss=0.0197, lr=2.30e-05, step=4610]Training:   2%|‚ñè         | 4611/200000 [1:38:36<70:17:09,  1.30s/it, loss=0.0176, lr=2.31e-05, step=4611]Training:   2%|‚ñè         | 4612/200000 [1:38:37<66:45:40,  1.23s/it, loss=0.0176, lr=2.31e-05, step=4611]Training:   2%|‚ñè         | 4612/200000 [1:38:37<66:45:40,  1.23s/it, loss=0.0246, lr=2.31e-05, step=4612]Training:   2%|‚ñè         | 4613/200000 [1:38:39<68:05:50,  1.25s/it, loss=0.0246, lr=2.31e-05, step=4612]Training:   2%|‚ñè         | 4613/200000 [1:38:39<68:05:50,  1.25s/it, loss=0.0187, lr=2.31e-05, step=4613]Training:   2%|‚ñè         | 4614/200000 [1:38:40<65:10:59,  1.20s/it, loss=0.0187, lr=2.31e-05, step=4613]Training:   2%|‚ñè         | 4614/200000 [1:38:40<65:10:59,  1.20s/it, loss=0.0183, lr=2.31e-05, step=4614]Training:   2%|‚ñè         | 4615/200000 [1:38:41<68:01:34,  1.25s/it, loss=0.0183, lr=2.31e-05, step=4614]Training:   2%|‚ñè         | 4615/200000 [1:38:41<68:01:34,  1.25s/it, loss=0.0238, lr=2.31e-05, step=4615]Training:   2%|‚ñè         | 4616/200000 [1:38:43<70:45:08,  1.30s/it, loss=0.0238, lr=2.31e-05, step=4615]Training:   2%|‚ñè         | 4616/200000 [1:38:43<70:45:08,  1.30s/it, loss=0.0264, lr=2.31e-05, step=4616]Training:   2%|‚ñè         | 4617/200000 [1:38:44<73:16:38,  1.35s/it, loss=0.0264, lr=2.31e-05, step=4616]Training:   2%|‚ñè         | 4617/200000 [1:38:44<73:16:38,  1.35s/it, loss=0.0163, lr=2.31e-05, step=4617]WARNING:root:Token length (51) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4618/200000 [1:38:45<74:26:34,  1.37s/it, loss=0.0163, lr=2.31e-05, step=4617]Training:   2%|‚ñè         | 4618/200000 [1:38:45<74:26:34,  1.37s/it, loss=0.0261, lr=2.31e-05, step=4618]Training:   2%|‚ñè         | 4619/200000 [1:38:47<69:39:29,  1.28s/it, loss=0.0261, lr=2.31e-05, step=4618]Training:   2%|‚ñè         | 4619/200000 [1:38:47<69:39:29,  1.28s/it, loss=0.0225, lr=2.31e-05, step=4619]Training:   2%|‚ñè         | 4620/200000 [1:38:48<66:17:19,  1.22s/it, loss=0.0225, lr=2.31e-05, step=4619]Training:   2%|‚ñè         | 4620/200000 [1:38:48<66:17:19,  1.22s/it, loss=0.0281, lr=2.31e-05, step=4620]Training:   2%|‚ñè         | 4621/200000 [1:38:49<68:18:10,  1.26s/it, loss=0.0281, lr=2.31e-05, step=4620]Training:   2%|‚ñè         | 4621/200000 [1:38:49<68:18:10,  1.26s/it, loss=0.0163, lr=2.31e-05, step=4621]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4622/200000 [1:38:50<69:14:40,  1.28s/it, loss=0.0163, lr=2.31e-05, step=4621]Training:   2%|‚ñè         | 4622/200000 [1:38:50<69:14:40,  1.28s/it, loss=0.0282, lr=2.31e-05, step=4622]Training:   2%|‚ñè         | 4623/200000 [1:38:51<65:57:45,  1.22s/it, loss=0.0282, lr=2.31e-05, step=4622]Training:   2%|‚ñè         | 4623/200000 [1:38:51<65:57:45,  1.22s/it, loss=0.0363, lr=2.31e-05, step=4623]Training:   2%|‚ñè         | 4624/200000 [1:38:53<67:43:02,  1.25s/it, loss=0.0363, lr=2.31e-05, step=4623]Training:   2%|‚ñè         | 4624/200000 [1:38:53<67:43:02,  1.25s/it, loss=0.0243, lr=2.31e-05, step=4624]Training:   2%|‚ñè         | 4625/200000 [1:38:54<64:55:44,  1.20s/it, loss=0.0243, lr=2.31e-05, step=4624]Training:   2%|‚ñè         | 4625/200000 [1:38:54<64:55:44,  1.20s/it, loss=0.0359, lr=2.31e-05, step=4625]Training:   2%|‚ñè         | 4626/200000 [1:38:55<69:12:15,  1.28s/it, loss=0.0359, lr=2.31e-05, step=4625]Training:   2%|‚ñè         | 4626/200000 [1:38:55<69:12:15,  1.28s/it, loss=0.0177, lr=2.31e-05, step=4626]Training:   2%|‚ñè         | 4627/200000 [1:38:56<65:57:15,  1.22s/it, loss=0.0177, lr=2.31e-05, step=4626]Training:   2%|‚ñè         | 4627/200000 [1:38:56<65:57:15,  1.22s/it, loss=0.0154, lr=2.31e-05, step=4627]Training:   2%|‚ñè         | 4628/200000 [1:38:58<69:01:13,  1.27s/it, loss=0.0154, lr=2.31e-05, step=4627]Training:   2%|‚ñè         | 4628/200000 [1:38:58<69:01:13,  1.27s/it, loss=0.0258, lr=2.31e-05, step=4628]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4629/200000 [1:38:59<71:50:48,  1.32s/it, loss=0.0258, lr=2.31e-05, step=4628]Training:   2%|‚ñè         | 4629/200000 [1:38:59<71:50:48,  1.32s/it, loss=0.0490, lr=2.31e-05, step=4629]Training:   2%|‚ñè         | 4630/200000 [1:39:00<67:50:08,  1.25s/it, loss=0.0490, lr=2.31e-05, step=4629]Training:   2%|‚ñè         | 4630/200000 [1:39:00<67:50:08,  1.25s/it, loss=0.0262, lr=2.31e-05, step=4630]Training:   2%|‚ñè         | 4631/200000 [1:39:01<64:58:13,  1.20s/it, loss=0.0262, lr=2.31e-05, step=4630]Training:   2%|‚ñè         | 4631/200000 [1:39:01<64:58:13,  1.20s/it, loss=0.0202, lr=2.32e-05, step=4631]Training:   2%|‚ñè         | 4632/200000 [1:39:03<68:11:43,  1.26s/it, loss=0.0202, lr=2.32e-05, step=4631]Training:   2%|‚ñè         | 4632/200000 [1:39:03<68:11:43,  1.26s/it, loss=0.0229, lr=2.32e-05, step=4632]Training:   2%|‚ñè         | 4633/200000 [1:39:04<71:10:32,  1.31s/it, loss=0.0229, lr=2.32e-05, step=4632]Training:   2%|‚ñè         | 4633/200000 [1:39:04<71:10:32,  1.31s/it, loss=0.0289, lr=2.32e-05, step=4633]Training:   2%|‚ñè         | 4634/200000 [1:39:05<67:22:10,  1.24s/it, loss=0.0289, lr=2.32e-05, step=4633]Training:   2%|‚ñè         | 4634/200000 [1:39:05<67:22:10,  1.24s/it, loss=0.0204, lr=2.32e-05, step=4634]Training:   2%|‚ñè         | 4635/200000 [1:39:07<71:12:54,  1.31s/it, loss=0.0204, lr=2.32e-05, step=4634]Training:   2%|‚ñè         | 4635/200000 [1:39:07<71:12:54,  1.31s/it, loss=0.0346, lr=2.32e-05, step=4635]Training:   2%|‚ñè         | 4636/200000 [1:39:08<72:09:57,  1.33s/it, loss=0.0346, lr=2.32e-05, step=4635]Training:   2%|‚ñè         | 4636/200000 [1:39:08<72:09:57,  1.33s/it, loss=0.0196, lr=2.32e-05, step=4636]Training:   2%|‚ñè         | 4637/200000 [1:39:09<72:37:49,  1.34s/it, loss=0.0196, lr=2.32e-05, step=4636]Training:   2%|‚ñè         | 4637/200000 [1:39:09<72:37:49,  1.34s/it, loss=0.0207, lr=2.32e-05, step=4637]Training:   2%|‚ñè         | 4638/200000 [1:39:11<68:20:54,  1.26s/it, loss=0.0207, lr=2.32e-05, step=4637]Training:   2%|‚ñè         | 4638/200000 [1:39:11<68:20:54,  1.26s/it, loss=0.0185, lr=2.32e-05, step=4638]Training:   2%|‚ñè         | 4639/200000 [1:39:12<72:07:32,  1.33s/it, loss=0.0185, lr=2.32e-05, step=4638]Training:   2%|‚ñè         | 4639/200000 [1:39:12<72:07:32,  1.33s/it, loss=0.0166, lr=2.32e-05, step=4639]Training:   2%|‚ñè         | 4640/200000 [1:39:14<75:16:03,  1.39s/it, loss=0.0166, lr=2.32e-05, step=4639]Training:   2%|‚ñè         | 4640/200000 [1:39:14<75:16:03,  1.39s/it, loss=0.0192, lr=2.32e-05, step=4640]Training:   2%|‚ñè         | 4641/200000 [1:39:15<70:12:49,  1.29s/it, loss=0.0192, lr=2.32e-05, step=4640]Training:   2%|‚ñè         | 4641/200000 [1:39:15<70:12:49,  1.29s/it, loss=0.0249, lr=2.32e-05, step=4641]Training:   2%|‚ñè         | 4642/200000 [1:39:16<66:42:21,  1.23s/it, loss=0.0249, lr=2.32e-05, step=4641]Training:   2%|‚ñè         | 4642/200000 [1:39:16<66:42:21,  1.23s/it, loss=0.0404, lr=2.32e-05, step=4642]Training:   2%|‚ñè         | 4643/200000 [1:39:17<70:26:16,  1.30s/it, loss=0.0404, lr=2.32e-05, step=4642]Training:   2%|‚ñè         | 4643/200000 [1:39:17<70:26:16,  1.30s/it, loss=0.0151, lr=2.32e-05, step=4643]Training:   2%|‚ñè         | 4644/200000 [1:39:18<66:50:08,  1.23s/it, loss=0.0151, lr=2.32e-05, step=4643]Training:   2%|‚ñè         | 4644/200000 [1:39:18<66:50:08,  1.23s/it, loss=0.0339, lr=2.32e-05, step=4644]Training:   2%|‚ñè         | 4645/200000 [1:39:20<68:34:46,  1.26s/it, loss=0.0339, lr=2.32e-05, step=4644]Training:   2%|‚ñè         | 4645/200000 [1:39:20<68:34:46,  1.26s/it, loss=0.0198, lr=2.32e-05, step=4645]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4646/200000 [1:39:21<65:31:02,  1.21s/it, loss=0.0198, lr=2.32e-05, step=4645]Training:   2%|‚ñè         | 4646/200000 [1:39:21<65:31:02,  1.21s/it, loss=0.0234, lr=2.32e-05, step=4646]Training:   2%|‚ñè         | 4647/200000 [1:39:22<69:15:42,  1.28s/it, loss=0.0234, lr=2.32e-05, step=4646]Training:   2%|‚ñè         | 4647/200000 [1:39:22<69:15:42,  1.28s/it, loss=0.0234, lr=2.32e-05, step=4647]Training:   2%|‚ñè         | 4648/200000 [1:39:24<71:55:51,  1.33s/it, loss=0.0234, lr=2.32e-05, step=4647]Training:   2%|‚ñè         | 4648/200000 [1:39:24<71:55:51,  1.33s/it, loss=0.0252, lr=2.32e-05, step=4648]Training:   2%|‚ñè         | 4649/200000 [1:39:25<73:52:37,  1.36s/it, loss=0.0252, lr=2.32e-05, step=4648]Training:   2%|‚ñè         | 4649/200000 [1:39:25<73:52:37,  1.36s/it, loss=0.0274, lr=2.32e-05, step=4649]Training:   2%|‚ñè         | 4650/200000 [1:39:26<75:08:55,  1.38s/it, loss=0.0274, lr=2.32e-05, step=4649]Training:   2%|‚ñè         | 4650/200000 [1:39:26<75:08:55,  1.38s/it, loss=0.0191, lr=2.32e-05, step=4650]Training:   2%|‚ñè         | 4651/200000 [1:39:27<70:07:35,  1.29s/it, loss=0.0191, lr=2.32e-05, step=4650]Training:   2%|‚ñè         | 4651/200000 [1:39:27<70:07:35,  1.29s/it, loss=0.0206, lr=2.33e-05, step=4651]Training:   2%|‚ñè         | 4652/200000 [1:39:29<66:38:48,  1.23s/it, loss=0.0206, lr=2.33e-05, step=4651]Training:   2%|‚ñè         | 4652/200000 [1:39:29<66:38:48,  1.23s/it, loss=0.0270, lr=2.33e-05, step=4652]Training:   2%|‚ñè         | 4653/200000 [1:39:30<68:34:37,  1.26s/it, loss=0.0270, lr=2.33e-05, step=4652]Training:   2%|‚ñè         | 4653/200000 [1:39:30<68:34:37,  1.26s/it, loss=0.0261, lr=2.33e-05, step=4653]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4654/200000 [1:39:31<70:40:53,  1.30s/it, loss=0.0261, lr=2.33e-05, step=4653]Training:   2%|‚ñè         | 4654/200000 [1:39:31<70:40:53,  1.30s/it, loss=0.0271, lr=2.33e-05, step=4654]Training:   2%|‚ñè         | 4655/200000 [1:39:32<66:58:29,  1.23s/it, loss=0.0271, lr=2.33e-05, step=4654]Training:   2%|‚ñè         | 4655/200000 [1:39:32<66:58:29,  1.23s/it, loss=0.0208, lr=2.33e-05, step=4655]Training:   2%|‚ñè         | 4656/200000 [1:39:34<70:15:23,  1.29s/it, loss=0.0208, lr=2.33e-05, step=4655]Training:   2%|‚ñè         | 4656/200000 [1:39:34<70:15:23,  1.29s/it, loss=0.0132, lr=2.33e-05, step=4656]Training:   2%|‚ñè         | 4657/200000 [1:39:35<71:14:13,  1.31s/it, loss=0.0132, lr=2.33e-05, step=4656]Training:   2%|‚ñè         | 4657/200000 [1:39:35<71:14:13,  1.31s/it, loss=0.0567, lr=2.33e-05, step=4657]Training:   2%|‚ñè         | 4658/200000 [1:39:37<72:32:50,  1.34s/it, loss=0.0567, lr=2.33e-05, step=4657]Training:   2%|‚ñè         | 4658/200000 [1:39:37<72:32:50,  1.34s/it, loss=0.0172, lr=2.33e-05, step=4658]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4659/200000 [1:39:38<68:18:48,  1.26s/it, loss=0.0172, lr=2.33e-05, step=4658]Training:   2%|‚ñè         | 4659/200000 [1:39:38<68:18:48,  1.26s/it, loss=0.0186, lr=2.33e-05, step=4659]Training:   2%|‚ñè         | 4660/200000 [1:39:39<71:56:29,  1.33s/it, loss=0.0186, lr=2.33e-05, step=4659]Training:   2%|‚ñè         | 4660/200000 [1:39:39<71:56:29,  1.33s/it, loss=0.0176, lr=2.33e-05, step=4660]Training:   2%|‚ñè         | 4661/200000 [1:39:41<74:41:30,  1.38s/it, loss=0.0176, lr=2.33e-05, step=4660]Training:   2%|‚ñè         | 4661/200000 [1:39:41<74:41:30,  1.38s/it, loss=0.0328, lr=2.33e-05, step=4661]Training:   2%|‚ñè         | 4662/200000 [1:39:42<69:47:04,  1.29s/it, loss=0.0328, lr=2.33e-05, step=4661]Training:   2%|‚ñè         | 4662/200000 [1:39:42<69:47:04,  1.29s/it, loss=0.0141, lr=2.33e-05, step=4662]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4663/200000 [1:39:43<66:21:54,  1.22s/it, loss=0.0141, lr=2.33e-05, step=4662]Training:   2%|‚ñè         | 4663/200000 [1:39:43<66:21:54,  1.22s/it, loss=0.0240, lr=2.33e-05, step=4663]Training:   2%|‚ñè         | 4664/200000 [1:39:44<70:15:01,  1.29s/it, loss=0.0240, lr=2.33e-05, step=4663]Training:   2%|‚ñè         | 4664/200000 [1:39:44<70:15:01,  1.29s/it, loss=0.0271, lr=2.33e-05, step=4664]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4665/200000 [1:39:45<66:42:13,  1.23s/it, loss=0.0271, lr=2.33e-05, step=4664]Training:   2%|‚ñè         | 4665/200000 [1:39:45<66:42:13,  1.23s/it, loss=0.0410, lr=2.33e-05, step=4665]Training:   2%|‚ñè         | 4666/200000 [1:39:47<67:53:26,  1.25s/it, loss=0.0410, lr=2.33e-05, step=4665]Training:   2%|‚ñè         | 4666/200000 [1:39:47<67:53:26,  1.25s/it, loss=0.0177, lr=2.33e-05, step=4666]Training:   2%|‚ñè         | 4667/200000 [1:39:48<65:01:40,  1.20s/it, loss=0.0177, lr=2.33e-05, step=4666]Training:   2%|‚ñè         | 4667/200000 [1:39:48<65:01:40,  1.20s/it, loss=0.0151, lr=2.33e-05, step=4667]Training:   2%|‚ñè         | 4668/200000 [1:39:49<67:54:30,  1.25s/it, loss=0.0151, lr=2.33e-05, step=4667]Training:   2%|‚ñè         | 4668/200000 [1:39:49<67:54:30,  1.25s/it, loss=0.0155, lr=2.33e-05, step=4668]Training:   2%|‚ñè         | 4669/200000 [1:39:51<71:21:25,  1.32s/it, loss=0.0155, lr=2.33e-05, step=4668]Training:   2%|‚ñè         | 4669/200000 [1:39:51<71:21:25,  1.32s/it, loss=0.0244, lr=2.33e-05, step=4669]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4670/200000 [1:39:52<73:48:46,  1.36s/it, loss=0.0244, lr=2.33e-05, step=4669]Training:   2%|‚ñè         | 4670/200000 [1:39:52<73:48:46,  1.36s/it, loss=0.0295, lr=2.33e-05, step=4670]Training:   2%|‚ñè         | 4671/200000 [1:39:53<74:39:46,  1.38s/it, loss=0.0295, lr=2.33e-05, step=4670]Training:   2%|‚ñè         | 4671/200000 [1:39:53<74:39:46,  1.38s/it, loss=0.0187, lr=2.34e-05, step=4671]Training:   2%|‚ñè         | 4672/200000 [1:39:54<69:47:51,  1.29s/it, loss=0.0187, lr=2.34e-05, step=4671]Training:   2%|‚ñè         | 4672/200000 [1:39:54<69:47:51,  1.29s/it, loss=0.0210, lr=2.34e-05, step=4672]Training:   2%|‚ñè         | 4673/200000 [1:39:56<66:22:55,  1.22s/it, loss=0.0210, lr=2.34e-05, step=4672]Training:   2%|‚ñè         | 4673/200000 [1:39:56<66:22:55,  1.22s/it, loss=0.0196, lr=2.34e-05, step=4673]Training:   2%|‚ñè         | 4674/200000 [1:39:57<68:17:15,  1.26s/it, loss=0.0196, lr=2.34e-05, step=4673]Training:   2%|‚ñè         | 4674/200000 [1:39:57<68:17:15,  1.26s/it, loss=0.0212, lr=2.34e-05, step=4674]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4675/200000 [1:39:58<69:29:39,  1.28s/it, loss=0.0212, lr=2.34e-05, step=4674]Training:   2%|‚ñè         | 4675/200000 [1:39:58<69:29:39,  1.28s/it, loss=0.0158, lr=2.34e-05, step=4675]Training:   2%|‚ñè         | 4676/200000 [1:39:59<66:10:16,  1.22s/it, loss=0.0158, lr=2.34e-05, step=4675]Training:   2%|‚ñè         | 4676/200000 [1:39:59<66:10:16,  1.22s/it, loss=0.0199, lr=2.34e-05, step=4676]Training:   2%|‚ñè         | 4677/200000 [1:40:01<69:03:21,  1.27s/it, loss=0.0199, lr=2.34e-05, step=4676]Training:   2%|‚ñè         | 4677/200000 [1:40:01<69:03:21,  1.27s/it, loss=0.0346, lr=2.34e-05, step=4677]Training:   2%|‚ñè         | 4678/200000 [1:40:02<65:51:43,  1.21s/it, loss=0.0346, lr=2.34e-05, step=4677]Training:   2%|‚ñè         | 4678/200000 [1:40:02<65:51:43,  1.21s/it, loss=0.0128, lr=2.34e-05, step=4678]Training:   2%|‚ñè         | 4679/200000 [1:40:03<69:47:00,  1.29s/it, loss=0.0128, lr=2.34e-05, step=4678]Training:   2%|‚ñè         | 4679/200000 [1:40:03<69:47:00,  1.29s/it, loss=0.0191, lr=2.34e-05, step=4679]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4680/200000 [1:40:04<66:23:00,  1.22s/it, loss=0.0191, lr=2.34e-05, step=4679]Training:   2%|‚ñè         | 4680/200000 [1:40:04<66:23:00,  1.22s/it, loss=0.0247, lr=2.34e-05, step=4680]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4681/200000 [1:40:06<69:18:11,  1.28s/it, loss=0.0247, lr=2.34e-05, step=4680]Training:   2%|‚ñè         | 4681/200000 [1:40:06<69:18:11,  1.28s/it, loss=0.0174, lr=2.34e-05, step=4681]Training:   2%|‚ñè         | 4682/200000 [1:40:07<71:55:47,  1.33s/it, loss=0.0174, lr=2.34e-05, step=4681]Training:   2%|‚ñè         | 4682/200000 [1:40:07<71:55:47,  1.33s/it, loss=0.0201, lr=2.34e-05, step=4682]Training:   2%|‚ñè         | 4683/200000 [1:40:08<67:52:18,  1.25s/it, loss=0.0201, lr=2.34e-05, step=4682]Training:   2%|‚ñè         | 4683/200000 [1:40:08<67:52:18,  1.25s/it, loss=0.0210, lr=2.34e-05, step=4683]Training:   2%|‚ñè         | 4684/200000 [1:40:09<65:02:27,  1.20s/it, loss=0.0210, lr=2.34e-05, step=4683]Training:   2%|‚ñè         | 4684/200000 [1:40:09<65:02:27,  1.20s/it, loss=0.0434, lr=2.34e-05, step=4684]Training:   2%|‚ñè         | 4685/200000 [1:40:11<68:23:26,  1.26s/it, loss=0.0434, lr=2.34e-05, step=4684]Training:   2%|‚ñè         | 4685/200000 [1:40:11<68:23:26,  1.26s/it, loss=0.0234, lr=2.34e-05, step=4685]Training:   2%|‚ñè         | 4686/200000 [1:40:12<71:13:18,  1.31s/it, loss=0.0234, lr=2.34e-05, step=4685]Training:   2%|‚ñè         | 4686/200000 [1:40:12<71:13:18,  1.31s/it, loss=0.0149, lr=2.34e-05, step=4686]Training:   2%|‚ñè         | 4687/200000 [1:40:13<67:22:19,  1.24s/it, loss=0.0149, lr=2.34e-05, step=4686]Training:   2%|‚ñè         | 4687/200000 [1:40:13<67:22:19,  1.24s/it, loss=0.0306, lr=2.34e-05, step=4687]Training:   2%|‚ñè         | 4688/200000 [1:40:15<69:39:21,  1.28s/it, loss=0.0306, lr=2.34e-05, step=4687]Training:   2%|‚ñè         | 4688/200000 [1:40:15<69:39:21,  1.28s/it, loss=0.0199, lr=2.34e-05, step=4688]Training:   2%|‚ñè         | 4689/200000 [1:40:16<70:57:02,  1.31s/it, loss=0.0199, lr=2.34e-05, step=4688]Training:   2%|‚ñè         | 4689/200000 [1:40:16<70:57:02,  1.31s/it, loss=0.0250, lr=2.34e-05, step=4689]Training:   2%|‚ñè         | 4690/200000 [1:40:17<71:44:08,  1.32s/it, loss=0.0250, lr=2.34e-05, step=4689]Training:   2%|‚ñè         | 4690/200000 [1:40:17<71:44:08,  1.32s/it, loss=0.0238, lr=2.34e-05, step=4690]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4691/200000 [1:40:18<67:43:29,  1.25s/it, loss=0.0238, lr=2.34e-05, step=4690]Training:   2%|‚ñè         | 4691/200000 [1:40:18<67:43:29,  1.25s/it, loss=0.0341, lr=2.35e-05, step=4691]Training:   2%|‚ñè         | 4692/200000 [1:40:20<71:47:51,  1.32s/it, loss=0.0341, lr=2.35e-05, step=4691]Training:   2%|‚ñè         | 4692/200000 [1:40:20<71:47:51,  1.32s/it, loss=0.0193, lr=2.35e-05, step=4692]Training:   2%|‚ñè         | 4693/200000 [1:40:21<75:05:33,  1.38s/it, loss=0.0193, lr=2.35e-05, step=4692]Training:   2%|‚ñè         | 4693/200000 [1:40:21<75:05:33,  1.38s/it, loss=0.0136, lr=2.35e-05, step=4693]Training:   2%|‚ñè         | 4694/200000 [1:40:22<70:05:54,  1.29s/it, loss=0.0136, lr=2.35e-05, step=4693]Training:   2%|‚ñè         | 4694/200000 [1:40:22<70:05:54,  1.29s/it, loss=0.0301, lr=2.35e-05, step=4694]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4695/200000 [1:40:24<66:36:29,  1.23s/it, loss=0.0301, lr=2.35e-05, step=4694]Training:   2%|‚ñè         | 4695/200000 [1:40:24<66:36:29,  1.23s/it, loss=0.0203, lr=2.35e-05, step=4695]Training:   2%|‚ñè         | 4696/200000 [1:40:25<70:36:03,  1.30s/it, loss=0.0203, lr=2.35e-05, step=4695]Training:   2%|‚ñè         | 4696/200000 [1:40:25<70:36:03,  1.30s/it, loss=0.0207, lr=2.35e-05, step=4696]Training:   2%|‚ñè         | 4697/200000 [1:40:26<66:57:20,  1.23s/it, loss=0.0207, lr=2.35e-05, step=4696]Training:   2%|‚ñè         | 4697/200000 [1:40:26<66:57:20,  1.23s/it, loss=0.0114, lr=2.35e-05, step=4697]Training:   2%|‚ñè         | 4698/200000 [1:40:27<68:36:43,  1.26s/it, loss=0.0114, lr=2.35e-05, step=4697]Training:   2%|‚ñè         | 4698/200000 [1:40:27<68:36:43,  1.26s/it, loss=0.0242, lr=2.35e-05, step=4698]Training:   2%|‚ñè         | 4699/200000 [1:40:29<65:37:01,  1.21s/it, loss=0.0242, lr=2.35e-05, step=4698]Training:   2%|‚ñè         | 4699/200000 [1:40:29<65:37:01,  1.21s/it, loss=0.0552, lr=2.35e-05, step=4699]Training:   2%|‚ñè         | 4700/200000 [1:40:30<69:23:18,  1.28s/it, loss=0.0552, lr=2.35e-05, step=4699]Training:   2%|‚ñè         | 4700/200000 [1:40:30<69:23:18,  1.28s/it, loss=0.0235, lr=2.35e-05, step=4700]00:33:45.267 [I] step=4700 loss=0.0240 lr=2.33e-05 grad_norm=0.51 time=128.0s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4701/200000 [1:40:31<72:42:40,  1.34s/it, loss=0.0235, lr=2.35e-05, step=4700]Training:   2%|‚ñè         | 4701/200000 [1:40:31<72:42:40,  1.34s/it, loss=0.0312, lr=2.35e-05, step=4701]Training:   2%|‚ñè         | 4702/200000 [1:40:33<74:50:38,  1.38s/it, loss=0.0312, lr=2.35e-05, step=4701]Training:   2%|‚ñè         | 4702/200000 [1:40:33<74:50:38,  1.38s/it, loss=0.0217, lr=2.35e-05, step=4702]Training:   2%|‚ñè         | 4703/200000 [1:40:34<75:42:45,  1.40s/it, loss=0.0217, lr=2.35e-05, step=4702]Training:   2%|‚ñè         | 4703/200000 [1:40:34<75:42:45,  1.40s/it, loss=0.0241, lr=2.35e-05, step=4703]Training:   2%|‚ñè         | 4704/200000 [1:40:35<70:31:30,  1.30s/it, loss=0.0241, lr=2.35e-05, step=4703]Training:   2%|‚ñè         | 4704/200000 [1:40:35<70:31:30,  1.30s/it, loss=0.0192, lr=2.35e-05, step=4704]Training:   2%|‚ñè         | 4705/200000 [1:40:37<66:52:45,  1.23s/it, loss=0.0192, lr=2.35e-05, step=4704]Training:   2%|‚ñè         | 4705/200000 [1:40:37<66:52:45,  1.23s/it, loss=0.0181, lr=2.35e-05, step=4705]Training:   2%|‚ñè         | 4706/200000 [1:40:38<69:21:23,  1.28s/it, loss=0.0181, lr=2.35e-05, step=4705]Training:   2%|‚ñè         | 4706/200000 [1:40:38<69:21:23,  1.28s/it, loss=0.0348, lr=2.35e-05, step=4706]Training:   2%|‚ñè         | 4707/200000 [1:40:39<71:53:59,  1.33s/it, loss=0.0348, lr=2.35e-05, step=4706]Training:   2%|‚ñè         | 4707/200000 [1:40:39<71:53:59,  1.33s/it, loss=0.0184, lr=2.35e-05, step=4707]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4708/200000 [1:40:40<67:53:50,  1.25s/it, loss=0.0184, lr=2.35e-05, step=4707]Training:   2%|‚ñè         | 4708/200000 [1:40:40<67:53:50,  1.25s/it, loss=0.0127, lr=2.35e-05, step=4708]Training:   2%|‚ñè         | 4709/200000 [1:40:42<71:02:15,  1.31s/it, loss=0.0127, lr=2.35e-05, step=4708]Training:   2%|‚ñè         | 4709/200000 [1:40:42<71:02:15,  1.31s/it, loss=0.0205, lr=2.35e-05, step=4709]Training:   2%|‚ñè         | 4710/200000 [1:40:43<71:45:08,  1.32s/it, loss=0.0205, lr=2.35e-05, step=4709]Training:   2%|‚ñè         | 4710/200000 [1:40:43<71:45:08,  1.32s/it, loss=0.0206, lr=2.35e-05, step=4710]Training:   2%|‚ñè         | 4711/200000 [1:40:45<72:53:22,  1.34s/it, loss=0.0206, lr=2.35e-05, step=4710]Training:   2%|‚ñè         | 4711/200000 [1:40:45<72:53:22,  1.34s/it, loss=0.0488, lr=2.36e-05, step=4711]Training:   2%|‚ñè         | 4712/200000 [1:40:46<68:30:32,  1.26s/it, loss=0.0488, lr=2.36e-05, step=4711]Training:   2%|‚ñè         | 4712/200000 [1:40:46<68:30:32,  1.26s/it, loss=0.0172, lr=2.36e-05, step=4712]Training:   2%|‚ñè         | 4713/200000 [1:40:47<71:49:59,  1.32s/it, loss=0.0172, lr=2.36e-05, step=4712]Training:   2%|‚ñè         | 4713/200000 [1:40:47<71:49:59,  1.32s/it, loss=0.0225, lr=2.36e-05, step=4713]Training:   2%|‚ñè         | 4714/200000 [1:40:49<74:33:16,  1.37s/it, loss=0.0225, lr=2.36e-05, step=4713]Training:   2%|‚ñè         | 4714/200000 [1:40:49<74:33:16,  1.37s/it, loss=0.0394, lr=2.36e-05, step=4714]Training:   2%|‚ñè         | 4715/200000 [1:40:50<69:42:10,  1.28s/it, loss=0.0394, lr=2.36e-05, step=4714]Training:   2%|‚ñè         | 4715/200000 [1:40:50<69:42:10,  1.28s/it, loss=0.0359, lr=2.36e-05, step=4715]Training:   2%|‚ñè         | 4716/200000 [1:40:51<66:19:40,  1.22s/it, loss=0.0359, lr=2.36e-05, step=4715]Training:   2%|‚ñè         | 4716/200000 [1:40:51<66:19:40,  1.22s/it, loss=0.0297, lr=2.36e-05, step=4716]Training:   2%|‚ñè         | 4717/200000 [1:40:52<69:28:54,  1.28s/it, loss=0.0297, lr=2.36e-05, step=4716]Training:   2%|‚ñè         | 4717/200000 [1:40:52<69:28:54,  1.28s/it, loss=0.0234, lr=2.36e-05, step=4717]Training:   2%|‚ñè         | 4718/200000 [1:40:53<66:08:44,  1.22s/it, loss=0.0234, lr=2.36e-05, step=4717]Training:   2%|‚ñè         | 4718/200000 [1:40:53<66:08:44,  1.22s/it, loss=0.0152, lr=2.36e-05, step=4718]Training:   2%|‚ñè         | 4719/200000 [1:40:55<67:32:19,  1.25s/it, loss=0.0152, lr=2.36e-05, step=4718]Training:   2%|‚ñè         | 4719/200000 [1:40:55<67:32:19,  1.25s/it, loss=0.0159, lr=2.36e-05, step=4719]Training:   2%|‚ñè         | 4720/200000 [1:40:56<64:48:16,  1.19s/it, loss=0.0159, lr=2.36e-05, step=4719]Training:   2%|‚ñè         | 4720/200000 [1:40:56<64:48:16,  1.19s/it, loss=0.0319, lr=2.36e-05, step=4720]Training:   2%|‚ñè         | 4721/200000 [1:40:57<67:41:35,  1.25s/it, loss=0.0319, lr=2.36e-05, step=4720]Training:   2%|‚ñè         | 4721/200000 [1:40:57<67:41:35,  1.25s/it, loss=0.0308, lr=2.36e-05, step=4721]Training:   2%|‚ñè         | 4722/200000 [1:40:58<70:28:55,  1.30s/it, loss=0.0308, lr=2.36e-05, step=4721]Training:   2%|‚ñè         | 4722/200000 [1:40:58<70:28:55,  1.30s/it, loss=0.0340, lr=2.36e-05, step=4722]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4723/200000 [1:41:00<73:07:36,  1.35s/it, loss=0.0340, lr=2.36e-05, step=4722]Training:   2%|‚ñè         | 4723/200000 [1:41:00<73:07:36,  1.35s/it, loss=0.0190, lr=2.36e-05, step=4723]Training:   2%|‚ñè         | 4724/200000 [1:41:01<74:34:42,  1.37s/it, loss=0.0190, lr=2.36e-05, step=4723]Training:   2%|‚ñè         | 4724/200000 [1:41:01<74:34:42,  1.37s/it, loss=0.0198, lr=2.36e-05, step=4724]Training:   2%|‚ñè         | 4725/200000 [1:41:02<69:42:58,  1.29s/it, loss=0.0198, lr=2.36e-05, step=4724]Training:   2%|‚ñè         | 4725/200000 [1:41:02<69:42:58,  1.29s/it, loss=0.0177, lr=2.36e-05, step=4725]Training:   2%|‚ñè         | 4726/200000 [1:41:04<66:19:56,  1.22s/it, loss=0.0177, lr=2.36e-05, step=4725]Training:   2%|‚ñè         | 4726/200000 [1:41:04<66:19:56,  1.22s/it, loss=0.0232, lr=2.36e-05, step=4726]Training:   2%|‚ñè         | 4727/200000 [1:41:05<68:50:58,  1.27s/it, loss=0.0232, lr=2.36e-05, step=4726]Training:   2%|‚ñè         | 4727/200000 [1:41:05<68:50:58,  1.27s/it, loss=0.0168, lr=2.36e-05, step=4727]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4728/200000 [1:41:06<69:51:15,  1.29s/it, loss=0.0168, lr=2.36e-05, step=4727]Training:   2%|‚ñè         | 4728/200000 [1:41:06<69:51:15,  1.29s/it, loss=0.0245, lr=2.36e-05, step=4728]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4729/200000 [1:41:07<66:26:31,  1.22s/it, loss=0.0245, lr=2.36e-05, step=4728]Training:   2%|‚ñè         | 4729/200000 [1:41:07<66:26:31,  1.22s/it, loss=0.0228, lr=2.36e-05, step=4729]Training:   2%|‚ñè         | 4730/200000 [1:41:09<68:52:06,  1.27s/it, loss=0.0228, lr=2.36e-05, step=4729]Training:   2%|‚ñè         | 4730/200000 [1:41:09<68:52:06,  1.27s/it, loss=0.0231, lr=2.36e-05, step=4730]Training:   2%|‚ñè         | 4731/200000 [1:41:10<65:41:04,  1.21s/it, loss=0.0231, lr=2.36e-05, step=4730]Training:   2%|‚ñè         | 4731/200000 [1:41:10<65:41:04,  1.21s/it, loss=0.0373, lr=2.37e-05, step=4731]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4732/200000 [1:41:11<69:46:41,  1.29s/it, loss=0.0373, lr=2.37e-05, step=4731]Training:   2%|‚ñè         | 4732/200000 [1:41:11<69:46:41,  1.29s/it, loss=0.0252, lr=2.37e-05, step=4732]Training:   2%|‚ñè         | 4733/200000 [1:41:12<66:21:24,  1.22s/it, loss=0.0252, lr=2.37e-05, step=4732]Training:   2%|‚ñè         | 4733/200000 [1:41:12<66:21:24,  1.22s/it, loss=0.0150, lr=2.37e-05, step=4733]Training:   2%|‚ñè         | 4734/200000 [1:41:14<69:10:22,  1.28s/it, loss=0.0150, lr=2.37e-05, step=4733]Training:   2%|‚ñè         | 4734/200000 [1:41:14<69:10:22,  1.28s/it, loss=0.0190, lr=2.37e-05, step=4734]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4735/200000 [1:41:15<71:53:08,  1.33s/it, loss=0.0190, lr=2.37e-05, step=4734]Training:   2%|‚ñè         | 4735/200000 [1:41:15<71:53:08,  1.33s/it, loss=0.0172, lr=2.37e-05, step=4735]Training:   2%|‚ñè         | 4736/200000 [1:41:16<67:50:41,  1.25s/it, loss=0.0172, lr=2.37e-05, step=4735]Training:   2%|‚ñè         | 4736/200000 [1:41:16<67:50:41,  1.25s/it, loss=0.0194, lr=2.37e-05, step=4736]Training:   2%|‚ñè         | 4737/200000 [1:41:17<64:59:45,  1.20s/it, loss=0.0194, lr=2.37e-05, step=4736]Training:   2%|‚ñè         | 4737/200000 [1:41:17<64:59:45,  1.20s/it, loss=0.0217, lr=2.37e-05, step=4737]Training:   2%|‚ñè         | 4738/200000 [1:41:19<68:20:01,  1.26s/it, loss=0.0217, lr=2.37e-05, step=4737]Training:   2%|‚ñè         | 4738/200000 [1:41:19<68:20:01,  1.26s/it, loss=0.0399, lr=2.37e-05, step=4738]Training:   2%|‚ñè         | 4739/200000 [1:41:20<71:57:13,  1.33s/it, loss=0.0399, lr=2.37e-05, step=4738]Training:   2%|‚ñè         | 4739/200000 [1:41:20<71:57:13,  1.33s/it, loss=0.0174, lr=2.37e-05, step=4739]Training:   2%|‚ñè         | 4740/200000 [1:41:21<67:53:45,  1.25s/it, loss=0.0174, lr=2.37e-05, step=4739]Training:   2%|‚ñè         | 4740/200000 [1:41:21<67:53:45,  1.25s/it, loss=0.0175, lr=2.37e-05, step=4740]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4741/200000 [1:41:23<70:45:36,  1.30s/it, loss=0.0175, lr=2.37e-05, step=4740]Training:   2%|‚ñè         | 4741/200000 [1:41:23<70:45:36,  1.30s/it, loss=0.0208, lr=2.37e-05, step=4741]Training:   2%|‚ñè         | 4742/200000 [1:41:24<71:34:52,  1.32s/it, loss=0.0208, lr=2.37e-05, step=4741]Training:   2%|‚ñè         | 4742/200000 [1:41:24<71:34:52,  1.32s/it, loss=0.0132, lr=2.37e-05, step=4742]Training:   2%|‚ñè         | 4743/200000 [1:41:25<72:10:27,  1.33s/it, loss=0.0132, lr=2.37e-05, step=4742]Training:   2%|‚ñè         | 4743/200000 [1:41:25<72:10:27,  1.33s/it, loss=0.0403, lr=2.37e-05, step=4743]Training:   2%|‚ñè         | 4744/200000 [1:41:26<68:02:42,  1.25s/it, loss=0.0403, lr=2.37e-05, step=4743]Training:   2%|‚ñè         | 4744/200000 [1:41:26<68:02:42,  1.25s/it, loss=0.0198, lr=2.37e-05, step=4744]Training:   2%|‚ñè         | 4745/200000 [1:41:28<71:59:46,  1.33s/it, loss=0.0198, lr=2.37e-05, step=4744]Training:   2%|‚ñè         | 4745/200000 [1:41:28<71:59:46,  1.33s/it, loss=0.0188, lr=2.37e-05, step=4745]Training:   2%|‚ñè         | 4746/200000 [1:41:29<75:12:15,  1.39s/it, loss=0.0188, lr=2.37e-05, step=4745]Training:   2%|‚ñè         | 4746/200000 [1:41:29<75:12:15,  1.39s/it, loss=0.0252, lr=2.37e-05, step=4746]Training:   2%|‚ñè         | 4747/200000 [1:41:31<70:09:47,  1.29s/it, loss=0.0252, lr=2.37e-05, step=4746]Training:   2%|‚ñè         | 4747/200000 [1:41:31<70:09:47,  1.29s/it, loss=0.0233, lr=2.37e-05, step=4747]Training:   2%|‚ñè         | 4748/200000 [1:41:32<66:39:36,  1.23s/it, loss=0.0233, lr=2.37e-05, step=4747]Training:   2%|‚ñè         | 4748/200000 [1:41:32<66:39:36,  1.23s/it, loss=0.0213, lr=2.37e-05, step=4748]Training:   2%|‚ñè         | 4749/200000 [1:41:33<70:22:35,  1.30s/it, loss=0.0213, lr=2.37e-05, step=4748]Training:   2%|‚ñè         | 4749/200000 [1:41:33<70:22:35,  1.30s/it, loss=0.0158, lr=2.37e-05, step=4749]Training:   2%|‚ñè         | 4750/200000 [1:41:34<66:46:51,  1.23s/it, loss=0.0158, lr=2.37e-05, step=4749]Training:   2%|‚ñè         | 4750/200000 [1:41:34<66:46:51,  1.23s/it, loss=0.0210, lr=2.37e-05, step=4750]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4751/200000 [1:41:36<69:10:32,  1.28s/it, loss=0.0210, lr=2.37e-05, step=4750]Training:   2%|‚ñè         | 4751/200000 [1:41:36<69:10:32,  1.28s/it, loss=0.0243, lr=2.38e-05, step=4751]Training:   2%|‚ñè         | 4752/200000 [1:41:37<65:53:52,  1.22s/it, loss=0.0243, lr=2.38e-05, step=4751]Training:   2%|‚ñè         | 4752/200000 [1:41:37<65:53:52,  1.22s/it, loss=0.0159, lr=2.38e-05, step=4752]Training:   2%|‚ñè         | 4753/200000 [1:41:38<69:28:38,  1.28s/it, loss=0.0159, lr=2.38e-05, step=4752]Training:   2%|‚ñè         | 4753/200000 [1:41:38<69:28:38,  1.28s/it, loss=0.0251, lr=2.38e-05, step=4753]Training:   2%|‚ñè         | 4754/200000 [1:41:39<72:02:40,  1.33s/it, loss=0.0251, lr=2.38e-05, step=4753]Training:   2%|‚ñè         | 4754/200000 [1:41:39<72:02:40,  1.33s/it, loss=0.0228, lr=2.38e-05, step=4754]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4755/200000 [1:41:41<74:06:11,  1.37s/it, loss=0.0228, lr=2.38e-05, step=4754]Training:   2%|‚ñè         | 4755/200000 [1:41:41<74:06:11,  1.37s/it, loss=0.0125, lr=2.38e-05, step=4755]Training:   2%|‚ñè         | 4756/200000 [1:41:42<75:19:16,  1.39s/it, loss=0.0125, lr=2.38e-05, step=4755]Training:   2%|‚ñè         | 4756/200000 [1:41:42<75:19:16,  1.39s/it, loss=0.0204, lr=2.38e-05, step=4756]Training:   2%|‚ñè         | 4757/200000 [1:41:43<70:13:17,  1.29s/it, loss=0.0204, lr=2.38e-05, step=4756]Training:   2%|‚ñè         | 4757/200000 [1:41:43<70:13:17,  1.29s/it, loss=0.0316, lr=2.38e-05, step=4757]Training:   2%|‚ñè         | 4758/200000 [1:41:45<66:39:16,  1.23s/it, loss=0.0316, lr=2.38e-05, step=4757]Training:   2%|‚ñè         | 4758/200000 [1:41:45<66:39:16,  1.23s/it, loss=0.0197, lr=2.38e-05, step=4758]Training:   2%|‚ñè         | 4759/200000 [1:41:46<68:29:43,  1.26s/it, loss=0.0197, lr=2.38e-05, step=4758]Training:   2%|‚ñè         | 4759/200000 [1:41:46<68:29:43,  1.26s/it, loss=0.0207, lr=2.38e-05, step=4759]Training:   2%|‚ñè         | 4760/200000 [1:41:47<70:43:01,  1.30s/it, loss=0.0207, lr=2.38e-05, step=4759]Training:   2%|‚ñè         | 4760/200000 [1:41:47<70:43:01,  1.30s/it, loss=0.0193, lr=2.38e-05, step=4760]Training:   2%|‚ñè         | 4761/200000 [1:41:48<67:00:40,  1.24s/it, loss=0.0193, lr=2.38e-05, step=4760]Training:   2%|‚ñè         | 4761/200000 [1:41:48<67:00:40,  1.24s/it, loss=0.0300, lr=2.38e-05, step=4761]Training:   2%|‚ñè         | 4762/200000 [1:41:50<70:04:33,  1.29s/it, loss=0.0300, lr=2.38e-05, step=4761]Training:   2%|‚ñè         | 4762/200000 [1:41:50<70:04:33,  1.29s/it, loss=0.0174, lr=2.38e-05, step=4762]Training:   2%|‚ñè         | 4763/200000 [1:41:51<71:06:08,  1.31s/it, loss=0.0174, lr=2.38e-05, step=4762]Training:   2%|‚ñè         | 4763/200000 [1:41:51<71:06:08,  1.31s/it, loss=0.0259, lr=2.38e-05, step=4763]Training:   2%|‚ñè         | 4764/200000 [1:41:53<72:21:35,  1.33s/it, loss=0.0259, lr=2.38e-05, step=4763]Training:   2%|‚ñè         | 4764/200000 [1:41:53<72:21:35,  1.33s/it, loss=0.0221, lr=2.38e-05, step=4764]Training:   2%|‚ñè         | 4765/200000 [1:41:54<68:09:29,  1.26s/it, loss=0.0221, lr=2.38e-05, step=4764]Training:   2%|‚ñè         | 4765/200000 [1:41:54<68:09:29,  1.26s/it, loss=0.0329, lr=2.38e-05, step=4765]Training:   2%|‚ñè         | 4766/200000 [1:41:55<71:48:36,  1.32s/it, loss=0.0329, lr=2.38e-05, step=4765]Training:   2%|‚ñè         | 4766/200000 [1:41:55<71:48:36,  1.32s/it, loss=0.0222, lr=2.38e-05, step=4766]Training:   2%|‚ñè         | 4767/200000 [1:41:57<73:55:57,  1.36s/it, loss=0.0222, lr=2.38e-05, step=4766]Training:   2%|‚ñè         | 4767/200000 [1:41:57<73:55:57,  1.36s/it, loss=0.0204, lr=2.38e-05, step=4767]Training:   2%|‚ñè         | 4768/200000 [1:41:58<69:16:08,  1.28s/it, loss=0.0204, lr=2.38e-05, step=4767]Training:   2%|‚ñè         | 4768/200000 [1:41:58<69:16:08,  1.28s/it, loss=0.0196, lr=2.38e-05, step=4768]Training:   2%|‚ñè         | 4769/200000 [1:41:59<66:00:58,  1.22s/it, loss=0.0196, lr=2.38e-05, step=4768]Training:   2%|‚ñè         | 4769/200000 [1:41:59<66:00:58,  1.22s/it, loss=0.0111, lr=2.38e-05, step=4769]Training:   2%|‚ñè         | 4770/200000 [1:42:00<69:51:03,  1.29s/it, loss=0.0111, lr=2.38e-05, step=4769]Training:   2%|‚ñè         | 4770/200000 [1:42:00<69:51:03,  1.29s/it, loss=0.0125, lr=2.38e-05, step=4770]Training:   2%|‚ñè         | 4771/200000 [1:42:01<66:22:39,  1.22s/it, loss=0.0125, lr=2.38e-05, step=4770]Training:   2%|‚ñè         | 4771/200000 [1:42:01<66:22:39,  1.22s/it, loss=0.0274, lr=2.39e-05, step=4771]Training:   2%|‚ñè         | 4772/200000 [1:42:03<68:35:04,  1.26s/it, loss=0.0274, lr=2.39e-05, step=4771]Training:   2%|‚ñè         | 4772/200000 [1:42:03<68:35:04,  1.26s/it, loss=0.0136, lr=2.39e-05, step=4772]Training:   2%|‚ñè         | 4773/200000 [1:42:04<65:28:57,  1.21s/it, loss=0.0136, lr=2.39e-05, step=4772]Training:   2%|‚ñè         | 4773/200000 [1:42:04<65:28:57,  1.21s/it, loss=0.0214, lr=2.39e-05, step=4773]Training:   2%|‚ñè         | 4774/200000 [1:42:05<68:13:18,  1.26s/it, loss=0.0214, lr=2.39e-05, step=4773]Training:   2%|‚ñè         | 4774/200000 [1:42:05<68:13:18,  1.26s/it, loss=0.0127, lr=2.39e-05, step=4774]Training:   2%|‚ñè         | 4775/200000 [1:42:06<71:32:24,  1.32s/it, loss=0.0127, lr=2.39e-05, step=4774]Training:   2%|‚ñè         | 4775/200000 [1:42:06<71:32:24,  1.32s/it, loss=0.0270, lr=2.39e-05, step=4775]Training:   2%|‚ñè         | 4776/200000 [1:42:08<73:55:08,  1.36s/it, loss=0.0270, lr=2.39e-05, step=4775]Training:   2%|‚ñè         | 4776/200000 [1:42:08<73:55:08,  1.36s/it, loss=0.0183, lr=2.39e-05, step=4776]Training:   2%|‚ñè         | 4777/200000 [1:42:09<74:17:57,  1.37s/it, loss=0.0183, lr=2.39e-05, step=4776]Training:   2%|‚ñè         | 4777/200000 [1:42:09<74:17:57,  1.37s/it, loss=0.0144, lr=2.39e-05, step=4777]Training:   2%|‚ñè         | 4778/200000 [1:42:10<69:31:17,  1.28s/it, loss=0.0144, lr=2.39e-05, step=4777]Training:   2%|‚ñè         | 4778/200000 [1:42:10<69:31:17,  1.28s/it, loss=0.0135, lr=2.39e-05, step=4778]Training:   2%|‚ñè         | 4779/200000 [1:42:11<66:08:15,  1.22s/it, loss=0.0135, lr=2.39e-05, step=4778]Training:   2%|‚ñè         | 4779/200000 [1:42:11<66:08:15,  1.22s/it, loss=0.0218, lr=2.39e-05, step=4779]Training:   2%|‚ñè         | 4780/200000 [1:42:13<67:58:54,  1.25s/it, loss=0.0218, lr=2.39e-05, step=4779]Training:   2%|‚ñè         | 4780/200000 [1:42:13<67:58:54,  1.25s/it, loss=0.0187, lr=2.39e-05, step=4780]Training:   2%|‚ñè         | 4781/200000 [1:42:14<69:24:33,  1.28s/it, loss=0.0187, lr=2.39e-05, step=4780]Training:   2%|‚ñè         | 4781/200000 [1:42:14<69:24:33,  1.28s/it, loss=0.0131, lr=2.39e-05, step=4781]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4782/200000 [1:42:15<66:06:23,  1.22s/it, loss=0.0131, lr=2.39e-05, step=4781]Training:   2%|‚ñè         | 4782/200000 [1:42:15<66:06:23,  1.22s/it, loss=0.0156, lr=2.39e-05, step=4782]Training:   2%|‚ñè         | 4783/200000 [1:42:17<68:09:11,  1.26s/it, loss=0.0156, lr=2.39e-05, step=4782]Training:   2%|‚ñè         | 4783/200000 [1:42:17<68:09:11,  1.26s/it, loss=0.0326, lr=2.39e-05, step=4783]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4784/200000 [1:42:18<65:12:26,  1.20s/it, loss=0.0326, lr=2.39e-05, step=4783]Training:   2%|‚ñè         | 4784/200000 [1:42:18<65:12:26,  1.20s/it, loss=0.0304, lr=2.39e-05, step=4784]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4785/200000 [1:42:19<69:23:09,  1.28s/it, loss=0.0304, lr=2.39e-05, step=4784]Training:   2%|‚ñè         | 4785/200000 [1:42:19<69:23:09,  1.28s/it, loss=0.0167, lr=2.39e-05, step=4785]Training:   2%|‚ñè         | 4786/200000 [1:42:20<66:05:54,  1.22s/it, loss=0.0167, lr=2.39e-05, step=4785]Training:   2%|‚ñè         | 4786/200000 [1:42:20<66:05:54,  1.22s/it, loss=0.0110, lr=2.39e-05, step=4786]Training:   2%|‚ñè         | 4787/200000 [1:42:22<69:17:47,  1.28s/it, loss=0.0110, lr=2.39e-05, step=4786]Training:   2%|‚ñè         | 4787/200000 [1:42:22<69:17:47,  1.28s/it, loss=0.0175, lr=2.39e-05, step=4787]Training:   2%|‚ñè         | 4788/200000 [1:42:23<72:03:30,  1.33s/it, loss=0.0175, lr=2.39e-05, step=4787]Training:   2%|‚ñè         | 4788/200000 [1:42:23<72:03:30,  1.33s/it, loss=0.0208, lr=2.39e-05, step=4788]Training:   2%|‚ñè         | 4789/200000 [1:42:24<67:55:59,  1.25s/it, loss=0.0208, lr=2.39e-05, step=4788]Training:   2%|‚ñè         | 4789/200000 [1:42:24<67:55:59,  1.25s/it, loss=0.0291, lr=2.39e-05, step=4789]Training:   2%|‚ñè         | 4790/200000 [1:42:25<65:03:41,  1.20s/it, loss=0.0291, lr=2.39e-05, step=4789]Training:   2%|‚ñè         | 4790/200000 [1:42:25<65:03:41,  1.20s/it, loss=0.0149, lr=2.39e-05, step=4790]Training:   2%|‚ñè         | 4791/200000 [1:42:27<68:21:41,  1.26s/it, loss=0.0149, lr=2.39e-05, step=4790]Training:   2%|‚ñè         | 4791/200000 [1:42:27<68:21:41,  1.26s/it, loss=0.0146, lr=2.40e-05, step=4791]Training:   2%|‚ñè         | 4792/200000 [1:42:28<71:00:37,  1.31s/it, loss=0.0146, lr=2.40e-05, step=4791]Training:   2%|‚ñè         | 4792/200000 [1:42:28<71:00:37,  1.31s/it, loss=0.0163, lr=2.40e-05, step=4792]Training:   2%|‚ñè         | 4793/200000 [1:42:29<67:12:36,  1.24s/it, loss=0.0163, lr=2.40e-05, step=4792]Training:   2%|‚ñè         | 4793/200000 [1:42:29<67:12:36,  1.24s/it, loss=0.0322, lr=2.40e-05, step=4793]Training:   2%|‚ñè         | 4794/200000 [1:42:31<70:48:05,  1.31s/it, loss=0.0322, lr=2.40e-05, step=4793]Training:   2%|‚ñè         | 4794/200000 [1:42:31<70:48:05,  1.31s/it, loss=0.0193, lr=2.40e-05, step=4794]Training:   2%|‚ñè         | 4795/200000 [1:42:32<71:53:03,  1.33s/it, loss=0.0193, lr=2.40e-05, step=4794]Training:   2%|‚ñè         | 4795/200000 [1:42:32<71:53:03,  1.33s/it, loss=0.0155, lr=2.40e-05, step=4795]Training:   2%|‚ñè         | 4796/200000 [1:42:33<72:24:39,  1.34s/it, loss=0.0155, lr=2.40e-05, step=4795]Training:   2%|‚ñè         | 4796/200000 [1:42:33<72:24:39,  1.34s/it, loss=0.0158, lr=2.40e-05, step=4796]Training:   2%|‚ñè         | 4797/200000 [1:42:34<68:10:45,  1.26s/it, loss=0.0158, lr=2.40e-05, step=4796]Training:   2%|‚ñè         | 4797/200000 [1:42:34<68:10:45,  1.26s/it, loss=0.0172, lr=2.40e-05, step=4797]Training:   2%|‚ñè         | 4798/200000 [1:42:36<72:01:30,  1.33s/it, loss=0.0172, lr=2.40e-05, step=4797]Training:   2%|‚ñè         | 4798/200000 [1:42:36<72:01:30,  1.33s/it, loss=0.0210, lr=2.40e-05, step=4798]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4799/200000 [1:42:37<75:15:11,  1.39s/it, loss=0.0210, lr=2.40e-05, step=4798]Training:   2%|‚ñè         | 4799/200000 [1:42:37<75:15:11,  1.39s/it, loss=0.0127, lr=2.40e-05, step=4799]Training:   2%|‚ñè         | 4800/200000 [1:42:38<70:06:37,  1.29s/it, loss=0.0127, lr=2.40e-05, step=4799]Training:   2%|‚ñè         | 4800/200000 [1:42:38<70:06:37,  1.29s/it, loss=0.0194, lr=2.40e-05, step=4800]00:35:53.363 [I] step=4800 loss=0.0217 lr=2.38e-05 grad_norm=0.46 time=128.1s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4801/200000 [1:42:40<66:34:18,  1.23s/it, loss=0.0194, lr=2.40e-05, step=4800]Training:   2%|‚ñè         | 4801/200000 [1:42:40<66:34:18,  1.23s/it, loss=0.0190, lr=2.40e-05, step=4801]Training:   2%|‚ñè         | 4802/200000 [1:42:41<70:28:47,  1.30s/it, loss=0.0190, lr=2.40e-05, step=4801]Training:   2%|‚ñè         | 4802/200000 [1:42:41<70:28:47,  1.30s/it, loss=0.0229, lr=2.40e-05, step=4802]Training:   2%|‚ñè         | 4803/200000 [1:42:42<66:48:52,  1.23s/it, loss=0.0229, lr=2.40e-05, step=4802]Training:   2%|‚ñè         | 4803/200000 [1:42:42<66:48:52,  1.23s/it, loss=0.0172, lr=2.40e-05, step=4803]Training:   2%|‚ñè         | 4804/200000 [1:42:43<69:01:14,  1.27s/it, loss=0.0172, lr=2.40e-05, step=4803]Training:   2%|‚ñè         | 4804/200000 [1:42:43<69:01:14,  1.27s/it, loss=0.0151, lr=2.40e-05, step=4804]Training:   2%|‚ñè         | 4805/200000 [1:42:45<65:47:15,  1.21s/it, loss=0.0151, lr=2.40e-05, step=4804]Training:   2%|‚ñè         | 4805/200000 [1:42:45<65:47:15,  1.21s/it, loss=0.0340, lr=2.40e-05, step=4805]Training:   2%|‚ñè         | 4806/200000 [1:42:46<69:25:45,  1.28s/it, loss=0.0340, lr=2.40e-05, step=4805]Training:   2%|‚ñè         | 4806/200000 [1:42:46<69:25:45,  1.28s/it, loss=0.0170, lr=2.40e-05, step=4806]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4807/200000 [1:42:47<72:43:18,  1.34s/it, loss=0.0170, lr=2.40e-05, step=4806]Training:   2%|‚ñè         | 4807/200000 [1:42:47<72:43:18,  1.34s/it, loss=0.0206, lr=2.40e-05, step=4807]Training:   2%|‚ñè         | 4808/200000 [1:42:49<74:45:56,  1.38s/it, loss=0.0206, lr=2.40e-05, step=4807]Training:   2%|‚ñè         | 4808/200000 [1:42:49<74:45:56,  1.38s/it, loss=0.0250, lr=2.40e-05, step=4808]Training:   2%|‚ñè         | 4809/200000 [1:42:50<75:26:00,  1.39s/it, loss=0.0250, lr=2.40e-05, step=4808]Training:   2%|‚ñè         | 4809/200000 [1:42:50<75:26:00,  1.39s/it, loss=0.0184, lr=2.40e-05, step=4809]Training:   2%|‚ñè         | 4810/200000 [1:42:51<70:18:38,  1.30s/it, loss=0.0184, lr=2.40e-05, step=4809]Training:   2%|‚ñè         | 4810/200000 [1:42:51<70:18:38,  1.30s/it, loss=0.0198, lr=2.40e-05, step=4810]Training:   2%|‚ñè         | 4811/200000 [1:42:52<66:42:52,  1.23s/it, loss=0.0198, lr=2.40e-05, step=4810]Training:   2%|‚ñè         | 4811/200000 [1:42:52<66:42:52,  1.23s/it, loss=0.0149, lr=2.41e-05, step=4811]Training:   2%|‚ñè         | 4812/200000 [1:42:54<68:46:41,  1.27s/it, loss=0.0149, lr=2.41e-05, step=4811]Training:   2%|‚ñè         | 4812/200000 [1:42:54<68:46:41,  1.27s/it, loss=0.0166, lr=2.41e-05, step=4812]Training:   2%|‚ñè         | 4813/200000 [1:42:55<71:27:42,  1.32s/it, loss=0.0166, lr=2.41e-05, step=4812]Training:   2%|‚ñè         | 4813/200000 [1:42:55<71:27:42,  1.32s/it, loss=0.0183, lr=2.41e-05, step=4813]Training:   2%|‚ñè         | 4814/200000 [1:42:56<67:32:08,  1.25s/it, loss=0.0183, lr=2.41e-05, step=4813]Training:   2%|‚ñè         | 4814/200000 [1:42:56<67:32:08,  1.25s/it, loss=0.0178, lr=2.41e-05, step=4814]Training:   2%|‚ñè         | 4815/200000 [1:42:58<71:04:09,  1.31s/it, loss=0.0178, lr=2.41e-05, step=4814]Training:   2%|‚ñè         | 4815/200000 [1:42:58<71:04:09,  1.31s/it, loss=0.0260, lr=2.41e-05, step=4815]Training:   2%|‚ñè         | 4816/200000 [1:42:59<71:47:00,  1.32s/it, loss=0.0260, lr=2.41e-05, step=4815]Training:   2%|‚ñè         | 4816/200000 [1:42:59<71:47:00,  1.32s/it, loss=0.0177, lr=2.41e-05, step=4816]Training:   2%|‚ñè         | 4817/200000 [1:43:01<72:16:45,  1.33s/it, loss=0.0177, lr=2.41e-05, step=4816]Training:   2%|‚ñè         | 4817/200000 [1:43:01<72:16:45,  1.33s/it, loss=0.0239, lr=2.41e-05, step=4817]Training:   2%|‚ñè         | 4818/200000 [1:43:02<68:04:56,  1.26s/it, loss=0.0239, lr=2.41e-05, step=4817]Training:   2%|‚ñè         | 4818/200000 [1:43:02<68:04:56,  1.26s/it, loss=0.0975, lr=2.41e-05, step=4818]Training:   2%|‚ñè         | 4819/200000 [1:43:03<71:40:56,  1.32s/it, loss=0.0975, lr=2.41e-05, step=4818]Training:   2%|‚ñè         | 4819/200000 [1:43:03<71:40:56,  1.32s/it, loss=0.0165, lr=2.41e-05, step=4819]Training:   2%|‚ñè         | 4820/200000 [1:43:05<73:52:12,  1.36s/it, loss=0.0165, lr=2.41e-05, step=4819]Training:   2%|‚ñè         | 4820/200000 [1:43:05<73:52:12,  1.36s/it, loss=0.0218, lr=2.41e-05, step=4820]Training:   2%|‚ñè         | 4821/200000 [1:43:06<69:09:40,  1.28s/it, loss=0.0218, lr=2.41e-05, step=4820]Training:   2%|‚ñè         | 4821/200000 [1:43:06<69:09:40,  1.28s/it, loss=0.0269, lr=2.41e-05, step=4821]Training:   2%|‚ñè         | 4822/200000 [1:43:07<65:54:10,  1.22s/it, loss=0.0269, lr=2.41e-05, step=4821]Training:   2%|‚ñè         | 4822/200000 [1:43:07<65:54:10,  1.22s/it, loss=0.0250, lr=2.41e-05, step=4822]Training:   2%|‚ñè         | 4823/200000 [1:43:08<69:56:29,  1.29s/it, loss=0.0250, lr=2.41e-05, step=4822]Training:   2%|‚ñè         | 4823/200000 [1:43:08<69:56:29,  1.29s/it, loss=0.0287, lr=2.41e-05, step=4823]Training:   2%|‚ñè         | 4824/200000 [1:43:09<66:25:39,  1.23s/it, loss=0.0287, lr=2.41e-05, step=4823]Training:   2%|‚ñè         | 4824/200000 [1:43:09<66:25:39,  1.23s/it, loss=0.0251, lr=2.41e-05, step=4824]Training:   2%|‚ñè         | 4825/200000 [1:43:11<68:42:02,  1.27s/it, loss=0.0251, lr=2.41e-05, step=4824]Training:   2%|‚ñè         | 4825/200000 [1:43:11<68:42:02,  1.27s/it, loss=0.0136, lr=2.41e-05, step=4825]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4826/200000 [1:43:12<65:34:53,  1.21s/it, loss=0.0136, lr=2.41e-05, step=4825]Training:   2%|‚ñè         | 4826/200000 [1:43:12<65:34:53,  1.21s/it, loss=0.0625, lr=2.41e-05, step=4826]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4827/200000 [1:43:13<68:16:31,  1.26s/it, loss=0.0625, lr=2.41e-05, step=4826]Training:   2%|‚ñè         | 4827/200000 [1:43:13<68:16:31,  1.26s/it, loss=0.0139, lr=2.41e-05, step=4827]Training:   2%|‚ñè         | 4828/200000 [1:43:14<70:51:18,  1.31s/it, loss=0.0139, lr=2.41e-05, step=4827]Training:   2%|‚ñè         | 4828/200000 [1:43:14<70:51:18,  1.31s/it, loss=0.0278, lr=2.41e-05, step=4828]Training:   2%|‚ñè         | 4829/200000 [1:43:16<73:23:02,  1.35s/it, loss=0.0278, lr=2.41e-05, step=4828]Training:   2%|‚ñè         | 4829/200000 [1:43:16<73:23:02,  1.35s/it, loss=0.0211, lr=2.41e-05, step=4829]Training:   2%|‚ñè         | 4830/200000 [1:43:17<74:36:14,  1.38s/it, loss=0.0211, lr=2.41e-05, step=4829]Training:   2%|‚ñè         | 4830/200000 [1:43:17<74:36:14,  1.38s/it, loss=0.0200, lr=2.41e-05, step=4830]Training:   2%|‚ñè         | 4831/200000 [1:43:18<69:41:56,  1.29s/it, loss=0.0200, lr=2.41e-05, step=4830]Training:   2%|‚ñè         | 4831/200000 [1:43:18<69:41:56,  1.29s/it, loss=0.0142, lr=2.42e-05, step=4831]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4832/200000 [1:43:20<66:15:59,  1.22s/it, loss=0.0142, lr=2.42e-05, step=4831]Training:   2%|‚ñè         | 4832/200000 [1:43:20<66:15:59,  1.22s/it, loss=0.0205, lr=2.42e-05, step=4832]Training:   2%|‚ñè         | 4833/200000 [1:43:21<68:55:08,  1.27s/it, loss=0.0205, lr=2.42e-05, step=4832]Training:   2%|‚ñè         | 4833/200000 [1:43:21<68:55:08,  1.27s/it, loss=0.0195, lr=2.42e-05, step=4833]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4834/200000 [1:43:22<69:57:01,  1.29s/it, loss=0.0195, lr=2.42e-05, step=4833]Training:   2%|‚ñè         | 4834/200000 [1:43:22<69:57:01,  1.29s/it, loss=0.0164, lr=2.42e-05, step=4834]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4835/200000 [1:43:23<66:27:38,  1.23s/it, loss=0.0164, lr=2.42e-05, step=4834]Training:   2%|‚ñè         | 4835/200000 [1:43:23<66:27:38,  1.23s/it, loss=0.0185, lr=2.42e-05, step=4835]Training:   2%|‚ñè         | 4836/200000 [1:43:25<68:49:59,  1.27s/it, loss=0.0185, lr=2.42e-05, step=4835]Training:   2%|‚ñè         | 4836/200000 [1:43:25<68:49:59,  1.27s/it, loss=0.0197, lr=2.42e-05, step=4836]Training:   2%|‚ñè         | 4837/200000 [1:43:26<65:38:57,  1.21s/it, loss=0.0197, lr=2.42e-05, step=4836]Training:   2%|‚ñè         | 4837/200000 [1:43:26<65:38:57,  1.21s/it, loss=0.0172, lr=2.42e-05, step=4837]Training:   2%|‚ñè         | 4838/200000 [1:43:27<69:40:09,  1.29s/it, loss=0.0172, lr=2.42e-05, step=4837]Training:   2%|‚ñè         | 4838/200000 [1:43:27<69:40:09,  1.29s/it, loss=0.0299, lr=2.42e-05, step=4838]Training:   2%|‚ñè         | 4839/200000 [1:43:28<66:12:42,  1.22s/it, loss=0.0299, lr=2.42e-05, step=4838]Training:   2%|‚ñè         | 4839/200000 [1:43:28<66:12:42,  1.22s/it, loss=0.0337, lr=2.42e-05, step=4839]Training:   2%|‚ñè         | 4840/200000 [1:43:30<69:22:22,  1.28s/it, loss=0.0337, lr=2.42e-05, step=4839]Training:   2%|‚ñè         | 4840/200000 [1:43:30<69:22:22,  1.28s/it, loss=0.0165, lr=2.42e-05, step=4840]Training:   2%|‚ñè         | 4841/200000 [1:43:31<71:24:01,  1.32s/it, loss=0.0165, lr=2.42e-05, step=4840]Training:   2%|‚ñè         | 4841/200000 [1:43:31<71:24:01,  1.32s/it, loss=0.0203, lr=2.42e-05, step=4841]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4842/200000 [1:43:32<67:26:56,  1.24s/it, loss=0.0203, lr=2.42e-05, step=4841]Training:   2%|‚ñè         | 4842/200000 [1:43:32<67:26:56,  1.24s/it, loss=0.0156, lr=2.42e-05, step=4842]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4843/200000 [1:43:33<64:40:22,  1.19s/it, loss=0.0156, lr=2.42e-05, step=4842]Training:   2%|‚ñè         | 4843/200000 [1:43:33<64:40:22,  1.19s/it, loss=0.0183, lr=2.42e-05, step=4843]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4844/200000 [1:43:35<67:53:30,  1.25s/it, loss=0.0183, lr=2.42e-05, step=4843]Training:   2%|‚ñè         | 4844/200000 [1:43:35<67:53:30,  1.25s/it, loss=0.0251, lr=2.42e-05, step=4844]Training:   2%|‚ñè         | 4845/200000 [1:43:36<70:58:01,  1.31s/it, loss=0.0251, lr=2.42e-05, step=4844]Training:   2%|‚ñè         | 4845/200000 [1:43:36<70:58:01,  1.31s/it, loss=0.0173, lr=2.42e-05, step=4845]Training:   2%|‚ñè         | 4846/200000 [1:43:37<67:09:53,  1.24s/it, loss=0.0173, lr=2.42e-05, step=4845]Training:   2%|‚ñè         | 4846/200000 [1:43:37<67:09:53,  1.24s/it, loss=0.0181, lr=2.42e-05, step=4846]Training:   2%|‚ñè         | 4847/200000 [1:43:39<70:36:14,  1.30s/it, loss=0.0181, lr=2.42e-05, step=4846]Training:   2%|‚ñè         | 4847/200000 [1:43:39<70:36:14,  1.30s/it, loss=0.0215, lr=2.42e-05, step=4847]Training:   2%|‚ñè         | 4848/200000 [1:43:40<71:41:38,  1.32s/it, loss=0.0215, lr=2.42e-05, step=4847]Training:   2%|‚ñè         | 4848/200000 [1:43:40<71:41:38,  1.32s/it, loss=0.0274, lr=2.42e-05, step=4848]Training:   2%|‚ñè         | 4849/200000 [1:43:41<72:15:02,  1.33s/it, loss=0.0274, lr=2.42e-05, step=4848]Training:   2%|‚ñè         | 4849/200000 [1:43:41<72:15:02,  1.33s/it, loss=0.0155, lr=2.42e-05, step=4849]Training:   2%|‚ñè         | 4850/200000 [1:43:42<68:02:57,  1.26s/it, loss=0.0155, lr=2.42e-05, step=4849]Training:   2%|‚ñè         | 4850/200000 [1:43:42<68:02:57,  1.26s/it, loss=0.0225, lr=2.42e-05, step=4850]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4851/200000 [1:43:44<71:58:44,  1.33s/it, loss=0.0225, lr=2.42e-05, step=4850]Training:   2%|‚ñè         | 4851/200000 [1:43:44<71:58:44,  1.33s/it, loss=0.0169, lr=2.43e-05, step=4851]Training:   2%|‚ñè         | 4852/200000 [1:43:45<74:33:20,  1.38s/it, loss=0.0169, lr=2.43e-05, step=4851]Training:   2%|‚ñè         | 4852/200000 [1:43:45<74:33:20,  1.38s/it, loss=0.0175, lr=2.43e-05, step=4852]Training:   2%|‚ñè         | 4853/200000 [1:43:46<69:40:50,  1.29s/it, loss=0.0175, lr=2.43e-05, step=4852]Training:   2%|‚ñè         | 4853/200000 [1:43:46<69:40:50,  1.29s/it, loss=0.0216, lr=2.43e-05, step=4853]Training:   2%|‚ñè         | 4854/200000 [1:43:48<66:17:15,  1.22s/it, loss=0.0216, lr=2.43e-05, step=4853]Training:   2%|‚ñè         | 4854/200000 [1:43:48<66:17:15,  1.22s/it, loss=0.0179, lr=2.43e-05, step=4854]Training:   2%|‚ñè         | 4855/200000 [1:43:49<70:16:13,  1.30s/it, loss=0.0179, lr=2.43e-05, step=4854]Training:   2%|‚ñè         | 4855/200000 [1:43:49<70:16:13,  1.30s/it, loss=0.0378, lr=2.43e-05, step=4855]Training:   2%|‚ñè         | 4856/200000 [1:43:50<66:38:26,  1.23s/it, loss=0.0378, lr=2.43e-05, step=4855]Training:   2%|‚ñè         | 4856/200000 [1:43:50<66:38:26,  1.23s/it, loss=0.0135, lr=2.43e-05, step=4856]Training:   2%|‚ñè         | 4857/200000 [1:43:51<68:22:31,  1.26s/it, loss=0.0135, lr=2.43e-05, step=4856]Training:   2%|‚ñè         | 4857/200000 [1:43:51<68:22:31,  1.26s/it, loss=0.0259, lr=2.43e-05, step=4857]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4858/200000 [1:43:52<65:19:30,  1.21s/it, loss=0.0259, lr=2.43e-05, step=4857]Training:   2%|‚ñè         | 4858/200000 [1:43:52<65:19:30,  1.21s/it, loss=0.0164, lr=2.43e-05, step=4858]Training:   2%|‚ñè         | 4859/200000 [1:43:54<69:01:30,  1.27s/it, loss=0.0164, lr=2.43e-05, step=4858]Training:   2%|‚ñè         | 4859/200000 [1:43:54<69:01:30,  1.27s/it, loss=0.0217, lr=2.43e-05, step=4859]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4860/200000 [1:43:55<71:44:27,  1.32s/it, loss=0.0217, lr=2.43e-05, step=4859]Training:   2%|‚ñè         | 4860/200000 [1:43:55<71:44:27,  1.32s/it, loss=0.0131, lr=2.43e-05, step=4860]Training:   2%|‚ñè         | 4861/200000 [1:43:57<74:04:21,  1.37s/it, loss=0.0131, lr=2.43e-05, step=4860]Training:   2%|‚ñè         | 4861/200000 [1:43:57<74:04:21,  1.37s/it, loss=0.0133, lr=2.43e-05, step=4861]Training:   2%|‚ñè         | 4862/200000 [1:43:58<75:12:53,  1.39s/it, loss=0.0133, lr=2.43e-05, step=4861]Training:   2%|‚ñè         | 4862/200000 [1:43:58<75:12:53,  1.39s/it, loss=0.0188, lr=2.43e-05, step=4862]Training:   2%|‚ñè         | 4863/200000 [1:43:59<70:06:38,  1.29s/it, loss=0.0188, lr=2.43e-05, step=4862]Training:   2%|‚ñè         | 4863/200000 [1:43:59<70:06:38,  1.29s/it, loss=0.0235, lr=2.43e-05, step=4863]Training:   2%|‚ñè         | 4864/200000 [1:44:00<66:31:52,  1.23s/it, loss=0.0235, lr=2.43e-05, step=4863]Training:   2%|‚ñè         | 4864/200000 [1:44:00<66:31:52,  1.23s/it, loss=0.0211, lr=2.43e-05, step=4864]Training:   2%|‚ñè         | 4865/200000 [1:44:02<68:31:57,  1.26s/it, loss=0.0211, lr=2.43e-05, step=4864]Training:   2%|‚ñè         | 4865/200000 [1:44:02<68:31:57,  1.26s/it, loss=0.0221, lr=2.43e-05, step=4865]Training:   2%|‚ñè         | 4866/200000 [1:44:03<70:38:18,  1.30s/it, loss=0.0221, lr=2.43e-05, step=4865]Training:   2%|‚ñè         | 4866/200000 [1:44:03<70:38:18,  1.30s/it, loss=0.0180, lr=2.43e-05, step=4866]Training:   2%|‚ñè         | 4867/200000 [1:44:04<66:53:57,  1.23s/it, loss=0.0180, lr=2.43e-05, step=4866]Training:   2%|‚ñè         | 4867/200000 [1:44:04<66:53:57,  1.23s/it, loss=0.0546, lr=2.43e-05, step=4867]Training:   2%|‚ñè         | 4868/200000 [1:44:06<69:38:49,  1.28s/it, loss=0.0546, lr=2.43e-05, step=4867]Training:   2%|‚ñè         | 4868/200000 [1:44:06<69:38:49,  1.28s/it, loss=0.0161, lr=2.43e-05, step=4868]Training:   2%|‚ñè         | 4869/200000 [1:44:07<70:43:53,  1.30s/it, loss=0.0161, lr=2.43e-05, step=4868]Training:   2%|‚ñè         | 4869/200000 [1:44:07<70:43:53,  1.30s/it, loss=0.0159, lr=2.43e-05, step=4869]Training:   2%|‚ñè         | 4870/200000 [1:44:08<72:16:18,  1.33s/it, loss=0.0159, lr=2.43e-05, step=4869]Training:   2%|‚ñè         | 4870/200000 [1:44:08<72:16:18,  1.33s/it, loss=0.0119, lr=2.43e-05, step=4870]Training:   2%|‚ñè         | 4871/200000 [1:44:09<68:01:35,  1.26s/it, loss=0.0119, lr=2.43e-05, step=4870]Training:   2%|‚ñè         | 4871/200000 [1:44:09<68:01:35,  1.26s/it, loss=0.0185, lr=2.44e-05, step=4871]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4872/200000 [1:44:11<71:34:31,  1.32s/it, loss=0.0185, lr=2.44e-05, step=4871]Training:   2%|‚ñè         | 4872/200000 [1:44:11<71:34:31,  1.32s/it, loss=0.0249, lr=2.44e-05, step=4872]Training:   2%|‚ñè         | 4873/200000 [1:44:12<73:51:26,  1.36s/it, loss=0.0249, lr=2.44e-05, step=4872]Training:   2%|‚ñè         | 4873/200000 [1:44:12<73:51:26,  1.36s/it, loss=0.0132, lr=2.44e-05, step=4873]Training:   2%|‚ñè         | 4874/200000 [1:44:13<69:10:47,  1.28s/it, loss=0.0132, lr=2.44e-05, step=4873]Training:   2%|‚ñè         | 4874/200000 [1:44:13<69:10:47,  1.28s/it, loss=0.0157, lr=2.44e-05, step=4874]Training:   2%|‚ñè         | 4875/200000 [1:44:15<65:53:54,  1.22s/it, loss=0.0157, lr=2.44e-05, step=4874]Training:   2%|‚ñè         | 4875/200000 [1:44:15<65:53:54,  1.22s/it, loss=0.0162, lr=2.44e-05, step=4875]Training:   2%|‚ñè         | 4876/200000 [1:44:16<69:47:15,  1.29s/it, loss=0.0162, lr=2.44e-05, step=4875]Training:   2%|‚ñè         | 4876/200000 [1:44:16<69:47:15,  1.29s/it, loss=0.0153, lr=2.44e-05, step=4876]Training:   2%|‚ñè         | 4877/200000 [1:44:17<66:17:31,  1.22s/it, loss=0.0153, lr=2.44e-05, step=4876]Training:   2%|‚ñè         | 4877/200000 [1:44:17<66:17:31,  1.22s/it, loss=0.0147, lr=2.44e-05, step=4877]Training:   2%|‚ñè         | 4878/200000 [1:44:18<67:47:22,  1.25s/it, loss=0.0147, lr=2.44e-05, step=4877]Training:   2%|‚ñè         | 4878/200000 [1:44:18<67:47:22,  1.25s/it, loss=0.0222, lr=2.44e-05, step=4878]Training:   2%|‚ñè         | 4879/200000 [1:44:19<64:53:57,  1.20s/it, loss=0.0222, lr=2.44e-05, step=4878]Training:   2%|‚ñè         | 4879/200000 [1:44:19<64:53:57,  1.20s/it, loss=0.0246, lr=2.44e-05, step=4879]Training:   2%|‚ñè         | 4880/200000 [1:44:21<67:47:28,  1.25s/it, loss=0.0246, lr=2.44e-05, step=4879]Training:   2%|‚ñè         | 4880/200000 [1:44:21<67:47:28,  1.25s/it, loss=0.0127, lr=2.44e-05, step=4880]Training:   2%|‚ñè         | 4881/200000 [1:44:22<70:27:54,  1.30s/it, loss=0.0127, lr=2.44e-05, step=4880]Training:   2%|‚ñè         | 4881/200000 [1:44:22<70:27:54,  1.30s/it, loss=0.0139, lr=2.44e-05, step=4881]Training:   2%|‚ñè         | 4882/200000 [1:44:24<72:49:27,  1.34s/it, loss=0.0139, lr=2.44e-05, step=4881]Training:   2%|‚ñè         | 4882/200000 [1:44:24<72:49:27,  1.34s/it, loss=0.0258, lr=2.44e-05, step=4882]Training:   2%|‚ñè         | 4883/200000 [1:44:25<74:14:45,  1.37s/it, loss=0.0258, lr=2.44e-05, step=4882]Training:   2%|‚ñè         | 4883/200000 [1:44:25<74:14:45,  1.37s/it, loss=0.0198, lr=2.44e-05, step=4883]Training:   2%|‚ñè         | 4884/200000 [1:44:26<69:26:40,  1.28s/it, loss=0.0198, lr=2.44e-05, step=4883]Training:   2%|‚ñè         | 4884/200000 [1:44:26<69:26:40,  1.28s/it, loss=0.0153, lr=2.44e-05, step=4884]Training:   2%|‚ñè         | 4885/200000 [1:44:27<66:03:16,  1.22s/it, loss=0.0153, lr=2.44e-05, step=4884]Training:   2%|‚ñè         | 4885/200000 [1:44:27<66:03:16,  1.22s/it, loss=0.0194, lr=2.44e-05, step=4885]Training:   2%|‚ñè         | 4886/200000 [1:44:29<67:22:30,  1.24s/it, loss=0.0194, lr=2.44e-05, step=4885]Training:   2%|‚ñè         | 4886/200000 [1:44:29<67:22:30,  1.24s/it, loss=0.0229, lr=2.44e-05, step=4886]Training:   2%|‚ñè         | 4887/200000 [1:44:30<68:26:43,  1.26s/it, loss=0.0229, lr=2.44e-05, step=4886]Training:   2%|‚ñè         | 4887/200000 [1:44:30<68:26:43,  1.26s/it, loss=0.0089, lr=2.44e-05, step=4887]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4888/200000 [1:44:31<65:22:41,  1.21s/it, loss=0.0089, lr=2.44e-05, step=4887]Training:   2%|‚ñè         | 4888/200000 [1:44:31<65:22:41,  1.21s/it, loss=0.0212, lr=2.44e-05, step=4888]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4889/200000 [1:44:32<68:17:14,  1.26s/it, loss=0.0212, lr=2.44e-05, step=4888]Training:   2%|‚ñè         | 4889/200000 [1:44:32<68:17:14,  1.26s/it, loss=0.0200, lr=2.44e-05, step=4889]Training:   2%|‚ñè         | 4890/200000 [1:44:33<65:17:21,  1.20s/it, loss=0.0200, lr=2.44e-05, step=4889]Training:   2%|‚ñè         | 4890/200000 [1:44:33<65:17:21,  1.20s/it, loss=0.0251, lr=2.44e-05, step=4890]Training:   2%|‚ñè         | 4891/200000 [1:44:35<69:25:30,  1.28s/it, loss=0.0251, lr=2.44e-05, step=4890]Training:   2%|‚ñè         | 4891/200000 [1:44:35<69:25:30,  1.28s/it, loss=0.0198, lr=2.45e-05, step=4891]Training:   2%|‚ñè         | 4892/200000 [1:44:36<66:05:11,  1.22s/it, loss=0.0198, lr=2.45e-05, step=4891]Training:   2%|‚ñè         | 4892/200000 [1:44:36<66:05:11,  1.22s/it, loss=0.0224, lr=2.45e-05, step=4892]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4893/200000 [1:44:37<69:11:00,  1.28s/it, loss=0.0224, lr=2.45e-05, step=4892]Training:   2%|‚ñè         | 4893/200000 [1:44:37<69:11:00,  1.28s/it, loss=0.0216, lr=2.45e-05, step=4893]Training:   2%|‚ñè         | 4894/200000 [1:44:39<71:16:25,  1.32s/it, loss=0.0216, lr=2.45e-05, step=4893]Training:   2%|‚ñè         | 4894/200000 [1:44:39<71:16:25,  1.32s/it, loss=0.0189, lr=2.45e-05, step=4894]Training:   2%|‚ñè         | 4895/200000 [1:44:40<67:19:09,  1.24s/it, loss=0.0189, lr=2.45e-05, step=4894]Training:   2%|‚ñè         | 4895/200000 [1:44:40<67:19:09,  1.24s/it, loss=0.0127, lr=2.45e-05, step=4895]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4896/200000 [1:44:41<64:38:14,  1.19s/it, loss=0.0127, lr=2.45e-05, step=4895]Training:   2%|‚ñè         | 4896/200000 [1:44:41<64:38:14,  1.19s/it, loss=0.0188, lr=2.45e-05, step=4896]Training:   2%|‚ñè         | 4897/200000 [1:44:42<67:12:18,  1.24s/it, loss=0.0188, lr=2.45e-05, step=4896]Training:   2%|‚ñè         | 4897/200000 [1:44:42<67:12:18,  1.24s/it, loss=0.0146, lr=2.45e-05, step=4897]Training:   2%|‚ñè         | 4898/200000 [1:44:44<69:44:18,  1.29s/it, loss=0.0146, lr=2.45e-05, step=4897]Training:   2%|‚ñè         | 4898/200000 [1:44:44<69:44:18,  1.29s/it, loss=0.0148, lr=2.45e-05, step=4898]Training:   2%|‚ñè         | 4899/200000 [1:44:45<66:16:04,  1.22s/it, loss=0.0148, lr=2.45e-05, step=4898]Training:   2%|‚ñè         | 4899/200000 [1:44:45<66:16:04,  1.22s/it, loss=0.0292, lr=2.45e-05, step=4899]Training:   2%|‚ñè         | 4900/200000 [1:44:46<70:08:29,  1.29s/it, loss=0.0292, lr=2.45e-05, step=4899]Training:   2%|‚ñè         | 4900/200000 [1:44:46<70:08:29,  1.29s/it, loss=0.0190, lr=2.45e-05, step=4900]00:38:01.378 [I] step=4900 loss=0.0214 lr=2.43e-05 grad_norm=0.45 time=128.0s                     (701675:train_pytorch.py:582)
Training:   2%|‚ñè         | 4901/200000 [1:44:48<71:23:04,  1.32s/it, loss=0.0190, lr=2.45e-05, step=4900]Training:   2%|‚ñè         | 4901/200000 [1:44:48<71:23:04,  1.32s/it, loss=0.0281, lr=2.45e-05, step=4901]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4902/200000 [1:44:49<72:04:05,  1.33s/it, loss=0.0281, lr=2.45e-05, step=4901]Training:   2%|‚ñè         | 4902/200000 [1:44:49<72:04:05,  1.33s/it, loss=0.0187, lr=2.45e-05, step=4902]Training:   2%|‚ñè         | 4903/200000 [1:44:50<67:54:33,  1.25s/it, loss=0.0187, lr=2.45e-05, step=4902]Training:   2%|‚ñè         | 4903/200000 [1:44:50<67:54:33,  1.25s/it, loss=0.0201, lr=2.45e-05, step=4903]Training:   2%|‚ñè         | 4904/200000 [1:44:52<71:58:30,  1.33s/it, loss=0.0201, lr=2.45e-05, step=4903]Training:   2%|‚ñè         | 4904/200000 [1:44:52<71:58:30,  1.33s/it, loss=0.0243, lr=2.45e-05, step=4904]Training:   2%|‚ñè         | 4905/200000 [1:44:53<74:18:11,  1.37s/it, loss=0.0243, lr=2.45e-05, step=4904]Training:   2%|‚ñè         | 4905/200000 [1:44:53<74:18:11,  1.37s/it, loss=0.0203, lr=2.45e-05, step=4905]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4906/200000 [1:44:54<69:29:41,  1.28s/it, loss=0.0203, lr=2.45e-05, step=4905]Training:   2%|‚ñè         | 4906/200000 [1:44:54<69:29:41,  1.28s/it, loss=0.0206, lr=2.45e-05, step=4906]Training:   2%|‚ñè         | 4907/200000 [1:44:55<66:07:29,  1.22s/it, loss=0.0206, lr=2.45e-05, step=4906]Training:   2%|‚ñè         | 4907/200000 [1:44:55<66:07:29,  1.22s/it, loss=0.0158, lr=2.45e-05, step=4907]Training:   2%|‚ñè         | 4908/200000 [1:44:57<70:24:25,  1.30s/it, loss=0.0158, lr=2.45e-05, step=4907]Training:   2%|‚ñè         | 4908/200000 [1:44:57<70:24:25,  1.30s/it, loss=0.0112, lr=2.45e-05, step=4908]Training:   2%|‚ñè         | 4909/200000 [1:44:58<66:43:51,  1.23s/it, loss=0.0112, lr=2.45e-05, step=4908]Training:   2%|‚ñè         | 4909/200000 [1:44:58<66:43:51,  1.23s/it, loss=0.0151, lr=2.45e-05, step=4909]Training:   2%|‚ñè         | 4910/200000 [1:44:59<68:24:22,  1.26s/it, loss=0.0151, lr=2.45e-05, step=4909]Training:   2%|‚ñè         | 4910/200000 [1:44:59<68:24:22,  1.26s/it, loss=0.0261, lr=2.45e-05, step=4910]Training:   2%|‚ñè         | 4911/200000 [1:45:00<65:19:24,  1.21s/it, loss=0.0261, lr=2.45e-05, step=4910]Training:   2%|‚ñè         | 4911/200000 [1:45:00<65:19:24,  1.21s/it, loss=0.0235, lr=2.46e-05, step=4911]Training:   2%|‚ñè         | 4912/200000 [1:45:02<69:02:49,  1.27s/it, loss=0.0235, lr=2.46e-05, step=4911]Training:   2%|‚ñè         | 4912/200000 [1:45:02<69:02:49,  1.27s/it, loss=0.0164, lr=2.46e-05, step=4912]Training:   2%|‚ñè         | 4913/200000 [1:45:03<71:38:03,  1.32s/it, loss=0.0164, lr=2.46e-05, step=4912]Training:   2%|‚ñè         | 4913/200000 [1:45:03<71:38:03,  1.32s/it, loss=0.0186, lr=2.46e-05, step=4913]Training:   2%|‚ñè         | 4914/200000 [1:45:04<74:03:41,  1.37s/it, loss=0.0186, lr=2.46e-05, step=4913]Training:   2%|‚ñè         | 4914/200000 [1:45:04<74:03:41,  1.37s/it, loss=0.0122, lr=2.46e-05, step=4914]Training:   2%|‚ñè         | 4915/200000 [1:45:06<74:58:20,  1.38s/it, loss=0.0122, lr=2.46e-05, step=4914]Training:   2%|‚ñè         | 4915/200000 [1:45:06<74:58:20,  1.38s/it, loss=0.0209, lr=2.46e-05, step=4915]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4916/200000 [1:45:07<69:58:52,  1.29s/it, loss=0.0209, lr=2.46e-05, step=4915]Training:   2%|‚ñè         | 4916/200000 [1:45:07<69:58:52,  1.29s/it, loss=0.0866, lr=2.46e-05, step=4916]Training:   2%|‚ñè         | 4917/200000 [1:45:08<66:24:36,  1.23s/it, loss=0.0866, lr=2.46e-05, step=4916]Training:   2%|‚ñè         | 4917/200000 [1:45:08<66:24:36,  1.23s/it, loss=0.0173, lr=2.46e-05, step=4917]Training:   2%|‚ñè         | 4918/200000 [1:45:09<68:12:17,  1.26s/it, loss=0.0173, lr=2.46e-05, step=4917]Training:   2%|‚ñè         | 4918/200000 [1:45:09<68:12:17,  1.26s/it, loss=0.0325, lr=2.46e-05, step=4918]Training:   2%|‚ñè         | 4919/200000 [1:45:11<70:25:29,  1.30s/it, loss=0.0325, lr=2.46e-05, step=4918]Training:   2%|‚ñè         | 4919/200000 [1:45:11<70:25:29,  1.30s/it, loss=0.0156, lr=2.46e-05, step=4919]Training:   2%|‚ñè         | 4920/200000 [1:45:12<66:48:02,  1.23s/it, loss=0.0156, lr=2.46e-05, step=4919]Training:   2%|‚ñè         | 4920/200000 [1:45:12<66:48:02,  1.23s/it, loss=0.0103, lr=2.46e-05, step=4920]Training:   2%|‚ñè         | 4921/200000 [1:45:13<70:27:04,  1.30s/it, loss=0.0103, lr=2.46e-05, step=4920]Training:   2%|‚ñè         | 4921/200000 [1:45:13<70:27:04,  1.30s/it, loss=0.0153, lr=2.46e-05, step=4921]Training:   2%|‚ñè         | 4922/200000 [1:45:15<71:15:40,  1.32s/it, loss=0.0153, lr=2.46e-05, step=4921]Training:   2%|‚ñè         | 4922/200000 [1:45:15<71:15:40,  1.32s/it, loss=0.0169, lr=2.46e-05, step=4922]Training:   2%|‚ñè         | 4923/200000 [1:45:16<72:22:31,  1.34s/it, loss=0.0169, lr=2.46e-05, step=4922]Training:   2%|‚ñè         | 4923/200000 [1:45:16<72:22:31,  1.34s/it, loss=0.0175, lr=2.46e-05, step=4923]Training:   2%|‚ñè         | 4924/200000 [1:45:17<68:06:24,  1.26s/it, loss=0.0175, lr=2.46e-05, step=4923]Training:   2%|‚ñè         | 4924/200000 [1:45:17<68:06:24,  1.26s/it, loss=0.0231, lr=2.46e-05, step=4924]Training:   2%|‚ñè         | 4925/200000 [1:45:19<71:25:49,  1.32s/it, loss=0.0231, lr=2.46e-05, step=4924]Training:   2%|‚ñè         | 4925/200000 [1:45:19<71:25:49,  1.32s/it, loss=0.0186, lr=2.46e-05, step=4925]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4926/200000 [1:45:20<73:49:35,  1.36s/it, loss=0.0186, lr=2.46e-05, step=4925]Training:   2%|‚ñè         | 4926/200000 [1:45:20<73:49:35,  1.36s/it, loss=0.0126, lr=2.46e-05, step=4926]Training:   2%|‚ñè         | 4927/200000 [1:45:21<69:08:06,  1.28s/it, loss=0.0126, lr=2.46e-05, step=4926]Training:   2%|‚ñè         | 4927/200000 [1:45:21<69:08:06,  1.28s/it, loss=0.0412, lr=2.46e-05, step=4927]Training:   2%|‚ñè         | 4928/200000 [1:45:22<65:54:15,  1.22s/it, loss=0.0412, lr=2.46e-05, step=4927]Training:   2%|‚ñè         | 4928/200000 [1:45:22<65:54:15,  1.22s/it, loss=0.0181, lr=2.46e-05, step=4928]Training:   2%|‚ñè         | 4929/200000 [1:45:24<69:50:41,  1.29s/it, loss=0.0181, lr=2.46e-05, step=4928]Training:   2%|‚ñè         | 4929/200000 [1:45:24<69:50:41,  1.29s/it, loss=0.0075, lr=2.46e-05, step=4929]Training:   2%|‚ñè         | 4930/200000 [1:45:25<66:20:30,  1.22s/it, loss=0.0075, lr=2.46e-05, step=4929]Training:   2%|‚ñè         | 4930/200000 [1:45:25<66:20:30,  1.22s/it, loss=0.0138, lr=2.46e-05, step=4930]Training:   2%|‚ñè         | 4931/200000 [1:45:26<68:20:41,  1.26s/it, loss=0.0138, lr=2.46e-05, step=4930]Training:   2%|‚ñè         | 4931/200000 [1:45:26<68:20:41,  1.26s/it, loss=0.0165, lr=2.47e-05, step=4931]Training:   2%|‚ñè         | 4932/200000 [1:45:27<65:19:55,  1.21s/it, loss=0.0165, lr=2.47e-05, step=4931]Training:   2%|‚ñè         | 4932/200000 [1:45:27<65:19:55,  1.21s/it, loss=0.0142, lr=2.47e-05, step=4932]Training:   2%|‚ñè         | 4933/200000 [1:45:28<67:47:04,  1.25s/it, loss=0.0142, lr=2.47e-05, step=4932]Training:   2%|‚ñè         | 4933/200000 [1:45:28<67:47:04,  1.25s/it, loss=0.0259, lr=2.47e-05, step=4933]Training:   2%|‚ñè         | 4934/200000 [1:45:30<70:32:33,  1.30s/it, loss=0.0259, lr=2.47e-05, step=4933]Training:   2%|‚ñè         | 4934/200000 [1:45:30<70:32:33,  1.30s/it, loss=0.0146, lr=2.47e-05, step=4934]Training:   2%|‚ñè         | 4935/200000 [1:45:31<73:06:31,  1.35s/it, loss=0.0146, lr=2.47e-05, step=4934]Training:   2%|‚ñè         | 4935/200000 [1:45:31<73:06:31,  1.35s/it, loss=0.0384, lr=2.47e-05, step=4935]Training:   2%|‚ñè         | 4936/200000 [1:45:33<73:52:16,  1.36s/it, loss=0.0384, lr=2.47e-05, step=4935]Training:   2%|‚ñè         | 4936/200000 [1:45:33<73:52:16,  1.36s/it, loss=0.0204, lr=2.47e-05, step=4936]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4937/200000 [1:45:34<69:09:25,  1.28s/it, loss=0.0204, lr=2.47e-05, step=4936]Training:   2%|‚ñè         | 4937/200000 [1:45:34<69:09:25,  1.28s/it, loss=0.0159, lr=2.47e-05, step=4937]Training:   2%|‚ñè         | 4938/200000 [1:45:35<65:53:16,  1.22s/it, loss=0.0159, lr=2.47e-05, step=4937]Training:   2%|‚ñè         | 4938/200000 [1:45:35<65:53:16,  1.22s/it, loss=0.0141, lr=2.47e-05, step=4938]Training:   2%|‚ñè         | 4939/200000 [1:45:36<68:16:22,  1.26s/it, loss=0.0141, lr=2.47e-05, step=4938]Training:   2%|‚ñè         | 4939/200000 [1:45:36<68:16:22,  1.26s/it, loss=0.0133, lr=2.47e-05, step=4939]Training:   2%|‚ñè         | 4940/200000 [1:45:38<68:51:41,  1.27s/it, loss=0.0133, lr=2.47e-05, step=4939]Training:   2%|‚ñè         | 4940/200000 [1:45:38<68:51:41,  1.27s/it, loss=0.0143, lr=2.47e-05, step=4940]Training:   2%|‚ñè         | 4941/200000 [1:45:39<65:40:28,  1.21s/it, loss=0.0143, lr=2.47e-05, step=4940]Training:   2%|‚ñè         | 4941/200000 [1:45:39<65:40:28,  1.21s/it, loss=0.0260, lr=2.47e-05, step=4941]Training:   2%|‚ñè         | 4942/200000 [1:45:40<68:21:41,  1.26s/it, loss=0.0260, lr=2.47e-05, step=4941]Training:   2%|‚ñè         | 4942/200000 [1:45:40<68:21:41,  1.26s/it, loss=0.0203, lr=2.47e-05, step=4942]Training:   2%|‚ñè         | 4943/200000 [1:45:41<65:16:50,  1.20s/it, loss=0.0203, lr=2.47e-05, step=4942]Training:   2%|‚ñè         | 4943/200000 [1:45:41<65:16:50,  1.20s/it, loss=0.0131, lr=2.47e-05, step=4943]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4944/200000 [1:45:43<69:23:42,  1.28s/it, loss=0.0131, lr=2.47e-05, step=4943]Training:   2%|‚ñè         | 4944/200000 [1:45:43<69:23:42,  1.28s/it, loss=0.0393, lr=2.47e-05, step=4944]Training:   2%|‚ñè         | 4945/200000 [1:45:44<66:02:32,  1.22s/it, loss=0.0393, lr=2.47e-05, step=4944]Training:   2%|‚ñè         | 4945/200000 [1:45:44<66:02:32,  1.22s/it, loss=0.0159, lr=2.47e-05, step=4945]Training:   2%|‚ñè         | 4946/200000 [1:45:45<69:22:16,  1.28s/it, loss=0.0159, lr=2.47e-05, step=4945]Training:   2%|‚ñè         | 4946/200000 [1:45:45<69:22:16,  1.28s/it, loss=0.0185, lr=2.47e-05, step=4946]Training:   2%|‚ñè         | 4947/200000 [1:45:46<71:49:44,  1.33s/it, loss=0.0185, lr=2.47e-05, step=4946]Training:   2%|‚ñè         | 4947/200000 [1:45:46<71:49:44,  1.33s/it, loss=0.0140, lr=2.47e-05, step=4947]Training:   2%|‚ñè         | 4948/200000 [1:45:48<67:46:57,  1.25s/it, loss=0.0140, lr=2.47e-05, step=4947]Training:   2%|‚ñè         | 4948/200000 [1:45:48<67:46:57,  1.25s/it, loss=0.0202, lr=2.47e-05, step=4948]Training:   2%|‚ñè         | 4949/200000 [1:45:49<64:54:44,  1.20s/it, loss=0.0202, lr=2.47e-05, step=4948]Training:   2%|‚ñè         | 4949/200000 [1:45:49<64:54:44,  1.20s/it, loss=0.0247, lr=2.47e-05, step=4949]Training:   2%|‚ñè         | 4950/200000 [1:45:50<68:07:23,  1.26s/it, loss=0.0247, lr=2.47e-05, step=4949]Training:   2%|‚ñè         | 4950/200000 [1:45:50<68:07:23,  1.26s/it, loss=0.0207, lr=2.47e-05, step=4950]Training:   2%|‚ñè         | 4951/200000 [1:45:51<70:35:35,  1.30s/it, loss=0.0207, lr=2.47e-05, step=4950]Training:   2%|‚ñè         | 4951/200000 [1:45:51<70:35:35,  1.30s/it, loss=0.0121, lr=2.48e-05, step=4951]Training:   2%|‚ñè         | 4952/200000 [1:45:52<66:53:01,  1.23s/it, loss=0.0121, lr=2.48e-05, step=4951]Training:   2%|‚ñè         | 4952/200000 [1:45:52<66:53:01,  1.23s/it, loss=0.0309, lr=2.48e-05, step=4952]Training:   2%|‚ñè         | 4953/200000 [1:45:54<70:39:52,  1.30s/it, loss=0.0309, lr=2.48e-05, step=4952]Training:   2%|‚ñè         | 4953/200000 [1:45:54<70:39:52,  1.30s/it, loss=0.0147, lr=2.48e-05, step=4953]Training:   2%|‚ñè         | 4954/200000 [1:45:55<71:32:19,  1.32s/it, loss=0.0147, lr=2.48e-05, step=4953]Training:   2%|‚ñè         | 4954/200000 [1:45:55<71:32:19,  1.32s/it, loss=0.0264, lr=2.48e-05, step=4954]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4955/200000 [1:45:57<72:21:18,  1.34s/it, loss=0.0264, lr=2.48e-05, step=4954]Training:   2%|‚ñè         | 4955/200000 [1:45:57<72:21:18,  1.34s/it, loss=0.0178, lr=2.48e-05, step=4955]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4956/200000 [1:45:58<68:09:42,  1.26s/it, loss=0.0178, lr=2.48e-05, step=4955]Training:   2%|‚ñè         | 4956/200000 [1:45:58<68:09:42,  1.26s/it, loss=0.0125, lr=2.48e-05, step=4956]Training:   2%|‚ñè         | 4957/200000 [1:45:59<71:58:21,  1.33s/it, loss=0.0125, lr=2.48e-05, step=4956]Training:   2%|‚ñè         | 4957/200000 [1:45:59<71:58:21,  1.33s/it, loss=0.0217, lr=2.48e-05, step=4957]Training:   2%|‚ñè         | 4958/200000 [1:46:01<74:40:34,  1.38s/it, loss=0.0217, lr=2.48e-05, step=4957]Training:   2%|‚ñè         | 4958/200000 [1:46:01<74:40:34,  1.38s/it, loss=0.0287, lr=2.48e-05, step=4958]Training:   2%|‚ñè         | 4959/200000 [1:46:02<69:51:59,  1.29s/it, loss=0.0287, lr=2.48e-05, step=4958]Training:   2%|‚ñè         | 4959/200000 [1:46:02<69:51:59,  1.29s/it, loss=0.0149, lr=2.48e-05, step=4959]Training:   2%|‚ñè         | 4960/200000 [1:46:03<66:25:39,  1.23s/it, loss=0.0149, lr=2.48e-05, step=4959]Training:   2%|‚ñè         | 4960/200000 [1:46:03<66:25:39,  1.23s/it, loss=0.0158, lr=2.48e-05, step=4960]Training:   2%|‚ñè         | 4961/200000 [1:46:04<70:26:47,  1.30s/it, loss=0.0158, lr=2.48e-05, step=4960]Training:   2%|‚ñè         | 4961/200000 [1:46:04<70:26:47,  1.30s/it, loss=0.0105, lr=2.48e-05, step=4961]Training:   2%|‚ñè         | 4962/200000 [1:46:05<66:47:54,  1.23s/it, loss=0.0105, lr=2.48e-05, step=4961]Training:   2%|‚ñè         | 4962/200000 [1:46:05<66:47:54,  1.23s/it, loss=0.0590, lr=2.48e-05, step=4962]Training:   2%|‚ñè         | 4963/200000 [1:46:07<68:14:53,  1.26s/it, loss=0.0590, lr=2.48e-05, step=4962]Training:   2%|‚ñè         | 4963/200000 [1:46:07<68:14:53,  1.26s/it, loss=0.0274, lr=2.48e-05, step=4963]Training:   2%|‚ñè         | 4964/200000 [1:46:08<65:16:05,  1.20s/it, loss=0.0274, lr=2.48e-05, step=4963]Training:   2%|‚ñè         | 4964/200000 [1:46:08<65:16:05,  1.20s/it, loss=0.0199, lr=2.48e-05, step=4964]Training:   2%|‚ñè         | 4965/200000 [1:46:09<69:01:23,  1.27s/it, loss=0.0199, lr=2.48e-05, step=4964]Training:   2%|‚ñè         | 4965/200000 [1:46:09<69:01:23,  1.27s/it, loss=0.0237, lr=2.48e-05, step=4965]Training:   2%|‚ñè         | 4966/200000 [1:46:11<71:36:46,  1.32s/it, loss=0.0237, lr=2.48e-05, step=4965]Training:   2%|‚ñè         | 4966/200000 [1:46:11<71:36:46,  1.32s/it, loss=0.0168, lr=2.48e-05, step=4966]Training:   2%|‚ñè         | 4967/200000 [1:46:12<73:45:30,  1.36s/it, loss=0.0168, lr=2.48e-05, step=4966]Training:   2%|‚ñè         | 4967/200000 [1:46:12<73:45:30,  1.36s/it, loss=0.0259, lr=2.48e-05, step=4967]Training:   2%|‚ñè         | 4968/200000 [1:46:14<74:51:57,  1.38s/it, loss=0.0259, lr=2.48e-05, step=4967]Training:   2%|‚ñè         | 4968/200000 [1:46:14<74:51:57,  1.38s/it, loss=0.0222, lr=2.48e-05, step=4968]Training:   2%|‚ñè         | 4969/200000 [1:46:15<69:50:18,  1.29s/it, loss=0.0222, lr=2.48e-05, step=4968]Training:   2%|‚ñè         | 4969/200000 [1:46:15<69:50:18,  1.29s/it, loss=0.0184, lr=2.48e-05, step=4969]Training:   2%|‚ñè         | 4970/200000 [1:46:16<66:24:00,  1.23s/it, loss=0.0184, lr=2.48e-05, step=4969]Training:   2%|‚ñè         | 4970/200000 [1:46:16<66:24:00,  1.23s/it, loss=0.0265, lr=2.48e-05, step=4970]Training:   2%|‚ñè         | 4971/200000 [1:46:17<68:15:33,  1.26s/it, loss=0.0265, lr=2.48e-05, step=4970]Training:   2%|‚ñè         | 4971/200000 [1:46:17<68:15:33,  1.26s/it, loss=0.0168, lr=2.49e-05, step=4971]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4972/200000 [1:46:18<70:18:10,  1.30s/it, loss=0.0168, lr=2.49e-05, step=4971]Training:   2%|‚ñè         | 4972/200000 [1:46:18<70:18:10,  1.30s/it, loss=0.0177, lr=2.49e-05, step=4972]Training:   2%|‚ñè         | 4973/200000 [1:46:20<66:40:00,  1.23s/it, loss=0.0177, lr=2.49e-05, step=4972]Training:   2%|‚ñè         | 4973/200000 [1:46:20<66:40:00,  1.23s/it, loss=0.0200, lr=2.49e-05, step=4973]Training:   2%|‚ñè         | 4974/200000 [1:46:21<69:58:46,  1.29s/it, loss=0.0200, lr=2.49e-05, step=4973]Training:   2%|‚ñè         | 4974/200000 [1:46:21<69:58:46,  1.29s/it, loss=0.0288, lr=2.49e-05, step=4974]Training:   2%|‚ñè         | 4975/200000 [1:46:22<70:58:08,  1.31s/it, loss=0.0288, lr=2.49e-05, step=4974]Training:   2%|‚ñè         | 4975/200000 [1:46:22<70:58:08,  1.31s/it, loss=0.0181, lr=2.49e-05, step=4975]Training:   2%|‚ñè         | 4976/200000 [1:46:24<71:57:38,  1.33s/it, loss=0.0181, lr=2.49e-05, step=4975]Training:   2%|‚ñè         | 4976/200000 [1:46:24<71:57:38,  1.33s/it, loss=0.0136, lr=2.49e-05, step=4976]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4977/200000 [1:46:25<67:49:16,  1.25s/it, loss=0.0136, lr=2.49e-05, step=4976]Training:   2%|‚ñè         | 4977/200000 [1:46:25<67:49:16,  1.25s/it, loss=0.0215, lr=2.49e-05, step=4977]Training:   2%|‚ñè         | 4978/200000 [1:46:26<71:32:47,  1.32s/it, loss=0.0215, lr=2.49e-05, step=4977]Training:   2%|‚ñè         | 4978/200000 [1:46:26<71:32:47,  1.32s/it, loss=0.0322, lr=2.49e-05, step=4978]Training:   2%|‚ñè         | 4979/200000 [1:46:28<73:51:15,  1.36s/it, loss=0.0322, lr=2.49e-05, step=4978]Training:   2%|‚ñè         | 4979/200000 [1:46:28<73:51:15,  1.36s/it, loss=0.0149, lr=2.49e-05, step=4979]Training:   2%|‚ñè         | 4980/200000 [1:46:29<69:09:09,  1.28s/it, loss=0.0149, lr=2.49e-05, step=4979]Training:   2%|‚ñè         | 4980/200000 [1:46:29<69:09:09,  1.28s/it, loss=0.0225, lr=2.49e-05, step=4980]Training:   2%|‚ñè         | 4981/200000 [1:46:30<65:52:06,  1.22s/it, loss=0.0225, lr=2.49e-05, step=4980]Training:   2%|‚ñè         | 4981/200000 [1:46:30<65:52:06,  1.22s/it, loss=0.0477, lr=2.49e-05, step=4981]Training:   2%|‚ñè         | 4982/200000 [1:46:31<69:52:41,  1.29s/it, loss=0.0477, lr=2.49e-05, step=4981]Training:   2%|‚ñè         | 4982/200000 [1:46:31<69:52:41,  1.29s/it, loss=0.0132, lr=2.49e-05, step=4982]Training:   2%|‚ñè         | 4983/200000 [1:46:32<66:21:03,  1.22s/it, loss=0.0132, lr=2.49e-05, step=4982]Training:   2%|‚ñè         | 4983/200000 [1:46:32<66:21:03,  1.22s/it, loss=0.0165, lr=2.49e-05, step=4983]Training:   2%|‚ñè         | 4984/200000 [1:46:34<67:28:27,  1.25s/it, loss=0.0165, lr=2.49e-05, step=4983]Training:   2%|‚ñè         | 4984/200000 [1:46:34<67:28:27,  1.25s/it, loss=0.0212, lr=2.49e-05, step=4984]Training:   2%|‚ñè         | 4985/200000 [1:46:35<64:40:16,  1.19s/it, loss=0.0212, lr=2.49e-05, step=4984]Training:   2%|‚ñè         | 4985/200000 [1:46:35<64:40:16,  1.19s/it, loss=0.0167, lr=2.49e-05, step=4985]Training:   2%|‚ñè         | 4986/200000 [1:46:36<67:35:04,  1.25s/it, loss=0.0167, lr=2.49e-05, step=4985]Training:   2%|‚ñè         | 4986/200000 [1:46:36<67:35:04,  1.25s/it, loss=0.0315, lr=2.49e-05, step=4986]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   2%|‚ñè         | 4987/200000 [1:46:38<70:20:21,  1.30s/it, loss=0.0315, lr=2.49e-05, step=4986]Training:   2%|‚ñè         | 4987/200000 [1:46:38<70:20:21,  1.30s/it, loss=0.0174, lr=2.49e-05, step=4987]Training:   2%|‚ñè         | 4988/200000 [1:46:39<72:52:35,  1.35s/it, loss=0.0174, lr=2.49e-05, step=4987]Training:   2%|‚ñè         | 4988/200000 [1:46:39<72:52:35,  1.35s/it, loss=0.0093, lr=2.49e-05, step=4988]Training:   2%|‚ñè         | 4989/200000 [1:46:40<74:13:28,  1.37s/it, loss=0.0093, lr=2.49e-05, step=4988]Training:   2%|‚ñè         | 4989/200000 [1:46:40<74:13:28,  1.37s/it, loss=0.0340, lr=2.49e-05, step=4989]Training:   2%|‚ñè         | 4990/200000 [1:46:42<69:24:09,  1.28s/it, loss=0.0340, lr=2.49e-05, step=4989]Training:   2%|‚ñè         | 4990/200000 [1:46:42<69:24:09,  1.28s/it, loss=0.0172, lr=2.49e-05, step=4990]Training:   2%|‚ñè         | 4991/200000 [1:46:43<66:02:39,  1.22s/it, loss=0.0172, lr=2.49e-05, step=4990]Training:   2%|‚ñè         | 4991/200000 [1:46:43<66:02:39,  1.22s/it, loss=0.0263, lr=2.50e-05, step=4991]Training:   2%|‚ñè         | 4992/200000 [1:46:44<68:03:41,  1.26s/it, loss=0.0263, lr=2.50e-05, step=4991]Training:   2%|‚ñè         | 4992/200000 [1:46:44<68:03:41,  1.26s/it, loss=0.0160, lr=2.50e-05, step=4992]Training:   2%|‚ñè         | 4993/200000 [1:46:45<69:28:21,  1.28s/it, loss=0.0160, lr=2.50e-05, step=4992]Training:   2%|‚ñè         | 4993/200000 [1:46:45<69:28:21,  1.28s/it, loss=0.0233, lr=2.50e-05, step=4993]Training:   2%|‚ñè         | 4994/200000 [1:46:46<66:05:32,  1.22s/it, loss=0.0233, lr=2.50e-05, step=4993]Training:   2%|‚ñè         | 4994/200000 [1:46:46<66:05:32,  1.22s/it, loss=0.1744, lr=2.50e-05, step=4994]Training:   2%|‚ñè         | 4995/200000 [1:46:48<68:26:39,  1.26s/it, loss=0.1744, lr=2.50e-05, step=4994]Training:   2%|‚ñè         | 4995/200000 [1:46:48<68:26:39,  1.26s/it, loss=0.0190, lr=2.50e-05, step=4995]Training:   2%|‚ñè         | 4996/200000 [1:46:49<65:21:46,  1.21s/it, loss=0.0190, lr=2.50e-05, step=4995]Training:   2%|‚ñè         | 4996/200000 [1:46:49<65:21:46,  1.21s/it, loss=0.0199, lr=2.50e-05, step=4996]Training:   2%|‚ñè         | 4997/200000 [1:46:50<69:26:52,  1.28s/it, loss=0.0199, lr=2.50e-05, step=4996]Training:   2%|‚ñè         | 4997/200000 [1:46:50<69:26:52,  1.28s/it, loss=0.0140, lr=2.50e-05, step=4997]Training:   2%|‚ñè         | 4998/200000 [1:46:51<66:05:49,  1.22s/it, loss=0.0140, lr=2.50e-05, step=4997]Training:   2%|‚ñè         | 4998/200000 [1:46:51<66:05:49,  1.22s/it, loss=0.0206, lr=2.50e-05, step=4998]Training:   2%|‚ñè         | 4999/200000 [1:46:53<69:09:00,  1.28s/it, loss=0.0206, lr=2.50e-05, step=4998]Training:   2%|‚ñè         | 4999/200000 [1:46:53<69:09:00,  1.28s/it, loss=0.0148, lr=2.50e-05, step=4999]00:40:42.794 [I] Saved checkpoint at step 5000 -> /project/peilab/wzj/RoboTwin/policy/openpi_test/checkpoints/pi0_base_aloha_robotwin_full_torch/robotwin_aloha_lerobot/5000 (701675:train_pytorch.py:190)
Training:   2%|‚ñé         | 5000/200000 [1:47:29<637:10:37, 11.76s/it, loss=0.0148, lr=2.50e-05, step=4999]Training:   2%|‚ñé         | 5000/200000 [1:47:29<637:10:37, 11.76s/it, loss=0.0186, lr=2.50e-05, step=5000]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
00:40:43.880 [I] step=5000 loss=0.0225 lr=2.48e-05 grad_norm=0.43 time=162.5s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5001/200000 [1:47:30<463:40:11,  8.56s/it, loss=0.0186, lr=2.50e-05, step=5000]Training:   3%|‚ñé         | 5001/200000 [1:47:30<463:40:11,  8.56s/it, loss=0.0203, lr=2.50e-05, step=5001]Training:   3%|‚ñé         | 5002/200000 [1:47:31<341:57:32,  6.31s/it, loss=0.0203, lr=2.50e-05, step=5001]Training:   3%|‚ñé         | 5002/200000 [1:47:31<341:57:32,  6.31s/it, loss=0.0118, lr=2.50e-05, step=5002]Training:   3%|‚ñé         | 5003/200000 [1:47:33<262:00:45,  4.84s/it, loss=0.0118, lr=2.50e-05, step=5002]Training:   3%|‚ñé         | 5003/200000 [1:47:33<262:00:45,  4.84s/it, loss=0.0230, lr=2.50e-05, step=5003]Training:   3%|‚ñé         | 5004/200000 [1:47:34<206:34:20,  3.81s/it, loss=0.0230, lr=2.50e-05, step=5003]Training:   3%|‚ñé         | 5004/200000 [1:47:34<206:34:20,  3.81s/it, loss=0.0194, lr=2.50e-05, step=5004]Training:   3%|‚ñé         | 5005/200000 [1:47:35<162:01:35,  2.99s/it, loss=0.0194, lr=2.50e-05, step=5004]Training:   3%|‚ñé         | 5005/200000 [1:47:35<162:01:35,  2.99s/it, loss=0.0182, lr=2.50e-05, step=5005]Training:   3%|‚ñé         | 5006/200000 [1:47:36<136:18:32,  2.52s/it, loss=0.0182, lr=2.50e-05, step=5005]Training:   3%|‚ñé         | 5006/200000 [1:47:36<136:18:32,  2.52s/it, loss=0.0129, lr=2.50e-05, step=5006]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5007/200000 [1:47:38<117:31:43,  2.17s/it, loss=0.0129, lr=2.50e-05, step=5006]Training:   3%|‚ñé         | 5007/200000 [1:47:38<117:31:43,  2.17s/it, loss=0.0230, lr=2.50e-05, step=5007]Training:   3%|‚ñé         | 5008/200000 [1:47:39<104:18:21,  1.93s/it, loss=0.0230, lr=2.50e-05, step=5007]Training:   3%|‚ñé         | 5008/200000 [1:47:39<104:18:21,  1.93s/it, loss=0.0116, lr=2.50e-05, step=5008]Training:   3%|‚ñé         | 5009/200000 [1:47:40<90:26:37,  1.67s/it, loss=0.0116, lr=2.50e-05, step=5008] Training:   3%|‚ñé         | 5009/200000 [1:47:40<90:26:37,  1.67s/it, loss=0.0208, lr=2.50e-05, step=5009]Training:   3%|‚ñé         | 5010/200000 [1:47:42<87:36:56,  1.62s/it, loss=0.0208, lr=2.50e-05, step=5009]Training:   3%|‚ñé         | 5010/200000 [1:47:42<87:36:56,  1.62s/it, loss=0.0447, lr=2.50e-05, step=5010]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5011/200000 [1:47:43<85:49:52,  1.58s/it, loss=0.0447, lr=2.50e-05, step=5010]Training:   3%|‚ñé         | 5011/200000 [1:47:43<85:49:52,  1.58s/it, loss=0.0149, lr=2.51e-05, step=5011]Training:   3%|‚ñé         | 5012/200000 [1:47:44<77:34:38,  1.43s/it, loss=0.0149, lr=2.51e-05, step=5011]Training:   3%|‚ñé         | 5012/200000 [1:47:44<77:34:38,  1.43s/it, loss=0.0231, lr=2.51e-05, step=5012]Training:   3%|‚ñé         | 5013/200000 [1:47:45<71:43:47,  1.32s/it, loss=0.0231, lr=2.51e-05, step=5012]Training:   3%|‚ñé         | 5013/200000 [1:47:45<71:43:47,  1.32s/it, loss=0.0186, lr=2.51e-05, step=5013]Training:   3%|‚ñé         | 5014/200000 [1:47:47<74:14:40,  1.37s/it, loss=0.0186, lr=2.51e-05, step=5013]Training:   3%|‚ñé         | 5014/200000 [1:47:47<74:14:40,  1.37s/it, loss=0.0206, lr=2.51e-05, step=5014]Training:   3%|‚ñé         | 5015/200000 [1:47:48<69:23:17,  1.28s/it, loss=0.0206, lr=2.51e-05, step=5014]Training:   3%|‚ñé         | 5015/200000 [1:47:48<69:23:17,  1.28s/it, loss=0.0144, lr=2.51e-05, step=5015]Training:   3%|‚ñé         | 5016/200000 [1:47:49<69:57:12,  1.29s/it, loss=0.0144, lr=2.51e-05, step=5015]Training:   3%|‚ñé         | 5016/200000 [1:47:49<69:57:12,  1.29s/it, loss=0.0198, lr=2.51e-05, step=5016]Training:   3%|‚ñé         | 5017/200000 [1:47:50<66:26:31,  1.23s/it, loss=0.0198, lr=2.51e-05, step=5016]Training:   3%|‚ñé         | 5017/200000 [1:47:50<66:26:31,  1.23s/it, loss=0.0124, lr=2.51e-05, step=5017]Training:   3%|‚ñé         | 5018/200000 [1:47:52<69:36:03,  1.29s/it, loss=0.0124, lr=2.51e-05, step=5017]Training:   3%|‚ñé         | 5018/200000 [1:47:52<69:36:03,  1.29s/it, loss=0.0200, lr=2.51e-05, step=5018]Training:   3%|‚ñé         | 5019/200000 [1:47:53<72:19:28,  1.34s/it, loss=0.0200, lr=2.51e-05, step=5018]Training:   3%|‚ñé         | 5019/200000 [1:47:53<72:19:28,  1.34s/it, loss=0.0170, lr=2.51e-05, step=5019]Training:   3%|‚ñé         | 5020/200000 [1:47:55<73:32:51,  1.36s/it, loss=0.0170, lr=2.51e-05, step=5019]Training:   3%|‚ñé         | 5020/200000 [1:47:55<73:32:51,  1.36s/it, loss=0.0339, lr=2.51e-05, step=5020]Training:   3%|‚ñé         | 5021/200000 [1:47:56<74:05:41,  1.37s/it, loss=0.0339, lr=2.51e-05, step=5020]Training:   3%|‚ñé         | 5021/200000 [1:47:56<74:05:41,  1.37s/it, loss=0.0157, lr=2.51e-05, step=5021]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5022/200000 [1:47:57<69:19:36,  1.28s/it, loss=0.0157, lr=2.51e-05, step=5021]Training:   3%|‚ñé         | 5022/200000 [1:47:57<69:19:36,  1.28s/it, loss=0.0324, lr=2.51e-05, step=5022]Training:   3%|‚ñé         | 5023/200000 [1:47:58<65:57:29,  1.22s/it, loss=0.0324, lr=2.51e-05, step=5022]Training:   3%|‚ñé         | 5023/200000 [1:47:58<65:57:29,  1.22s/it, loss=0.0200, lr=2.51e-05, step=5023]Training:   3%|‚ñé         | 5024/200000 [1:48:00<68:53:43,  1.27s/it, loss=0.0200, lr=2.51e-05, step=5023]Training:   3%|‚ñé         | 5024/200000 [1:48:00<68:53:43,  1.27s/it, loss=0.0181, lr=2.51e-05, step=5024]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5025/200000 [1:48:01<71:39:15,  1.32s/it, loss=0.0181, lr=2.51e-05, step=5024]Training:   3%|‚ñé         | 5025/200000 [1:48:01<71:39:15,  1.32s/it, loss=0.0169, lr=2.51e-05, step=5025]Training:   3%|‚ñé         | 5026/200000 [1:48:02<67:36:37,  1.25s/it, loss=0.0169, lr=2.51e-05, step=5025]Training:   3%|‚ñé         | 5026/200000 [1:48:02<67:36:37,  1.25s/it, loss=0.0194, lr=2.51e-05, step=5026]Training:   3%|‚ñé         | 5027/200000 [1:48:03<70:03:02,  1.29s/it, loss=0.0194, lr=2.51e-05, step=5026]Training:   3%|‚ñé         | 5027/200000 [1:48:03<70:03:02,  1.29s/it, loss=0.0344, lr=2.51e-05, step=5027]Training:   3%|‚ñé         | 5028/200000 [1:48:05<70:41:23,  1.31s/it, loss=0.0344, lr=2.51e-05, step=5027]Training:   3%|‚ñé         | 5028/200000 [1:48:05<70:41:23,  1.31s/it, loss=0.0146, lr=2.51e-05, step=5028]Training:   3%|‚ñé         | 5029/200000 [1:48:06<71:17:46,  1.32s/it, loss=0.0146, lr=2.51e-05, step=5028]Training:   3%|‚ñé         | 5029/200000 [1:48:06<71:17:46,  1.32s/it, loss=0.0189, lr=2.51e-05, step=5029]Training:   3%|‚ñé         | 5030/200000 [1:48:07<67:20:33,  1.24s/it, loss=0.0189, lr=2.51e-05, step=5029]Training:   3%|‚ñé         | 5030/200000 [1:48:07<67:20:33,  1.24s/it, loss=0.0169, lr=2.51e-05, step=5030]Training:   3%|‚ñé         | 5031/200000 [1:48:09<71:13:07,  1.32s/it, loss=0.0169, lr=2.51e-05, step=5030]Training:   3%|‚ñé         | 5031/200000 [1:48:09<71:13:07,  1.32s/it, loss=0.0200, lr=2.52e-05, step=5031]Training:   3%|‚ñé         | 5032/200000 [1:48:10<73:38:01,  1.36s/it, loss=0.0200, lr=2.52e-05, step=5031]Training:   3%|‚ñé         | 5032/200000 [1:48:10<73:38:01,  1.36s/it, loss=0.0142, lr=2.52e-05, step=5032]Training:   3%|‚ñé         | 5033/200000 [1:48:11<68:59:20,  1.27s/it, loss=0.0142, lr=2.52e-05, step=5032]Training:   3%|‚ñé         | 5033/200000 [1:48:11<68:59:20,  1.27s/it, loss=0.0125, lr=2.52e-05, step=5033]Training:   3%|‚ñé         | 5034/200000 [1:48:12<65:45:39,  1.21s/it, loss=0.0125, lr=2.52e-05, step=5033]Training:   3%|‚ñé         | 5034/200000 [1:48:12<65:45:39,  1.21s/it, loss=0.0183, lr=2.52e-05, step=5034]Training:   3%|‚ñé         | 5035/200000 [1:48:14<69:42:27,  1.29s/it, loss=0.0183, lr=2.52e-05, step=5034]Training:   3%|‚ñé         | 5035/200000 [1:48:14<69:42:27,  1.29s/it, loss=0.0139, lr=2.52e-05, step=5035]Training:   3%|‚ñé         | 5036/200000 [1:48:15<66:13:06,  1.22s/it, loss=0.0139, lr=2.52e-05, step=5035]Training:   3%|‚ñé         | 5036/200000 [1:48:15<66:13:06,  1.22s/it, loss=0.0194, lr=2.52e-05, step=5036]Training:   3%|‚ñé         | 5037/200000 [1:48:16<67:09:22,  1.24s/it, loss=0.0194, lr=2.52e-05, step=5036]Training:   3%|‚ñé         | 5037/200000 [1:48:16<67:09:22,  1.24s/it, loss=0.0335, lr=2.52e-05, step=5037]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5038/200000 [1:48:17<64:27:10,  1.19s/it, loss=0.0335, lr=2.52e-05, step=5037]Training:   3%|‚ñé         | 5038/200000 [1:48:17<64:27:10,  1.19s/it, loss=0.0192, lr=2.52e-05, step=5038]Training:   3%|‚ñé         | 5039/200000 [1:48:19<68:20:21,  1.26s/it, loss=0.0192, lr=2.52e-05, step=5038]Training:   3%|‚ñé         | 5039/200000 [1:48:19<68:20:21,  1.26s/it, loss=0.0191, lr=2.52e-05, step=5039]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5040/200000 [1:48:20<70:48:37,  1.31s/it, loss=0.0191, lr=2.52e-05, step=5039]Training:   3%|‚ñé         | 5040/200000 [1:48:20<70:48:37,  1.31s/it, loss=0.0270, lr=2.52e-05, step=5040]Training:   3%|‚ñé         | 5041/200000 [1:48:21<72:33:23,  1.34s/it, loss=0.0270, lr=2.52e-05, step=5040]Training:   3%|‚ñé         | 5041/200000 [1:48:21<72:33:23,  1.34s/it, loss=0.0187, lr=2.52e-05, step=5041]Training:   3%|‚ñé         | 5042/200000 [1:48:23<74:00:04,  1.37s/it, loss=0.0187, lr=2.52e-05, step=5041]Training:   3%|‚ñé         | 5042/200000 [1:48:23<74:00:04,  1.37s/it, loss=0.0235, lr=2.52e-05, step=5042]Training:   3%|‚ñé         | 5043/200000 [1:48:24<69:13:25,  1.28s/it, loss=0.0235, lr=2.52e-05, step=5042]Training:   3%|‚ñé         | 5043/200000 [1:48:24<69:13:25,  1.28s/it, loss=0.0183, lr=2.52e-05, step=5043]Training:   3%|‚ñé         | 5044/200000 [1:48:25<65:54:06,  1.22s/it, loss=0.0183, lr=2.52e-05, step=5043]Training:   3%|‚ñé         | 5044/200000 [1:48:25<65:54:06,  1.22s/it, loss=0.0197, lr=2.52e-05, step=5044]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5045/200000 [1:48:26<67:57:14,  1.25s/it, loss=0.0197, lr=2.52e-05, step=5044]Training:   3%|‚ñé         | 5045/200000 [1:48:26<67:57:14,  1.25s/it, loss=0.0156, lr=2.52e-05, step=5045]Training:   3%|‚ñé         | 5046/200000 [1:48:28<68:52:23,  1.27s/it, loss=0.0156, lr=2.52e-05, step=5045]Training:   3%|‚ñé         | 5046/200000 [1:48:28<68:52:23,  1.27s/it, loss=0.0225, lr=2.52e-05, step=5046]Training:   3%|‚ñé         | 5047/200000 [1:48:29<65:39:40,  1.21s/it, loss=0.0225, lr=2.52e-05, step=5046]Training:   3%|‚ñé         | 5047/200000 [1:48:29<65:39:40,  1.21s/it, loss=0.0233, lr=2.52e-05, step=5047]Training:   3%|‚ñé         | 5048/200000 [1:48:30<67:40:11,  1.25s/it, loss=0.0233, lr=2.52e-05, step=5047]Training:   3%|‚ñé         | 5048/200000 [1:48:30<67:40:11,  1.25s/it, loss=0.0144, lr=2.52e-05, step=5048]Training:   3%|‚ñé         | 5049/200000 [1:48:31<69:19:21,  1.28s/it, loss=0.0144, lr=2.52e-05, step=5048]Training:   3%|‚ñé         | 5049/200000 [1:48:31<69:19:21,  1.28s/it, loss=0.0146, lr=2.52e-05, step=5049]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5050/200000 [1:48:33<70:06:32,  1.29s/it, loss=0.0146, lr=2.52e-05, step=5049]Training:   3%|‚ñé         | 5050/200000 [1:48:33<70:06:32,  1.29s/it, loss=0.0140, lr=2.52e-05, step=5050]Training:   3%|‚ñé         | 5051/200000 [1:48:34<66:31:45,  1.23s/it, loss=0.0140, lr=2.52e-05, step=5050]Training:   3%|‚ñé         | 5051/200000 [1:48:34<66:31:45,  1.23s/it, loss=0.0142, lr=2.53e-05, step=5051]Training:   3%|‚ñé         | 5052/200000 [1:48:35<69:32:50,  1.28s/it, loss=0.0142, lr=2.53e-05, step=5051]Training:   3%|‚ñé         | 5052/200000 [1:48:35<69:32:50,  1.28s/it, loss=0.0196, lr=2.53e-05, step=5052]Training:   3%|‚ñé         | 5053/200000 [1:48:37<71:13:12,  1.32s/it, loss=0.0196, lr=2.53e-05, step=5052]Training:   3%|‚ñé         | 5053/200000 [1:48:37<71:13:12,  1.32s/it, loss=0.0163, lr=2.53e-05, step=5053]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5054/200000 [1:48:38<67:17:02,  1.24s/it, loss=0.0163, lr=2.53e-05, step=5053]Training:   3%|‚ñé         | 5054/200000 [1:48:38<67:17:02,  1.24s/it, loss=0.0154, lr=2.53e-05, step=5054]Training:   3%|‚ñé         | 5055/200000 [1:48:39<64:31:09,  1.19s/it, loss=0.0154, lr=2.53e-05, step=5054]Training:   3%|‚ñé         | 5055/200000 [1:48:39<64:31:09,  1.19s/it, loss=0.0127, lr=2.53e-05, step=5055]Training:   3%|‚ñé         | 5056/200000 [1:48:40<67:54:54,  1.25s/it, loss=0.0127, lr=2.53e-05, step=5055]Training:   3%|‚ñé         | 5056/200000 [1:48:40<67:54:54,  1.25s/it, loss=0.0527, lr=2.53e-05, step=5056]Training:   3%|‚ñé         | 5057/200000 [1:48:42<70:39:49,  1.30s/it, loss=0.0527, lr=2.53e-05, step=5056]Training:   3%|‚ñé         | 5057/200000 [1:48:42<70:39:49,  1.30s/it, loss=0.0142, lr=2.53e-05, step=5057]Training:   3%|‚ñé         | 5058/200000 [1:48:43<66:56:01,  1.24s/it, loss=0.0142, lr=2.53e-05, step=5057]Training:   3%|‚ñé         | 5058/200000 [1:48:43<66:56:01,  1.24s/it, loss=0.0207, lr=2.53e-05, step=5058]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5059/200000 [1:48:44<70:08:51,  1.30s/it, loss=0.0207, lr=2.53e-05, step=5058]Training:   3%|‚ñé         | 5059/200000 [1:48:44<70:08:51,  1.30s/it, loss=0.0138, lr=2.53e-05, step=5059]Training:   3%|‚ñé         | 5060/200000 [1:48:46<71:29:04,  1.32s/it, loss=0.0138, lr=2.53e-05, step=5059]Training:   3%|‚ñé         | 5060/200000 [1:48:46<71:29:04,  1.32s/it, loss=0.0204, lr=2.53e-05, step=5060]Training:   3%|‚ñé         | 5061/200000 [1:48:47<72:47:49,  1.34s/it, loss=0.0204, lr=2.53e-05, step=5060]Training:   3%|‚ñé         | 5061/200000 [1:48:47<72:47:49,  1.34s/it, loss=0.0110, lr=2.53e-05, step=5061]Training:   3%|‚ñé         | 5062/200000 [1:48:48<68:26:10,  1.26s/it, loss=0.0110, lr=2.53e-05, step=5061]Training:   3%|‚ñé         | 5062/200000 [1:48:48<68:26:10,  1.26s/it, loss=0.0220, lr=2.53e-05, step=5062]Training:   3%|‚ñé         | 5063/200000 [1:48:49<72:12:45,  1.33s/it, loss=0.0220, lr=2.53e-05, step=5062]Training:   3%|‚ñé         | 5063/200000 [1:48:49<72:12:45,  1.33s/it, loss=0.0361, lr=2.53e-05, step=5063]Training:   3%|‚ñé         | 5064/200000 [1:48:51<74:41:05,  1.38s/it, loss=0.0361, lr=2.53e-05, step=5063]Training:   3%|‚ñé         | 5064/200000 [1:48:51<74:41:05,  1.38s/it, loss=0.0146, lr=2.53e-05, step=5064]Training:   3%|‚ñé         | 5065/200000 [1:48:52<69:45:08,  1.29s/it, loss=0.0146, lr=2.53e-05, step=5064]Training:   3%|‚ñé         | 5065/200000 [1:48:52<69:45:08,  1.29s/it, loss=0.0122, lr=2.53e-05, step=5065]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5066/200000 [1:48:53<66:17:25,  1.22s/it, loss=0.0122, lr=2.53e-05, step=5065]Training:   3%|‚ñé         | 5066/200000 [1:48:53<66:17:25,  1.22s/it, loss=0.0190, lr=2.53e-05, step=5066]Training:   3%|‚ñé         | 5067/200000 [1:48:55<70:17:20,  1.30s/it, loss=0.0190, lr=2.53e-05, step=5066]Training:   3%|‚ñé         | 5067/200000 [1:48:55<70:17:20,  1.30s/it, loss=0.0167, lr=2.53e-05, step=5067]Training:   3%|‚ñé         | 5068/200000 [1:48:56<66:38:44,  1.23s/it, loss=0.0167, lr=2.53e-05, step=5067]Training:   3%|‚ñé         | 5068/200000 [1:48:56<66:38:44,  1.23s/it, loss=0.0263, lr=2.53e-05, step=5068]Training:   3%|‚ñé         | 5069/200000 [1:48:57<68:08:49,  1.26s/it, loss=0.0263, lr=2.53e-05, step=5068]Training:   3%|‚ñé         | 5069/200000 [1:48:57<68:08:49,  1.26s/it, loss=0.0247, lr=2.53e-05, step=5069]Training:   3%|‚ñé         | 5070/200000 [1:48:58<65:07:42,  1.20s/it, loss=0.0247, lr=2.53e-05, step=5069]Training:   3%|‚ñé         | 5070/200000 [1:48:58<65:07:42,  1.20s/it, loss=0.0119, lr=2.53e-05, step=5070]Training:   3%|‚ñé         | 5071/200000 [1:49:00<69:16:29,  1.28s/it, loss=0.0119, lr=2.53e-05, step=5070]Training:   3%|‚ñé         | 5071/200000 [1:49:00<69:16:29,  1.28s/it, loss=0.0235, lr=2.54e-05, step=5071]Training:   3%|‚ñé         | 5072/200000 [1:49:01<71:52:26,  1.33s/it, loss=0.0235, lr=2.54e-05, step=5071]Training:   3%|‚ñé         | 5072/200000 [1:49:01<71:52:26,  1.33s/it, loss=0.0140, lr=2.54e-05, step=5072]Training:   3%|‚ñé         | 5073/200000 [1:49:02<73:05:25,  1.35s/it, loss=0.0140, lr=2.54e-05, step=5072]Training:   3%|‚ñé         | 5073/200000 [1:49:02<73:05:25,  1.35s/it, loss=0.0222, lr=2.54e-05, step=5073]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5074/200000 [1:49:04<74:08:40,  1.37s/it, loss=0.0222, lr=2.54e-05, step=5073]Training:   3%|‚ñé         | 5074/200000 [1:49:04<74:08:40,  1.37s/it, loss=0.0224, lr=2.54e-05, step=5074]Training:   3%|‚ñé         | 5075/200000 [1:49:05<69:23:04,  1.28s/it, loss=0.0224, lr=2.54e-05, step=5074]Training:   3%|‚ñé         | 5075/200000 [1:49:05<69:23:04,  1.28s/it, loss=0.0267, lr=2.54e-05, step=5075]Training:   3%|‚ñé         | 5076/200000 [1:49:06<66:03:00,  1.22s/it, loss=0.0267, lr=2.54e-05, step=5075]Training:   3%|‚ñé         | 5076/200000 [1:49:06<66:03:00,  1.22s/it, loss=0.0385, lr=2.54e-05, step=5076]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5077/200000 [1:49:07<67:41:45,  1.25s/it, loss=0.0385, lr=2.54e-05, step=5076]Training:   3%|‚ñé         | 5077/200000 [1:49:07<67:41:45,  1.25s/it, loss=0.0228, lr=2.54e-05, step=5077]Training:   3%|‚ñé         | 5078/200000 [1:49:09<69:59:13,  1.29s/it, loss=0.0228, lr=2.54e-05, step=5077]Training:   3%|‚ñé         | 5078/200000 [1:49:09<69:59:13,  1.29s/it, loss=0.0493, lr=2.54e-05, step=5078]Training:   3%|‚ñé         | 5079/200000 [1:49:10<66:24:51,  1.23s/it, loss=0.0493, lr=2.54e-05, step=5078]Training:   3%|‚ñé         | 5079/200000 [1:49:10<66:24:51,  1.23s/it, loss=0.0196, lr=2.54e-05, step=5079]Training:   3%|‚ñé         | 5080/200000 [1:49:11<68:55:50,  1.27s/it, loss=0.0196, lr=2.54e-05, step=5079]Training:   3%|‚ñé         | 5080/200000 [1:49:11<68:55:50,  1.27s/it, loss=0.0345, lr=2.54e-05, step=5080]Training:   3%|‚ñé         | 5081/200000 [1:49:12<70:11:31,  1.30s/it, loss=0.0345, lr=2.54e-05, step=5080]Training:   3%|‚ñé         | 5081/200000 [1:49:12<70:11:31,  1.30s/it, loss=0.0135, lr=2.54e-05, step=5081]Training:   3%|‚ñé         | 5082/200000 [1:49:14<70:53:30,  1.31s/it, loss=0.0135, lr=2.54e-05, step=5081]Training:   3%|‚ñé         | 5082/200000 [1:49:14<70:53:30,  1.31s/it, loss=0.0298, lr=2.54e-05, step=5082]Training:   3%|‚ñé         | 5083/200000 [1:49:15<67:02:48,  1.24s/it, loss=0.0298, lr=2.54e-05, step=5082]Training:   3%|‚ñé         | 5083/200000 [1:49:15<67:02:48,  1.24s/it, loss=0.0209, lr=2.54e-05, step=5083]Training:   3%|‚ñé         | 5084/200000 [1:49:16<70:46:38,  1.31s/it, loss=0.0209, lr=2.54e-05, step=5083]Training:   3%|‚ñé         | 5084/200000 [1:49:16<70:46:38,  1.31s/it, loss=0.0225, lr=2.54e-05, step=5084]Training:   3%|‚ñé         | 5085/200000 [1:49:18<73:10:10,  1.35s/it, loss=0.0225, lr=2.54e-05, step=5084]Training:   3%|‚ñé         | 5085/200000 [1:49:18<73:10:10,  1.35s/it, loss=0.0328, lr=2.54e-05, step=5085]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5086/200000 [1:49:19<68:40:28,  1.27s/it, loss=0.0328, lr=2.54e-05, step=5085]Training:   3%|‚ñé         | 5086/200000 [1:49:19<68:40:28,  1.27s/it, loss=0.0146, lr=2.54e-05, step=5086]Training:   3%|‚ñé         | 5087/200000 [1:49:20<65:30:21,  1.21s/it, loss=0.0146, lr=2.54e-05, step=5086]Training:   3%|‚ñé         | 5087/200000 [1:49:20<65:30:21,  1.21s/it, loss=0.0463, lr=2.54e-05, step=5087]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5088/200000 [1:49:21<69:43:45,  1.29s/it, loss=0.0463, lr=2.54e-05, step=5087]Training:   3%|‚ñé         | 5088/200000 [1:49:21<69:43:45,  1.29s/it, loss=0.0172, lr=2.54e-05, step=5088]Training:   3%|‚ñé         | 5089/200000 [1:49:22<66:13:32,  1.22s/it, loss=0.0172, lr=2.54e-05, step=5088]Training:   3%|‚ñé         | 5089/200000 [1:49:22<66:13:32,  1.22s/it, loss=0.0207, lr=2.54e-05, step=5089]Training:   3%|‚ñé         | 5090/200000 [1:49:24<67:13:46,  1.24s/it, loss=0.0207, lr=2.54e-05, step=5089]Training:   3%|‚ñé         | 5090/200000 [1:49:24<67:13:46,  1.24s/it, loss=0.3557, lr=2.54e-05, step=5090]Training:   3%|‚ñé         | 5091/200000 [1:49:25<64:28:38,  1.19s/it, loss=0.3557, lr=2.54e-05, step=5090]Training:   3%|‚ñé         | 5091/200000 [1:49:25<64:28:38,  1.19s/it, loss=0.0130, lr=2.55e-05, step=5091]Training:   3%|‚ñé         | 5092/200000 [1:49:26<68:32:43,  1.27s/it, loss=0.0130, lr=2.55e-05, step=5091]Training:   3%|‚ñé         | 5092/200000 [1:49:26<68:32:43,  1.27s/it, loss=0.0277, lr=2.55e-05, step=5092]Training:   3%|‚ñé         | 5093/200000 [1:49:28<71:42:59,  1.32s/it, loss=0.0277, lr=2.55e-05, step=5092]Training:   3%|‚ñé         | 5093/200000 [1:49:28<71:42:59,  1.32s/it, loss=0.0273, lr=2.55e-05, step=5093]Training:   3%|‚ñé         | 5094/200000 [1:49:29<73:01:40,  1.35s/it, loss=0.0273, lr=2.55e-05, step=5093]Training:   3%|‚ñé         | 5094/200000 [1:49:29<73:01:40,  1.35s/it, loss=0.0209, lr=2.55e-05, step=5094]Training:   3%|‚ñé         | 5095/200000 [1:49:31<74:19:28,  1.37s/it, loss=0.0209, lr=2.55e-05, step=5094]Training:   3%|‚ñé         | 5095/200000 [1:49:31<74:19:28,  1.37s/it, loss=0.0201, lr=2.55e-05, step=5095]Training:   3%|‚ñé         | 5096/200000 [1:49:32<69:29:07,  1.28s/it, loss=0.0201, lr=2.55e-05, step=5095]Training:   3%|‚ñé         | 5096/200000 [1:49:32<69:29:07,  1.28s/it, loss=0.0154, lr=2.55e-05, step=5096]Training:   3%|‚ñé         | 5097/200000 [1:49:33<66:04:54,  1.22s/it, loss=0.0154, lr=2.55e-05, step=5096]Training:   3%|‚ñé         | 5097/200000 [1:49:33<66:04:54,  1.22s/it, loss=0.0229, lr=2.55e-05, step=5097]Training:   3%|‚ñé         | 5098/200000 [1:49:34<67:54:50,  1.25s/it, loss=0.0229, lr=2.55e-05, step=5097]Training:   3%|‚ñé         | 5098/200000 [1:49:34<67:54:50,  1.25s/it, loss=0.0176, lr=2.55e-05, step=5098]Training:   3%|‚ñé         | 5099/200000 [1:49:35<69:29:54,  1.28s/it, loss=0.0176, lr=2.55e-05, step=5098]Training:   3%|‚ñé         | 5099/200000 [1:49:35<69:29:54,  1.28s/it, loss=0.0266, lr=2.55e-05, step=5099]Training:   3%|‚ñé         | 5100/200000 [1:49:36<66:08:31,  1.22s/it, loss=0.0266, lr=2.55e-05, step=5099]Training:   3%|‚ñé         | 5100/200000 [1:49:36<66:08:31,  1.22s/it, loss=0.0169, lr=2.55e-05, step=5100]00:42:51.614 [I] step=5100 loss=0.0244 lr=2.53e-05 grad_norm=0.44 time=127.7s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5101/200000 [1:49:38<68:02:48,  1.26s/it, loss=0.0169, lr=2.55e-05, step=5100]Training:   3%|‚ñé         | 5101/200000 [1:49:38<68:02:48,  1.26s/it, loss=0.0158, lr=2.55e-05, step=5101]Training:   3%|‚ñé         | 5102/200000 [1:49:39<69:39:08,  1.29s/it, loss=0.0158, lr=2.55e-05, step=5101]Training:   3%|‚ñé         | 5102/200000 [1:49:39<69:39:08,  1.29s/it, loss=0.0168, lr=2.55e-05, step=5102]Training:   3%|‚ñé         | 5103/200000 [1:49:40<70:12:52,  1.30s/it, loss=0.0168, lr=2.55e-05, step=5102]Training:   3%|‚ñé         | 5103/200000 [1:49:40<70:12:52,  1.30s/it, loss=0.0118, lr=2.55e-05, step=5103]Training:   3%|‚ñé         | 5104/200000 [1:49:42<66:35:29,  1.23s/it, loss=0.0118, lr=2.55e-05, step=5103]Training:   3%|‚ñé         | 5104/200000 [1:49:42<66:35:29,  1.23s/it, loss=0.0249, lr=2.55e-05, step=5104]Training:   3%|‚ñé         | 5105/200000 [1:49:43<69:30:01,  1.28s/it, loss=0.0249, lr=2.55e-05, step=5104]Training:   3%|‚ñé         | 5105/200000 [1:49:43<69:30:01,  1.28s/it, loss=0.0206, lr=2.55e-05, step=5105]Training:   3%|‚ñé         | 5106/200000 [1:49:44<72:06:37,  1.33s/it, loss=0.0206, lr=2.55e-05, step=5105]Training:   3%|‚ñé         | 5106/200000 [1:49:44<72:06:37,  1.33s/it, loss=0.0237, lr=2.55e-05, step=5106]Training:   3%|‚ñé         | 5107/200000 [1:49:45<67:57:17,  1.26s/it, loss=0.0237, lr=2.55e-05, step=5106]Training:   3%|‚ñé         | 5107/200000 [1:49:45<67:57:17,  1.26s/it, loss=0.0119, lr=2.55e-05, step=5107]Training:   3%|‚ñé         | 5108/200000 [1:49:47<65:00:39,  1.20s/it, loss=0.0119, lr=2.55e-05, step=5107]Training:   3%|‚ñé         | 5108/200000 [1:49:47<65:00:39,  1.20s/it, loss=0.0169, lr=2.55e-05, step=5108]Training:   3%|‚ñé         | 5109/200000 [1:49:48<68:08:28,  1.26s/it, loss=0.0169, lr=2.55e-05, step=5108]Training:   3%|‚ñé         | 5109/200000 [1:49:48<68:08:28,  1.26s/it, loss=0.0147, lr=2.55e-05, step=5109]Training:   3%|‚ñé         | 5110/200000 [1:49:49<71:06:42,  1.31s/it, loss=0.0147, lr=2.55e-05, step=5109]Training:   3%|‚ñé         | 5110/200000 [1:49:49<71:06:42,  1.31s/it, loss=0.0220, lr=2.55e-05, step=5110]Training:   3%|‚ñé         | 5111/200000 [1:49:50<67:14:49,  1.24s/it, loss=0.0220, lr=2.55e-05, step=5110]Training:   3%|‚ñé         | 5111/200000 [1:49:50<67:14:49,  1.24s/it, loss=0.0183, lr=2.56e-05, step=5111]Training:   3%|‚ñé         | 5112/200000 [1:49:52<69:39:44,  1.29s/it, loss=0.0183, lr=2.56e-05, step=5111]Training:   3%|‚ñé         | 5112/200000 [1:49:52<69:39:44,  1.29s/it, loss=0.0240, lr=2.56e-05, step=5112]Training:   3%|‚ñé         | 5113/200000 [1:49:53<71:10:12,  1.31s/it, loss=0.0240, lr=2.56e-05, step=5112]Training:   3%|‚ñé         | 5113/200000 [1:49:53<71:10:12,  1.31s/it, loss=0.0184, lr=2.56e-05, step=5113]Training:   3%|‚ñé         | 5114/200000 [1:49:55<71:54:55,  1.33s/it, loss=0.0184, lr=2.56e-05, step=5113]Training:   3%|‚ñé         | 5114/200000 [1:49:55<71:54:55,  1.33s/it, loss=0.0120, lr=2.56e-05, step=5114]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5115/200000 [1:49:56<67:47:22,  1.25s/it, loss=0.0120, lr=2.56e-05, step=5114]Training:   3%|‚ñé         | 5115/200000 [1:49:56<67:47:22,  1.25s/it, loss=0.0171, lr=2.56e-05, step=5115]Training:   3%|‚ñé         | 5116/200000 [1:49:57<71:49:36,  1.33s/it, loss=0.0171, lr=2.56e-05, step=5115]Training:   3%|‚ñé         | 5116/200000 [1:49:57<71:49:36,  1.33s/it, loss=0.0113, lr=2.56e-05, step=5116]Training:   3%|‚ñé         | 5117/200000 [1:49:59<74:45:59,  1.38s/it, loss=0.0113, lr=2.56e-05, step=5116]Training:   3%|‚ñé         | 5117/200000 [1:49:59<74:45:59,  1.38s/it, loss=0.0209, lr=2.56e-05, step=5117]Training:   3%|‚ñé         | 5118/200000 [1:50:00<69:48:48,  1.29s/it, loss=0.0209, lr=2.56e-05, step=5117]Training:   3%|‚ñé         | 5118/200000 [1:50:00<69:48:48,  1.29s/it, loss=0.0255, lr=2.56e-05, step=5118]Training:   3%|‚ñé         | 5119/200000 [1:50:01<66:18:33,  1.22s/it, loss=0.0255, lr=2.56e-05, step=5118]Training:   3%|‚ñé         | 5119/200000 [1:50:01<66:18:33,  1.22s/it, loss=0.0167, lr=2.56e-05, step=5119]Training:   3%|‚ñé         | 5120/200000 [1:50:02<70:25:26,  1.30s/it, loss=0.0167, lr=2.56e-05, step=5119]Training:   3%|‚ñé         | 5120/200000 [1:50:02<70:25:26,  1.30s/it, loss=0.0159, lr=2.56e-05, step=5120]Training:   3%|‚ñé         | 5121/200000 [1:50:03<66:44:39,  1.23s/it, loss=0.0159, lr=2.56e-05, step=5120]Training:   3%|‚ñé         | 5121/200000 [1:50:03<66:44:39,  1.23s/it, loss=0.0147, lr=2.56e-05, step=5121]Training:   3%|‚ñé         | 5122/200000 [1:50:05<68:32:19,  1.27s/it, loss=0.0147, lr=2.56e-05, step=5121]Training:   3%|‚ñé         | 5122/200000 [1:50:05<68:32:19,  1.27s/it, loss=0.0163, lr=2.56e-05, step=5122]Training:   3%|‚ñé         | 5123/200000 [1:50:06<65:26:37,  1.21s/it, loss=0.0163, lr=2.56e-05, step=5122]Training:   3%|‚ñé         | 5123/200000 [1:50:06<65:26:37,  1.21s/it, loss=0.0480, lr=2.56e-05, step=5123]Training:   3%|‚ñé         | 5124/200000 [1:50:07<69:35:19,  1.29s/it, loss=0.0480, lr=2.56e-05, step=5123]Training:   3%|‚ñé         | 5124/200000 [1:50:07<69:35:19,  1.29s/it, loss=0.0182, lr=2.56e-05, step=5124]Training:   3%|‚ñé         | 5125/200000 [1:50:09<72:47:42,  1.34s/it, loss=0.0182, lr=2.56e-05, step=5124]Training:   3%|‚ñé         | 5125/200000 [1:50:09<72:47:42,  1.34s/it, loss=0.0400, lr=2.56e-05, step=5125]Training:   3%|‚ñé         | 5126/200000 [1:50:10<73:48:24,  1.36s/it, loss=0.0400, lr=2.56e-05, step=5125]Training:   3%|‚ñé         | 5126/200000 [1:50:10<73:48:24,  1.36s/it, loss=0.0544, lr=2.56e-05, step=5126]Training:   3%|‚ñé         | 5127/200000 [1:50:12<74:56:16,  1.38s/it, loss=0.0544, lr=2.56e-05, step=5126]Training:   3%|‚ñé         | 5127/200000 [1:50:12<74:56:16,  1.38s/it, loss=0.0148, lr=2.56e-05, step=5127]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5128/200000 [1:50:13<69:56:20,  1.29s/it, loss=0.0148, lr=2.56e-05, step=5127]Training:   3%|‚ñé         | 5128/200000 [1:50:13<69:56:20,  1.29s/it, loss=0.0159, lr=2.56e-05, step=5128]Training:   3%|‚ñé         | 5129/200000 [1:50:14<66:25:08,  1.23s/it, loss=0.0159, lr=2.56e-05, step=5128]Training:   3%|‚ñé         | 5129/200000 [1:50:14<66:25:08,  1.23s/it, loss=0.0187, lr=2.56e-05, step=5129]Training:   3%|‚ñé         | 5130/200000 [1:50:15<68:30:43,  1.27s/it, loss=0.0187, lr=2.56e-05, step=5129]Training:   3%|‚ñé         | 5130/200000 [1:50:15<68:30:43,  1.27s/it, loss=0.0126, lr=2.56e-05, step=5130]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5131/200000 [1:50:17<70:53:35,  1.31s/it, loss=0.0126, lr=2.56e-05, step=5130]Training:   3%|‚ñé         | 5131/200000 [1:50:17<70:53:35,  1.31s/it, loss=0.0120, lr=2.57e-05, step=5131]Training:   3%|‚ñé         | 5132/200000 [1:50:18<67:05:34,  1.24s/it, loss=0.0120, lr=2.57e-05, step=5131]Training:   3%|‚ñé         | 5132/200000 [1:50:18<67:05:34,  1.24s/it, loss=0.0569, lr=2.57e-05, step=5132]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5133/200000 [1:50:19<69:49:57,  1.29s/it, loss=0.0569, lr=2.57e-05, step=5132]Training:   3%|‚ñé         | 5133/200000 [1:50:19<69:49:57,  1.29s/it, loss=0.0151, lr=2.57e-05, step=5133]Training:   3%|‚ñé         | 5134/200000 [1:50:20<71:11:24,  1.32s/it, loss=0.0151, lr=2.57e-05, step=5133]Training:   3%|‚ñé         | 5134/200000 [1:50:20<71:11:24,  1.32s/it, loss=0.0177, lr=2.57e-05, step=5134]Training:   3%|‚ñé         | 5135/200000 [1:50:22<71:46:21,  1.33s/it, loss=0.0177, lr=2.57e-05, step=5134]Training:   3%|‚ñé         | 5135/200000 [1:50:22<71:46:21,  1.33s/it, loss=0.0218, lr=2.57e-05, step=5135]Training:   3%|‚ñé         | 5136/200000 [1:50:23<67:41:50,  1.25s/it, loss=0.0218, lr=2.57e-05, step=5135]Training:   3%|‚ñé         | 5136/200000 [1:50:23<67:41:50,  1.25s/it, loss=0.0159, lr=2.57e-05, step=5136]Training:   3%|‚ñé         | 5137/200000 [1:50:24<71:20:45,  1.32s/it, loss=0.0159, lr=2.57e-05, step=5136]Training:   3%|‚ñé         | 5137/200000 [1:50:24<71:20:45,  1.32s/it, loss=0.0153, lr=2.57e-05, step=5137]Training:   3%|‚ñé         | 5138/200000 [1:50:26<73:54:42,  1.37s/it, loss=0.0153, lr=2.57e-05, step=5137]Training:   3%|‚ñé         | 5138/200000 [1:50:26<73:54:42,  1.37s/it, loss=0.0176, lr=2.57e-05, step=5138]Training:   3%|‚ñé         | 5139/200000 [1:50:27<69:13:34,  1.28s/it, loss=0.0176, lr=2.57e-05, step=5138]Training:   3%|‚ñé         | 5139/200000 [1:50:27<69:13:34,  1.28s/it, loss=0.0139, lr=2.57e-05, step=5139]Training:   3%|‚ñé         | 5140/200000 [1:50:28<65:55:47,  1.22s/it, loss=0.0139, lr=2.57e-05, step=5139]Training:   3%|‚ñé         | 5140/200000 [1:50:28<65:55:47,  1.22s/it, loss=0.0151, lr=2.57e-05, step=5140]Training:   3%|‚ñé         | 5141/200000 [1:50:29<69:09:38,  1.28s/it, loss=0.0151, lr=2.57e-05, step=5140]Training:   3%|‚ñé         | 5141/200000 [1:50:29<69:09:38,  1.28s/it, loss=0.0104, lr=2.57e-05, step=5141]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5142/200000 [1:50:30<65:53:43,  1.22s/it, loss=0.0104, lr=2.57e-05, step=5141]Training:   3%|‚ñé         | 5142/200000 [1:50:30<65:53:43,  1.22s/it, loss=0.0170, lr=2.57e-05, step=5142]Training:   3%|‚ñé         | 5143/200000 [1:50:32<67:27:14,  1.25s/it, loss=0.0170, lr=2.57e-05, step=5142]Training:   3%|‚ñé         | 5143/200000 [1:50:32<67:27:14,  1.25s/it, loss=0.0151, lr=2.57e-05, step=5143]Training:   3%|‚ñé         | 5144/200000 [1:50:33<64:42:54,  1.20s/it, loss=0.0151, lr=2.57e-05, step=5143]Training:   3%|‚ñé         | 5144/200000 [1:50:33<64:42:54,  1.20s/it, loss=0.0154, lr=2.57e-05, step=5144]Training:   3%|‚ñé         | 5145/200000 [1:50:34<68:39:39,  1.27s/it, loss=0.0154, lr=2.57e-05, step=5144]Training:   3%|‚ñé         | 5145/200000 [1:50:34<68:39:39,  1.27s/it, loss=0.0170, lr=2.57e-05, step=5145]Training:   3%|‚ñé         | 5146/200000 [1:50:36<71:08:24,  1.31s/it, loss=0.0170, lr=2.57e-05, step=5145]Training:   3%|‚ñé         | 5146/200000 [1:50:36<71:08:24,  1.31s/it, loss=0.0145, lr=2.57e-05, step=5146]Training:   3%|‚ñé         | 5147/200000 [1:50:37<72:18:17,  1.34s/it, loss=0.0145, lr=2.57e-05, step=5146]Training:   3%|‚ñé         | 5147/200000 [1:50:37<72:18:17,  1.34s/it, loss=0.0208, lr=2.57e-05, step=5147]Training:   3%|‚ñé         | 5148/200000 [1:50:38<73:52:34,  1.36s/it, loss=0.0208, lr=2.57e-05, step=5147]Training:   3%|‚ñé         | 5148/200000 [1:50:38<73:52:34,  1.36s/it, loss=0.0241, lr=2.57e-05, step=5148]Training:   3%|‚ñé         | 5149/200000 [1:50:40<69:08:45,  1.28s/it, loss=0.0241, lr=2.57e-05, step=5148]Training:   3%|‚ñé         | 5149/200000 [1:50:40<69:08:45,  1.28s/it, loss=0.0174, lr=2.57e-05, step=5149]Training:   3%|‚ñé         | 5150/200000 [1:50:41<65:50:34,  1.22s/it, loss=0.0174, lr=2.57e-05, step=5149]Training:   3%|‚ñé         | 5150/200000 [1:50:41<65:50:34,  1.22s/it, loss=0.0157, lr=2.57e-05, step=5150]Training:   3%|‚ñé         | 5151/200000 [1:50:42<67:47:00,  1.25s/it, loss=0.0157, lr=2.57e-05, step=5150]Training:   3%|‚ñé         | 5151/200000 [1:50:42<67:47:00,  1.25s/it, loss=0.0167, lr=2.58e-05, step=5151]Training:   3%|‚ñé         | 5152/200000 [1:50:43<69:03:28,  1.28s/it, loss=0.0167, lr=2.58e-05, step=5151]Training:   3%|‚ñé         | 5152/200000 [1:50:43<69:03:28,  1.28s/it, loss=0.0270, lr=2.58e-05, step=5152]Training:   3%|‚ñé         | 5153/200000 [1:50:44<65:48:13,  1.22s/it, loss=0.0270, lr=2.58e-05, step=5152]Training:   3%|‚ñé         | 5153/200000 [1:50:44<65:48:13,  1.22s/it, loss=0.0353, lr=2.58e-05, step=5153]Training:   3%|‚ñé         | 5154/200000 [1:50:46<67:38:13,  1.25s/it, loss=0.0353, lr=2.58e-05, step=5153]Training:   3%|‚ñé         | 5154/200000 [1:50:46<67:38:13,  1.25s/it, loss=0.0201, lr=2.58e-05, step=5154]Training:   3%|‚ñé         | 5155/200000 [1:50:47<69:18:38,  1.28s/it, loss=0.0201, lr=2.58e-05, step=5154]Training:   3%|‚ñé         | 5155/200000 [1:50:47<69:18:38,  1.28s/it, loss=0.0162, lr=2.58e-05, step=5155]Training:   3%|‚ñé         | 5156/200000 [1:50:48<70:24:29,  1.30s/it, loss=0.0162, lr=2.58e-05, step=5155]Training:   3%|‚ñé         | 5156/200000 [1:50:48<70:24:29,  1.30s/it, loss=0.0146, lr=2.58e-05, step=5156]Training:   3%|‚ñé         | 5157/200000 [1:50:49<66:44:45,  1.23s/it, loss=0.0146, lr=2.58e-05, step=5156]Training:   3%|‚ñé         | 5157/200000 [1:50:49<66:44:45,  1.23s/it, loss=0.0141, lr=2.58e-05, step=5157]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5158/200000 [1:50:51<70:33:42,  1.30s/it, loss=0.0141, lr=2.58e-05, step=5157]Training:   3%|‚ñé         | 5158/200000 [1:50:51<70:33:42,  1.30s/it, loss=0.0181, lr=2.58e-05, step=5158]Training:   3%|‚ñé         | 5159/200000 [1:50:52<72:35:59,  1.34s/it, loss=0.0181, lr=2.58e-05, step=5158]Training:   3%|‚ñé         | 5159/200000 [1:50:52<72:35:59,  1.34s/it, loss=0.0159, lr=2.58e-05, step=5159]Training:   3%|‚ñé         | 5160/200000 [1:50:53<68:17:26,  1.26s/it, loss=0.0159, lr=2.58e-05, step=5159]Training:   3%|‚ñé         | 5160/200000 [1:50:53<68:17:26,  1.26s/it, loss=0.0388, lr=2.58e-05, step=5160]Training:   3%|‚ñé         | 5161/200000 [1:50:55<65:16:04,  1.21s/it, loss=0.0388, lr=2.58e-05, step=5160]Training:   3%|‚ñé         | 5161/200000 [1:50:55<65:16:04,  1.21s/it, loss=0.0283, lr=2.58e-05, step=5161]Training:   3%|‚ñé         | 5162/200000 [1:50:56<68:32:47,  1.27s/it, loss=0.0283, lr=2.58e-05, step=5161]Training:   3%|‚ñé         | 5162/200000 [1:50:56<68:32:47,  1.27s/it, loss=0.0148, lr=2.58e-05, step=5162]Training:   3%|‚ñé         | 5163/200000 [1:50:57<71:37:18,  1.32s/it, loss=0.0148, lr=2.58e-05, step=5162]Training:   3%|‚ñé         | 5163/200000 [1:50:57<71:37:18,  1.32s/it, loss=0.0193, lr=2.58e-05, step=5163]Training:   3%|‚ñé         | 5164/200000 [1:50:58<67:34:24,  1.25s/it, loss=0.0193, lr=2.58e-05, step=5163]Training:   3%|‚ñé         | 5164/200000 [1:50:58<67:34:24,  1.25s/it, loss=0.0296, lr=2.58e-05, step=5164]Training:   3%|‚ñé         | 5165/200000 [1:51:00<70:24:57,  1.30s/it, loss=0.0296, lr=2.58e-05, step=5164]Training:   3%|‚ñé         | 5165/200000 [1:51:00<70:24:57,  1.30s/it, loss=0.0127, lr=2.58e-05, step=5165]Training:   3%|‚ñé         | 5166/200000 [1:51:01<71:04:01,  1.31s/it, loss=0.0127, lr=2.58e-05, step=5165]Training:   3%|‚ñé         | 5166/200000 [1:51:01<71:04:01,  1.31s/it, loss=0.0159, lr=2.58e-05, step=5166]Training:   3%|‚ñé         | 5167/200000 [1:51:03<72:27:36,  1.34s/it, loss=0.0159, lr=2.58e-05, step=5166]Training:   3%|‚ñé         | 5167/200000 [1:51:03<72:27:36,  1.34s/it, loss=0.0159, lr=2.58e-05, step=5167]Training:   3%|‚ñé         | 5168/200000 [1:51:04<68:13:40,  1.26s/it, loss=0.0159, lr=2.58e-05, step=5167]Training:   3%|‚ñé         | 5168/200000 [1:51:04<68:13:40,  1.26s/it, loss=0.0137, lr=2.58e-05, step=5168]Training:   3%|‚ñé         | 5169/200000 [1:51:05<72:58:35,  1.35s/it, loss=0.0137, lr=2.58e-05, step=5168]Training:   3%|‚ñé         | 5169/200000 [1:51:05<72:58:35,  1.35s/it, loss=0.0169, lr=2.58e-05, step=5169]Training:   3%|‚ñé         | 5170/200000 [1:51:07<75:25:45,  1.39s/it, loss=0.0169, lr=2.58e-05, step=5169]Training:   3%|‚ñé         | 5170/200000 [1:51:07<75:25:45,  1.39s/it, loss=0.0225, lr=2.58e-05, step=5170]Training:   3%|‚ñé         | 5171/200000 [1:51:08<70:14:41,  1.30s/it, loss=0.0225, lr=2.58e-05, step=5170]Training:   3%|‚ñé         | 5171/200000 [1:51:08<70:14:41,  1.30s/it, loss=0.0142, lr=2.59e-05, step=5171]Training:   3%|‚ñé         | 5172/200000 [1:51:09<66:39:14,  1.23s/it, loss=0.0142, lr=2.59e-05, step=5171]Training:   3%|‚ñé         | 5172/200000 [1:51:09<66:39:14,  1.23s/it, loss=0.0143, lr=2.59e-05, step=5172]Training:   3%|‚ñé         | 5173/200000 [1:51:10<70:29:40,  1.30s/it, loss=0.0143, lr=2.59e-05, step=5172]Training:   3%|‚ñé         | 5173/200000 [1:51:10<70:29:40,  1.30s/it, loss=0.0148, lr=2.59e-05, step=5173]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5174/200000 [1:51:11<66:50:52,  1.24s/it, loss=0.0148, lr=2.59e-05, step=5173]Training:   3%|‚ñé         | 5174/200000 [1:51:11<66:50:52,  1.24s/it, loss=0.0147, lr=2.59e-05, step=5174]Training:   3%|‚ñé         | 5175/200000 [1:51:13<67:47:56,  1.25s/it, loss=0.0147, lr=2.59e-05, step=5174]Training:   3%|‚ñé         | 5175/200000 [1:51:13<67:47:56,  1.25s/it, loss=0.0197, lr=2.59e-05, step=5175]Training:   3%|‚ñé         | 5176/200000 [1:51:14<64:56:39,  1.20s/it, loss=0.0197, lr=2.59e-05, step=5175]Training:   3%|‚ñé         | 5176/200000 [1:51:14<64:56:39,  1.20s/it, loss=0.0181, lr=2.59e-05, step=5176]Training:   3%|‚ñé         | 5177/200000 [1:51:15<69:11:00,  1.28s/it, loss=0.0181, lr=2.59e-05, step=5176]Training:   3%|‚ñé         | 5177/200000 [1:51:15<69:11:00,  1.28s/it, loss=0.0177, lr=2.59e-05, step=5177]Training:   3%|‚ñé         | 5178/200000 [1:51:17<71:42:42,  1.33s/it, loss=0.0177, lr=2.59e-05, step=5177]Training:   3%|‚ñé         | 5178/200000 [1:51:17<71:42:42,  1.33s/it, loss=0.0260, lr=2.59e-05, step=5178]Training:   3%|‚ñé         | 5179/200000 [1:51:18<74:06:50,  1.37s/it, loss=0.0260, lr=2.59e-05, step=5178]Training:   3%|‚ñé         | 5179/200000 [1:51:18<74:06:50,  1.37s/it, loss=0.0133, lr=2.59e-05, step=5179]Training:   3%|‚ñé         | 5180/200000 [1:51:20<74:45:47,  1.38s/it, loss=0.0133, lr=2.59e-05, step=5179]Training:   3%|‚ñé         | 5180/200000 [1:51:20<74:45:47,  1.38s/it, loss=0.0189, lr=2.59e-05, step=5180]Training:   3%|‚ñé         | 5181/200000 [1:51:21<69:49:12,  1.29s/it, loss=0.0189, lr=2.59e-05, step=5180]Training:   3%|‚ñé         | 5181/200000 [1:51:21<69:49:12,  1.29s/it, loss=0.0221, lr=2.59e-05, step=5181]Training:   3%|‚ñé         | 5182/200000 [1:51:22<66:21:34,  1.23s/it, loss=0.0221, lr=2.59e-05, step=5181]Training:   3%|‚ñé         | 5182/200000 [1:51:22<66:21:34,  1.23s/it, loss=0.0108, lr=2.59e-05, step=5182]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5183/200000 [1:51:23<68:18:48,  1.26s/it, loss=0.0108, lr=2.59e-05, step=5182]Training:   3%|‚ñé         | 5183/200000 [1:51:23<68:18:48,  1.26s/it, loss=0.0274, lr=2.59e-05, step=5183]Training:   3%|‚ñé         | 5184/200000 [1:51:24<70:23:11,  1.30s/it, loss=0.0274, lr=2.59e-05, step=5183]Training:   3%|‚ñé         | 5184/200000 [1:51:24<70:23:11,  1.30s/it, loss=0.0227, lr=2.59e-05, step=5184]Training:   3%|‚ñé         | 5185/200000 [1:51:26<66:41:11,  1.23s/it, loss=0.0227, lr=2.59e-05, step=5184]Training:   3%|‚ñé         | 5185/200000 [1:51:26<66:41:11,  1.23s/it, loss=0.0311, lr=2.59e-05, step=5185]Training:   3%|‚ñé         | 5186/200000 [1:51:27<69:09:51,  1.28s/it, loss=0.0311, lr=2.59e-05, step=5185]Training:   3%|‚ñé         | 5186/200000 [1:51:27<69:09:51,  1.28s/it, loss=0.0119, lr=2.59e-05, step=5186]Training:   3%|‚ñé         | 5187/200000 [1:51:28<70:31:19,  1.30s/it, loss=0.0119, lr=2.59e-05, step=5186]Training:   3%|‚ñé         | 5187/200000 [1:51:28<70:31:19,  1.30s/it, loss=0.0275, lr=2.59e-05, step=5187]Training:   3%|‚ñé         | 5188/200000 [1:51:30<71:01:03,  1.31s/it, loss=0.0275, lr=2.59e-05, step=5187]Training:   3%|‚ñé         | 5188/200000 [1:51:30<71:01:03,  1.31s/it, loss=0.0179, lr=2.59e-05, step=5188]Training:   3%|‚ñé         | 5189/200000 [1:51:31<67:10:31,  1.24s/it, loss=0.0179, lr=2.59e-05, step=5188]Training:   3%|‚ñé         | 5189/200000 [1:51:31<67:10:31,  1.24s/it, loss=0.0196, lr=2.59e-05, step=5189]Training:   3%|‚ñé         | 5190/200000 [1:51:32<70:56:00,  1.31s/it, loss=0.0196, lr=2.59e-05, step=5189]Training:   3%|‚ñé         | 5190/200000 [1:51:32<70:56:00,  1.31s/it, loss=0.0144, lr=2.59e-05, step=5190]Training:   3%|‚ñé         | 5191/200000 [1:51:34<73:28:20,  1.36s/it, loss=0.0144, lr=2.59e-05, step=5190]Training:   3%|‚ñé         | 5191/200000 [1:51:34<73:28:20,  1.36s/it, loss=0.0388, lr=2.60e-05, step=5191]Training:   3%|‚ñé         | 5192/200000 [1:51:35<68:52:06,  1.27s/it, loss=0.0388, lr=2.60e-05, step=5191]Training:   3%|‚ñé         | 5192/200000 [1:51:35<68:52:06,  1.27s/it, loss=0.0309, lr=2.60e-05, step=5192]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5193/200000 [1:51:36<65:37:48,  1.21s/it, loss=0.0309, lr=2.60e-05, step=5192]Training:   3%|‚ñé         | 5193/200000 [1:51:36<65:37:48,  1.21s/it, loss=0.0090, lr=2.60e-05, step=5193]Training:   3%|‚ñé         | 5194/200000 [1:51:37<69:03:54,  1.28s/it, loss=0.0090, lr=2.60e-05, step=5193]Training:   3%|‚ñé         | 5194/200000 [1:51:37<69:03:54,  1.28s/it, loss=0.0347, lr=2.60e-05, step=5194]Training:   3%|‚ñé         | 5195/200000 [1:51:38<65:45:58,  1.22s/it, loss=0.0347, lr=2.60e-05, step=5194]Training:   3%|‚ñé         | 5195/200000 [1:51:38<65:45:58,  1.22s/it, loss=0.0146, lr=2.60e-05, step=5195]Training:   3%|‚ñé         | 5196/200000 [1:51:40<67:21:34,  1.24s/it, loss=0.0146, lr=2.60e-05, step=5195]Training:   3%|‚ñé         | 5196/200000 [1:51:40<67:21:34,  1.24s/it, loss=0.0222, lr=2.60e-05, step=5196]Training:   3%|‚ñé         | 5197/200000 [1:51:41<64:32:59,  1.19s/it, loss=0.0222, lr=2.60e-05, step=5196]Training:   3%|‚ñé         | 5197/200000 [1:51:41<64:32:59,  1.19s/it, loss=0.0245, lr=2.60e-05, step=5197]Training:   3%|‚ñé         | 5198/200000 [1:51:42<68:37:51,  1.27s/it, loss=0.0245, lr=2.60e-05, step=5197]Training:   3%|‚ñé         | 5198/200000 [1:51:42<68:37:51,  1.27s/it, loss=0.0170, lr=2.60e-05, step=5198]Training:   3%|‚ñé         | 5199/200000 [1:51:44<71:38:42,  1.32s/it, loss=0.0170, lr=2.60e-05, step=5198]Training:   3%|‚ñé         | 5199/200000 [1:51:44<71:38:42,  1.32s/it, loss=0.0210, lr=2.60e-05, step=5199]Training:   3%|‚ñé         | 5200/200000 [1:51:45<73:42:30,  1.36s/it, loss=0.0210, lr=2.60e-05, step=5199]Training:   3%|‚ñé         | 5200/200000 [1:51:45<73:42:30,  1.36s/it, loss=0.0183, lr=2.60e-05, step=5200]00:45:00.271 [I] step=5200 loss=0.0201 lr=2.58e-05 grad_norm=0.41 time=128.7s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5201/200000 [1:51:46<74:51:52,  1.38s/it, loss=0.0183, lr=2.60e-05, step=5200]Training:   3%|‚ñé         | 5201/200000 [1:51:46<74:51:52,  1.38s/it, loss=0.0309, lr=2.60e-05, step=5201]Training:   3%|‚ñé         | 5202/200000 [1:51:48<69:49:10,  1.29s/it, loss=0.0309, lr=2.60e-05, step=5201]Training:   3%|‚ñé         | 5202/200000 [1:51:48<69:49:10,  1.29s/it, loss=0.0288, lr=2.60e-05, step=5202]Training:   3%|‚ñé         | 5203/200000 [1:51:49<66:14:34,  1.22s/it, loss=0.0288, lr=2.60e-05, step=5202]Training:   3%|‚ñé         | 5203/200000 [1:51:49<66:14:34,  1.22s/it, loss=0.0249, lr=2.60e-05, step=5203]Training:   3%|‚ñé         | 5204/200000 [1:51:50<68:06:58,  1.26s/it, loss=0.0249, lr=2.60e-05, step=5203]Training:   3%|‚ñé         | 5204/200000 [1:51:50<68:06:58,  1.26s/it, loss=0.0153, lr=2.60e-05, step=5204]Training:   3%|‚ñé         | 5205/200000 [1:51:51<69:24:49,  1.28s/it, loss=0.0153, lr=2.60e-05, step=5204]Training:   3%|‚ñé         | 5205/200000 [1:51:51<69:24:49,  1.28s/it, loss=0.0175, lr=2.60e-05, step=5205]Training:   3%|‚ñé         | 5206/200000 [1:51:52<66:01:28,  1.22s/it, loss=0.0175, lr=2.60e-05, step=5205]Training:   3%|‚ñé         | 5206/200000 [1:51:52<66:01:28,  1.22s/it, loss=0.0160, lr=2.60e-05, step=5206]Training:   3%|‚ñé         | 5207/200000 [1:51:54<67:58:30,  1.26s/it, loss=0.0160, lr=2.60e-05, step=5206]Training:   3%|‚ñé         | 5207/200000 [1:51:54<67:58:30,  1.26s/it, loss=0.0203, lr=2.60e-05, step=5207]Training:   3%|‚ñé         | 5208/200000 [1:51:55<69:30:58,  1.28s/it, loss=0.0203, lr=2.60e-05, step=5207]Training:   3%|‚ñé         | 5208/200000 [1:51:55<69:30:58,  1.28s/it, loss=0.0354, lr=2.60e-05, step=5208]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5209/200000 [1:51:56<70:28:53,  1.30s/it, loss=0.0354, lr=2.60e-05, step=5208]Training:   3%|‚ñé         | 5209/200000 [1:51:56<70:28:53,  1.30s/it, loss=0.0126, lr=2.60e-05, step=5209]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5210/200000 [1:51:57<66:45:40,  1.23s/it, loss=0.0126, lr=2.60e-05, step=5209]Training:   3%|‚ñé         | 5210/200000 [1:51:57<66:45:40,  1.23s/it, loss=0.0198, lr=2.60e-05, step=5210]Training:   3%|‚ñé         | 5211/200000 [1:51:59<69:38:08,  1.29s/it, loss=0.0198, lr=2.60e-05, step=5210]Training:   3%|‚ñé         | 5211/200000 [1:51:59<69:38:08,  1.29s/it, loss=0.0180, lr=2.61e-05, step=5211]Training:   3%|‚ñé         | 5212/200000 [1:52:00<71:26:08,  1.32s/it, loss=0.0180, lr=2.61e-05, step=5211]Training:   3%|‚ñé         | 5212/200000 [1:52:00<71:26:08,  1.32s/it, loss=0.0223, lr=2.61e-05, step=5212]Training:   3%|‚ñé         | 5213/200000 [1:52:01<67:25:26,  1.25s/it, loss=0.0223, lr=2.61e-05, step=5212]Training:   3%|‚ñé         | 5213/200000 [1:52:01<67:25:26,  1.25s/it, loss=0.0515, lr=2.61e-05, step=5213]Training:   3%|‚ñé         | 5214/200000 [1:52:02<64:36:46,  1.19s/it, loss=0.0515, lr=2.61e-05, step=5213]Training:   3%|‚ñé         | 5214/200000 [1:52:02<64:36:46,  1.19s/it, loss=0.0205, lr=2.61e-05, step=5214]Training:   3%|‚ñé         | 5215/200000 [1:52:04<67:57:12,  1.26s/it, loss=0.0205, lr=2.61e-05, step=5214]Training:   3%|‚ñé         | 5215/200000 [1:52:04<67:57:12,  1.26s/it, loss=0.0221, lr=2.61e-05, step=5215]Training:   3%|‚ñé         | 5216/200000 [1:52:05<70:16:02,  1.30s/it, loss=0.0221, lr=2.61e-05, step=5215]Training:   3%|‚ñé         | 5216/200000 [1:52:05<70:16:02,  1.30s/it, loss=0.0183, lr=2.61e-05, step=5216]Training:   3%|‚ñé         | 5217/200000 [1:52:06<66:34:59,  1.23s/it, loss=0.0183, lr=2.61e-05, step=5216]Training:   3%|‚ñé         | 5217/200000 [1:52:06<66:34:59,  1.23s/it, loss=0.0170, lr=2.61e-05, step=5217]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5218/200000 [1:52:08<69:23:43,  1.28s/it, loss=0.0170, lr=2.61e-05, step=5217]Training:   3%|‚ñé         | 5218/200000 [1:52:08<69:23:43,  1.28s/it, loss=0.0161, lr=2.61e-05, step=5218]Training:   3%|‚ñé         | 5219/200000 [1:52:09<70:21:25,  1.30s/it, loss=0.0161, lr=2.61e-05, step=5218]Training:   3%|‚ñé         | 5219/200000 [1:52:09<70:21:25,  1.30s/it, loss=0.0162, lr=2.61e-05, step=5219]Training:   3%|‚ñé         | 5220/200000 [1:52:10<71:17:13,  1.32s/it, loss=0.0162, lr=2.61e-05, step=5219]Training:   3%|‚ñé         | 5220/200000 [1:52:10<71:17:13,  1.32s/it, loss=0.0509, lr=2.61e-05, step=5220]Training:   3%|‚ñé         | 5221/200000 [1:52:11<67:20:06,  1.24s/it, loss=0.0509, lr=2.61e-05, step=5220]Training:   3%|‚ñé         | 5221/200000 [1:52:11<67:20:06,  1.24s/it, loss=0.0208, lr=2.61e-05, step=5221]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5222/200000 [1:52:13<71:27:32,  1.32s/it, loss=0.0208, lr=2.61e-05, step=5221]Training:   3%|‚ñé         | 5222/200000 [1:52:13<71:27:32,  1.32s/it, loss=0.0167, lr=2.61e-05, step=5222]Training:   3%|‚ñé         | 5223/200000 [1:52:14<74:46:37,  1.38s/it, loss=0.0167, lr=2.61e-05, step=5222]Training:   3%|‚ñé         | 5223/200000 [1:52:14<74:46:37,  1.38s/it, loss=0.0154, lr=2.61e-05, step=5223]Training:   3%|‚ñé         | 5224/200000 [1:52:16<69:49:39,  1.29s/it, loss=0.0154, lr=2.61e-05, step=5223]Training:   3%|‚ñé         | 5224/200000 [1:52:16<69:49:39,  1.29s/it, loss=0.0128, lr=2.61e-05, step=5224]Training:   3%|‚ñé         | 5225/200000 [1:52:17<66:19:36,  1.23s/it, loss=0.0128, lr=2.61e-05, step=5224]Training:   3%|‚ñé         | 5225/200000 [1:52:17<66:19:36,  1.23s/it, loss=0.0258, lr=2.61e-05, step=5225]Training:   3%|‚ñé         | 5226/200000 [1:52:18<70:25:49,  1.30s/it, loss=0.0258, lr=2.61e-05, step=5225]Training:   3%|‚ñé         | 5226/200000 [1:52:18<70:25:49,  1.30s/it, loss=0.0174, lr=2.61e-05, step=5226]Training:   3%|‚ñé         | 5227/200000 [1:52:19<66:44:26,  1.23s/it, loss=0.0174, lr=2.61e-05, step=5226]Training:   3%|‚ñé         | 5227/200000 [1:52:19<66:44:26,  1.23s/it, loss=0.0258, lr=2.61e-05, step=5227]Training:   3%|‚ñé         | 5228/200000 [1:52:21<68:45:30,  1.27s/it, loss=0.0258, lr=2.61e-05, step=5227]Training:   3%|‚ñé         | 5228/200000 [1:52:21<68:45:30,  1.27s/it, loss=0.0179, lr=2.61e-05, step=5228]Training:   3%|‚ñé         | 5229/200000 [1:52:22<65:34:53,  1.21s/it, loss=0.0179, lr=2.61e-05, step=5228]Training:   3%|‚ñé         | 5229/200000 [1:52:22<65:34:53,  1.21s/it, loss=0.0285, lr=2.61e-05, step=5229]Training:   3%|‚ñé         | 5230/200000 [1:52:23<69:34:44,  1.29s/it, loss=0.0285, lr=2.61e-05, step=5229]Training:   3%|‚ñé         | 5230/200000 [1:52:23<69:34:44,  1.29s/it, loss=0.0150, lr=2.61e-05, step=5230]Training:   3%|‚ñé         | 5231/200000 [1:52:25<72:47:00,  1.35s/it, loss=0.0150, lr=2.61e-05, step=5230]Training:   3%|‚ñé         | 5231/200000 [1:52:25<72:47:00,  1.35s/it, loss=0.0172, lr=2.62e-05, step=5231]Training:   3%|‚ñé         | 5232/200000 [1:52:26<73:55:39,  1.37s/it, loss=0.0172, lr=2.62e-05, step=5231]Training:   3%|‚ñé         | 5232/200000 [1:52:26<73:55:39,  1.37s/it, loss=0.0205, lr=2.62e-05, step=5232]Training:   3%|‚ñé         | 5233/200000 [1:52:27<75:07:29,  1.39s/it, loss=0.0205, lr=2.62e-05, step=5232]Training:   3%|‚ñé         | 5233/200000 [1:52:27<75:07:29,  1.39s/it, loss=0.0283, lr=2.62e-05, step=5233]Training:   3%|‚ñé         | 5234/200000 [1:52:29<70:05:37,  1.30s/it, loss=0.0283, lr=2.62e-05, step=5233]Training:   3%|‚ñé         | 5234/200000 [1:52:29<70:05:37,  1.30s/it, loss=0.0184, lr=2.62e-05, step=5234]Training:   3%|‚ñé         | 5235/200000 [1:52:30<66:29:52,  1.23s/it, loss=0.0184, lr=2.62e-05, step=5234]Training:   3%|‚ñé         | 5235/200000 [1:52:30<66:29:52,  1.23s/it, loss=0.0234, lr=2.62e-05, step=5235]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5236/200000 [1:52:31<68:25:33,  1.26s/it, loss=0.0234, lr=2.62e-05, step=5235]Training:   3%|‚ñé         | 5236/200000 [1:52:31<68:25:33,  1.26s/it, loss=0.0154, lr=2.62e-05, step=5236]Training:   3%|‚ñé         | 5237/200000 [1:52:32<70:49:38,  1.31s/it, loss=0.0154, lr=2.62e-05, step=5236]Training:   3%|‚ñé         | 5237/200000 [1:52:32<70:49:38,  1.31s/it, loss=0.0152, lr=2.62e-05, step=5237]Training:   3%|‚ñé         | 5238/200000 [1:52:33<67:00:02,  1.24s/it, loss=0.0152, lr=2.62e-05, step=5237]Training:   3%|‚ñé         | 5238/200000 [1:52:33<67:00:02,  1.24s/it, loss=0.0501, lr=2.62e-05, step=5238]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5239/200000 [1:52:35<69:38:17,  1.29s/it, loss=0.0501, lr=2.62e-05, step=5238]Training:   3%|‚ñé         | 5239/200000 [1:52:35<69:38:17,  1.29s/it, loss=0.0209, lr=2.62e-05, step=5239]Training:   3%|‚ñé         | 5240/200000 [1:52:36<70:59:16,  1.31s/it, loss=0.0209, lr=2.62e-05, step=5239]Training:   3%|‚ñé         | 5240/200000 [1:52:36<70:59:16,  1.31s/it, loss=0.0234, lr=2.62e-05, step=5240]Training:   3%|‚ñé         | 5241/200000 [1:52:38<71:41:55,  1.33s/it, loss=0.0234, lr=2.62e-05, step=5240]Training:   3%|‚ñé         | 5241/200000 [1:52:38<71:41:55,  1.33s/it, loss=0.0171, lr=2.62e-05, step=5241]Training:   3%|‚ñé         | 5242/200000 [1:52:39<67:39:36,  1.25s/it, loss=0.0171, lr=2.62e-05, step=5241]Training:   3%|‚ñé         | 5242/200000 [1:52:39<67:39:36,  1.25s/it, loss=0.0263, lr=2.62e-05, step=5242]Training:   3%|‚ñé         | 5243/200000 [1:52:40<71:16:46,  1.32s/it, loss=0.0263, lr=2.62e-05, step=5242]Training:   3%|‚ñé         | 5243/200000 [1:52:40<71:16:46,  1.32s/it, loss=0.0290, lr=2.62e-05, step=5243]Training:   3%|‚ñé         | 5244/200000 [1:52:42<74:19:53,  1.37s/it, loss=0.0290, lr=2.62e-05, step=5243]Training:   3%|‚ñé         | 5244/200000 [1:52:42<74:19:53,  1.37s/it, loss=0.0080, lr=2.62e-05, step=5244]Training:   3%|‚ñé         | 5245/200000 [1:52:43<69:28:50,  1.28s/it, loss=0.0080, lr=2.62e-05, step=5244]Training:   3%|‚ñé         | 5245/200000 [1:52:43<69:28:50,  1.28s/it, loss=0.0221, lr=2.62e-05, step=5245]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5246/200000 [1:52:44<66:05:08,  1.22s/it, loss=0.0221, lr=2.62e-05, step=5245]Training:   3%|‚ñé         | 5246/200000 [1:52:44<66:05:08,  1.22s/it, loss=0.0131, lr=2.62e-05, step=5246]Training:   3%|‚ñé         | 5247/200000 [1:52:45<69:25:00,  1.28s/it, loss=0.0131, lr=2.62e-05, step=5246]Training:   3%|‚ñé         | 5247/200000 [1:52:45<69:25:00,  1.28s/it, loss=0.0165, lr=2.62e-05, step=5247]Training:   3%|‚ñé         | 5248/200000 [1:52:46<66:03:24,  1.22s/it, loss=0.0165, lr=2.62e-05, step=5247]Training:   3%|‚ñé         | 5248/200000 [1:52:46<66:03:24,  1.22s/it, loss=0.0334, lr=2.62e-05, step=5248]Training:   3%|‚ñé         | 5249/200000 [1:52:48<67:48:36,  1.25s/it, loss=0.0334, lr=2.62e-05, step=5248]Training:   3%|‚ñé         | 5249/200000 [1:52:48<67:48:36,  1.25s/it, loss=0.0185, lr=2.62e-05, step=5249]Training:   3%|‚ñé         | 5250/200000 [1:52:49<64:58:08,  1.20s/it, loss=0.0185, lr=2.62e-05, step=5249]Training:   3%|‚ñé         | 5250/200000 [1:52:49<64:58:08,  1.20s/it, loss=0.0206, lr=2.62e-05, step=5250]Training:   3%|‚ñé         | 5251/200000 [1:52:50<68:50:17,  1.27s/it, loss=0.0206, lr=2.62e-05, step=5250]Training:   3%|‚ñé         | 5251/200000 [1:52:50<68:50:17,  1.27s/it, loss=0.0125, lr=2.63e-05, step=5251]Training:   3%|‚ñé         | 5252/200000 [1:52:52<71:16:06,  1.32s/it, loss=0.0125, lr=2.63e-05, step=5251]Training:   3%|‚ñé         | 5252/200000 [1:52:52<71:16:06,  1.32s/it, loss=0.0159, lr=2.63e-05, step=5252]Training:   3%|‚ñé         | 5253/200000 [1:52:53<72:42:41,  1.34s/it, loss=0.0159, lr=2.63e-05, step=5252]Training:   3%|‚ñé         | 5253/200000 [1:52:53<72:42:41,  1.34s/it, loss=0.0193, lr=2.63e-05, step=5253]Training:   3%|‚ñé         | 5254/200000 [1:52:54<74:11:03,  1.37s/it, loss=0.0193, lr=2.63e-05, step=5253]Training:   3%|‚ñé         | 5254/200000 [1:52:54<74:11:03,  1.37s/it, loss=0.0090, lr=2.63e-05, step=5254]Training:   3%|‚ñé         | 5255/200000 [1:52:55<69:23:02,  1.28s/it, loss=0.0090, lr=2.63e-05, step=5254]Training:   3%|‚ñé         | 5255/200000 [1:52:55<69:23:02,  1.28s/it, loss=0.0127, lr=2.63e-05, step=5255]Training:   3%|‚ñé         | 5256/200000 [1:52:57<66:01:36,  1.22s/it, loss=0.0127, lr=2.63e-05, step=5255]Training:   3%|‚ñé         | 5256/200000 [1:52:57<66:01:36,  1.22s/it, loss=0.0107, lr=2.63e-05, step=5256]Training:   3%|‚ñé         | 5257/200000 [1:52:58<67:43:41,  1.25s/it, loss=0.0107, lr=2.63e-05, step=5256]Training:   3%|‚ñé         | 5257/200000 [1:52:58<67:43:41,  1.25s/it, loss=0.0246, lr=2.63e-05, step=5257]Training:   3%|‚ñé         | 5258/200000 [1:52:59<69:18:14,  1.28s/it, loss=0.0246, lr=2.63e-05, step=5257]Training:   3%|‚ñé         | 5258/200000 [1:52:59<69:18:14,  1.28s/it, loss=0.0127, lr=2.63e-05, step=5258]Training:   3%|‚ñé         | 5259/200000 [1:53:00<65:57:05,  1.22s/it, loss=0.0127, lr=2.63e-05, step=5258]Training:   3%|‚ñé         | 5259/200000 [1:53:00<65:57:05,  1.22s/it, loss=0.0201, lr=2.63e-05, step=5259]Training:   3%|‚ñé         | 5260/200000 [1:53:02<67:47:18,  1.25s/it, loss=0.0201, lr=2.63e-05, step=5259]Training:   3%|‚ñé         | 5260/200000 [1:53:02<67:47:18,  1.25s/it, loss=0.0163, lr=2.63e-05, step=5260]Training:   3%|‚ñé         | 5261/200000 [1:53:03<69:25:20,  1.28s/it, loss=0.0163, lr=2.63e-05, step=5260]Training:   3%|‚ñé         | 5261/200000 [1:53:03<69:25:20,  1.28s/it, loss=0.0159, lr=2.63e-05, step=5261]Training:   3%|‚ñé         | 5262/200000 [1:53:04<70:08:48,  1.30s/it, loss=0.0159, lr=2.63e-05, step=5261]Training:   3%|‚ñé         | 5262/200000 [1:53:04<70:08:48,  1.30s/it, loss=0.0207, lr=2.63e-05, step=5262]Training:   3%|‚ñé         | 5263/200000 [1:53:05<66:32:37,  1.23s/it, loss=0.0207, lr=2.63e-05, step=5262]Training:   3%|‚ñé         | 5263/200000 [1:53:05<66:32:37,  1.23s/it, loss=0.0268, lr=2.63e-05, step=5263]Training:   3%|‚ñé         | 5264/200000 [1:53:07<69:34:31,  1.29s/it, loss=0.0268, lr=2.63e-05, step=5263]Training:   3%|‚ñé         | 5264/200000 [1:53:07<69:34:31,  1.29s/it, loss=0.0185, lr=2.63e-05, step=5264]Training:   3%|‚ñé         | 5265/200000 [1:53:08<72:11:42,  1.33s/it, loss=0.0185, lr=2.63e-05, step=5264]Training:   3%|‚ñé         | 5265/200000 [1:53:08<72:11:42,  1.33s/it, loss=0.0226, lr=2.63e-05, step=5265]Training:   3%|‚ñé         | 5266/200000 [1:53:09<67:59:46,  1.26s/it, loss=0.0226, lr=2.63e-05, step=5265]Training:   3%|‚ñé         | 5266/200000 [1:53:09<67:59:46,  1.26s/it, loss=0.0156, lr=2.63e-05, step=5266]Training:   3%|‚ñé         | 5267/200000 [1:53:10<65:01:37,  1.20s/it, loss=0.0156, lr=2.63e-05, step=5266]Training:   3%|‚ñé         | 5267/200000 [1:53:10<65:01:37,  1.20s/it, loss=0.0368, lr=2.63e-05, step=5267]Training:   3%|‚ñé         | 5268/200000 [1:53:12<68:17:44,  1.26s/it, loss=0.0368, lr=2.63e-05, step=5267]Training:   3%|‚ñé         | 5268/200000 [1:53:12<68:17:44,  1.26s/it, loss=0.0126, lr=2.63e-05, step=5268]Training:   3%|‚ñé         | 5269/200000 [1:53:13<71:25:40,  1.32s/it, loss=0.0126, lr=2.63e-05, step=5268]Training:   3%|‚ñé         | 5269/200000 [1:53:13<71:25:40,  1.32s/it, loss=0.0133, lr=2.63e-05, step=5269]Training:   3%|‚ñé         | 5270/200000 [1:53:14<67:27:28,  1.25s/it, loss=0.0133, lr=2.63e-05, step=5269]Training:   3%|‚ñé         | 5270/200000 [1:53:14<67:27:28,  1.25s/it, loss=0.0137, lr=2.63e-05, step=5270]Training:   3%|‚ñé         | 5271/200000 [1:53:16<70:26:13,  1.30s/it, loss=0.0137, lr=2.63e-05, step=5270]Training:   3%|‚ñé         | 5271/200000 [1:53:16<70:26:13,  1.30s/it, loss=0.0211, lr=2.64e-05, step=5271]Training:   3%|‚ñé         | 5272/200000 [1:53:17<70:58:14,  1.31s/it, loss=0.0211, lr=2.64e-05, step=5271]Training:   3%|‚ñé         | 5272/200000 [1:53:17<70:58:14,  1.31s/it, loss=0.0221, lr=2.64e-05, step=5272]Training:   3%|‚ñé         | 5273/200000 [1:53:18<72:24:30,  1.34s/it, loss=0.0221, lr=2.64e-05, step=5272]Training:   3%|‚ñé         | 5273/200000 [1:53:18<72:24:30,  1.34s/it, loss=0.0152, lr=2.64e-05, step=5273]Training:   3%|‚ñé         | 5274/200000 [1:53:20<68:09:36,  1.26s/it, loss=0.0152, lr=2.64e-05, step=5273]Training:   3%|‚ñé         | 5274/200000 [1:53:20<68:09:36,  1.26s/it, loss=0.0166, lr=2.64e-05, step=5274]Training:   3%|‚ñé         | 5275/200000 [1:53:21<72:08:32,  1.33s/it, loss=0.0166, lr=2.64e-05, step=5274]Training:   3%|‚ñé         | 5275/200000 [1:53:21<72:08:32,  1.33s/it, loss=0.0215, lr=2.64e-05, step=5275]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5276/200000 [1:53:23<75:15:14,  1.39s/it, loss=0.0215, lr=2.64e-05, step=5275]Training:   3%|‚ñé         | 5276/200000 [1:53:23<75:15:14,  1.39s/it, loss=0.0212, lr=2.64e-05, step=5276]Training:   3%|‚ñé         | 5277/200000 [1:53:24<70:08:26,  1.30s/it, loss=0.0212, lr=2.64e-05, step=5276]Training:   3%|‚ñé         | 5277/200000 [1:53:24<70:08:26,  1.30s/it, loss=0.0215, lr=2.64e-05, step=5277]Training:   3%|‚ñé         | 5278/200000 [1:53:25<66:34:30,  1.23s/it, loss=0.0215, lr=2.64e-05, step=5277]Training:   3%|‚ñé         | 5278/200000 [1:53:25<66:34:30,  1.23s/it, loss=0.0182, lr=2.64e-05, step=5278]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5279/200000 [1:53:26<70:38:38,  1.31s/it, loss=0.0182, lr=2.64e-05, step=5278]Training:   3%|‚ñé         | 5279/200000 [1:53:26<70:38:38,  1.31s/it, loss=0.0321, lr=2.64e-05, step=5279]Training:   3%|‚ñé         | 5280/200000 [1:53:27<66:55:57,  1.24s/it, loss=0.0321, lr=2.64e-05, step=5279]Training:   3%|‚ñé         | 5280/200000 [1:53:27<66:55:57,  1.24s/it, loss=0.0191, lr=2.64e-05, step=5280]Training:   3%|‚ñé         | 5281/200000 [1:53:29<68:24:37,  1.26s/it, loss=0.0191, lr=2.64e-05, step=5280]Training:   3%|‚ñé         | 5281/200000 [1:53:29<68:24:37,  1.26s/it, loss=0.0133, lr=2.64e-05, step=5281]Training:   3%|‚ñé         | 5282/200000 [1:53:30<65:21:50,  1.21s/it, loss=0.0133, lr=2.64e-05, step=5281]Training:   3%|‚ñé         | 5282/200000 [1:53:30<65:21:50,  1.21s/it, loss=0.0195, lr=2.64e-05, step=5282]Training:   3%|‚ñé         | 5283/200000 [1:53:31<69:23:06,  1.28s/it, loss=0.0195, lr=2.64e-05, step=5282]Training:   3%|‚ñé         | 5283/200000 [1:53:31<69:23:06,  1.28s/it, loss=0.0180, lr=2.64e-05, step=5283]Training:   3%|‚ñé         | 5284/200000 [1:53:33<71:53:27,  1.33s/it, loss=0.0180, lr=2.64e-05, step=5283]Training:   3%|‚ñé         | 5284/200000 [1:53:33<71:53:27,  1.33s/it, loss=0.0183, lr=2.64e-05, step=5284]Training:   3%|‚ñé         | 5285/200000 [1:53:34<73:05:37,  1.35s/it, loss=0.0183, lr=2.64e-05, step=5284]Training:   3%|‚ñé         | 5285/200000 [1:53:34<73:05:37,  1.35s/it, loss=0.0223, lr=2.64e-05, step=5285]Training:   3%|‚ñé         | 5286/200000 [1:53:35<74:31:48,  1.38s/it, loss=0.0223, lr=2.64e-05, step=5285]Training:   3%|‚ñé         | 5286/200000 [1:53:35<74:31:48,  1.38s/it, loss=0.0338, lr=2.64e-05, step=5286]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5287/200000 [1:53:37<69:37:16,  1.29s/it, loss=0.0338, lr=2.64e-05, step=5286]Training:   3%|‚ñé         | 5287/200000 [1:53:37<69:37:16,  1.29s/it, loss=0.0164, lr=2.64e-05, step=5287]Training:   3%|‚ñé         | 5288/200000 [1:53:38<66:11:28,  1.22s/it, loss=0.0164, lr=2.64e-05, step=5287]Training:   3%|‚ñé         | 5288/200000 [1:53:38<66:11:28,  1.22s/it, loss=0.0201, lr=2.64e-05, step=5288]Training:   3%|‚ñé         | 5289/200000 [1:53:39<68:10:48,  1.26s/it, loss=0.0201, lr=2.64e-05, step=5288]Training:   3%|‚ñé         | 5289/200000 [1:53:39<68:10:48,  1.26s/it, loss=0.0260, lr=2.64e-05, step=5289]Training:   3%|‚ñé         | 5290/200000 [1:53:40<70:35:35,  1.31s/it, loss=0.0260, lr=2.64e-05, step=5289]Training:   3%|‚ñé         | 5290/200000 [1:53:40<70:35:35,  1.31s/it, loss=0.0093, lr=2.64e-05, step=5290]Training:   3%|‚ñé         | 5291/200000 [1:53:41<66:52:13,  1.24s/it, loss=0.0093, lr=2.64e-05, step=5290]Training:   3%|‚ñé         | 5291/200000 [1:53:41<66:52:13,  1.24s/it, loss=0.0132, lr=2.65e-05, step=5291]Training:   3%|‚ñé         | 5292/200000 [1:53:43<69:39:17,  1.29s/it, loss=0.0132, lr=2.65e-05, step=5291]Training:   3%|‚ñé         | 5292/200000 [1:53:43<69:39:17,  1.29s/it, loss=0.1665, lr=2.65e-05, step=5292]Training:   3%|‚ñé         | 5293/200000 [1:53:44<71:00:14,  1.31s/it, loss=0.1665, lr=2.65e-05, step=5292]Training:   3%|‚ñé         | 5293/200000 [1:53:44<71:00:14,  1.31s/it, loss=0.2038, lr=2.65e-05, step=5293]Training:   3%|‚ñé         | 5294/200000 [1:53:46<71:35:08,  1.32s/it, loss=0.2038, lr=2.65e-05, step=5293]Training:   3%|‚ñé         | 5294/200000 [1:53:46<71:35:08,  1.32s/it, loss=0.0287, lr=2.65e-05, step=5294]Training:   3%|‚ñé         | 5295/200000 [1:53:47<67:33:55,  1.25s/it, loss=0.0287, lr=2.65e-05, step=5294]Training:   3%|‚ñé         | 5295/200000 [1:53:47<67:33:55,  1.25s/it, loss=0.0234, lr=2.65e-05, step=5295]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5296/200000 [1:53:48<71:03:08,  1.31s/it, loss=0.0234, lr=2.65e-05, step=5295]Training:   3%|‚ñé         | 5296/200000 [1:53:48<71:03:08,  1.31s/it, loss=0.0108, lr=2.65e-05, step=5296]Training:   3%|‚ñé         | 5297/200000 [1:53:50<74:05:49,  1.37s/it, loss=0.0108, lr=2.65e-05, step=5296]Training:   3%|‚ñé         | 5297/200000 [1:53:50<74:05:49,  1.37s/it, loss=0.0192, lr=2.65e-05, step=5297]Training:   3%|‚ñé         | 5298/200000 [1:53:51<69:20:05,  1.28s/it, loss=0.0192, lr=2.65e-05, step=5297]Training:   3%|‚ñé         | 5298/200000 [1:53:51<69:20:05,  1.28s/it, loss=0.0209, lr=2.65e-05, step=5298]Training:   3%|‚ñé         | 5299/200000 [1:53:52<65:58:30,  1.22s/it, loss=0.0209, lr=2.65e-05, step=5298]Training:   3%|‚ñé         | 5299/200000 [1:53:52<65:58:30,  1.22s/it, loss=0.0215, lr=2.65e-05, step=5299]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5300/200000 [1:53:53<69:22:35,  1.28s/it, loss=0.0215, lr=2.65e-05, step=5299]Training:   3%|‚ñé         | 5300/200000 [1:53:53<69:22:35,  1.28s/it, loss=0.0227, lr=2.65e-05, step=5300]00:47:08.049 [I] step=5300 loss=0.0238 lr=2.63e-05 grad_norm=0.42 time=127.8s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5301/200000 [1:53:54<66:00:58,  1.22s/it, loss=0.0227, lr=2.65e-05, step=5300]Training:   3%|‚ñé         | 5301/200000 [1:53:54<66:00:58,  1.22s/it, loss=0.0203, lr=2.65e-05, step=5301]Training:   3%|‚ñé         | 5302/200000 [1:53:56<66:53:18,  1.24s/it, loss=0.0203, lr=2.65e-05, step=5301]Training:   3%|‚ñé         | 5302/200000 [1:53:56<66:53:18,  1.24s/it, loss=0.0159, lr=2.65e-05, step=5302]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5303/200000 [1:53:57<64:14:36,  1.19s/it, loss=0.0159, lr=2.65e-05, step=5302]Training:   3%|‚ñé         | 5303/200000 [1:53:57<64:14:36,  1.19s/it, loss=0.0188, lr=2.65e-05, step=5303]Training:   3%|‚ñé         | 5304/200000 [1:53:58<68:21:35,  1.26s/it, loss=0.0188, lr=2.65e-05, step=5303]Training:   3%|‚ñé         | 5304/200000 [1:53:58<68:21:35,  1.26s/it, loss=0.0194, lr=2.65e-05, step=5304]Training:   3%|‚ñé         | 5305/200000 [1:53:59<71:25:36,  1.32s/it, loss=0.0194, lr=2.65e-05, step=5304]Training:   3%|‚ñé         | 5305/200000 [1:53:59<71:25:36,  1.32s/it, loss=0.0196, lr=2.65e-05, step=5305]Training:   3%|‚ñé         | 5306/200000 [1:54:01<72:49:27,  1.35s/it, loss=0.0196, lr=2.65e-05, step=5305]Training:   3%|‚ñé         | 5306/200000 [1:54:01<72:49:27,  1.35s/it, loss=0.0211, lr=2.65e-05, step=5306]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5307/200000 [1:54:02<74:01:25,  1.37s/it, loss=0.0211, lr=2.65e-05, step=5306]Training:   3%|‚ñé         | 5307/200000 [1:54:02<74:01:25,  1.37s/it, loss=0.0207, lr=2.65e-05, step=5307]Training:   3%|‚ñé         | 5308/200000 [1:54:03<69:17:09,  1.28s/it, loss=0.0207, lr=2.65e-05, step=5307]Training:   3%|‚ñé         | 5308/200000 [1:54:03<69:17:09,  1.28s/it, loss=0.0178, lr=2.65e-05, step=5308]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5309/200000 [1:54:04<65:56:32,  1.22s/it, loss=0.0178, lr=2.65e-05, step=5308]Training:   3%|‚ñé         | 5309/200000 [1:54:04<65:56:32,  1.22s/it, loss=0.0245, lr=2.65e-05, step=5309]Training:   3%|‚ñé         | 5310/200000 [1:54:06<67:49:45,  1.25s/it, loss=0.0245, lr=2.65e-05, step=5309]Training:   3%|‚ñé         | 5310/200000 [1:54:06<67:49:45,  1.25s/it, loss=0.0260, lr=2.65e-05, step=5310]Training:   3%|‚ñé         | 5311/200000 [1:54:07<69:59:55,  1.29s/it, loss=0.0260, lr=2.65e-05, step=5310]Training:   3%|‚ñé         | 5311/200000 [1:54:07<69:59:55,  1.29s/it, loss=0.0140, lr=2.66e-05, step=5311]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5312/200000 [1:54:08<66:27:36,  1.23s/it, loss=0.0140, lr=2.66e-05, step=5311]Training:   3%|‚ñé         | 5312/200000 [1:54:08<66:27:36,  1.23s/it, loss=0.0138, lr=2.66e-05, step=5312]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5313/200000 [1:54:10<68:23:24,  1.26s/it, loss=0.0138, lr=2.66e-05, step=5312]Training:   3%|‚ñé         | 5313/200000 [1:54:10<68:23:24,  1.26s/it, loss=0.0187, lr=2.66e-05, step=5313]Training:   3%|‚ñé         | 5314/200000 [1:54:11<69:53:37,  1.29s/it, loss=0.0187, lr=2.66e-05, step=5313]Training:   3%|‚ñé         | 5314/200000 [1:54:11<69:53:37,  1.29s/it, loss=0.0155, lr=2.66e-05, step=5314]Training:   3%|‚ñé         | 5315/200000 [1:54:12<70:23:26,  1.30s/it, loss=0.0155, lr=2.66e-05, step=5314]Training:   3%|‚ñé         | 5315/200000 [1:54:12<70:23:26,  1.30s/it, loss=0.0337, lr=2.66e-05, step=5315]Training:   3%|‚ñé         | 5316/200000 [1:54:13<66:44:20,  1.23s/it, loss=0.0337, lr=2.66e-05, step=5315]Training:   3%|‚ñé         | 5316/200000 [1:54:13<66:44:20,  1.23s/it, loss=0.0228, lr=2.66e-05, step=5316]Training:   3%|‚ñé         | 5317/200000 [1:54:15<70:24:23,  1.30s/it, loss=0.0228, lr=2.66e-05, step=5316]Training:   3%|‚ñé         | 5317/200000 [1:54:15<70:24:23,  1.30s/it, loss=0.0141, lr=2.66e-05, step=5317]Training:   3%|‚ñé         | 5318/200000 [1:54:16<71:54:42,  1.33s/it, loss=0.0141, lr=2.66e-05, step=5317]Training:   3%|‚ñé         | 5318/200000 [1:54:16<71:54:42,  1.33s/it, loss=0.0141, lr=2.66e-05, step=5318]Training:   3%|‚ñé         | 5319/200000 [1:54:17<67:48:28,  1.25s/it, loss=0.0141, lr=2.66e-05, step=5318]Training:   3%|‚ñé         | 5319/200000 [1:54:17<67:48:28,  1.25s/it, loss=0.0312, lr=2.66e-05, step=5319]Training:   3%|‚ñé         | 5320/200000 [1:54:18<64:55:48,  1.20s/it, loss=0.0312, lr=2.66e-05, step=5319]Training:   3%|‚ñé         | 5320/200000 [1:54:18<64:55:48,  1.20s/it, loss=0.0157, lr=2.66e-05, step=5320]Training:   3%|‚ñé         | 5321/200000 [1:54:20<68:10:04,  1.26s/it, loss=0.0157, lr=2.66e-05, step=5320]Training:   3%|‚ñé         | 5321/200000 [1:54:20<68:10:04,  1.26s/it, loss=0.0139, lr=2.66e-05, step=5321]Training:   3%|‚ñé         | 5322/200000 [1:54:21<70:29:22,  1.30s/it, loss=0.0139, lr=2.66e-05, step=5321]Training:   3%|‚ñé         | 5322/200000 [1:54:21<70:29:22,  1.30s/it, loss=0.0175, lr=2.66e-05, step=5322]Training:   3%|‚ñé         | 5323/200000 [1:54:22<66:46:14,  1.23s/it, loss=0.0175, lr=2.66e-05, step=5322]Training:   3%|‚ñé         | 5323/200000 [1:54:22<66:46:14,  1.23s/it, loss=0.0226, lr=2.66e-05, step=5323]Training:   3%|‚ñé         | 5324/200000 [1:54:24<68:59:11,  1.28s/it, loss=0.0226, lr=2.66e-05, step=5323]Training:   3%|‚ñé         | 5324/200000 [1:54:24<68:59:11,  1.28s/it, loss=0.0247, lr=2.66e-05, step=5324]Training:   3%|‚ñé         | 5325/200000 [1:54:25<70:00:23,  1.29s/it, loss=0.0247, lr=2.66e-05, step=5324]Training:   3%|‚ñé         | 5325/200000 [1:54:25<70:00:23,  1.29s/it, loss=0.0195, lr=2.66e-05, step=5325]Training:   3%|‚ñé         | 5326/200000 [1:54:26<71:01:38,  1.31s/it, loss=0.0195, lr=2.66e-05, step=5325]Training:   3%|‚ñé         | 5326/200000 [1:54:26<71:01:38,  1.31s/it, loss=0.0315, lr=2.66e-05, step=5326]Training:   3%|‚ñé         | 5327/200000 [1:54:27<67:09:16,  1.24s/it, loss=0.0315, lr=2.66e-05, step=5326]Training:   3%|‚ñé         | 5327/200000 [1:54:27<67:09:16,  1.24s/it, loss=0.0200, lr=2.66e-05, step=5327]Training:   3%|‚ñé         | 5328/200000 [1:54:29<72:11:46,  1.34s/it, loss=0.0200, lr=2.66e-05, step=5327]Training:   3%|‚ñé         | 5328/200000 [1:54:29<72:11:46,  1.34s/it, loss=0.0137, lr=2.66e-05, step=5328]Training:   3%|‚ñé         | 5329/200000 [1:54:30<75:04:15,  1.39s/it, loss=0.0137, lr=2.66e-05, step=5328]Training:   3%|‚ñé         | 5329/200000 [1:54:30<75:04:15,  1.39s/it, loss=0.0229, lr=2.66e-05, step=5329]Training:   3%|‚ñé         | 5330/200000 [1:54:32<70:01:43,  1.30s/it, loss=0.0229, lr=2.66e-05, step=5329]Training:   3%|‚ñé         | 5330/200000 [1:54:32<70:01:43,  1.30s/it, loss=0.0652, lr=2.66e-05, step=5330]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5331/200000 [1:54:33<66:26:29,  1.23s/it, loss=0.0652, lr=2.66e-05, step=5330]Training:   3%|‚ñé         | 5331/200000 [1:54:33<66:26:29,  1.23s/it, loss=0.0356, lr=2.67e-05, step=5331]Training:   3%|‚ñé         | 5332/200000 [1:54:34<70:35:05,  1.31s/it, loss=0.0356, lr=2.67e-05, step=5331]Training:   3%|‚ñé         | 5332/200000 [1:54:34<70:35:05,  1.31s/it, loss=0.0189, lr=2.67e-05, step=5332]Training:   3%|‚ñé         | 5333/200000 [1:54:35<66:51:35,  1.24s/it, loss=0.0189, lr=2.67e-05, step=5332]Training:   3%|‚ñé         | 5333/200000 [1:54:35<66:51:35,  1.24s/it, loss=0.0097, lr=2.67e-05, step=5333]Training:   3%|‚ñé         | 5334/200000 [1:54:37<68:22:22,  1.26s/it, loss=0.0097, lr=2.67e-05, step=5333]Training:   3%|‚ñé         | 5334/200000 [1:54:37<68:22:22,  1.26s/it, loss=0.0213, lr=2.67e-05, step=5334]Training:   3%|‚ñé         | 5335/200000 [1:54:38<65:18:25,  1.21s/it, loss=0.0213, lr=2.67e-05, step=5334]Training:   3%|‚ñé         | 5335/200000 [1:54:38<65:18:25,  1.21s/it, loss=0.0219, lr=2.67e-05, step=5335]Training:   3%|‚ñé         | 5336/200000 [1:54:39<68:51:33,  1.27s/it, loss=0.0219, lr=2.67e-05, step=5335]Training:   3%|‚ñé         | 5336/200000 [1:54:39<68:51:33,  1.27s/it, loss=0.0177, lr=2.67e-05, step=5336]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5337/200000 [1:54:40<72:14:32,  1.34s/it, loss=0.0177, lr=2.67e-05, step=5336]Training:   3%|‚ñé         | 5337/200000 [1:54:40<72:14:32,  1.34s/it, loss=0.0144, lr=2.67e-05, step=5337]Training:   3%|‚ñé         | 5338/200000 [1:54:42<73:37:50,  1.36s/it, loss=0.0144, lr=2.67e-05, step=5337]Training:   3%|‚ñé         | 5338/200000 [1:54:42<73:37:50,  1.36s/it, loss=0.0121, lr=2.67e-05, step=5338]Training:   3%|‚ñé         | 5339/200000 [1:54:43<74:49:21,  1.38s/it, loss=0.0121, lr=2.67e-05, step=5338]Training:   3%|‚ñé         | 5339/200000 [1:54:43<74:49:21,  1.38s/it, loss=0.0217, lr=2.67e-05, step=5339]Training:   3%|‚ñé         | 5340/200000 [1:54:44<69:49:46,  1.29s/it, loss=0.0217, lr=2.67e-05, step=5339]Training:   3%|‚ñé         | 5340/200000 [1:54:44<69:49:46,  1.29s/it, loss=0.0241, lr=2.67e-05, step=5340]Training:   3%|‚ñé         | 5341/200000 [1:54:45<66:20:35,  1.23s/it, loss=0.0241, lr=2.67e-05, step=5340]Training:   3%|‚ñé         | 5341/200000 [1:54:45<66:20:35,  1.23s/it, loss=0.0197, lr=2.67e-05, step=5341]Training:   3%|‚ñé         | 5342/200000 [1:54:47<68:29:31,  1.27s/it, loss=0.0197, lr=2.67e-05, step=5341]Training:   3%|‚ñé         | 5342/200000 [1:54:47<68:29:31,  1.27s/it, loss=0.0133, lr=2.67e-05, step=5342]Training:   3%|‚ñé         | 5343/200000 [1:54:48<71:13:06,  1.32s/it, loss=0.0133, lr=2.67e-05, step=5342]Training:   3%|‚ñé         | 5343/200000 [1:54:48<71:13:06,  1.32s/it, loss=0.0170, lr=2.67e-05, step=5343]Training:   3%|‚ñé         | 5344/200000 [1:54:49<67:18:41,  1.24s/it, loss=0.0170, lr=2.67e-05, step=5343]Training:   3%|‚ñé         | 5344/200000 [1:54:49<67:18:41,  1.24s/it, loss=0.0171, lr=2.67e-05, step=5344]Training:   3%|‚ñé         | 5345/200000 [1:54:51<69:49:56,  1.29s/it, loss=0.0171, lr=2.67e-05, step=5344]Training:   3%|‚ñé         | 5345/200000 [1:54:51<69:49:56,  1.29s/it, loss=0.0139, lr=2.67e-05, step=5345]Training:   3%|‚ñé         | 5346/200000 [1:54:52<71:04:18,  1.31s/it, loss=0.0139, lr=2.67e-05, step=5345]Training:   3%|‚ñé         | 5346/200000 [1:54:52<71:04:18,  1.31s/it, loss=0.0102, lr=2.67e-05, step=5346]Training:   3%|‚ñé         | 5347/200000 [1:54:53<71:36:10,  1.32s/it, loss=0.0102, lr=2.67e-05, step=5346]Training:   3%|‚ñé         | 5347/200000 [1:54:53<71:36:10,  1.32s/it, loss=0.0149, lr=2.67e-05, step=5347]Training:   3%|‚ñé         | 5348/200000 [1:54:55<67:34:39,  1.25s/it, loss=0.0149, lr=2.67e-05, step=5347]Training:   3%|‚ñé         | 5348/200000 [1:54:55<67:34:39,  1.25s/it, loss=0.0261, lr=2.67e-05, step=5348]Training:   3%|‚ñé         | 5349/200000 [1:54:56<72:08:15,  1.33s/it, loss=0.0261, lr=2.67e-05, step=5348]Training:   3%|‚ñé         | 5349/200000 [1:54:56<72:08:15,  1.33s/it, loss=0.0262, lr=2.67e-05, step=5349]Training:   3%|‚ñé         | 5350/200000 [1:54:58<74:59:03,  1.39s/it, loss=0.0262, lr=2.67e-05, step=5349]Training:   3%|‚ñé         | 5350/200000 [1:54:58<74:59:03,  1.39s/it, loss=0.0194, lr=2.67e-05, step=5350]Training:   3%|‚ñé         | 5351/200000 [1:54:59<69:55:10,  1.29s/it, loss=0.0194, lr=2.67e-05, step=5350]Training:   3%|‚ñé         | 5351/200000 [1:54:59<69:55:10,  1.29s/it, loss=0.0138, lr=2.68e-05, step=5351]Training:   3%|‚ñé         | 5352/200000 [1:55:00<66:23:07,  1.23s/it, loss=0.0138, lr=2.68e-05, step=5351]Training:   3%|‚ñé         | 5352/200000 [1:55:00<66:23:07,  1.23s/it, loss=0.0184, lr=2.68e-05, step=5352]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5353/200000 [1:55:01<69:38:43,  1.29s/it, loss=0.0184, lr=2.68e-05, step=5352]Training:   3%|‚ñé         | 5353/200000 [1:55:01<69:38:43,  1.29s/it, loss=0.0251, lr=2.68e-05, step=5353]Training:   3%|‚ñé         | 5354/200000 [1:55:02<66:12:47,  1.22s/it, loss=0.0251, lr=2.68e-05, step=5353]Training:   3%|‚ñé         | 5354/200000 [1:55:02<66:12:47,  1.22s/it, loss=0.0222, lr=2.68e-05, step=5354]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5355/200000 [1:55:04<67:37:14,  1.25s/it, loss=0.0222, lr=2.68e-05, step=5354]Training:   3%|‚ñé         | 5355/200000 [1:55:04<67:37:14,  1.25s/it, loss=0.0189, lr=2.68e-05, step=5355]Training:   3%|‚ñé         | 5356/200000 [1:55:05<64:44:09,  1.20s/it, loss=0.0189, lr=2.68e-05, step=5355]Training:   3%|‚ñé         | 5356/200000 [1:55:05<64:44:09,  1.20s/it, loss=0.0216, lr=2.68e-05, step=5356]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5357/200000 [1:55:06<68:31:55,  1.27s/it, loss=0.0216, lr=2.68e-05, step=5356]Training:   3%|‚ñé         | 5357/200000 [1:55:06<68:31:55,  1.27s/it, loss=0.0221, lr=2.68e-05, step=5357]Training:   3%|‚ñé         | 5358/200000 [1:55:07<71:01:54,  1.31s/it, loss=0.0221, lr=2.68e-05, step=5357]Training:   3%|‚ñé         | 5358/200000 [1:55:07<71:01:54,  1.31s/it, loss=0.0186, lr=2.68e-05, step=5358]Training:   3%|‚ñé         | 5359/200000 [1:55:09<72:05:30,  1.33s/it, loss=0.0186, lr=2.68e-05, step=5358]Training:   3%|‚ñé         | 5359/200000 [1:55:09<72:05:30,  1.33s/it, loss=0.0207, lr=2.68e-05, step=5359]Training:   3%|‚ñé         | 5360/200000 [1:55:10<73:37:22,  1.36s/it, loss=0.0207, lr=2.68e-05, step=5359]Training:   3%|‚ñé         | 5360/200000 [1:55:10<73:37:22,  1.36s/it, loss=0.0179, lr=2.68e-05, step=5360]Training:   3%|‚ñé         | 5361/200000 [1:55:11<69:00:00,  1.28s/it, loss=0.0179, lr=2.68e-05, step=5360]Training:   3%|‚ñé         | 5361/200000 [1:55:11<69:00:00,  1.28s/it, loss=0.0184, lr=2.68e-05, step=5361]Training:   3%|‚ñé         | 5362/200000 [1:55:12<65:47:44,  1.22s/it, loss=0.0184, lr=2.68e-05, step=5361]Training:   3%|‚ñé         | 5362/200000 [1:55:12<65:47:44,  1.22s/it, loss=0.0159, lr=2.68e-05, step=5362]Training:   3%|‚ñé         | 5363/200000 [1:55:14<67:52:51,  1.26s/it, loss=0.0159, lr=2.68e-05, step=5362]Training:   3%|‚ñé         | 5363/200000 [1:55:14<67:52:51,  1.26s/it, loss=0.0330, lr=2.68e-05, step=5363]Training:   3%|‚ñé         | 5364/200000 [1:55:15<68:30:36,  1.27s/it, loss=0.0330, lr=2.68e-05, step=5363]Training:   3%|‚ñé         | 5364/200000 [1:55:15<68:30:36,  1.27s/it, loss=0.0186, lr=2.68e-05, step=5364]Training:   3%|‚ñé         | 5365/200000 [1:55:16<65:26:01,  1.21s/it, loss=0.0186, lr=2.68e-05, step=5364]Training:   3%|‚ñé         | 5365/200000 [1:55:16<65:26:01,  1.21s/it, loss=0.0174, lr=2.68e-05, step=5365]Training:   3%|‚ñé         | 5366/200000 [1:55:17<67:22:01,  1.25s/it, loss=0.0174, lr=2.68e-05, step=5365]Training:   3%|‚ñé         | 5366/200000 [1:55:18<67:22:01,  1.25s/it, loss=0.0203, lr=2.68e-05, step=5366]Training:   3%|‚ñé         | 5367/200000 [1:55:19<69:06:36,  1.28s/it, loss=0.0203, lr=2.68e-05, step=5366]Training:   3%|‚ñé         | 5367/200000 [1:55:19<69:06:36,  1.28s/it, loss=0.0140, lr=2.68e-05, step=5367]Training:   3%|‚ñé         | 5368/200000 [1:55:20<69:50:51,  1.29s/it, loss=0.0140, lr=2.68e-05, step=5367]Training:   3%|‚ñé         | 5368/200000 [1:55:20<69:50:51,  1.29s/it, loss=0.0161, lr=2.68e-05, step=5368]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5369/200000 [1:55:21<66:21:09,  1.23s/it, loss=0.0161, lr=2.68e-05, step=5368]Training:   3%|‚ñé         | 5369/200000 [1:55:21<66:21:09,  1.23s/it, loss=0.0125, lr=2.68e-05, step=5369]Training:   3%|‚ñé         | 5370/200000 [1:55:23<69:29:07,  1.29s/it, loss=0.0125, lr=2.68e-05, step=5369]Training:   3%|‚ñé         | 5370/200000 [1:55:23<69:29:07,  1.29s/it, loss=0.0149, lr=2.68e-05, step=5370]Training:   3%|‚ñé         | 5371/200000 [1:55:24<72:02:01,  1.33s/it, loss=0.0149, lr=2.68e-05, step=5370]Training:   3%|‚ñé         | 5371/200000 [1:55:24<72:02:01,  1.33s/it, loss=0.0174, lr=2.69e-05, step=5371]Training:   3%|‚ñé         | 5372/200000 [1:55:25<67:54:41,  1.26s/it, loss=0.0174, lr=2.69e-05, step=5371]Training:   3%|‚ñé         | 5372/200000 [1:55:25<67:54:41,  1.26s/it, loss=0.0213, lr=2.69e-05, step=5372]Training:   3%|‚ñé         | 5373/200000 [1:55:26<65:00:09,  1.20s/it, loss=0.0213, lr=2.69e-05, step=5372]Training:   3%|‚ñé         | 5373/200000 [1:55:26<65:00:09,  1.20s/it, loss=0.0191, lr=2.69e-05, step=5373]Training:   3%|‚ñé         | 5374/200000 [1:55:28<68:07:02,  1.26s/it, loss=0.0191, lr=2.69e-05, step=5373]Training:   3%|‚ñé         | 5374/200000 [1:55:28<68:07:02,  1.26s/it, loss=0.0162, lr=2.69e-05, step=5374]Training:   3%|‚ñé         | 5375/200000 [1:55:29<70:35:36,  1.31s/it, loss=0.0162, lr=2.69e-05, step=5374]Training:   3%|‚ñé         | 5375/200000 [1:55:29<70:35:36,  1.31s/it, loss=0.0144, lr=2.69e-05, step=5375]Training:   3%|‚ñé         | 5376/200000 [1:55:30<66:51:16,  1.24s/it, loss=0.0144, lr=2.69e-05, step=5375]Training:   3%|‚ñé         | 5376/200000 [1:55:30<66:51:16,  1.24s/it, loss=0.0366, lr=2.69e-05, step=5376]Training:   3%|‚ñé         | 5377/200000 [1:55:32<69:50:15,  1.29s/it, loss=0.0366, lr=2.69e-05, step=5376]Training:   3%|‚ñé         | 5377/200000 [1:55:32<69:50:15,  1.29s/it, loss=0.0172, lr=2.69e-05, step=5377]Training:   3%|‚ñé         | 5378/200000 [1:55:33<70:45:16,  1.31s/it, loss=0.0172, lr=2.69e-05, step=5377]Training:   3%|‚ñé         | 5378/200000 [1:55:33<70:45:16,  1.31s/it, loss=0.0187, lr=2.69e-05, step=5378]Training:   3%|‚ñé         | 5379/200000 [1:55:34<72:17:31,  1.34s/it, loss=0.0187, lr=2.69e-05, step=5378]Training:   3%|‚ñé         | 5379/200000 [1:55:34<72:17:31,  1.34s/it, loss=0.0208, lr=2.69e-05, step=5379]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5380/200000 [1:55:35<68:05:03,  1.26s/it, loss=0.0208, lr=2.69e-05, step=5379]Training:   3%|‚ñé         | 5380/200000 [1:55:35<68:05:03,  1.26s/it, loss=0.0123, lr=2.69e-05, step=5380]Training:   3%|‚ñé         | 5381/200000 [1:55:37<71:53:32,  1.33s/it, loss=0.0123, lr=2.69e-05, step=5380]Training:   3%|‚ñé         | 5381/200000 [1:55:37<71:53:32,  1.33s/it, loss=0.0276, lr=2.69e-05, step=5381]Training:   3%|‚ñé         | 5382/200000 [1:55:38<75:02:04,  1.39s/it, loss=0.0276, lr=2.69e-05, step=5381]Training:   3%|‚ñé         | 5382/200000 [1:55:38<75:02:04,  1.39s/it, loss=0.0371, lr=2.69e-05, step=5382]Training:   3%|‚ñé         | 5383/200000 [1:55:39<69:57:41,  1.29s/it, loss=0.0371, lr=2.69e-05, step=5382]Training:   3%|‚ñé         | 5383/200000 [1:55:39<69:57:41,  1.29s/it, loss=0.0297, lr=2.69e-05, step=5383]Training:   3%|‚ñé         | 5384/200000 [1:55:41<66:25:15,  1.23s/it, loss=0.0297, lr=2.69e-05, step=5383]Training:   3%|‚ñé         | 5384/200000 [1:55:41<66:25:15,  1.23s/it, loss=0.0132, lr=2.69e-05, step=5384]Training:   3%|‚ñé         | 5385/200000 [1:55:42<70:26:45,  1.30s/it, loss=0.0132, lr=2.69e-05, step=5384]Training:   3%|‚ñé         | 5385/200000 [1:55:42<70:26:45,  1.30s/it, loss=0.0114, lr=2.69e-05, step=5385]Training:   3%|‚ñé         | 5386/200000 [1:55:43<66:46:09,  1.24s/it, loss=0.0114, lr=2.69e-05, step=5385]Training:   3%|‚ñé         | 5386/200000 [1:55:43<66:46:09,  1.24s/it, loss=0.0182, lr=2.69e-05, step=5386]Training:   3%|‚ñé         | 5387/200000 [1:55:44<67:57:21,  1.26s/it, loss=0.0182, lr=2.69e-05, step=5386]Training:   3%|‚ñé         | 5387/200000 [1:55:44<67:57:21,  1.26s/it, loss=0.0105, lr=2.69e-05, step=5387]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5388/200000 [1:55:46<65:01:40,  1.20s/it, loss=0.0105, lr=2.69e-05, step=5387]Training:   3%|‚ñé         | 5388/200000 [1:55:46<65:01:40,  1.20s/it, loss=0.0127, lr=2.69e-05, step=5388]Training:   3%|‚ñé         | 5389/200000 [1:55:47<69:12:16,  1.28s/it, loss=0.0127, lr=2.69e-05, step=5388]Training:   3%|‚ñé         | 5389/200000 [1:55:47<69:12:16,  1.28s/it, loss=0.0158, lr=2.69e-05, step=5389]Training:   3%|‚ñé         | 5390/200000 [1:55:48<71:47:05,  1.33s/it, loss=0.0158, lr=2.69e-05, step=5389]Training:   3%|‚ñé         | 5390/200000 [1:55:48<71:47:05,  1.33s/it, loss=0.0223, lr=2.69e-05, step=5390]Training:   3%|‚ñé         | 5391/200000 [1:55:50<72:50:53,  1.35s/it, loss=0.0223, lr=2.69e-05, step=5390]Training:   3%|‚ñé         | 5391/200000 [1:55:50<72:50:53,  1.35s/it, loss=0.0150, lr=2.70e-05, step=5391]Training:   3%|‚ñé         | 5392/200000 [1:55:51<74:17:38,  1.37s/it, loss=0.0150, lr=2.70e-05, step=5391]Training:   3%|‚ñé         | 5392/200000 [1:55:51<74:17:38,  1.37s/it, loss=0.0187, lr=2.70e-05, step=5392]Training:   3%|‚ñé         | 5393/200000 [1:55:52<69:26:31,  1.28s/it, loss=0.0187, lr=2.70e-05, step=5392]Training:   3%|‚ñé         | 5393/200000 [1:55:52<69:26:31,  1.28s/it, loss=0.4137, lr=2.70e-05, step=5393]Training:   3%|‚ñé         | 5394/200000 [1:55:53<66:05:00,  1.22s/it, loss=0.4137, lr=2.70e-05, step=5393]Training:   3%|‚ñé         | 5394/200000 [1:55:53<66:05:00,  1.22s/it, loss=0.0187, lr=2.70e-05, step=5394]Training:   3%|‚ñé         | 5395/200000 [1:55:55<68:03:16,  1.26s/it, loss=0.0187, lr=2.70e-05, step=5394]Training:   3%|‚ñé         | 5395/200000 [1:55:55<68:03:16,  1.26s/it, loss=0.0157, lr=2.70e-05, step=5395]Training:   3%|‚ñé         | 5396/200000 [1:55:56<69:38:55,  1.29s/it, loss=0.0157, lr=2.70e-05, step=5395]Training:   3%|‚ñé         | 5396/200000 [1:55:56<69:38:55,  1.29s/it, loss=0.0245, lr=2.70e-05, step=5396]Training:   3%|‚ñé         | 5397/200000 [1:55:57<66:11:37,  1.22s/it, loss=0.0245, lr=2.70e-05, step=5396]Training:   3%|‚ñé         | 5397/200000 [1:55:57<66:11:37,  1.22s/it, loss=0.0266, lr=2.70e-05, step=5397]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5398/200000 [1:55:59<68:53:53,  1.27s/it, loss=0.0266, lr=2.70e-05, step=5397]Training:   3%|‚ñé         | 5398/200000 [1:55:59<68:53:53,  1.27s/it, loss=0.0128, lr=2.70e-05, step=5398]Training:   3%|‚ñé         | 5399/200000 [1:56:00<69:57:47,  1.29s/it, loss=0.0128, lr=2.70e-05, step=5398]Training:   3%|‚ñé         | 5399/200000 [1:56:00<69:57:47,  1.29s/it, loss=0.0201, lr=2.70e-05, step=5399]Training:   3%|‚ñé         | 5400/200000 [1:56:01<71:02:56,  1.31s/it, loss=0.0201, lr=2.70e-05, step=5399]Training:   3%|‚ñé         | 5400/200000 [1:56:01<71:02:56,  1.31s/it, loss=0.0211, lr=2.70e-05, step=5400]00:49:16.147 [I] step=5400 loss=0.0240 lr=2.68e-05 grad_norm=0.41 time=128.1s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5401/200000 [1:56:02<67:09:38,  1.24s/it, loss=0.0211, lr=2.70e-05, step=5400]Training:   3%|‚ñé         | 5401/200000 [1:56:02<67:09:38,  1.24s/it, loss=0.0355, lr=2.70e-05, step=5401]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5402/200000 [1:56:04<70:49:25,  1.31s/it, loss=0.0355, lr=2.70e-05, step=5401]Training:   3%|‚ñé         | 5402/200000 [1:56:04<70:49:25,  1.31s/it, loss=0.0193, lr=2.70e-05, step=5402]Training:   3%|‚ñé         | 5403/200000 [1:56:05<73:56:19,  1.37s/it, loss=0.0193, lr=2.70e-05, step=5402]Training:   3%|‚ñé         | 5403/200000 [1:56:05<73:56:19,  1.37s/it, loss=0.0115, lr=2.70e-05, step=5403]Training:   3%|‚ñé         | 5404/200000 [1:56:06<69:13:12,  1.28s/it, loss=0.0115, lr=2.70e-05, step=5403]Training:   3%|‚ñé         | 5404/200000 [1:56:06<69:13:12,  1.28s/it, loss=0.0121, lr=2.70e-05, step=5404]Training:   3%|‚ñé         | 5405/200000 [1:56:07<65:55:05,  1.22s/it, loss=0.0121, lr=2.70e-05, step=5404]Training:   3%|‚ñé         | 5405/200000 [1:56:07<65:55:05,  1.22s/it, loss=0.0138, lr=2.70e-05, step=5405]Training:   3%|‚ñé         | 5406/200000 [1:56:09<69:50:24,  1.29s/it, loss=0.0138, lr=2.70e-05, step=5405]Training:   3%|‚ñé         | 5406/200000 [1:56:09<69:50:24,  1.29s/it, loss=0.0263, lr=2.70e-05, step=5406]Training:   3%|‚ñé         | 5407/200000 [1:56:10<66:19:35,  1.23s/it, loss=0.0263, lr=2.70e-05, step=5406]Training:   3%|‚ñé         | 5407/200000 [1:56:10<66:19:35,  1.23s/it, loss=0.0338, lr=2.70e-05, step=5407]Training:   3%|‚ñé         | 5408/200000 [1:56:11<67:36:30,  1.25s/it, loss=0.0338, lr=2.70e-05, step=5407]Training:   3%|‚ñé         | 5408/200000 [1:56:11<67:36:30,  1.25s/it, loss=0.0189, lr=2.70e-05, step=5408]Training:   3%|‚ñé         | 5409/200000 [1:56:12<64:47:09,  1.20s/it, loss=0.0189, lr=2.70e-05, step=5408]Training:   3%|‚ñé         | 5409/200000 [1:56:12<64:47:09,  1.20s/it, loss=0.0193, lr=2.70e-05, step=5409]Training:   3%|‚ñé         | 5410/200000 [1:56:14<68:44:13,  1.27s/it, loss=0.0193, lr=2.70e-05, step=5409]Training:   3%|‚ñé         | 5410/200000 [1:56:14<68:44:13,  1.27s/it, loss=0.0209, lr=2.70e-05, step=5410]Training:   3%|‚ñé         | 5411/200000 [1:56:15<71:45:39,  1.33s/it, loss=0.0209, lr=2.70e-05, step=5410]Training:   3%|‚ñé         | 5411/200000 [1:56:15<71:45:39,  1.33s/it, loss=0.0228, lr=2.71e-05, step=5411]Training:   3%|‚ñé         | 5412/200000 [1:56:17<72:57:39,  1.35s/it, loss=0.0228, lr=2.71e-05, step=5411]Training:   3%|‚ñé         | 5412/200000 [1:56:17<72:57:39,  1.35s/it, loss=0.0182, lr=2.71e-05, step=5412]Training:   3%|‚ñé         | 5413/200000 [1:56:18<74:04:04,  1.37s/it, loss=0.0182, lr=2.71e-05, step=5412]Training:   3%|‚ñé         | 5413/200000 [1:56:18<74:04:04,  1.37s/it, loss=0.0115, lr=2.71e-05, step=5413]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5414/200000 [1:56:19<69:18:17,  1.28s/it, loss=0.0115, lr=2.71e-05, step=5413]Training:   3%|‚ñé         | 5414/200000 [1:56:19<69:18:17,  1.28s/it, loss=0.0138, lr=2.71e-05, step=5414]Training:   3%|‚ñé         | 5415/200000 [1:56:20<65:55:47,  1.22s/it, loss=0.0138, lr=2.71e-05, step=5414]Training:   3%|‚ñé         | 5415/200000 [1:56:20<65:55:47,  1.22s/it, loss=0.0345, lr=2.71e-05, step=5415]Training:   3%|‚ñé         | 5416/200000 [1:56:22<67:53:45,  1.26s/it, loss=0.0345, lr=2.71e-05, step=5415]Training:   3%|‚ñé         | 5416/200000 [1:56:22<67:53:45,  1.26s/it, loss=0.0149, lr=2.71e-05, step=5416]Training:   3%|‚ñé         | 5417/200000 [1:56:23<68:59:12,  1.28s/it, loss=0.0149, lr=2.71e-05, step=5416]Training:   3%|‚ñé         | 5417/200000 [1:56:23<68:59:12,  1.28s/it, loss=0.0099, lr=2.71e-05, step=5417]Training:   3%|‚ñé         | 5418/200000 [1:56:24<65:44:10,  1.22s/it, loss=0.0099, lr=2.71e-05, step=5417]Training:   3%|‚ñé         | 5418/200000 [1:56:24<65:44:10,  1.22s/it, loss=0.0316, lr=2.71e-05, step=5418]Training:   3%|‚ñé         | 5419/200000 [1:56:25<67:38:09,  1.25s/it, loss=0.0316, lr=2.71e-05, step=5418]Training:   3%|‚ñé         | 5419/200000 [1:56:25<67:38:09,  1.25s/it, loss=0.0236, lr=2.71e-05, step=5419]Training:   3%|‚ñé         | 5420/200000 [1:56:27<69:17:18,  1.28s/it, loss=0.0236, lr=2.71e-05, step=5419]Training:   3%|‚ñé         | 5420/200000 [1:56:27<69:17:18,  1.28s/it, loss=0.0308, lr=2.71e-05, step=5420]Training:   3%|‚ñé         | 5421/200000 [1:56:28<70:07:02,  1.30s/it, loss=0.0308, lr=2.71e-05, step=5420]Training:   3%|‚ñé         | 5421/200000 [1:56:28<70:07:02,  1.30s/it, loss=0.0161, lr=2.71e-05, step=5421]Training:   3%|‚ñé         | 5422/200000 [1:56:29<66:31:26,  1.23s/it, loss=0.0161, lr=2.71e-05, step=5421]Training:   3%|‚ñé         | 5422/200000 [1:56:29<66:31:26,  1.23s/it, loss=0.0478, lr=2.71e-05, step=5422]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5423/200000 [1:56:31<69:31:26,  1.29s/it, loss=0.0478, lr=2.71e-05, step=5422]Training:   3%|‚ñé         | 5423/200000 [1:56:31<69:31:26,  1.29s/it, loss=0.0179, lr=2.71e-05, step=5423]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5424/200000 [1:56:32<72:07:32,  1.33s/it, loss=0.0179, lr=2.71e-05, step=5423]Training:   3%|‚ñé         | 5424/200000 [1:56:32<72:07:32,  1.33s/it, loss=0.0218, lr=2.71e-05, step=5424]Training:   3%|‚ñé         | 5425/200000 [1:56:33<67:55:47,  1.26s/it, loss=0.0218, lr=2.71e-05, step=5424]Training:   3%|‚ñé         | 5425/200000 [1:56:33<67:55:47,  1.26s/it, loss=0.0129, lr=2.71e-05, step=5425]Training:   3%|‚ñé         | 5426/200000 [1:56:34<65:02:04,  1.20s/it, loss=0.0129, lr=2.71e-05, step=5425]Training:   3%|‚ñé         | 5426/200000 [1:56:34<65:02:04,  1.20s/it, loss=0.0146, lr=2.71e-05, step=5426]Training:   3%|‚ñé         | 5427/200000 [1:56:36<68:13:15,  1.26s/it, loss=0.0146, lr=2.71e-05, step=5426]Training:   3%|‚ñé         | 5427/200000 [1:56:36<68:13:15,  1.26s/it, loss=0.0150, lr=2.71e-05, step=5427]Training:   3%|‚ñé         | 5428/200000 [1:56:37<70:14:46,  1.30s/it, loss=0.0150, lr=2.71e-05, step=5427]Training:   3%|‚ñé         | 5428/200000 [1:56:37<70:14:46,  1.30s/it, loss=0.0217, lr=2.71e-05, step=5428]Training:   3%|‚ñé         | 5429/200000 [1:56:38<66:37:09,  1.23s/it, loss=0.0217, lr=2.71e-05, step=5428]Training:   3%|‚ñé         | 5429/200000 [1:56:38<66:37:09,  1.23s/it, loss=0.0191, lr=2.71e-05, step=5429]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5430/200000 [1:56:39<69:25:26,  1.28s/it, loss=0.0191, lr=2.71e-05, step=5429]Training:   3%|‚ñé         | 5430/200000 [1:56:39<69:25:26,  1.28s/it, loss=0.0202, lr=2.71e-05, step=5430]Training:   3%|‚ñé         | 5431/200000 [1:56:41<70:48:43,  1.31s/it, loss=0.0202, lr=2.71e-05, step=5430]Training:   3%|‚ñé         | 5431/200000 [1:56:41<70:48:43,  1.31s/it, loss=0.0179, lr=2.72e-05, step=5431]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5432/200000 [1:56:42<72:18:57,  1.34s/it, loss=0.0179, lr=2.72e-05, step=5431]Training:   3%|‚ñé         | 5432/200000 [1:56:42<72:18:57,  1.34s/it, loss=0.0130, lr=2.72e-05, step=5432]Training:   3%|‚ñé         | 5433/200000 [1:56:43<68:05:48,  1.26s/it, loss=0.0130, lr=2.72e-05, step=5432]Training:   3%|‚ñé         | 5433/200000 [1:56:43<68:05:48,  1.26s/it, loss=0.0447, lr=2.72e-05, step=5433]Training:   3%|‚ñé         | 5434/200000 [1:56:45<71:54:37,  1.33s/it, loss=0.0447, lr=2.72e-05, step=5433]Training:   3%|‚ñé         | 5434/200000 [1:56:45<71:54:37,  1.33s/it, loss=0.0145, lr=2.72e-05, step=5434]Training:   3%|‚ñé         | 5435/200000 [1:56:46<75:02:40,  1.39s/it, loss=0.0145, lr=2.72e-05, step=5434]Training:   3%|‚ñé         | 5435/200000 [1:56:46<75:02:40,  1.39s/it, loss=0.0295, lr=2.72e-05, step=5435]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5436/200000 [1:56:47<70:01:07,  1.30s/it, loss=0.0295, lr=2.72e-05, step=5435]Training:   3%|‚ñé         | 5436/200000 [1:56:47<70:01:07,  1.30s/it, loss=0.0119, lr=2.72e-05, step=5436]Training:   3%|‚ñé         | 5437/200000 [1:56:48<66:26:13,  1.23s/it, loss=0.0119, lr=2.72e-05, step=5436]Training:   3%|‚ñé         | 5437/200000 [1:56:48<66:26:13,  1.23s/it, loss=0.0135, lr=2.72e-05, step=5437]Training:   3%|‚ñé         | 5438/200000 [1:56:50<70:31:45,  1.31s/it, loss=0.0135, lr=2.72e-05, step=5437]Training:   3%|‚ñé         | 5438/200000 [1:56:50<70:31:45,  1.31s/it, loss=0.0166, lr=2.72e-05, step=5438]Training:   3%|‚ñé         | 5439/200000 [1:56:51<66:47:26,  1.24s/it, loss=0.0166, lr=2.72e-05, step=5438]Training:   3%|‚ñé         | 5439/200000 [1:56:51<66:47:26,  1.24s/it, loss=0.0149, lr=2.72e-05, step=5439]Training:   3%|‚ñé         | 5440/200000 [1:56:52<69:02:40,  1.28s/it, loss=0.0149, lr=2.72e-05, step=5439]Training:   3%|‚ñé         | 5440/200000 [1:56:52<69:02:40,  1.28s/it, loss=0.0186, lr=2.72e-05, step=5440]Training:   3%|‚ñé         | 5441/200000 [1:56:53<65:46:28,  1.22s/it, loss=0.0186, lr=2.72e-05, step=5440]Training:   3%|‚ñé         | 5441/200000 [1:56:53<65:46:28,  1.22s/it, loss=0.0105, lr=2.72e-05, step=5441]Training:   3%|‚ñé         | 5442/200000 [1:56:55<69:42:11,  1.29s/it, loss=0.0105, lr=2.72e-05, step=5441]Training:   3%|‚ñé         | 5442/200000 [1:56:55<69:42:11,  1.29s/it, loss=0.0192, lr=2.72e-05, step=5442]Training:   3%|‚ñé         | 5443/200000 [1:56:56<72:41:41,  1.35s/it, loss=0.0192, lr=2.72e-05, step=5442]Training:   3%|‚ñé         | 5443/200000 [1:56:56<72:41:41,  1.35s/it, loss=0.0123, lr=2.72e-05, step=5443]Training:   3%|‚ñé         | 5444/200000 [1:56:58<74:33:57,  1.38s/it, loss=0.0123, lr=2.72e-05, step=5443]Training:   3%|‚ñé         | 5444/200000 [1:56:58<74:33:57,  1.38s/it, loss=0.0135, lr=2.72e-05, step=5444]Training:   3%|‚ñé         | 5445/200000 [1:56:59<75:00:30,  1.39s/it, loss=0.0135, lr=2.72e-05, step=5444]Training:   3%|‚ñé         | 5445/200000 [1:56:59<75:00:30,  1.39s/it, loss=0.0191, lr=2.72e-05, step=5445]Training:   3%|‚ñé         | 5446/200000 [1:57:00<69:58:05,  1.29s/it, loss=0.0191, lr=2.72e-05, step=5445]Training:   3%|‚ñé         | 5446/200000 [1:57:00<69:58:05,  1.29s/it, loss=0.0237, lr=2.72e-05, step=5446]Training:   3%|‚ñé         | 5447/200000 [1:57:01<66:24:39,  1.23s/it, loss=0.0237, lr=2.72e-05, step=5446]Training:   3%|‚ñé         | 5447/200000 [1:57:01<66:24:39,  1.23s/it, loss=0.0238, lr=2.72e-05, step=5447]Training:   3%|‚ñé         | 5448/200000 [1:57:03<68:25:38,  1.27s/it, loss=0.0238, lr=2.72e-05, step=5447]Training:   3%|‚ñé         | 5448/200000 [1:57:03<68:25:38,  1.27s/it, loss=0.0176, lr=2.72e-05, step=5448]Training:   3%|‚ñé         | 5449/200000 [1:57:04<70:37:58,  1.31s/it, loss=0.0176, lr=2.72e-05, step=5448]Training:   3%|‚ñé         | 5449/200000 [1:57:04<70:37:58,  1.31s/it, loss=0.0702, lr=2.72e-05, step=5449]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5450/200000 [1:57:05<66:53:19,  1.24s/it, loss=0.0702, lr=2.72e-05, step=5449]Training:   3%|‚ñé         | 5450/200000 [1:57:05<66:53:19,  1.24s/it, loss=0.0238, lr=2.72e-05, step=5450]Training:   3%|‚ñé         | 5451/200000 [1:57:07<69:34:29,  1.29s/it, loss=0.0238, lr=2.72e-05, step=5450]Training:   3%|‚ñé         | 5451/200000 [1:57:07<69:34:29,  1.29s/it, loss=0.0116, lr=2.73e-05, step=5451]Training:   3%|‚ñé         | 5452/200000 [1:57:08<70:52:27,  1.31s/it, loss=0.0116, lr=2.73e-05, step=5451]Training:   3%|‚ñé         | 5452/200000 [1:57:08<70:52:27,  1.31s/it, loss=0.0349, lr=2.73e-05, step=5452]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5453/200000 [1:57:09<72:04:21,  1.33s/it, loss=0.0349, lr=2.73e-05, step=5452]Training:   3%|‚ñé         | 5453/200000 [1:57:09<72:04:21,  1.33s/it, loss=0.0170, lr=2.73e-05, step=5453]Training:   3%|‚ñé         | 5454/200000 [1:57:10<67:51:21,  1.26s/it, loss=0.0170, lr=2.73e-05, step=5453]Training:   3%|‚ñé         | 5454/200000 [1:57:10<67:51:21,  1.26s/it, loss=0.0442, lr=2.73e-05, step=5454]Training:   3%|‚ñé         | 5455/200000 [1:57:12<71:21:04,  1.32s/it, loss=0.0442, lr=2.73e-05, step=5454]Training:   3%|‚ñé         | 5455/200000 [1:57:12<71:21:04,  1.32s/it, loss=0.0175, lr=2.73e-05, step=5455]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5456/200000 [1:57:13<74:23:55,  1.38s/it, loss=0.0175, lr=2.73e-05, step=5455]Training:   3%|‚ñé         | 5456/200000 [1:57:13<74:23:55,  1.38s/it, loss=0.0176, lr=2.73e-05, step=5456]Training:   3%|‚ñé         | 5457/200000 [1:57:14<69:30:05,  1.29s/it, loss=0.0176, lr=2.73e-05, step=5456]Training:   3%|‚ñé         | 5457/200000 [1:57:14<69:30:05,  1.29s/it, loss=0.0163, lr=2.73e-05, step=5457]Training:   3%|‚ñé         | 5458/200000 [1:57:16<66:07:26,  1.22s/it, loss=0.0163, lr=2.73e-05, step=5457]Training:   3%|‚ñé         | 5458/200000 [1:57:16<66:07:26,  1.22s/it, loss=0.0321, lr=2.73e-05, step=5458]Training:   3%|‚ñé         | 5459/200000 [1:57:17<69:43:23,  1.29s/it, loss=0.0321, lr=2.73e-05, step=5458]Training:   3%|‚ñé         | 5459/200000 [1:57:17<69:43:23,  1.29s/it, loss=0.0125, lr=2.73e-05, step=5459]Training:   3%|‚ñé         | 5460/200000 [1:57:18<66:14:32,  1.23s/it, loss=0.0125, lr=2.73e-05, step=5459]Training:   3%|‚ñé         | 5460/200000 [1:57:18<66:14:32,  1.23s/it, loss=0.0146, lr=2.73e-05, step=5460]Training:   3%|‚ñé         | 5461/200000 [1:57:19<67:40:08,  1.25s/it, loss=0.0146, lr=2.73e-05, step=5460]Training:   3%|‚ñé         | 5461/200000 [1:57:19<67:40:08,  1.25s/it, loss=0.0140, lr=2.73e-05, step=5461]Training:   3%|‚ñé         | 5462/200000 [1:57:20<64:48:35,  1.20s/it, loss=0.0140, lr=2.73e-05, step=5461]Training:   3%|‚ñé         | 5462/200000 [1:57:20<64:48:35,  1.20s/it, loss=0.0249, lr=2.73e-05, step=5462]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5463/200000 [1:57:22<68:40:27,  1.27s/it, loss=0.0249, lr=2.73e-05, step=5462]Training:   3%|‚ñé         | 5463/200000 [1:57:22<68:40:27,  1.27s/it, loss=0.0281, lr=2.73e-05, step=5463]Training:   3%|‚ñé         | 5464/200000 [1:57:23<71:02:21,  1.31s/it, loss=0.0281, lr=2.73e-05, step=5463]Training:   3%|‚ñé         | 5464/200000 [1:57:23<71:02:21,  1.31s/it, loss=0.0167, lr=2.73e-05, step=5464]Training:   3%|‚ñé         | 5465/200000 [1:57:25<73:16:35,  1.36s/it, loss=0.0167, lr=2.73e-05, step=5464]Training:   3%|‚ñé         | 5465/200000 [1:57:25<73:16:35,  1.36s/it, loss=0.0221, lr=2.73e-05, step=5465]Training:   3%|‚ñé         | 5466/200000 [1:57:26<74:14:29,  1.37s/it, loss=0.0221, lr=2.73e-05, step=5465]Training:   3%|‚ñé         | 5466/200000 [1:57:26<74:14:29,  1.37s/it, loss=0.0179, lr=2.73e-05, step=5466]Training:   3%|‚ñé         | 5467/200000 [1:57:27<69:25:07,  1.28s/it, loss=0.0179, lr=2.73e-05, step=5466]Training:   3%|‚ñé         | 5467/200000 [1:57:27<69:25:07,  1.28s/it, loss=0.0324, lr=2.73e-05, step=5467]Training:   3%|‚ñé         | 5468/200000 [1:57:28<66:02:05,  1.22s/it, loss=0.0324, lr=2.73e-05, step=5467]Training:   3%|‚ñé         | 5468/200000 [1:57:28<66:02:05,  1.22s/it, loss=0.0150, lr=2.73e-05, step=5468]Training:   3%|‚ñé         | 5469/200000 [1:57:30<67:40:34,  1.25s/it, loss=0.0150, lr=2.73e-05, step=5468]Training:   3%|‚ñé         | 5469/200000 [1:57:30<67:40:34,  1.25s/it, loss=0.0159, lr=2.73e-05, step=5469]Training:   3%|‚ñé         | 5470/200000 [1:57:31<68:34:10,  1.27s/it, loss=0.0159, lr=2.73e-05, step=5469]Training:   3%|‚ñé         | 5470/200000 [1:57:31<68:34:10,  1.27s/it, loss=0.0191, lr=2.73e-05, step=5470]Training:   3%|‚ñé         | 5471/200000 [1:57:32<65:24:59,  1.21s/it, loss=0.0191, lr=2.73e-05, step=5470]Training:   3%|‚ñé         | 5471/200000 [1:57:32<65:24:59,  1.21s/it, loss=0.0113, lr=2.74e-05, step=5471]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5472/200000 [1:57:33<66:59:48,  1.24s/it, loss=0.0113, lr=2.74e-05, step=5471]Training:   3%|‚ñé         | 5472/200000 [1:57:33<66:59:48,  1.24s/it, loss=0.0184, lr=2.74e-05, step=5472]Training:   3%|‚ñé         | 5473/200000 [1:57:35<68:48:50,  1.27s/it, loss=0.0184, lr=2.74e-05, step=5472]Training:   3%|‚ñé         | 5473/200000 [1:57:35<68:48:50,  1.27s/it, loss=0.0125, lr=2.74e-05, step=5473]Training:   3%|‚ñé         | 5474/200000 [1:57:36<70:39:53,  1.31s/it, loss=0.0125, lr=2.74e-05, step=5473]Training:   3%|‚ñé         | 5474/200000 [1:57:36<70:39:53,  1.31s/it, loss=0.0211, lr=2.74e-05, step=5474]Training:   3%|‚ñé         | 5475/200000 [1:57:37<66:54:38,  1.24s/it, loss=0.0211, lr=2.74e-05, step=5474]Training:   3%|‚ñé         | 5475/200000 [1:57:37<66:54:38,  1.24s/it, loss=0.0145, lr=2.74e-05, step=5475]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5476/200000 [1:57:39<69:47:24,  1.29s/it, loss=0.0145, lr=2.74e-05, step=5475]Training:   3%|‚ñé         | 5476/200000 [1:57:39<69:47:24,  1.29s/it, loss=0.0178, lr=2.74e-05, step=5476]Training:   3%|‚ñé         | 5477/200000 [1:57:40<72:13:57,  1.34s/it, loss=0.0178, lr=2.74e-05, step=5476]Training:   3%|‚ñé         | 5477/200000 [1:57:40<72:13:57,  1.34s/it, loss=0.0285, lr=2.74e-05, step=5477]Training:   3%|‚ñé         | 5478/200000 [1:57:41<68:00:24,  1.26s/it, loss=0.0285, lr=2.74e-05, step=5477]Training:   3%|‚ñé         | 5478/200000 [1:57:41<68:00:24,  1.26s/it, loss=0.0264, lr=2.74e-05, step=5478]Training:   3%|‚ñé         | 5479/200000 [1:57:42<65:00:40,  1.20s/it, loss=0.0264, lr=2.74e-05, step=5478]Training:   3%|‚ñé         | 5479/200000 [1:57:42<65:00:40,  1.20s/it, loss=0.0147, lr=2.74e-05, step=5479]Training:   3%|‚ñé         | 5480/200000 [1:57:44<68:11:26,  1.26s/it, loss=0.0147, lr=2.74e-05, step=5479]Training:   3%|‚ñé         | 5480/200000 [1:57:44<68:11:26,  1.26s/it, loss=0.0169, lr=2.74e-05, step=5480]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5481/200000 [1:57:45<71:14:41,  1.32s/it, loss=0.0169, lr=2.74e-05, step=5480]Training:   3%|‚ñé         | 5481/200000 [1:57:45<71:14:41,  1.32s/it, loss=0.0141, lr=2.74e-05, step=5481]Training:   3%|‚ñé         | 5482/200000 [1:57:46<67:18:17,  1.25s/it, loss=0.0141, lr=2.74e-05, step=5481]Training:   3%|‚ñé         | 5482/200000 [1:57:46<67:18:17,  1.25s/it, loss=0.0626, lr=2.74e-05, step=5482]Training:   3%|‚ñé         | 5483/200000 [1:57:48<70:17:33,  1.30s/it, loss=0.0626, lr=2.74e-05, step=5482]Training:   3%|‚ñé         | 5483/200000 [1:57:48<70:17:33,  1.30s/it, loss=0.0188, lr=2.74e-05, step=5483]Training:   3%|‚ñé         | 5484/200000 [1:57:49<71:38:29,  1.33s/it, loss=0.0188, lr=2.74e-05, step=5483]Training:   3%|‚ñé         | 5484/200000 [1:57:49<71:38:29,  1.33s/it, loss=0.0189, lr=2.74e-05, step=5484]Training:   3%|‚ñé         | 5485/200000 [1:57:50<72:16:13,  1.34s/it, loss=0.0189, lr=2.74e-05, step=5484]Training:   3%|‚ñé         | 5485/200000 [1:57:50<72:16:13,  1.34s/it, loss=0.0179, lr=2.74e-05, step=5485]Training:   3%|‚ñé         | 5486/200000 [1:57:51<68:02:14,  1.26s/it, loss=0.0179, lr=2.74e-05, step=5485]Training:   3%|‚ñé         | 5486/200000 [1:57:51<68:02:14,  1.26s/it, loss=0.0244, lr=2.74e-05, step=5486]Training:   3%|‚ñé         | 5487/200000 [1:57:53<71:58:32,  1.33s/it, loss=0.0244, lr=2.74e-05, step=5486]Training:   3%|‚ñé         | 5487/200000 [1:57:53<71:58:32,  1.33s/it, loss=0.0170, lr=2.74e-05, step=5487]Training:   3%|‚ñé         | 5488/200000 [1:57:54<75:09:23,  1.39s/it, loss=0.0170, lr=2.74e-05, step=5487]Training:   3%|‚ñé         | 5488/200000 [1:57:54<75:09:23,  1.39s/it, loss=0.0207, lr=2.74e-05, step=5488]Training:   3%|‚ñé         | 5489/200000 [1:57:55<70:00:42,  1.30s/it, loss=0.0207, lr=2.74e-05, step=5488]Training:   3%|‚ñé         | 5489/200000 [1:57:55<70:00:42,  1.30s/it, loss=0.0460, lr=2.74e-05, step=5489]Training:   3%|‚ñé         | 5490/200000 [1:57:57<66:27:12,  1.23s/it, loss=0.0460, lr=2.74e-05, step=5489]Training:   3%|‚ñé         | 5490/200000 [1:57:57<66:27:12,  1.23s/it, loss=0.0256, lr=2.74e-05, step=5490]Training:   3%|‚ñé         | 5491/200000 [1:57:58<70:32:31,  1.31s/it, loss=0.0256, lr=2.74e-05, step=5490]Training:   3%|‚ñé         | 5491/200000 [1:57:58<70:32:31,  1.31s/it, loss=0.0167, lr=2.75e-05, step=5491]Training:   3%|‚ñé         | 5492/200000 [1:57:59<66:48:52,  1.24s/it, loss=0.0167, lr=2.75e-05, step=5491]Training:   3%|‚ñé         | 5492/200000 [1:57:59<66:48:52,  1.24s/it, loss=0.0154, lr=2.75e-05, step=5492]Training:   3%|‚ñé         | 5493/200000 [1:58:00<68:22:42,  1.27s/it, loss=0.0154, lr=2.75e-05, step=5492]Training:   3%|‚ñé         | 5493/200000 [1:58:00<68:22:42,  1.27s/it, loss=0.0113, lr=2.75e-05, step=5493]Training:   3%|‚ñé         | 5494/200000 [1:58:02<65:21:26,  1.21s/it, loss=0.0113, lr=2.75e-05, step=5493]Training:   3%|‚ñé         | 5494/200000 [1:58:02<65:21:26,  1.21s/it, loss=0.0116, lr=2.75e-05, step=5494]Training:   3%|‚ñé         | 5495/200000 [1:58:03<69:17:48,  1.28s/it, loss=0.0116, lr=2.75e-05, step=5494]Training:   3%|‚ñé         | 5495/200000 [1:58:03<69:17:48,  1.28s/it, loss=0.0260, lr=2.75e-05, step=5495]Training:   3%|‚ñé         | 5496/200000 [1:58:04<72:28:46,  1.34s/it, loss=0.0260, lr=2.75e-05, step=5495]Training:   3%|‚ñé         | 5496/200000 [1:58:04<72:28:46,  1.34s/it, loss=0.0154, lr=2.75e-05, step=5496]Training:   3%|‚ñé         | 5497/200000 [1:58:06<74:34:05,  1.38s/it, loss=0.0154, lr=2.75e-05, step=5496]Training:   3%|‚ñé         | 5497/200000 [1:58:06<74:34:05,  1.38s/it, loss=0.0176, lr=2.75e-05, step=5497]Training:   3%|‚ñé         | 5498/200000 [1:58:07<75:31:13,  1.40s/it, loss=0.0176, lr=2.75e-05, step=5497]Training:   3%|‚ñé         | 5498/200000 [1:58:07<75:31:13,  1.40s/it, loss=0.0143, lr=2.75e-05, step=5498]Training:   3%|‚ñé         | 5499/200000 [1:58:08<70:16:36,  1.30s/it, loss=0.0143, lr=2.75e-05, step=5498]Training:   3%|‚ñé         | 5499/200000 [1:58:08<70:16:36,  1.30s/it, loss=0.0154, lr=2.75e-05, step=5499]Training:   3%|‚ñé         | 5500/200000 [1:58:09<66:39:07,  1.23s/it, loss=0.0154, lr=2.75e-05, step=5499]Training:   3%|‚ñé         | 5500/200000 [1:58:09<66:39:07,  1.23s/it, loss=0.0130, lr=2.75e-05, step=5500]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
00:51:24.648 [I] step=5500 loss=0.0206 lr=2.73e-05 grad_norm=0.44 time=128.5s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5501/200000 [1:58:11<68:23:03,  1.27s/it, loss=0.0130, lr=2.75e-05, step=5500]Training:   3%|‚ñé         | 5501/200000 [1:58:11<68:23:03,  1.27s/it, loss=0.0107, lr=2.75e-05, step=5501]Training:   3%|‚ñé         | 5502/200000 [1:58:12<70:13:11,  1.30s/it, loss=0.0107, lr=2.75e-05, step=5501]Training:   3%|‚ñé         | 5502/200000 [1:58:12<70:13:11,  1.30s/it, loss=0.0222, lr=2.75e-05, step=5502]Training:   3%|‚ñé         | 5503/200000 [1:58:13<66:34:06,  1.23s/it, loss=0.0222, lr=2.75e-05, step=5502]Training:   3%|‚ñé         | 5503/200000 [1:58:13<66:34:06,  1.23s/it, loss=0.0174, lr=2.75e-05, step=5503]Training:   3%|‚ñé         | 5504/200000 [1:58:15<68:36:34,  1.27s/it, loss=0.0174, lr=2.75e-05, step=5503]Training:   3%|‚ñé         | 5504/200000 [1:58:15<68:36:34,  1.27s/it, loss=0.0156, lr=2.75e-05, step=5504]Training:   3%|‚ñé         | 5505/200000 [1:58:16<70:04:15,  1.30s/it, loss=0.0156, lr=2.75e-05, step=5504]Training:   3%|‚ñé         | 5505/200000 [1:58:16<70:04:15,  1.30s/it, loss=0.0174, lr=2.75e-05, step=5505]Training:   3%|‚ñé         | 5506/200000 [1:58:17<71:43:08,  1.33s/it, loss=0.0174, lr=2.75e-05, step=5505]Training:   3%|‚ñé         | 5506/200000 [1:58:17<71:43:08,  1.33s/it, loss=0.0152, lr=2.75e-05, step=5506]Training:   3%|‚ñé         | 5507/200000 [1:58:18<67:38:41,  1.25s/it, loss=0.0152, lr=2.75e-05, step=5506]Training:   3%|‚ñé         | 5507/200000 [1:58:18<67:38:41,  1.25s/it, loss=0.0200, lr=2.75e-05, step=5507]Training:   3%|‚ñé         | 5508/200000 [1:58:20<71:19:13,  1.32s/it, loss=0.0200, lr=2.75e-05, step=5507]Training:   3%|‚ñé         | 5508/200000 [1:58:20<71:19:13,  1.32s/it, loss=0.0752, lr=2.75e-05, step=5508]Training:   3%|‚ñé         | 5509/200000 [1:58:21<74:17:38,  1.38s/it, loss=0.0752, lr=2.75e-05, step=5508]Training:   3%|‚ñé         | 5509/200000 [1:58:21<74:17:38,  1.38s/it, loss=0.0133, lr=2.75e-05, step=5509]Training:   3%|‚ñé         | 5510/200000 [1:58:23<69:27:34,  1.29s/it, loss=0.0133, lr=2.75e-05, step=5509]Training:   3%|‚ñé         | 5510/200000 [1:58:23<69:27:34,  1.29s/it, loss=0.0245, lr=2.75e-05, step=5510]Training:   3%|‚ñé         | 5511/200000 [1:58:24<66:05:03,  1.22s/it, loss=0.0245, lr=2.75e-05, step=5510]Training:   3%|‚ñé         | 5511/200000 [1:58:24<66:05:03,  1.22s/it, loss=0.0240, lr=2.76e-05, step=5511]Training:   3%|‚ñé         | 5512/200000 [1:58:25<69:42:11,  1.29s/it, loss=0.0240, lr=2.76e-05, step=5511]Training:   3%|‚ñé         | 5512/200000 [1:58:25<69:42:11,  1.29s/it, loss=0.0278, lr=2.76e-05, step=5512]Training:   3%|‚ñé         | 5513/200000 [1:58:26<66:14:15,  1.23s/it, loss=0.0278, lr=2.76e-05, step=5512]Training:   3%|‚ñé         | 5513/200000 [1:58:26<66:14:15,  1.23s/it, loss=0.0166, lr=2.76e-05, step=5513]Training:   3%|‚ñé         | 5514/200000 [1:58:27<67:49:46,  1.26s/it, loss=0.0166, lr=2.76e-05, step=5513]Training:   3%|‚ñé         | 5514/200000 [1:58:27<67:49:46,  1.26s/it, loss=0.0217, lr=2.76e-05, step=5514]Training:   3%|‚ñé         | 5515/200000 [1:58:29<64:53:47,  1.20s/it, loss=0.0217, lr=2.76e-05, step=5514]Training:   3%|‚ñé         | 5515/200000 [1:58:29<64:53:47,  1.20s/it, loss=0.0176, lr=2.76e-05, step=5515]Training:   3%|‚ñé         | 5516/200000 [1:58:30<68:48:33,  1.27s/it, loss=0.0176, lr=2.76e-05, step=5515]Training:   3%|‚ñé         | 5516/200000 [1:58:30<68:48:33,  1.27s/it, loss=0.0183, lr=2.76e-05, step=5516]Training:   3%|‚ñé         | 5517/200000 [1:58:31<71:01:37,  1.31s/it, loss=0.0183, lr=2.76e-05, step=5516]Training:   3%|‚ñé         | 5517/200000 [1:58:31<71:01:37,  1.31s/it, loss=0.0211, lr=2.76e-05, step=5517]Training:   3%|‚ñé         | 5518/200000 [1:58:33<72:23:50,  1.34s/it, loss=0.0211, lr=2.76e-05, step=5517]Training:   3%|‚ñé         | 5518/200000 [1:58:33<72:23:50,  1.34s/it, loss=0.0239, lr=2.76e-05, step=5518]Training:   3%|‚ñé         | 5519/200000 [1:58:34<73:52:26,  1.37s/it, loss=0.0239, lr=2.76e-05, step=5518]Training:   3%|‚ñé         | 5519/200000 [1:58:34<73:52:26,  1.37s/it, loss=0.0207, lr=2.76e-05, step=5519]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5520/200000 [1:58:35<69:08:46,  1.28s/it, loss=0.0207, lr=2.76e-05, step=5519]Training:   3%|‚ñé         | 5520/200000 [1:58:35<69:08:46,  1.28s/it, loss=0.0326, lr=2.76e-05, step=5520]Training:   3%|‚ñé         | 5521/200000 [1:58:36<65:49:04,  1.22s/it, loss=0.0326, lr=2.76e-05, step=5520]Training:   3%|‚ñé         | 5521/200000 [1:58:36<65:49:04,  1.22s/it, loss=0.0216, lr=2.76e-05, step=5521]Training:   3%|‚ñé         | 5522/200000 [1:58:38<67:50:04,  1.26s/it, loss=0.0216, lr=2.76e-05, step=5521]Training:   3%|‚ñé         | 5522/200000 [1:58:38<67:50:04,  1.26s/it, loss=0.0247, lr=2.76e-05, step=5522]Training:   3%|‚ñé         | 5523/200000 [1:58:39<69:11:45,  1.28s/it, loss=0.0247, lr=2.76e-05, step=5522]Training:   3%|‚ñé         | 5523/200000 [1:58:39<69:11:45,  1.28s/it, loss=0.0148, lr=2.76e-05, step=5523]Training:   3%|‚ñé         | 5524/200000 [1:58:40<65:53:45,  1.22s/it, loss=0.0148, lr=2.76e-05, step=5523]Training:   3%|‚ñé         | 5524/200000 [1:58:40<65:53:45,  1.22s/it, loss=0.0195, lr=2.76e-05, step=5524]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5525/200000 [1:58:41<67:44:41,  1.25s/it, loss=0.0195, lr=2.76e-05, step=5524]Training:   3%|‚ñé         | 5525/200000 [1:58:41<67:44:41,  1.25s/it, loss=0.0244, lr=2.76e-05, step=5525]Training:   3%|‚ñé         | 5526/200000 [1:58:43<69:21:10,  1.28s/it, loss=0.0244, lr=2.76e-05, step=5525]Training:   3%|‚ñé         | 5526/200000 [1:58:43<69:21:10,  1.28s/it, loss=0.0114, lr=2.76e-05, step=5526]Training:   3%|‚ñé         | 5527/200000 [1:58:44<70:56:16,  1.31s/it, loss=0.0114, lr=2.76e-05, step=5526]Training:   3%|‚ñé         | 5527/200000 [1:58:44<70:56:16,  1.31s/it, loss=0.0349, lr=2.76e-05, step=5527]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5528/200000 [1:58:45<67:07:12,  1.24s/it, loss=0.0349, lr=2.76e-05, step=5527]Training:   3%|‚ñé         | 5528/200000 [1:58:45<67:07:12,  1.24s/it, loss=0.0271, lr=2.76e-05, step=5528]Training:   3%|‚ñé         | 5529/200000 [1:58:47<69:56:34,  1.29s/it, loss=0.0271, lr=2.76e-05, step=5528]Training:   3%|‚ñé         | 5529/200000 [1:58:47<69:56:34,  1.29s/it, loss=0.0268, lr=2.76e-05, step=5529]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5530/200000 [1:58:48<72:28:04,  1.34s/it, loss=0.0268, lr=2.76e-05, step=5529]Training:   3%|‚ñé         | 5530/200000 [1:58:48<72:28:04,  1.34s/it, loss=0.0214, lr=2.76e-05, step=5530]Training:   3%|‚ñé         | 5531/200000 [1:58:49<68:10:44,  1.26s/it, loss=0.0214, lr=2.76e-05, step=5530]Training:   3%|‚ñé         | 5531/200000 [1:58:49<68:10:44,  1.26s/it, loss=0.0293, lr=2.77e-05, step=5531]Training:   3%|‚ñé         | 5532/200000 [1:58:50<65:11:17,  1.21s/it, loss=0.0293, lr=2.77e-05, step=5531]Training:   3%|‚ñé         | 5532/200000 [1:58:50<65:11:17,  1.21s/it, loss=0.0231, lr=2.77e-05, step=5532]Training:   3%|‚ñé         | 5533/200000 [1:58:52<68:13:28,  1.26s/it, loss=0.0231, lr=2.77e-05, step=5532]Training:   3%|‚ñé         | 5533/200000 [1:58:52<68:13:28,  1.26s/it, loss=0.0208, lr=2.77e-05, step=5533]Training:   3%|‚ñé         | 5534/200000 [1:58:53<70:27:17,  1.30s/it, loss=0.0208, lr=2.77e-05, step=5533]Training:   3%|‚ñé         | 5534/200000 [1:58:53<70:27:17,  1.30s/it, loss=0.0157, lr=2.77e-05, step=5534]Training:   3%|‚ñé         | 5535/200000 [1:58:54<66:45:18,  1.24s/it, loss=0.0157, lr=2.77e-05, step=5534]Training:   3%|‚ñé         | 5535/200000 [1:58:54<66:45:18,  1.24s/it, loss=0.0165, lr=2.77e-05, step=5535]Training:   3%|‚ñé         | 5536/200000 [1:58:56<69:32:40,  1.29s/it, loss=0.0165, lr=2.77e-05, step=5535]Training:   3%|‚ñé         | 5536/200000 [1:58:56<69:32:40,  1.29s/it, loss=0.0200, lr=2.77e-05, step=5536]Training:   3%|‚ñé         | 5537/200000 [1:58:57<70:24:23,  1.30s/it, loss=0.0200, lr=2.77e-05, step=5536]Training:   3%|‚ñé         | 5537/200000 [1:58:57<70:24:23,  1.30s/it, loss=0.0197, lr=2.77e-05, step=5537]Training:   3%|‚ñé         | 5538/200000 [1:58:58<71:58:13,  1.33s/it, loss=0.0197, lr=2.77e-05, step=5537]Training:   3%|‚ñé         | 5538/200000 [1:58:58<71:58:13,  1.33s/it, loss=0.0121, lr=2.77e-05, step=5538]Training:   3%|‚ñé         | 5539/200000 [1:58:59<67:48:56,  1.26s/it, loss=0.0121, lr=2.77e-05, step=5538]Training:   3%|‚ñé         | 5539/200000 [1:58:59<67:48:56,  1.26s/it, loss=0.0226, lr=2.77e-05, step=5539]Training:   3%|‚ñé         | 5540/200000 [1:59:01<71:51:12,  1.33s/it, loss=0.0226, lr=2.77e-05, step=5539]Training:   3%|‚ñé         | 5540/200000 [1:59:01<71:51:12,  1.33s/it, loss=0.0141, lr=2.77e-05, step=5540]Training:   3%|‚ñé         | 5541/200000 [1:59:02<74:58:09,  1.39s/it, loss=0.0141, lr=2.77e-05, step=5540]Training:   3%|‚ñé         | 5541/200000 [1:59:02<74:58:09,  1.39s/it, loss=0.0189, lr=2.77e-05, step=5541]Training:   3%|‚ñé         | 5542/200000 [1:59:04<69:55:25,  1.29s/it, loss=0.0189, lr=2.77e-05, step=5541]Training:   3%|‚ñé         | 5542/200000 [1:59:04<69:55:25,  1.29s/it, loss=0.0265, lr=2.77e-05, step=5542]Training:   3%|‚ñé         | 5543/200000 [1:59:05<66:24:25,  1.23s/it, loss=0.0265, lr=2.77e-05, step=5542]Training:   3%|‚ñé         | 5543/200000 [1:59:05<66:24:25,  1.23s/it, loss=0.0217, lr=2.77e-05, step=5543]Training:   3%|‚ñé         | 5544/200000 [1:59:06<70:30:46,  1.31s/it, loss=0.0217, lr=2.77e-05, step=5543]Training:   3%|‚ñé         | 5544/200000 [1:59:06<70:30:46,  1.31s/it, loss=0.0245, lr=2.77e-05, step=5544]Training:   3%|‚ñé         | 5545/200000 [1:59:07<66:47:52,  1.24s/it, loss=0.0245, lr=2.77e-05, step=5544]Training:   3%|‚ñé         | 5545/200000 [1:59:07<66:47:52,  1.24s/it, loss=0.0137, lr=2.77e-05, step=5545]Training:   3%|‚ñé         | 5546/200000 [1:59:08<68:15:37,  1.26s/it, loss=0.0137, lr=2.77e-05, step=5545]Training:   3%|‚ñé         | 5546/200000 [1:59:08<68:15:37,  1.26s/it, loss=0.0287, lr=2.77e-05, step=5546]Training:   3%|‚ñé         | 5547/200000 [1:59:10<65:12:07,  1.21s/it, loss=0.0287, lr=2.77e-05, step=5546]Training:   3%|‚ñé         | 5547/200000 [1:59:10<65:12:07,  1.21s/it, loss=0.0217, lr=2.77e-05, step=5547]Training:   3%|‚ñé         | 5548/200000 [1:59:11<69:17:32,  1.28s/it, loss=0.0217, lr=2.77e-05, step=5547]Training:   3%|‚ñé         | 5548/200000 [1:59:11<69:17:32,  1.28s/it, loss=0.0121, lr=2.77e-05, step=5548]Training:   3%|‚ñé         | 5549/200000 [1:59:12<71:44:06,  1.33s/it, loss=0.0121, lr=2.77e-05, step=5548]Training:   3%|‚ñé         | 5549/200000 [1:59:12<71:44:06,  1.33s/it, loss=0.0255, lr=2.77e-05, step=5549]Training:   3%|‚ñé         | 5550/200000 [1:59:14<74:00:26,  1.37s/it, loss=0.0255, lr=2.77e-05, step=5549]Training:   3%|‚ñé         | 5550/200000 [1:59:14<74:00:26,  1.37s/it, loss=0.0190, lr=2.77e-05, step=5550]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5551/200000 [1:59:15<74:55:25,  1.39s/it, loss=0.0190, lr=2.77e-05, step=5550]Training:   3%|‚ñé         | 5551/200000 [1:59:15<74:55:25,  1.39s/it, loss=0.0200, lr=2.78e-05, step=5551]Training:   3%|‚ñé         | 5552/200000 [1:59:16<69:53:27,  1.29s/it, loss=0.0200, lr=2.78e-05, step=5551]Training:   3%|‚ñé         | 5552/200000 [1:59:16<69:53:27,  1.29s/it, loss=0.0200, lr=2.78e-05, step=5552]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5553/200000 [1:59:17<66:20:17,  1.23s/it, loss=0.0200, lr=2.78e-05, step=5552]Training:   3%|‚ñé         | 5553/200000 [1:59:17<66:20:17,  1.23s/it, loss=0.0927, lr=2.78e-05, step=5553]Training:   3%|‚ñé         | 5554/200000 [1:59:19<68:21:10,  1.27s/it, loss=0.0927, lr=2.78e-05, step=5553]Training:   3%|‚ñé         | 5554/200000 [1:59:19<68:21:10,  1.27s/it, loss=0.0126, lr=2.78e-05, step=5554]Training:   3%|‚ñé         | 5555/200000 [1:59:20<70:37:46,  1.31s/it, loss=0.0126, lr=2.78e-05, step=5554]Training:   3%|‚ñé         | 5555/200000 [1:59:20<70:37:46,  1.31s/it, loss=0.0130, lr=2.78e-05, step=5555]Training:   3%|‚ñé         | 5556/200000 [1:59:21<66:52:02,  1.24s/it, loss=0.0130, lr=2.78e-05, step=5555]Training:   3%|‚ñé         | 5556/200000 [1:59:21<66:52:02,  1.24s/it, loss=0.0224, lr=2.78e-05, step=5556]Training:   3%|‚ñé         | 5557/200000 [1:59:23<69:30:26,  1.29s/it, loss=0.0224, lr=2.78e-05, step=5556]Training:   3%|‚ñé         | 5557/200000 [1:59:23<69:30:26,  1.29s/it, loss=0.0174, lr=2.78e-05, step=5557]Training:   3%|‚ñé         | 5558/200000 [1:59:24<70:43:43,  1.31s/it, loss=0.0174, lr=2.78e-05, step=5557]Training:   3%|‚ñé         | 5558/200000 [1:59:24<70:43:43,  1.31s/it, loss=0.0141, lr=2.78e-05, step=5558]Training:   3%|‚ñé         | 5559/200000 [1:59:25<71:58:24,  1.33s/it, loss=0.0141, lr=2.78e-05, step=5558]Training:   3%|‚ñé         | 5559/200000 [1:59:25<71:58:24,  1.33s/it, loss=0.0138, lr=2.78e-05, step=5559]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5560/200000 [1:59:27<67:49:19,  1.26s/it, loss=0.0138, lr=2.78e-05, step=5559]Training:   3%|‚ñé         | 5560/200000 [1:59:27<67:49:19,  1.26s/it, loss=0.0245, lr=2.78e-05, step=5560]Training:   3%|‚ñé         | 5561/200000 [1:59:28<71:22:05,  1.32s/it, loss=0.0245, lr=2.78e-05, step=5560]Training:   3%|‚ñé         | 5561/200000 [1:59:28<71:22:05,  1.32s/it, loss=0.0199, lr=2.78e-05, step=5561]Training:   3%|‚ñé         | 5562/200000 [1:59:30<74:21:37,  1.38s/it, loss=0.0199, lr=2.78e-05, step=5561]Training:   3%|‚ñé         | 5562/200000 [1:59:30<74:21:37,  1.38s/it, loss=0.0147, lr=2.78e-05, step=5562]Training:   3%|‚ñé         | 5563/200000 [1:59:31<69:28:55,  1.29s/it, loss=0.0147, lr=2.78e-05, step=5562]Training:   3%|‚ñé         | 5563/200000 [1:59:31<69:28:55,  1.29s/it, loss=0.0164, lr=2.78e-05, step=5563]Training:   3%|‚ñé         | 5564/200000 [1:59:32<66:03:30,  1.22s/it, loss=0.0164, lr=2.78e-05, step=5563]Training:   3%|‚ñé         | 5564/200000 [1:59:32<66:03:30,  1.22s/it, loss=0.0209, lr=2.78e-05, step=5564]Training:   3%|‚ñé         | 5565/200000 [1:59:33<69:34:20,  1.29s/it, loss=0.0209, lr=2.78e-05, step=5564]Training:   3%|‚ñé         | 5565/200000 [1:59:33<69:34:20,  1.29s/it, loss=0.0177, lr=2.78e-05, step=5565]Training:   3%|‚ñé         | 5566/200000 [1:59:34<66:07:31,  1.22s/it, loss=0.0177, lr=2.78e-05, step=5565]Training:   3%|‚ñé         | 5566/200000 [1:59:34<66:07:31,  1.22s/it, loss=0.0206, lr=2.78e-05, step=5566]Training:   3%|‚ñé         | 5567/200000 [1:59:36<68:34:16,  1.27s/it, loss=0.0206, lr=2.78e-05, step=5566]Training:   3%|‚ñé         | 5567/200000 [1:59:36<68:34:16,  1.27s/it, loss=0.0220, lr=2.78e-05, step=5567]Training:   3%|‚ñé         | 5568/200000 [1:59:37<65:23:41,  1.21s/it, loss=0.0220, lr=2.78e-05, step=5567]Training:   3%|‚ñé         | 5568/200000 [1:59:37<65:23:41,  1.21s/it, loss=0.0214, lr=2.78e-05, step=5568]Training:   3%|‚ñé         | 5569/200000 [1:59:38<68:55:50,  1.28s/it, loss=0.0214, lr=2.78e-05, step=5568]Training:   3%|‚ñé         | 5569/200000 [1:59:38<68:55:50,  1.28s/it, loss=0.0189, lr=2.78e-05, step=5569]Training:   3%|‚ñé         | 5570/200000 [1:59:39<71:13:47,  1.32s/it, loss=0.0189, lr=2.78e-05, step=5569]Training:   3%|‚ñé         | 5570/200000 [1:59:39<71:13:47,  1.32s/it, loss=0.0164, lr=2.78e-05, step=5570]Training:   3%|‚ñé         | 5571/200000 [1:59:41<72:35:40,  1.34s/it, loss=0.0164, lr=2.78e-05, step=5570]Training:   3%|‚ñé         | 5571/200000 [1:59:41<72:35:40,  1.34s/it, loss=0.0152, lr=2.79e-05, step=5571]Training:   3%|‚ñé         | 5572/200000 [1:59:42<74:07:27,  1.37s/it, loss=0.0152, lr=2.79e-05, step=5571]Training:   3%|‚ñé         | 5572/200000 [1:59:42<74:07:27,  1.37s/it, loss=0.0214, lr=2.79e-05, step=5572]Training:   3%|‚ñé         | 5573/200000 [1:59:43<69:19:16,  1.28s/it, loss=0.0214, lr=2.79e-05, step=5572]Training:   3%|‚ñé         | 5573/200000 [1:59:43<69:19:16,  1.28s/it, loss=0.0167, lr=2.79e-05, step=5573]Training:   3%|‚ñé         | 5574/200000 [1:59:44<65:59:49,  1.22s/it, loss=0.0167, lr=2.79e-05, step=5573]Training:   3%|‚ñé         | 5574/200000 [1:59:44<65:59:49,  1.22s/it, loss=0.0150, lr=2.79e-05, step=5574]Training:   3%|‚ñé         | 5575/200000 [1:59:46<67:54:58,  1.26s/it, loss=0.0150, lr=2.79e-05, step=5574]Training:   3%|‚ñé         | 5575/200000 [1:59:46<67:54:58,  1.26s/it, loss=0.0196, lr=2.79e-05, step=5575]Training:   3%|‚ñé         | 5576/200000 [1:59:47<68:44:47,  1.27s/it, loss=0.0196, lr=2.79e-05, step=5575]Training:   3%|‚ñé         | 5576/200000 [1:59:47<68:44:47,  1.27s/it, loss=0.0113, lr=2.79e-05, step=5576]Training:   3%|‚ñé         | 5577/200000 [1:59:48<65:32:16,  1.21s/it, loss=0.0113, lr=2.79e-05, step=5576]Training:   3%|‚ñé         | 5577/200000 [1:59:48<65:32:16,  1.21s/it, loss=0.0259, lr=2.79e-05, step=5577]Training:   3%|‚ñé         | 5578/200000 [1:59:50<67:23:26,  1.25s/it, loss=0.0259, lr=2.79e-05, step=5577]Training:   3%|‚ñé         | 5578/200000 [1:59:50<67:23:26,  1.25s/it, loss=0.0210, lr=2.79e-05, step=5578]Training:   3%|‚ñé         | 5579/200000 [1:59:51<69:06:39,  1.28s/it, loss=0.0210, lr=2.79e-05, step=5578]Training:   3%|‚ñé         | 5579/200000 [1:59:51<69:06:39,  1.28s/it, loss=0.0184, lr=2.79e-05, step=5579]Training:   3%|‚ñé         | 5580/200000 [1:59:52<70:29:23,  1.31s/it, loss=0.0184, lr=2.79e-05, step=5579]Training:   3%|‚ñé         | 5580/200000 [1:59:52<70:29:23,  1.31s/it, loss=0.0375, lr=2.79e-05, step=5580]Training:   3%|‚ñé         | 5581/200000 [1:59:53<66:46:11,  1.24s/it, loss=0.0375, lr=2.79e-05, step=5580]Training:   3%|‚ñé         | 5581/200000 [1:59:53<66:46:11,  1.24s/it, loss=0.0153, lr=2.79e-05, step=5581]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5582/200000 [1:59:55<69:33:08,  1.29s/it, loss=0.0153, lr=2.79e-05, step=5581]Training:   3%|‚ñé         | 5582/200000 [1:59:55<69:33:08,  1.29s/it, loss=0.0134, lr=2.79e-05, step=5582]Training:   3%|‚ñé         | 5583/200000 [1:59:56<71:56:16,  1.33s/it, loss=0.0134, lr=2.79e-05, step=5582]Training:   3%|‚ñé         | 5583/200000 [1:59:56<71:56:16,  1.33s/it, loss=0.0144, lr=2.79e-05, step=5583]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5584/200000 [1:59:57<67:47:06,  1.26s/it, loss=0.0144, lr=2.79e-05, step=5583]Training:   3%|‚ñé         | 5584/200000 [1:59:57<67:47:06,  1.26s/it, loss=0.0358, lr=2.79e-05, step=5584]Training:   3%|‚ñé         | 5585/200000 [1:59:58<64:52:31,  1.20s/it, loss=0.0358, lr=2.79e-05, step=5584]Training:   3%|‚ñé         | 5585/200000 [1:59:58<64:52:31,  1.20s/it, loss=0.0156, lr=2.79e-05, step=5585]Training:   3%|‚ñé         | 5586/200000 [2:00:00<67:57:10,  1.26s/it, loss=0.0156, lr=2.79e-05, step=5585]Training:   3%|‚ñé         | 5586/200000 [2:00:00<67:57:10,  1.26s/it, loss=0.0167, lr=2.79e-05, step=5586]Training:   3%|‚ñé         | 5587/200000 [2:00:01<71:24:32,  1.32s/it, loss=0.0167, lr=2.79e-05, step=5586]Training:   3%|‚ñé         | 5587/200000 [2:00:01<71:24:32,  1.32s/it, loss=0.0065, lr=2.79e-05, step=5587]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5588/200000 [2:00:02<67:26:56,  1.25s/it, loss=0.0065, lr=2.79e-05, step=5587]Training:   3%|‚ñé         | 5588/200000 [2:00:02<67:26:56,  1.25s/it, loss=0.0138, lr=2.79e-05, step=5588]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5589/200000 [2:00:04<70:17:46,  1.30s/it, loss=0.0138, lr=2.79e-05, step=5588]Training:   3%|‚ñé         | 5589/200000 [2:00:04<70:17:46,  1.30s/it, loss=0.0194, lr=2.79e-05, step=5589]Training:   3%|‚ñé         | 5590/200000 [2:00:05<70:52:05,  1.31s/it, loss=0.0194, lr=2.79e-05, step=5589]Training:   3%|‚ñé         | 5590/200000 [2:00:05<70:52:05,  1.31s/it, loss=0.0315, lr=2.79e-05, step=5590]Training:   3%|‚ñé         | 5591/200000 [2:00:06<71:33:14,  1.33s/it, loss=0.0315, lr=2.79e-05, step=5590]Training:   3%|‚ñé         | 5591/200000 [2:00:06<71:33:14,  1.33s/it, loss=0.0208, lr=2.80e-05, step=5591]Training:   3%|‚ñé         | 5592/200000 [2:00:07<67:32:36,  1.25s/it, loss=0.0208, lr=2.80e-05, step=5591]Training:   3%|‚ñé         | 5592/200000 [2:00:07<67:32:36,  1.25s/it, loss=0.0187, lr=2.80e-05, step=5592]Training:   3%|‚ñé         | 5593/200000 [2:00:09<71:31:29,  1.32s/it, loss=0.0187, lr=2.80e-05, step=5592]Training:   3%|‚ñé         | 5593/200000 [2:00:09<71:31:29,  1.32s/it, loss=0.0172, lr=2.80e-05, step=5593]Training:   3%|‚ñé         | 5594/200000 [2:00:10<74:49:24,  1.39s/it, loss=0.0172, lr=2.80e-05, step=5593]Training:   3%|‚ñé         | 5594/200000 [2:00:10<74:49:24,  1.39s/it, loss=0.0250, lr=2.80e-05, step=5594]Training:   3%|‚ñé         | 5595/200000 [2:00:12<69:48:08,  1.29s/it, loss=0.0250, lr=2.80e-05, step=5594]Training:   3%|‚ñé         | 5595/200000 [2:00:12<69:48:08,  1.29s/it, loss=0.0129, lr=2.80e-05, step=5595]Training:   3%|‚ñé         | 5596/200000 [2:00:13<66:21:29,  1.23s/it, loss=0.0129, lr=2.80e-05, step=5595]Training:   3%|‚ñé         | 5596/200000 [2:00:13<66:21:29,  1.23s/it, loss=0.0208, lr=2.80e-05, step=5596]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5597/200000 [2:00:14<70:23:30,  1.30s/it, loss=0.0208, lr=2.80e-05, step=5596]Training:   3%|‚ñé         | 5597/200000 [2:00:14<70:23:30,  1.30s/it, loss=0.0154, lr=2.80e-05, step=5597]Training:   3%|‚ñé         | 5598/200000 [2:00:15<66:41:53,  1.24s/it, loss=0.0154, lr=2.80e-05, step=5597]Training:   3%|‚ñé         | 5598/200000 [2:00:15<66:41:53,  1.24s/it, loss=0.0115, lr=2.80e-05, step=5598]Training:   3%|‚ñé         | 5599/200000 [2:00:17<68:34:05,  1.27s/it, loss=0.0115, lr=2.80e-05, step=5598]Training:   3%|‚ñé         | 5599/200000 [2:00:17<68:34:05,  1.27s/it, loss=0.0124, lr=2.80e-05, step=5599]Training:   3%|‚ñé         | 5600/200000 [2:00:18<65:26:24,  1.21s/it, loss=0.0124, lr=2.80e-05, step=5599]Training:   3%|‚ñé         | 5600/200000 [2:00:18<65:26:24,  1.21s/it, loss=0.0119, lr=2.80e-05, step=5600]00:53:32.871 [I] step=5600 loss=0.0209 lr=2.78e-05 grad_norm=0.51 time=128.2s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5601/200000 [2:00:19<69:09:27,  1.28s/it, loss=0.0119, lr=2.80e-05, step=5600]Training:   3%|‚ñé         | 5601/200000 [2:00:19<69:09:27,  1.28s/it, loss=0.0199, lr=2.80e-05, step=5601]Training:   3%|‚ñé         | 5602/200000 [2:00:21<71:42:23,  1.33s/it, loss=0.0199, lr=2.80e-05, step=5601]Training:   3%|‚ñé         | 5602/200000 [2:00:21<71:42:23,  1.33s/it, loss=0.0212, lr=2.80e-05, step=5602]Training:   3%|‚ñé         | 5603/200000 [2:00:22<74:01:57,  1.37s/it, loss=0.0212, lr=2.80e-05, step=5602]Training:   3%|‚ñé         | 5603/200000 [2:00:22<74:01:57,  1.37s/it, loss=0.0698, lr=2.80e-05, step=5603]Training:   3%|‚ñé         | 5604/200000 [2:00:23<75:08:22,  1.39s/it, loss=0.0698, lr=2.80e-05, step=5603]Training:   3%|‚ñé         | 5604/200000 [2:00:23<75:08:22,  1.39s/it, loss=0.0124, lr=2.80e-05, step=5604]Training:   3%|‚ñé         | 5605/200000 [2:00:24<70:01:09,  1.30s/it, loss=0.0124, lr=2.80e-05, step=5604]Training:   3%|‚ñé         | 5605/200000 [2:00:24<70:01:09,  1.30s/it, loss=0.0201, lr=2.80e-05, step=5605]Training:   3%|‚ñé         | 5606/200000 [2:00:26<66:27:47,  1.23s/it, loss=0.0201, lr=2.80e-05, step=5605]Training:   3%|‚ñé         | 5606/200000 [2:00:26<66:27:47,  1.23s/it, loss=0.0174, lr=2.80e-05, step=5606]Training:   3%|‚ñé         | 5607/200000 [2:00:27<68:23:25,  1.27s/it, loss=0.0174, lr=2.80e-05, step=5606]Training:   3%|‚ñé         | 5607/200000 [2:00:27<68:23:25,  1.27s/it, loss=0.0155, lr=2.80e-05, step=5607]Training:   3%|‚ñé         | 5608/200000 [2:00:28<70:27:49,  1.30s/it, loss=0.0155, lr=2.80e-05, step=5607]Training:   3%|‚ñé         | 5608/200000 [2:00:28<70:27:49,  1.30s/it, loss=0.0132, lr=2.80e-05, step=5608]Training:   3%|‚ñé         | 5609/200000 [2:00:29<66:43:51,  1.24s/it, loss=0.0132, lr=2.80e-05, step=5608]Training:   3%|‚ñé         | 5609/200000 [2:00:29<66:43:51,  1.24s/it, loss=0.0155, lr=2.80e-05, step=5609]Training:   3%|‚ñé         | 5610/200000 [2:00:31<69:15:52,  1.28s/it, loss=0.0155, lr=2.80e-05, step=5609]Training:   3%|‚ñé         | 5610/200000 [2:00:31<69:15:52,  1.28s/it, loss=0.0173, lr=2.80e-05, step=5610]Training:   3%|‚ñé         | 5611/200000 [2:00:32<70:42:10,  1.31s/it, loss=0.0173, lr=2.80e-05, step=5610]Training:   3%|‚ñé         | 5611/200000 [2:00:32<70:42:10,  1.31s/it, loss=0.0106, lr=2.81e-05, step=5611]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5612/200000 [2:00:34<72:06:37,  1.34s/it, loss=0.0106, lr=2.81e-05, step=5611]Training:   3%|‚ñé         | 5612/200000 [2:00:34<72:06:37,  1.34s/it, loss=0.0167, lr=2.81e-05, step=5612]Training:   3%|‚ñé         | 5613/200000 [2:00:35<67:51:53,  1.26s/it, loss=0.0167, lr=2.81e-05, step=5612]Training:   3%|‚ñé         | 5613/200000 [2:00:35<67:51:53,  1.26s/it, loss=0.0176, lr=2.81e-05, step=5613]Training:   3%|‚ñé         | 5614/200000 [2:00:36<71:28:25,  1.32s/it, loss=0.0176, lr=2.81e-05, step=5613]Training:   3%|‚ñé         | 5614/200000 [2:00:36<71:28:25,  1.32s/it, loss=0.0162, lr=2.81e-05, step=5614]Training:   3%|‚ñé         | 5615/200000 [2:00:38<74:22:14,  1.38s/it, loss=0.0162, lr=2.81e-05, step=5614]Training:   3%|‚ñé         | 5615/200000 [2:00:38<74:22:14,  1.38s/it, loss=0.0208, lr=2.81e-05, step=5615]Training:   3%|‚ñé         | 5616/200000 [2:00:39<69:27:37,  1.29s/it, loss=0.0208, lr=2.81e-05, step=5615]Training:   3%|‚ñé         | 5616/200000 [2:00:39<69:27:37,  1.29s/it, loss=0.0189, lr=2.81e-05, step=5616]Training:   3%|‚ñé         | 5617/200000 [2:00:40<66:02:35,  1.22s/it, loss=0.0189, lr=2.81e-05, step=5616]Training:   3%|‚ñé         | 5617/200000 [2:00:40<66:02:35,  1.22s/it, loss=0.0133, lr=2.81e-05, step=5617]Training:   3%|‚ñé         | 5618/200000 [2:00:41<69:25:08,  1.29s/it, loss=0.0133, lr=2.81e-05, step=5617]Training:   3%|‚ñé         | 5618/200000 [2:00:41<69:25:08,  1.29s/it, loss=0.0165, lr=2.81e-05, step=5618]Training:   3%|‚ñé         | 5619/200000 [2:00:42<66:00:53,  1.22s/it, loss=0.0165, lr=2.81e-05, step=5618]Training:   3%|‚ñé         | 5619/200000 [2:00:42<66:00:53,  1.22s/it, loss=0.0246, lr=2.81e-05, step=5619]Training:   3%|‚ñé         | 5620/200000 [2:00:44<67:01:28,  1.24s/it, loss=0.0246, lr=2.81e-05, step=5619]Training:   3%|‚ñé         | 5620/200000 [2:00:44<67:01:28,  1.24s/it, loss=0.0170, lr=2.81e-05, step=5620]Training:   3%|‚ñé         | 5621/200000 [2:00:45<64:16:55,  1.19s/it, loss=0.0170, lr=2.81e-05, step=5620]Training:   3%|‚ñé         | 5621/200000 [2:00:45<64:16:55,  1.19s/it, loss=0.0170, lr=2.81e-05, step=5621]Training:   3%|‚ñé         | 5622/200000 [2:00:46<68:19:03,  1.27s/it, loss=0.0170, lr=2.81e-05, step=5621]Training:   3%|‚ñé         | 5622/200000 [2:00:46<68:19:03,  1.27s/it, loss=0.0147, lr=2.81e-05, step=5622]Training:   3%|‚ñé         | 5623/200000 [2:00:47<70:50:50,  1.31s/it, loss=0.0147, lr=2.81e-05, step=5622]Training:   3%|‚ñé         | 5623/200000 [2:00:47<70:50:50,  1.31s/it, loss=0.0105, lr=2.81e-05, step=5623]Training:   3%|‚ñé         | 5624/200000 [2:00:49<72:21:30,  1.34s/it, loss=0.0105, lr=2.81e-05, step=5623]Training:   3%|‚ñé         | 5624/200000 [2:00:49<72:21:30,  1.34s/it, loss=0.0192, lr=2.81e-05, step=5624]Training:   3%|‚ñé         | 5625/200000 [2:00:50<73:43:54,  1.37s/it, loss=0.0192, lr=2.81e-05, step=5624]Training:   3%|‚ñé         | 5625/200000 [2:00:50<73:43:54,  1.37s/it, loss=0.0174, lr=2.81e-05, step=5625]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5626/200000 [2:00:51<69:04:11,  1.28s/it, loss=0.0174, lr=2.81e-05, step=5625]Training:   3%|‚ñé         | 5626/200000 [2:00:51<69:04:11,  1.28s/it, loss=0.0186, lr=2.81e-05, step=5626]Training:   3%|‚ñé         | 5627/200000 [2:00:52<65:45:43,  1.22s/it, loss=0.0186, lr=2.81e-05, step=5626]Training:   3%|‚ñé         | 5627/200000 [2:00:52<65:45:43,  1.22s/it, loss=0.0160, lr=2.81e-05, step=5627]Training:   3%|‚ñé         | 5628/200000 [2:00:54<67:43:41,  1.25s/it, loss=0.0160, lr=2.81e-05, step=5627]Training:   3%|‚ñé         | 5628/200000 [2:00:54<67:43:41,  1.25s/it, loss=0.0116, lr=2.81e-05, step=5628]Training:   3%|‚ñé         | 5629/200000 [2:00:55<69:30:31,  1.29s/it, loss=0.0116, lr=2.81e-05, step=5628]Training:   3%|‚ñé         | 5629/200000 [2:00:55<69:30:31,  1.29s/it, loss=0.0603, lr=2.81e-05, step=5629]Training:   3%|‚ñé         | 5630/200000 [2:00:56<66:04:53,  1.22s/it, loss=0.0603, lr=2.81e-05, step=5629]Training:   3%|‚ñé         | 5630/200000 [2:00:56<66:04:53,  1.22s/it, loss=0.0115, lr=2.81e-05, step=5630]Training:   3%|‚ñé         | 5631/200000 [2:00:58<67:30:24,  1.25s/it, loss=0.0115, lr=2.81e-05, step=5630]Training:   3%|‚ñé         | 5631/200000 [2:00:58<67:30:24,  1.25s/it, loss=0.0115, lr=2.82e-05, step=5631]Training:   3%|‚ñé         | 5632/200000 [2:00:59<69:13:14,  1.28s/it, loss=0.0115, lr=2.82e-05, step=5631]Training:   3%|‚ñé         | 5632/200000 [2:00:59<69:13:14,  1.28s/it, loss=0.0166, lr=2.82e-05, step=5632]Training:   3%|‚ñé         | 5633/200000 [2:01:00<70:03:56,  1.30s/it, loss=0.0166, lr=2.82e-05, step=5632]Training:   3%|‚ñé         | 5633/200000 [2:01:00<70:03:56,  1.30s/it, loss=0.0140, lr=2.82e-05, step=5633]Training:   3%|‚ñé         | 5634/200000 [2:01:01<66:27:54,  1.23s/it, loss=0.0140, lr=2.82e-05, step=5633]Training:   3%|‚ñé         | 5634/200000 [2:01:01<66:27:54,  1.23s/it, loss=0.0138, lr=2.82e-05, step=5634]Training:   3%|‚ñé         | 5635/200000 [2:01:03<70:09:13,  1.30s/it, loss=0.0138, lr=2.82e-05, step=5634]Training:   3%|‚ñé         | 5635/200000 [2:01:03<70:09:13,  1.30s/it, loss=0.0179, lr=2.82e-05, step=5635]Training:   3%|‚ñé         | 5636/200000 [2:01:04<72:28:19,  1.34s/it, loss=0.0179, lr=2.82e-05, step=5635]Training:   3%|‚ñé         | 5636/200000 [2:01:04<72:28:19,  1.34s/it, loss=0.0124, lr=2.82e-05, step=5636]Training:   3%|‚ñé         | 5637/200000 [2:01:05<68:08:04,  1.26s/it, loss=0.0124, lr=2.82e-05, step=5636]Training:   3%|‚ñé         | 5637/200000 [2:01:05<68:08:04,  1.26s/it, loss=0.0261, lr=2.82e-05, step=5637]Training:   3%|‚ñé         | 5638/200000 [2:01:06<65:08:25,  1.21s/it, loss=0.0261, lr=2.82e-05, step=5637]Training:   3%|‚ñé         | 5638/200000 [2:01:06<65:08:25,  1.21s/it, loss=0.0218, lr=2.82e-05, step=5638]Training:   3%|‚ñé         | 5639/200000 [2:01:08<68:06:30,  1.26s/it, loss=0.0218, lr=2.82e-05, step=5638]Training:   3%|‚ñé         | 5639/200000 [2:01:08<68:06:30,  1.26s/it, loss=0.0127, lr=2.82e-05, step=5639]Training:   3%|‚ñé         | 5640/200000 [2:01:09<71:08:33,  1.32s/it, loss=0.0127, lr=2.82e-05, step=5639]Training:   3%|‚ñé         | 5640/200000 [2:01:09<71:08:33,  1.32s/it, loss=0.0180, lr=2.82e-05, step=5640]Training:   3%|‚ñé         | 5641/200000 [2:01:10<67:13:55,  1.25s/it, loss=0.0180, lr=2.82e-05, step=5640]Training:   3%|‚ñé         | 5641/200000 [2:01:10<67:13:55,  1.25s/it, loss=0.0172, lr=2.82e-05, step=5641]Training:   3%|‚ñé         | 5642/200000 [2:01:12<69:56:47,  1.30s/it, loss=0.0172, lr=2.82e-05, step=5641]Training:   3%|‚ñé         | 5642/200000 [2:01:12<69:56:47,  1.30s/it, loss=0.0144, lr=2.82e-05, step=5642]Training:   3%|‚ñé         | 5643/200000 [2:01:13<70:46:17,  1.31s/it, loss=0.0144, lr=2.82e-05, step=5642]Training:   3%|‚ñé         | 5643/200000 [2:01:13<70:46:17,  1.31s/it, loss=0.0160, lr=2.82e-05, step=5643]Training:   3%|‚ñé         | 5644/200000 [2:01:14<71:27:50,  1.32s/it, loss=0.0160, lr=2.82e-05, step=5643]Training:   3%|‚ñé         | 5644/200000 [2:01:14<71:27:50,  1.32s/it, loss=0.0118, lr=2.82e-05, step=5644]Training:   3%|‚ñé         | 5645/200000 [2:01:15<67:27:23,  1.25s/it, loss=0.0118, lr=2.82e-05, step=5644]Training:   3%|‚ñé         | 5645/200000 [2:01:15<67:27:23,  1.25s/it, loss=0.0191, lr=2.82e-05, step=5645]Training:   3%|‚ñé         | 5646/200000 [2:01:17<71:30:26,  1.32s/it, loss=0.0191, lr=2.82e-05, step=5645]Training:   3%|‚ñé         | 5646/200000 [2:01:17<71:30:26,  1.32s/it, loss=0.0178, lr=2.82e-05, step=5646]Training:   3%|‚ñé         | 5647/200000 [2:01:18<74:39:43,  1.38s/it, loss=0.0178, lr=2.82e-05, step=5646]Training:   3%|‚ñé         | 5647/200000 [2:01:18<74:39:43,  1.38s/it, loss=0.0314, lr=2.82e-05, step=5647]Training:   3%|‚ñé         | 5648/200000 [2:01:20<69:42:02,  1.29s/it, loss=0.0314, lr=2.82e-05, step=5647]Training:   3%|‚ñé         | 5648/200000 [2:01:20<69:42:02,  1.29s/it, loss=0.0390, lr=2.82e-05, step=5648]Training:   3%|‚ñé         | 5649/200000 [2:01:21<66:12:10,  1.23s/it, loss=0.0390, lr=2.82e-05, step=5648]Training:   3%|‚ñé         | 5649/200000 [2:01:21<66:12:10,  1.23s/it, loss=0.0133, lr=2.82e-05, step=5649]Training:   3%|‚ñé         | 5650/200000 [2:01:22<70:13:08,  1.30s/it, loss=0.0133, lr=2.82e-05, step=5649]Training:   3%|‚ñé         | 5650/200000 [2:01:22<70:13:08,  1.30s/it, loss=0.0109, lr=2.82e-05, step=5650]Training:   3%|‚ñé         | 5651/200000 [2:01:23<66:35:33,  1.23s/it, loss=0.0109, lr=2.82e-05, step=5650]Training:   3%|‚ñé         | 5651/200000 [2:01:23<66:35:33,  1.23s/it, loss=0.0149, lr=2.83e-05, step=5651]Training:   3%|‚ñé         | 5652/200000 [2:01:24<67:42:40,  1.25s/it, loss=0.0149, lr=2.83e-05, step=5651]Training:   3%|‚ñé         | 5652/200000 [2:01:24<67:42:40,  1.25s/it, loss=0.0206, lr=2.83e-05, step=5652]Training:   3%|‚ñé         | 5653/200000 [2:01:26<64:47:26,  1.20s/it, loss=0.0206, lr=2.83e-05, step=5652]Training:   3%|‚ñé         | 5653/200000 [2:01:26<64:47:26,  1.20s/it, loss=0.0215, lr=2.83e-05, step=5653]Training:   3%|‚ñé         | 5654/200000 [2:01:27<68:59:33,  1.28s/it, loss=0.0215, lr=2.83e-05, step=5653]Training:   3%|‚ñé         | 5654/200000 [2:01:27<68:59:33,  1.28s/it, loss=0.0172, lr=2.83e-05, step=5654]Training:   3%|‚ñé         | 5655/200000 [2:01:29<72:13:41,  1.34s/it, loss=0.0172, lr=2.83e-05, step=5654]Training:   3%|‚ñé         | 5655/200000 [2:01:29<72:13:41,  1.34s/it, loss=0.0209, lr=2.83e-05, step=5655]Training:   3%|‚ñé         | 5656/200000 [2:01:30<74:05:52,  1.37s/it, loss=0.0209, lr=2.83e-05, step=5655]Training:   3%|‚ñé         | 5656/200000 [2:01:30<74:05:52,  1.37s/it, loss=0.0212, lr=2.83e-05, step=5656]Training:   3%|‚ñé         | 5657/200000 [2:01:31<75:06:48,  1.39s/it, loss=0.0212, lr=2.83e-05, step=5656]Training:   3%|‚ñé         | 5657/200000 [2:01:31<75:06:48,  1.39s/it, loss=0.0163, lr=2.83e-05, step=5657]Training:   3%|‚ñé         | 5658/200000 [2:01:32<70:00:02,  1.30s/it, loss=0.0163, lr=2.83e-05, step=5657]Training:   3%|‚ñé         | 5658/200000 [2:01:32<70:00:02,  1.30s/it, loss=0.0177, lr=2.83e-05, step=5658]Training:   3%|‚ñé         | 5659/200000 [2:01:34<66:25:07,  1.23s/it, loss=0.0177, lr=2.83e-05, step=5658]Training:   3%|‚ñé         | 5659/200000 [2:01:34<66:25:07,  1.23s/it, loss=0.0170, lr=2.83e-05, step=5659]Training:   3%|‚ñé         | 5660/200000 [2:01:35<68:31:37,  1.27s/it, loss=0.0170, lr=2.83e-05, step=5659]Training:   3%|‚ñé         | 5660/200000 [2:01:35<68:31:37,  1.27s/it, loss=0.0118, lr=2.83e-05, step=5660]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5661/200000 [2:01:36<71:10:36,  1.32s/it, loss=0.0118, lr=2.83e-05, step=5660]Training:   3%|‚ñé         | 5661/200000 [2:01:36<71:10:36,  1.32s/it, loss=0.0121, lr=2.83e-05, step=5661]Training:   3%|‚ñé         | 5662/200000 [2:01:37<67:15:23,  1.25s/it, loss=0.0121, lr=2.83e-05, step=5661]Training:   3%|‚ñé         | 5662/200000 [2:01:37<67:15:23,  1.25s/it, loss=0.0153, lr=2.83e-05, step=5662]Training:   3%|‚ñé         | 5663/200000 [2:01:39<70:02:36,  1.30s/it, loss=0.0153, lr=2.83e-05, step=5662]Training:   3%|‚ñé         | 5663/200000 [2:01:39<70:02:36,  1.30s/it, loss=0.0173, lr=2.83e-05, step=5663]Training:   3%|‚ñé         | 5664/200000 [2:01:40<71:05:07,  1.32s/it, loss=0.0173, lr=2.83e-05, step=5663]Training:   3%|‚ñé         | 5664/200000 [2:01:40<71:05:07,  1.32s/it, loss=0.0185, lr=2.83e-05, step=5664]Training:   3%|‚ñé         | 5665/200000 [2:01:42<71:18:30,  1.32s/it, loss=0.0185, lr=2.83e-05, step=5664]Training:   3%|‚ñé         | 5665/200000 [2:01:42<71:18:30,  1.32s/it, loss=0.0211, lr=2.83e-05, step=5665]Training:   3%|‚ñé         | 5666/200000 [2:01:43<67:21:15,  1.25s/it, loss=0.0211, lr=2.83e-05, step=5665]Training:   3%|‚ñé         | 5666/200000 [2:01:43<67:21:15,  1.25s/it, loss=0.0095, lr=2.83e-05, step=5666]Training:   3%|‚ñé         | 5667/200000 [2:01:44<71:52:31,  1.33s/it, loss=0.0095, lr=2.83e-05, step=5666]Training:   3%|‚ñé         | 5667/200000 [2:01:44<71:52:31,  1.33s/it, loss=0.0165, lr=2.83e-05, step=5667]Training:   3%|‚ñé         | 5668/200000 [2:01:46<74:45:36,  1.38s/it, loss=0.0165, lr=2.83e-05, step=5667]Training:   3%|‚ñé         | 5668/200000 [2:01:46<74:45:36,  1.38s/it, loss=0.0191, lr=2.83e-05, step=5668]Training:   3%|‚ñé         | 5669/200000 [2:01:47<69:43:02,  1.29s/it, loss=0.0191, lr=2.83e-05, step=5668]Training:   3%|‚ñé         | 5669/200000 [2:01:47<69:43:02,  1.29s/it, loss=0.0207, lr=2.83e-05, step=5669]Training:   3%|‚ñé         | 5670/200000 [2:01:48<66:13:27,  1.23s/it, loss=0.0207, lr=2.83e-05, step=5669]Training:   3%|‚ñé         | 5670/200000 [2:01:48<66:13:27,  1.23s/it, loss=0.0140, lr=2.83e-05, step=5670]Training:   3%|‚ñé         | 5671/200000 [2:01:49<69:23:54,  1.29s/it, loss=0.0140, lr=2.83e-05, step=5670]Training:   3%|‚ñé         | 5671/200000 [2:01:49<69:23:54,  1.29s/it, loss=0.0170, lr=2.84e-05, step=5671]Training:   3%|‚ñé         | 5672/200000 [2:01:50<66:00:45,  1.22s/it, loss=0.0170, lr=2.84e-05, step=5671]Training:   3%|‚ñé         | 5672/200000 [2:01:50<66:00:45,  1.22s/it, loss=0.0428, lr=2.84e-05, step=5672]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5673/200000 [2:01:52<68:34:07,  1.27s/it, loss=0.0428, lr=2.84e-05, step=5672]Training:   3%|‚ñé         | 5673/200000 [2:01:52<68:34:07,  1.27s/it, loss=0.0159, lr=2.84e-05, step=5673]Training:   3%|‚ñé         | 5674/200000 [2:01:53<65:23:57,  1.21s/it, loss=0.0159, lr=2.84e-05, step=5673]Training:   3%|‚ñé         | 5674/200000 [2:01:53<65:23:57,  1.21s/it, loss=0.0159, lr=2.84e-05, step=5674]Training:   3%|‚ñé         | 5675/200000 [2:01:54<68:25:19,  1.27s/it, loss=0.0159, lr=2.84e-05, step=5674]Training:   3%|‚ñé         | 5675/200000 [2:01:54<68:25:19,  1.27s/it, loss=0.0177, lr=2.84e-05, step=5675]Training:   3%|‚ñé         | 5676/200000 [2:01:56<70:48:51,  1.31s/it, loss=0.0177, lr=2.84e-05, step=5675]Training:   3%|‚ñé         | 5676/200000 [2:01:56<70:48:51,  1.31s/it, loss=0.0153, lr=2.84e-05, step=5676]Training:   3%|‚ñé         | 5677/200000 [2:01:57<72:04:11,  1.34s/it, loss=0.0153, lr=2.84e-05, step=5676]Training:   3%|‚ñé         | 5677/200000 [2:01:57<72:04:11,  1.34s/it, loss=0.0150, lr=2.84e-05, step=5677]Training:   3%|‚ñé         | 5678/200000 [2:01:58<73:22:26,  1.36s/it, loss=0.0150, lr=2.84e-05, step=5677]Training:   3%|‚ñé         | 5678/200000 [2:01:58<73:22:26,  1.36s/it, loss=0.0493, lr=2.84e-05, step=5678]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5679/200000 [2:01:59<68:47:13,  1.27s/it, loss=0.0493, lr=2.84e-05, step=5678]Training:   3%|‚ñé         | 5679/200000 [2:01:59<68:47:13,  1.27s/it, loss=0.0203, lr=2.84e-05, step=5679]Training:   3%|‚ñé         | 5680/200000 [2:02:01<65:36:13,  1.22s/it, loss=0.0203, lr=2.84e-05, step=5679]Training:   3%|‚ñé         | 5680/200000 [2:02:01<65:36:13,  1.22s/it, loss=0.0203, lr=2.84e-05, step=5680]Training:   3%|‚ñé         | 5681/200000 [2:02:02<67:37:32,  1.25s/it, loss=0.0203, lr=2.84e-05, step=5680]Training:   3%|‚ñé         | 5681/200000 [2:02:02<67:37:32,  1.25s/it, loss=0.0177, lr=2.84e-05, step=5681]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5682/200000 [2:02:03<68:51:57,  1.28s/it, loss=0.0177, lr=2.84e-05, step=5681]Training:   3%|‚ñé         | 5682/200000 [2:02:03<68:51:57,  1.28s/it, loss=0.0134, lr=2.84e-05, step=5682]Training:   3%|‚ñé         | 5683/200000 [2:02:04<65:38:32,  1.22s/it, loss=0.0134, lr=2.84e-05, step=5682]Training:   3%|‚ñé         | 5683/200000 [2:02:04<65:38:32,  1.22s/it, loss=0.0226, lr=2.84e-05, step=5683]Training:   3%|‚ñé         | 5684/200000 [2:02:06<67:28:57,  1.25s/it, loss=0.0226, lr=2.84e-05, step=5683]Training:   3%|‚ñé         | 5684/200000 [2:02:06<67:28:57,  1.25s/it, loss=0.0144, lr=2.84e-05, step=5684]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5685/200000 [2:02:07<69:11:01,  1.28s/it, loss=0.0144, lr=2.84e-05, step=5684]Training:   3%|‚ñé         | 5685/200000 [2:02:07<69:11:01,  1.28s/it, loss=0.0217, lr=2.84e-05, step=5685]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5686/200000 [2:02:08<69:56:04,  1.30s/it, loss=0.0217, lr=2.84e-05, step=5685]Training:   3%|‚ñé         | 5686/200000 [2:02:08<69:56:04,  1.30s/it, loss=0.0250, lr=2.84e-05, step=5686]Training:   3%|‚ñé         | 5687/200000 [2:02:09<66:22:02,  1.23s/it, loss=0.0250, lr=2.84e-05, step=5686]Training:   3%|‚ñé         | 5687/200000 [2:02:09<66:22:02,  1.23s/it, loss=0.0233, lr=2.84e-05, step=5687]Training:   3%|‚ñé         | 5688/200000 [2:02:11<69:24:56,  1.29s/it, loss=0.0233, lr=2.84e-05, step=5687]Training:   3%|‚ñé         | 5688/200000 [2:02:11<69:24:56,  1.29s/it, loss=0.0180, lr=2.84e-05, step=5688]Training:   3%|‚ñé         | 5689/200000 [2:02:12<72:00:22,  1.33s/it, loss=0.0180, lr=2.84e-05, step=5688]Training:   3%|‚ñé         | 5689/200000 [2:02:12<72:00:22,  1.33s/it, loss=0.0183, lr=2.84e-05, step=5689]Training:   3%|‚ñé         | 5690/200000 [2:02:13<67:49:18,  1.26s/it, loss=0.0183, lr=2.84e-05, step=5689]Training:   3%|‚ñé         | 5690/200000 [2:02:13<67:49:18,  1.26s/it, loss=0.0140, lr=2.84e-05, step=5690]Training:   3%|‚ñé         | 5691/200000 [2:02:14<64:53:11,  1.20s/it, loss=0.0140, lr=2.84e-05, step=5690]Training:   3%|‚ñé         | 5691/200000 [2:02:14<64:53:11,  1.20s/it, loss=0.0119, lr=2.85e-05, step=5691]Training:   3%|‚ñé         | 5692/200000 [2:02:16<68:07:00,  1.26s/it, loss=0.0119, lr=2.85e-05, step=5691]Training:   3%|‚ñé         | 5692/200000 [2:02:16<68:07:00,  1.26s/it, loss=0.0298, lr=2.85e-05, step=5692]Training:   3%|‚ñé         | 5693/200000 [2:02:17<71:52:29,  1.33s/it, loss=0.0298, lr=2.85e-05, step=5692]Training:   3%|‚ñé         | 5693/200000 [2:02:17<71:52:29,  1.33s/it, loss=0.0183, lr=2.85e-05, step=5693]Training:   3%|‚ñé         | 5694/200000 [2:02:18<67:44:40,  1.26s/it, loss=0.0183, lr=2.85e-05, step=5693]Training:   3%|‚ñé         | 5694/200000 [2:02:18<67:44:40,  1.26s/it, loss=0.0185, lr=2.85e-05, step=5694]Training:   3%|‚ñé         | 5695/200000 [2:02:20<70:28:46,  1.31s/it, loss=0.0185, lr=2.85e-05, step=5694]Training:   3%|‚ñé         | 5695/200000 [2:02:20<70:28:46,  1.31s/it, loss=0.0135, lr=2.85e-05, step=5695]Training:   3%|‚ñé         | 5696/200000 [2:02:21<71:12:51,  1.32s/it, loss=0.0135, lr=2.85e-05, step=5695]Training:   3%|‚ñé         | 5696/200000 [2:02:21<71:12:51,  1.32s/it, loss=0.0139, lr=2.85e-05, step=5696]Training:   3%|‚ñé         | 5697/200000 [2:02:22<71:43:50,  1.33s/it, loss=0.0139, lr=2.85e-05, step=5696]Training:   3%|‚ñé         | 5697/200000 [2:02:22<71:43:50,  1.33s/it, loss=0.0509, lr=2.85e-05, step=5697]Training:   3%|‚ñé         | 5698/200000 [2:02:24<67:37:35,  1.25s/it, loss=0.0509, lr=2.85e-05, step=5697]Training:   3%|‚ñé         | 5698/200000 [2:02:24<67:37:35,  1.25s/it, loss=0.0147, lr=2.85e-05, step=5698]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5699/200000 [2:02:25<71:48:35,  1.33s/it, loss=0.0147, lr=2.85e-05, step=5698]Training:   3%|‚ñé         | 5699/200000 [2:02:25<71:48:35,  1.33s/it, loss=0.0084, lr=2.85e-05, step=5699]Training:   3%|‚ñé         | 5700/200000 [2:02:27<74:53:24,  1.39s/it, loss=0.0084, lr=2.85e-05, step=5699]Training:   3%|‚ñé         | 5700/200000 [2:02:27<74:53:24,  1.39s/it, loss=0.0234, lr=2.85e-05, step=5700]00:55:41.461 [I] step=5700 loss=0.0191 lr=2.83e-05 grad_norm=0.37 time=128.6s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5701/200000 [2:02:28<69:51:50,  1.29s/it, loss=0.0234, lr=2.85e-05, step=5700]Training:   3%|‚ñé         | 5701/200000 [2:02:28<69:51:50,  1.29s/it, loss=0.0191, lr=2.85e-05, step=5701]Training:   3%|‚ñé         | 5702/200000 [2:02:29<66:20:32,  1.23s/it, loss=0.0191, lr=2.85e-05, step=5701]Training:   3%|‚ñé         | 5702/200000 [2:02:29<66:20:32,  1.23s/it, loss=0.0137, lr=2.85e-05, step=5702]Training:   3%|‚ñé         | 5703/200000 [2:02:30<70:24:13,  1.30s/it, loss=0.0137, lr=2.85e-05, step=5702]Training:   3%|‚ñé         | 5703/200000 [2:02:30<70:24:13,  1.30s/it, loss=0.0129, lr=2.85e-05, step=5703]Training:   3%|‚ñé         | 5704/200000 [2:02:31<66:43:15,  1.24s/it, loss=0.0129, lr=2.85e-05, step=5703]Training:   3%|‚ñé         | 5704/200000 [2:02:31<66:43:15,  1.24s/it, loss=0.0129, lr=2.85e-05, step=5704]Training:   3%|‚ñé         | 5705/200000 [2:02:33<68:23:50,  1.27s/it, loss=0.0129, lr=2.85e-05, step=5704]Training:   3%|‚ñé         | 5705/200000 [2:02:33<68:23:50,  1.27s/it, loss=0.0116, lr=2.85e-05, step=5705]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5706/200000 [2:02:34<65:18:43,  1.21s/it, loss=0.0116, lr=2.85e-05, step=5705]Training:   3%|‚ñé         | 5706/200000 [2:02:34<65:18:43,  1.21s/it, loss=0.0147, lr=2.85e-05, step=5706]Training:   3%|‚ñé         | 5707/200000 [2:02:35<69:16:54,  1.28s/it, loss=0.0147, lr=2.85e-05, step=5706]Training:   3%|‚ñé         | 5707/200000 [2:02:35<69:16:54,  1.28s/it, loss=0.0096, lr=2.85e-05, step=5707]Training:   3%|‚ñé         | 5708/200000 [2:02:37<71:45:42,  1.33s/it, loss=0.0096, lr=2.85e-05, step=5707]Training:   3%|‚ñé         | 5708/200000 [2:02:37<71:45:42,  1.33s/it, loss=0.0226, lr=2.85e-05, step=5708]Training:   3%|‚ñé         | 5709/200000 [2:02:38<74:01:42,  1.37s/it, loss=0.0226, lr=2.85e-05, step=5708]Training:   3%|‚ñé         | 5709/200000 [2:02:38<74:01:42,  1.37s/it, loss=0.0126, lr=2.85e-05, step=5709]Training:   3%|‚ñé         | 5710/200000 [2:02:39<74:42:13,  1.38s/it, loss=0.0126, lr=2.85e-05, step=5709]Training:   3%|‚ñé         | 5710/200000 [2:02:39<74:42:13,  1.38s/it, loss=0.0171, lr=2.85e-05, step=5710]Training:   3%|‚ñé         | 5711/200000 [2:02:41<69:39:41,  1.29s/it, loss=0.0171, lr=2.85e-05, step=5710]Training:   3%|‚ñé         | 5711/200000 [2:02:41<69:39:41,  1.29s/it, loss=0.0214, lr=2.86e-05, step=5711]Training:   3%|‚ñé         | 5712/200000 [2:02:42<66:11:38,  1.23s/it, loss=0.0214, lr=2.86e-05, step=5711]Training:   3%|‚ñé         | 5712/200000 [2:02:42<66:11:38,  1.23s/it, loss=0.0112, lr=2.86e-05, step=5712]Training:   3%|‚ñé         | 5713/200000 [2:02:43<68:12:37,  1.26s/it, loss=0.0112, lr=2.86e-05, step=5712]Training:   3%|‚ñé         | 5713/200000 [2:02:43<68:12:37,  1.26s/it, loss=0.0117, lr=2.86e-05, step=5713]Training:   3%|‚ñé         | 5714/200000 [2:02:44<70:17:55,  1.30s/it, loss=0.0117, lr=2.86e-05, step=5713]Training:   3%|‚ñé         | 5714/200000 [2:02:44<70:17:55,  1.30s/it, loss=0.0097, lr=2.86e-05, step=5714]Training:   3%|‚ñé         | 5715/200000 [2:02:45<66:37:27,  1.23s/it, loss=0.0097, lr=2.86e-05, step=5714]Training:   3%|‚ñé         | 5715/200000 [2:02:45<66:37:27,  1.23s/it, loss=0.0122, lr=2.86e-05, step=5715]Training:   3%|‚ñé         | 5716/200000 [2:02:47<68:57:31,  1.28s/it, loss=0.0122, lr=2.86e-05, step=5715]Training:   3%|‚ñé         | 5716/200000 [2:02:47<68:57:31,  1.28s/it, loss=0.0306, lr=2.86e-05, step=5716]Training:   3%|‚ñé         | 5717/200000 [2:02:48<70:23:57,  1.30s/it, loss=0.0306, lr=2.86e-05, step=5716]Training:   3%|‚ñé         | 5717/200000 [2:02:48<70:23:57,  1.30s/it, loss=0.0147, lr=2.86e-05, step=5717]Training:   3%|‚ñé         | 5718/200000 [2:02:50<71:11:50,  1.32s/it, loss=0.0147, lr=2.86e-05, step=5717]Training:   3%|‚ñé         | 5718/200000 [2:02:50<71:11:50,  1.32s/it, loss=0.0142, lr=2.86e-05, step=5718]Training:   3%|‚ñé         | 5719/200000 [2:02:51<67:15:37,  1.25s/it, loss=0.0142, lr=2.86e-05, step=5718]Training:   3%|‚ñé         | 5719/200000 [2:02:51<67:15:37,  1.25s/it, loss=0.0185, lr=2.86e-05, step=5719]Training:   3%|‚ñé         | 5720/200000 [2:02:52<71:06:17,  1.32s/it, loss=0.0185, lr=2.86e-05, step=5719]Training:   3%|‚ñé         | 5720/200000 [2:02:52<71:06:17,  1.32s/it, loss=0.0158, lr=2.86e-05, step=5720]Training:   3%|‚ñé         | 5721/200000 [2:02:54<74:07:51,  1.37s/it, loss=0.0158, lr=2.86e-05, step=5720]Training:   3%|‚ñé         | 5721/200000 [2:02:54<74:07:51,  1.37s/it, loss=0.0329, lr=2.86e-05, step=5721]Training:   3%|‚ñé         | 5722/200000 [2:02:55<69:18:21,  1.28s/it, loss=0.0329, lr=2.86e-05, step=5721]Training:   3%|‚ñé         | 5722/200000 [2:02:55<69:18:21,  1.28s/it, loss=0.0108, lr=2.86e-05, step=5722]Training:   3%|‚ñé         | 5723/200000 [2:02:56<65:55:11,  1.22s/it, loss=0.0108, lr=2.86e-05, step=5722]Training:   3%|‚ñé         | 5723/200000 [2:02:56<65:55:11,  1.22s/it, loss=0.0166, lr=2.86e-05, step=5723]Training:   3%|‚ñé         | 5724/200000 [2:02:57<69:19:51,  1.28s/it, loss=0.0166, lr=2.86e-05, step=5723]Training:   3%|‚ñé         | 5724/200000 [2:02:57<69:19:51,  1.28s/it, loss=0.0144, lr=2.86e-05, step=5724]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5725/200000 [2:02:58<65:53:56,  1.22s/it, loss=0.0144, lr=2.86e-05, step=5724]Training:   3%|‚ñé         | 5725/200000 [2:02:58<65:53:56,  1.22s/it, loss=0.0160, lr=2.86e-05, step=5725]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5726/200000 [2:03:00<68:13:59,  1.26s/it, loss=0.0160, lr=2.86e-05, step=5725]Training:   3%|‚ñé         | 5726/200000 [2:03:00<68:13:59,  1.26s/it, loss=0.0198, lr=2.86e-05, step=5726]Training:   3%|‚ñé         | 5727/200000 [2:03:01<65:10:36,  1.21s/it, loss=0.0198, lr=2.86e-05, step=5726]Training:   3%|‚ñé         | 5727/200000 [2:03:01<65:10:36,  1.21s/it, loss=0.0208, lr=2.86e-05, step=5727]Training:   3%|‚ñé         | 5728/200000 [2:03:02<69:08:09,  1.28s/it, loss=0.0208, lr=2.86e-05, step=5727]Training:   3%|‚ñé         | 5728/200000 [2:03:02<69:08:09,  1.28s/it, loss=0.0175, lr=2.86e-05, step=5728]Training:   3%|‚ñé         | 5729/200000 [2:03:04<71:55:29,  1.33s/it, loss=0.0175, lr=2.86e-05, step=5728]Training:   3%|‚ñé         | 5729/200000 [2:03:04<71:55:29,  1.33s/it, loss=0.0154, lr=2.86e-05, step=5729]Training:   3%|‚ñé         | 5730/200000 [2:03:05<73:19:32,  1.36s/it, loss=0.0154, lr=2.86e-05, step=5729]Training:   3%|‚ñé         | 5730/200000 [2:03:05<73:19:32,  1.36s/it, loss=0.0175, lr=2.86e-05, step=5730]Training:   3%|‚ñé         | 5731/200000 [2:03:06<74:07:19,  1.37s/it, loss=0.0175, lr=2.86e-05, step=5730]Training:   3%|‚ñé         | 5731/200000 [2:03:06<74:07:19,  1.37s/it, loss=0.0225, lr=2.87e-05, step=5731]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5732/200000 [2:03:08<69:17:34,  1.28s/it, loss=0.0225, lr=2.87e-05, step=5731]Training:   3%|‚ñé         | 5732/200000 [2:03:08<69:17:34,  1.28s/it, loss=0.0142, lr=2.87e-05, step=5732]Training:   3%|‚ñé         | 5733/200000 [2:03:09<65:52:11,  1.22s/it, loss=0.0142, lr=2.87e-05, step=5732]Training:   3%|‚ñé         | 5733/200000 [2:03:09<65:52:11,  1.22s/it, loss=0.0126, lr=2.87e-05, step=5733]Training:   3%|‚ñé         | 5734/200000 [2:03:10<67:44:40,  1.26s/it, loss=0.0126, lr=2.87e-05, step=5733]Training:   3%|‚ñé         | 5734/200000 [2:03:10<67:44:40,  1.26s/it, loss=0.0187, lr=2.87e-05, step=5734]Training:   3%|‚ñé         | 5735/200000 [2:03:11<69:28:24,  1.29s/it, loss=0.0187, lr=2.87e-05, step=5734]Training:   3%|‚ñé         | 5735/200000 [2:03:11<69:28:24,  1.29s/it, loss=0.0153, lr=2.87e-05, step=5735]Training:   3%|‚ñé         | 5736/200000 [2:03:12<66:02:57,  1.22s/it, loss=0.0153, lr=2.87e-05, step=5735]Training:   3%|‚ñé         | 5736/200000 [2:03:12<66:02:57,  1.22s/it, loss=0.0129, lr=2.87e-05, step=5736]Training:   3%|‚ñé         | 5737/200000 [2:03:14<68:15:25,  1.26s/it, loss=0.0129, lr=2.87e-05, step=5736]Training:   3%|‚ñé         | 5737/200000 [2:03:14<68:15:25,  1.26s/it, loss=0.0142, lr=2.87e-05, step=5737]Training:   3%|‚ñé         | 5738/200000 [2:03:15<69:42:54,  1.29s/it, loss=0.0142, lr=2.87e-05, step=5737]Training:   3%|‚ñé         | 5738/200000 [2:03:15<69:42:54,  1.29s/it, loss=0.0222, lr=2.87e-05, step=5738]Training:   3%|‚ñé         | 5739/200000 [2:03:16<70:33:57,  1.31s/it, loss=0.0222, lr=2.87e-05, step=5738]Training:   3%|‚ñé         | 5739/200000 [2:03:16<70:33:57,  1.31s/it, loss=0.0111, lr=2.87e-05, step=5739]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5740/200000 [2:03:17<66:49:20,  1.24s/it, loss=0.0111, lr=2.87e-05, step=5739]Training:   3%|‚ñé         | 5740/200000 [2:03:17<66:49:20,  1.24s/it, loss=0.0129, lr=2.87e-05, step=5740]Training:   3%|‚ñé         | 5741/200000 [2:03:19<69:38:58,  1.29s/it, loss=0.0129, lr=2.87e-05, step=5740]Training:   3%|‚ñé         | 5741/200000 [2:03:19<69:38:58,  1.29s/it, loss=0.0113, lr=2.87e-05, step=5741]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5742/200000 [2:03:20<72:11:04,  1.34s/it, loss=0.0113, lr=2.87e-05, step=5741]Training:   3%|‚ñé         | 5742/200000 [2:03:20<72:11:04,  1.34s/it, loss=0.0266, lr=2.87e-05, step=5742]Training:   3%|‚ñé         | 5743/200000 [2:03:21<67:56:27,  1.26s/it, loss=0.0266, lr=2.87e-05, step=5742]Training:   3%|‚ñé         | 5743/200000 [2:03:21<67:56:27,  1.26s/it, loss=0.0146, lr=2.87e-05, step=5743]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5744/200000 [2:03:23<64:58:35,  1.20s/it, loss=0.0146, lr=2.87e-05, step=5743]Training:   3%|‚ñé         | 5744/200000 [2:03:23<64:58:35,  1.20s/it, loss=0.0191, lr=2.87e-05, step=5744]Training:   3%|‚ñé         | 5745/200000 [2:03:24<68:03:49,  1.26s/it, loss=0.0191, lr=2.87e-05, step=5744]Training:   3%|‚ñé         | 5745/200000 [2:03:24<68:03:49,  1.26s/it, loss=0.0123, lr=2.87e-05, step=5745]Training:   3%|‚ñé         | 5746/200000 [2:03:25<70:07:16,  1.30s/it, loss=0.0123, lr=2.87e-05, step=5745]Training:   3%|‚ñé         | 5746/200000 [2:03:25<70:07:16,  1.30s/it, loss=0.0230, lr=2.87e-05, step=5746]Training:   3%|‚ñé         | 5747/200000 [2:03:26<66:29:00,  1.23s/it, loss=0.0230, lr=2.87e-05, step=5746]Training:   3%|‚ñé         | 5747/200000 [2:03:26<66:29:00,  1.23s/it, loss=0.0360, lr=2.87e-05, step=5747]Training:   3%|‚ñé         | 5748/200000 [2:03:28<69:44:33,  1.29s/it, loss=0.0360, lr=2.87e-05, step=5747]Training:   3%|‚ñé         | 5748/200000 [2:03:28<69:44:33,  1.29s/it, loss=0.0197, lr=2.87e-05, step=5748]Training:   3%|‚ñé         | 5749/200000 [2:03:29<70:32:26,  1.31s/it, loss=0.0197, lr=2.87e-05, step=5748]Training:   3%|‚ñé         | 5749/200000 [2:03:29<70:32:26,  1.31s/it, loss=0.0206, lr=2.87e-05, step=5749]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5750/200000 [2:03:31<72:01:49,  1.33s/it, loss=0.0206, lr=2.87e-05, step=5749]Training:   3%|‚ñé         | 5750/200000 [2:03:31<72:01:49,  1.33s/it, loss=0.0226, lr=2.87e-05, step=5750]Training:   3%|‚ñé         | 5751/200000 [2:03:32<67:48:30,  1.26s/it, loss=0.0226, lr=2.87e-05, step=5750]Training:   3%|‚ñé         | 5751/200000 [2:03:32<67:48:30,  1.26s/it, loss=0.0174, lr=2.88e-05, step=5751]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5752/200000 [2:03:33<71:44:13,  1.33s/it, loss=0.0174, lr=2.88e-05, step=5751]Training:   3%|‚ñé         | 5752/200000 [2:03:33<71:44:13,  1.33s/it, loss=0.0157, lr=2.88e-05, step=5752]Training:   3%|‚ñé         | 5753/200000 [2:03:35<74:55:09,  1.39s/it, loss=0.0157, lr=2.88e-05, step=5752]Training:   3%|‚ñé         | 5753/200000 [2:03:35<74:55:09,  1.39s/it, loss=0.0342, lr=2.88e-05, step=5753]Training:   3%|‚ñé         | 5754/200000 [2:03:36<69:52:29,  1.30s/it, loss=0.0342, lr=2.88e-05, step=5753]Training:   3%|‚ñé         | 5754/200000 [2:03:36<69:52:29,  1.30s/it, loss=0.0171, lr=2.88e-05, step=5754]Training:   3%|‚ñé         | 5755/200000 [2:03:37<66:19:23,  1.23s/it, loss=0.0171, lr=2.88e-05, step=5754]Training:   3%|‚ñé         | 5755/200000 [2:03:37<66:19:23,  1.23s/it, loss=0.0162, lr=2.88e-05, step=5755]Training:   3%|‚ñé         | 5756/200000 [2:03:38<70:24:01,  1.30s/it, loss=0.0162, lr=2.88e-05, step=5755]Training:   3%|‚ñé         | 5756/200000 [2:03:38<70:24:01,  1.30s/it, loss=0.0152, lr=2.88e-05, step=5756]Training:   3%|‚ñé         | 5757/200000 [2:03:39<66:40:50,  1.24s/it, loss=0.0152, lr=2.88e-05, step=5756]Training:   3%|‚ñé         | 5757/200000 [2:03:39<66:40:50,  1.24s/it, loss=0.0135, lr=2.88e-05, step=5757]Training:   3%|‚ñé         | 5758/200000 [2:03:41<68:55:55,  1.28s/it, loss=0.0135, lr=2.88e-05, step=5757]Training:   3%|‚ñé         | 5758/200000 [2:03:41<68:55:55,  1.28s/it, loss=0.0198, lr=2.88e-05, step=5758]Training:   3%|‚ñé         | 5759/200000 [2:03:42<65:39:56,  1.22s/it, loss=0.0198, lr=2.88e-05, step=5758]Training:   3%|‚ñé         | 5759/200000 [2:03:42<65:39:56,  1.22s/it, loss=0.0157, lr=2.88e-05, step=5759]Training:   3%|‚ñé         | 5760/200000 [2:03:43<69:31:00,  1.29s/it, loss=0.0157, lr=2.88e-05, step=5759]Training:   3%|‚ñé         | 5760/200000 [2:03:43<69:31:00,  1.29s/it, loss=0.3991, lr=2.88e-05, step=5760]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5761/200000 [2:03:45<71:50:11,  1.33s/it, loss=0.3991, lr=2.88e-05, step=5760]Training:   3%|‚ñé         | 5761/200000 [2:03:45<71:50:11,  1.33s/it, loss=0.0111, lr=2.88e-05, step=5761]Training:   3%|‚ñé         | 5762/200000 [2:03:46<72:36:38,  1.35s/it, loss=0.0111, lr=2.88e-05, step=5761]Training:   3%|‚ñé         | 5762/200000 [2:03:46<72:36:38,  1.35s/it, loss=0.0167, lr=2.88e-05, step=5762]Training:   3%|‚ñé         | 5763/200000 [2:03:48<74:03:50,  1.37s/it, loss=0.0167, lr=2.88e-05, step=5762]Training:   3%|‚ñé         | 5763/200000 [2:03:48<74:03:50,  1.37s/it, loss=0.0136, lr=2.88e-05, step=5763]Training:   3%|‚ñé         | 5764/200000 [2:03:49<69:17:44,  1.28s/it, loss=0.0136, lr=2.88e-05, step=5763]Training:   3%|‚ñé         | 5764/200000 [2:03:49<69:17:44,  1.28s/it, loss=0.0161, lr=2.88e-05, step=5764]Training:   3%|‚ñé         | 5765/200000 [2:03:50<65:57:19,  1.22s/it, loss=0.0161, lr=2.88e-05, step=5764]Training:   3%|‚ñé         | 5765/200000 [2:03:50<65:57:19,  1.22s/it, loss=0.0179, lr=2.88e-05, step=5765]Training:   3%|‚ñé         | 5766/200000 [2:03:51<68:06:21,  1.26s/it, loss=0.0179, lr=2.88e-05, step=5765]Training:   3%|‚ñé         | 5766/200000 [2:03:51<68:06:21,  1.26s/it, loss=0.0236, lr=2.88e-05, step=5766]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5767/200000 [2:03:52<70:24:32,  1.30s/it, loss=0.0236, lr=2.88e-05, step=5766]Training:   3%|‚ñé         | 5767/200000 [2:03:52<70:24:32,  1.30s/it, loss=0.0219, lr=2.88e-05, step=5767]Training:   3%|‚ñé         | 5768/200000 [2:03:53<66:40:54,  1.24s/it, loss=0.0219, lr=2.88e-05, step=5767]Training:   3%|‚ñé         | 5768/200000 [2:03:53<66:40:54,  1.24s/it, loss=0.0293, lr=2.88e-05, step=5768]Training:   3%|‚ñé         | 5769/200000 [2:03:55<69:18:09,  1.28s/it, loss=0.0293, lr=2.88e-05, step=5768]Training:   3%|‚ñé         | 5769/200000 [2:03:55<69:18:09,  1.28s/it, loss=0.0187, lr=2.88e-05, step=5769]Training:   3%|‚ñé         | 5770/200000 [2:03:56<70:19:07,  1.30s/it, loss=0.0187, lr=2.88e-05, step=5769]Training:   3%|‚ñé         | 5770/200000 [2:03:56<70:19:07,  1.30s/it, loss=0.0267, lr=2.88e-05, step=5770]Training:   3%|‚ñé         | 5771/200000 [2:03:58<71:08:45,  1.32s/it, loss=0.0267, lr=2.88e-05, step=5770]Training:   3%|‚ñé         | 5771/200000 [2:03:58<71:08:45,  1.32s/it, loss=0.0182, lr=2.89e-05, step=5771]Training:   3%|‚ñé         | 5772/200000 [2:03:59<67:12:36,  1.25s/it, loss=0.0182, lr=2.89e-05, step=5771]Training:   3%|‚ñé         | 5772/200000 [2:03:59<67:12:36,  1.25s/it, loss=0.0147, lr=2.89e-05, step=5772]Training:   3%|‚ñé         | 5773/200000 [2:04:00<71:51:02,  1.33s/it, loss=0.0147, lr=2.89e-05, step=5772]Training:   3%|‚ñé         | 5773/200000 [2:04:00<71:51:02,  1.33s/it, loss=0.0127, lr=2.89e-05, step=5773]Training:   3%|‚ñé         | 5774/200000 [2:04:02<74:39:30,  1.38s/it, loss=0.0127, lr=2.89e-05, step=5773]Training:   3%|‚ñé         | 5774/200000 [2:04:02<74:39:30,  1.38s/it, loss=0.0315, lr=2.89e-05, step=5774]Training:   3%|‚ñé         | 5775/200000 [2:04:03<69:40:25,  1.29s/it, loss=0.0315, lr=2.89e-05, step=5774]Training:   3%|‚ñé         | 5775/200000 [2:04:03<69:40:25,  1.29s/it, loss=0.0164, lr=2.89e-05, step=5775]Training:   3%|‚ñé         | 5776/200000 [2:04:04<66:11:37,  1.23s/it, loss=0.0164, lr=2.89e-05, step=5775]Training:   3%|‚ñé         | 5776/200000 [2:04:04<66:11:37,  1.23s/it, loss=0.0438, lr=2.89e-05, step=5776]Training:   3%|‚ñé         | 5777/200000 [2:04:05<69:15:22,  1.28s/it, loss=0.0438, lr=2.89e-05, step=5776]Training:   3%|‚ñé         | 5777/200000 [2:04:05<69:15:22,  1.28s/it, loss=0.0219, lr=2.89e-05, step=5777]Training:   3%|‚ñé         | 5778/200000 [2:04:06<65:54:05,  1.22s/it, loss=0.0219, lr=2.89e-05, step=5777]Training:   3%|‚ñé         | 5778/200000 [2:04:06<65:54:05,  1.22s/it, loss=0.0153, lr=2.89e-05, step=5778]Training:   3%|‚ñé         | 5779/200000 [2:04:08<67:47:29,  1.26s/it, loss=0.0153, lr=2.89e-05, step=5778]Training:   3%|‚ñé         | 5779/200000 [2:04:08<67:47:29,  1.26s/it, loss=0.0168, lr=2.89e-05, step=5779]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5780/200000 [2:04:09<64:53:30,  1.20s/it, loss=0.0168, lr=2.89e-05, step=5779]Training:   3%|‚ñé         | 5780/200000 [2:04:09<64:53:30,  1.20s/it, loss=0.0183, lr=2.89e-05, step=5780]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5781/200000 [2:04:10<68:57:34,  1.28s/it, loss=0.0183, lr=2.89e-05, step=5780]Training:   3%|‚ñé         | 5781/200000 [2:04:10<68:57:34,  1.28s/it, loss=0.0218, lr=2.89e-05, step=5781]Training:   3%|‚ñé         | 5782/200000 [2:04:12<71:53:34,  1.33s/it, loss=0.0218, lr=2.89e-05, step=5781]Training:   3%|‚ñé         | 5782/200000 [2:04:12<71:53:34,  1.33s/it, loss=0.0118, lr=2.89e-05, step=5782]Training:   3%|‚ñé         | 5783/200000 [2:04:13<72:23:50,  1.34s/it, loss=0.0118, lr=2.89e-05, step=5782]Training:   3%|‚ñé         | 5783/200000 [2:04:13<72:23:50,  1.34s/it, loss=0.0454, lr=2.89e-05, step=5783]Training:   3%|‚ñé         | 5784/200000 [2:04:14<73:43:32,  1.37s/it, loss=0.0454, lr=2.89e-05, step=5783]Training:   3%|‚ñé         | 5784/200000 [2:04:14<73:43:32,  1.37s/it, loss=0.0174, lr=2.89e-05, step=5784]Training:   3%|‚ñé         | 5785/200000 [2:04:16<68:59:53,  1.28s/it, loss=0.0174, lr=2.89e-05, step=5784]Training:   3%|‚ñé         | 5785/200000 [2:04:16<68:59:53,  1.28s/it, loss=0.0198, lr=2.89e-05, step=5785]Training:   3%|‚ñé         | 5786/200000 [2:04:17<65:41:47,  1.22s/it, loss=0.0198, lr=2.89e-05, step=5785]Training:   3%|‚ñé         | 5786/200000 [2:04:17<65:41:47,  1.22s/it, loss=0.0173, lr=2.89e-05, step=5786]Training:   3%|‚ñé         | 5787/200000 [2:04:18<67:40:46,  1.25s/it, loss=0.0173, lr=2.89e-05, step=5786]Training:   3%|‚ñé         | 5787/200000 [2:04:18<67:40:46,  1.25s/it, loss=0.0235, lr=2.89e-05, step=5787]Training:   3%|‚ñé         | 5788/200000 [2:04:19<68:49:39,  1.28s/it, loss=0.0235, lr=2.89e-05, step=5787]Training:   3%|‚ñé         | 5788/200000 [2:04:19<68:49:39,  1.28s/it, loss=0.0094, lr=2.89e-05, step=5788]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5789/200000 [2:04:20<65:34:18,  1.22s/it, loss=0.0094, lr=2.89e-05, step=5788]Training:   3%|‚ñé         | 5789/200000 [2:04:20<65:34:18,  1.22s/it, loss=0.0115, lr=2.89e-05, step=5789]Training:   3%|‚ñé         | 5790/200000 [2:04:22<67:03:00,  1.24s/it, loss=0.0115, lr=2.89e-05, step=5789]Training:   3%|‚ñé         | 5790/200000 [2:04:22<67:03:00,  1.24s/it, loss=0.0153, lr=2.89e-05, step=5790]Training:   3%|‚ñé         | 5791/200000 [2:04:23<68:49:08,  1.28s/it, loss=0.0153, lr=2.89e-05, step=5790]Training:   3%|‚ñé         | 5791/200000 [2:04:23<68:49:08,  1.28s/it, loss=0.0182, lr=2.90e-05, step=5791]WARNING:root:Token length (49) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5792/200000 [2:04:24<69:50:10,  1.29s/it, loss=0.0182, lr=2.90e-05, step=5791]Training:   3%|‚ñé         | 5792/200000 [2:04:24<69:50:10,  1.29s/it, loss=0.0247, lr=2.90e-05, step=5792]Training:   3%|‚ñé         | 5793/200000 [2:04:25<66:16:25,  1.23s/it, loss=0.0247, lr=2.90e-05, step=5792]Training:   3%|‚ñé         | 5793/200000 [2:04:25<66:16:25,  1.23s/it, loss=0.0136, lr=2.90e-05, step=5793]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5794/200000 [2:04:27<69:16:35,  1.28s/it, loss=0.0136, lr=2.90e-05, step=5793]Training:   3%|‚ñé         | 5794/200000 [2:04:27<69:16:35,  1.28s/it, loss=0.0199, lr=2.90e-05, step=5794]Training:   3%|‚ñé         | 5795/200000 [2:04:28<71:42:50,  1.33s/it, loss=0.0199, lr=2.90e-05, step=5794]Training:   3%|‚ñé         | 5795/200000 [2:04:28<71:42:50,  1.33s/it, loss=0.0238, lr=2.90e-05, step=5795]Training:   3%|‚ñé         | 5796/200000 [2:04:29<67:37:53,  1.25s/it, loss=0.0238, lr=2.90e-05, step=5795]Training:   3%|‚ñé         | 5796/200000 [2:04:29<67:37:53,  1.25s/it, loss=0.0157, lr=2.90e-05, step=5796]Training:   3%|‚ñé         | 5797/200000 [2:04:30<64:44:06,  1.20s/it, loss=0.0157, lr=2.90e-05, step=5796]Training:   3%|‚ñé         | 5797/200000 [2:04:30<64:44:06,  1.20s/it, loss=0.0315, lr=2.90e-05, step=5797]Training:   3%|‚ñé         | 5798/200000 [2:04:32<68:01:16,  1.26s/it, loss=0.0315, lr=2.90e-05, step=5797]Training:   3%|‚ñé         | 5798/200000 [2:04:32<68:01:16,  1.26s/it, loss=0.0238, lr=2.90e-05, step=5798]Training:   3%|‚ñé         | 5799/200000 [2:04:33<71:22:41,  1.32s/it, loss=0.0238, lr=2.90e-05, step=5798]Training:   3%|‚ñé         | 5799/200000 [2:04:33<71:22:41,  1.32s/it, loss=0.0178, lr=2.90e-05, step=5799]Training:   3%|‚ñé         | 5800/200000 [2:04:34<67:22:54,  1.25s/it, loss=0.0178, lr=2.90e-05, step=5799]Training:   3%|‚ñé         | 5800/200000 [2:04:34<67:22:54,  1.25s/it, loss=0.1082, lr=2.90e-05, step=5800]00:57:49.625 [I] step=5800 loss=0.0230 lr=2.88e-05 grad_norm=0.40 time=128.2s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5801/200000 [2:04:36<70:23:30,  1.30s/it, loss=0.1082, lr=2.90e-05, step=5800]Training:   3%|‚ñé         | 5801/200000 [2:04:36<70:23:30,  1.30s/it, loss=0.0117, lr=2.90e-05, step=5801]Training:   3%|‚ñé         | 5802/200000 [2:04:37<71:47:50,  1.33s/it, loss=0.0117, lr=2.90e-05, step=5801]Training:   3%|‚ñé         | 5802/200000 [2:04:37<71:47:50,  1.33s/it, loss=0.0285, lr=2.90e-05, step=5802]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5803/200000 [2:04:39<72:01:28,  1.34s/it, loss=0.0285, lr=2.90e-05, step=5802]Training:   3%|‚ñé         | 5803/200000 [2:04:39<72:01:28,  1.34s/it, loss=0.0136, lr=2.90e-05, step=5803]Training:   3%|‚ñé         | 5804/200000 [2:04:40<67:50:03,  1.26s/it, loss=0.0136, lr=2.90e-05, step=5803]Training:   3%|‚ñé         | 5804/200000 [2:04:40<67:50:03,  1.26s/it, loss=0.0200, lr=2.90e-05, step=5804]Training:   3%|‚ñé         | 5805/200000 [2:04:41<71:35:31,  1.33s/it, loss=0.0200, lr=2.90e-05, step=5804]Training:   3%|‚ñé         | 5805/200000 [2:04:41<71:35:31,  1.33s/it, loss=0.0238, lr=2.90e-05, step=5805]Training:   3%|‚ñé         | 5806/200000 [2:04:43<74:47:04,  1.39s/it, loss=0.0238, lr=2.90e-05, step=5805]Training:   3%|‚ñé         | 5806/200000 [2:04:43<74:47:04,  1.39s/it, loss=0.0181, lr=2.90e-05, step=5806]Training:   3%|‚ñé         | 5807/200000 [2:04:44<69:45:54,  1.29s/it, loss=0.0181, lr=2.90e-05, step=5806]Training:   3%|‚ñé         | 5807/200000 [2:04:44<69:45:54,  1.29s/it, loss=0.0185, lr=2.90e-05, step=5807]Training:   3%|‚ñé         | 5808/200000 [2:04:45<66:14:02,  1.23s/it, loss=0.0185, lr=2.90e-05, step=5807]Training:   3%|‚ñé         | 5808/200000 [2:04:45<66:14:02,  1.23s/it, loss=0.0193, lr=2.90e-05, step=5808]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5809/200000 [2:04:46<70:20:30,  1.30s/it, loss=0.0193, lr=2.90e-05, step=5808]Training:   3%|‚ñé         | 5809/200000 [2:04:46<70:20:30,  1.30s/it, loss=0.0156, lr=2.90e-05, step=5809]Training:   3%|‚ñé         | 5810/200000 [2:04:47<66:39:58,  1.24s/it, loss=0.0156, lr=2.90e-05, step=5809]Training:   3%|‚ñé         | 5810/200000 [2:04:47<66:39:58,  1.24s/it, loss=0.0152, lr=2.90e-05, step=5810]Training:   3%|‚ñé         | 5811/200000 [2:04:49<68:00:55,  1.26s/it, loss=0.0152, lr=2.90e-05, step=5810]Training:   3%|‚ñé         | 5811/200000 [2:04:49<68:00:55,  1.26s/it, loss=0.0138, lr=2.91e-05, step=5811]Training:   3%|‚ñé         | 5812/200000 [2:04:50<65:01:36,  1.21s/it, loss=0.0138, lr=2.91e-05, step=5811]Training:   3%|‚ñé         | 5812/200000 [2:04:50<65:01:36,  1.21s/it, loss=0.0122, lr=2.91e-05, step=5812]Training:   3%|‚ñé         | 5813/200000 [2:04:51<68:41:17,  1.27s/it, loss=0.0122, lr=2.91e-05, step=5812]Training:   3%|‚ñé         | 5813/200000 [2:04:51<68:41:17,  1.27s/it, loss=0.0242, lr=2.91e-05, step=5813]Training:   3%|‚ñé         | 5814/200000 [2:04:53<71:19:05,  1.32s/it, loss=0.0242, lr=2.91e-05, step=5813]Training:   3%|‚ñé         | 5814/200000 [2:04:53<71:19:05,  1.32s/it, loss=0.0174, lr=2.91e-05, step=5814]Training:   3%|‚ñé         | 5815/200000 [2:04:54<73:15:35,  1.36s/it, loss=0.0174, lr=2.91e-05, step=5814]Training:   3%|‚ñé         | 5815/200000 [2:04:54<73:15:35,  1.36s/it, loss=0.0145, lr=2.91e-05, step=5815]Training:   3%|‚ñé         | 5816/200000 [2:04:55<74:24:06,  1.38s/it, loss=0.0145, lr=2.91e-05, step=5815]Training:   3%|‚ñé         | 5816/200000 [2:04:55<74:24:06,  1.38s/it, loss=0.0395, lr=2.91e-05, step=5816]Training:   3%|‚ñé         | 5817/200000 [2:04:57<69:25:24,  1.29s/it, loss=0.0395, lr=2.91e-05, step=5816]Training:   3%|‚ñé         | 5817/200000 [2:04:57<69:25:24,  1.29s/it, loss=0.0121, lr=2.91e-05, step=5817]Training:   3%|‚ñé         | 5818/200000 [2:04:58<66:00:13,  1.22s/it, loss=0.0121, lr=2.91e-05, step=5817]Training:   3%|‚ñé         | 5818/200000 [2:04:58<66:00:13,  1.22s/it, loss=0.0129, lr=2.91e-05, step=5818]Training:   3%|‚ñé         | 5819/200000 [2:04:59<67:38:35,  1.25s/it, loss=0.0129, lr=2.91e-05, step=5818]Training:   3%|‚ñé         | 5819/200000 [2:04:59<67:38:35,  1.25s/it, loss=0.0501, lr=2.91e-05, step=5819]Training:   3%|‚ñé         | 5820/200000 [2:05:00<69:33:53,  1.29s/it, loss=0.0501, lr=2.91e-05, step=5819]Training:   3%|‚ñé         | 5820/200000 [2:05:00<69:33:53,  1.29s/it, loss=0.0153, lr=2.91e-05, step=5820]Training:   3%|‚ñé         | 5821/200000 [2:05:01<66:04:34,  1.23s/it, loss=0.0153, lr=2.91e-05, step=5820]Training:   3%|‚ñé         | 5821/200000 [2:05:01<66:04:34,  1.23s/it, loss=0.0192, lr=2.91e-05, step=5821]Training:   3%|‚ñé         | 5822/200000 [2:05:03<68:53:58,  1.28s/it, loss=0.0192, lr=2.91e-05, step=5821]Training:   3%|‚ñé         | 5822/200000 [2:05:03<68:53:58,  1.28s/it, loss=0.0191, lr=2.91e-05, step=5822]Training:   3%|‚ñé         | 5823/200000 [2:05:04<70:20:57,  1.30s/it, loss=0.0191, lr=2.91e-05, step=5822]Training:   3%|‚ñé         | 5823/200000 [2:05:04<70:20:57,  1.30s/it, loss=0.0154, lr=2.91e-05, step=5823]Training:   3%|‚ñé         | 5824/200000 [2:05:06<71:03:56,  1.32s/it, loss=0.0154, lr=2.91e-05, step=5823]Training:   3%|‚ñé         | 5824/200000 [2:05:06<71:03:56,  1.32s/it, loss=0.0189, lr=2.91e-05, step=5824]Training:   3%|‚ñé         | 5825/200000 [2:05:07<67:09:08,  1.25s/it, loss=0.0189, lr=2.91e-05, step=5824]Training:   3%|‚ñé         | 5825/200000 [2:05:07<67:09:08,  1.25s/it, loss=0.0141, lr=2.91e-05, step=5825]Training:   3%|‚ñé         | 5826/200000 [2:05:08<70:52:44,  1.31s/it, loss=0.0141, lr=2.91e-05, step=5825]Training:   3%|‚ñé         | 5826/200000 [2:05:08<70:52:44,  1.31s/it, loss=0.0140, lr=2.91e-05, step=5826]Training:   3%|‚ñé         | 5827/200000 [2:05:10<73:57:29,  1.37s/it, loss=0.0140, lr=2.91e-05, step=5826]Training:   3%|‚ñé         | 5827/200000 [2:05:10<73:57:29,  1.37s/it, loss=0.0285, lr=2.91e-05, step=5827]Training:   3%|‚ñé         | 5828/200000 [2:05:11<69:10:20,  1.28s/it, loss=0.0285, lr=2.91e-05, step=5827]Training:   3%|‚ñé         | 5828/200000 [2:05:11<69:10:20,  1.28s/it, loss=0.0433, lr=2.91e-05, step=5828]Training:   3%|‚ñé         | 5829/200000 [2:05:12<65:49:49,  1.22s/it, loss=0.0433, lr=2.91e-05, step=5828]Training:   3%|‚ñé         | 5829/200000 [2:05:12<65:49:49,  1.22s/it, loss=0.0171, lr=2.91e-05, step=5829]Training:   3%|‚ñé         | 5830/200000 [2:05:13<69:12:35,  1.28s/it, loss=0.0171, lr=2.91e-05, step=5829]Training:   3%|‚ñé         | 5830/200000 [2:05:13<69:12:35,  1.28s/it, loss=0.0138, lr=2.91e-05, step=5830]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5831/200000 [2:05:14<65:51:32,  1.22s/it, loss=0.0138, lr=2.91e-05, step=5830]Training:   3%|‚ñé         | 5831/200000 [2:05:14<65:51:32,  1.22s/it, loss=0.0162, lr=2.92e-05, step=5831]Training:   3%|‚ñé         | 5832/200000 [2:05:16<67:27:38,  1.25s/it, loss=0.0162, lr=2.92e-05, step=5831]Training:   3%|‚ñé         | 5832/200000 [2:05:16<67:27:38,  1.25s/it, loss=0.0228, lr=2.92e-05, step=5832]Training:   3%|‚ñé         | 5833/200000 [2:05:17<64:34:48,  1.20s/it, loss=0.0228, lr=2.92e-05, step=5832]Training:   3%|‚ñé         | 5833/200000 [2:05:17<64:34:48,  1.20s/it, loss=0.0111, lr=2.92e-05, step=5833]Training:   3%|‚ñé         | 5834/200000 [2:05:18<68:42:50,  1.27s/it, loss=0.0111, lr=2.92e-05, step=5833]Training:   3%|‚ñé         | 5834/200000 [2:05:18<68:42:50,  1.27s/it, loss=0.0138, lr=2.92e-05, step=5834]Training:   3%|‚ñé         | 5835/200000 [2:05:20<71:43:59,  1.33s/it, loss=0.0138, lr=2.92e-05, step=5834]Training:   3%|‚ñé         | 5835/200000 [2:05:20<71:43:59,  1.33s/it, loss=0.0158, lr=2.92e-05, step=5835]Training:   3%|‚ñé         | 5836/200000 [2:05:21<73:36:42,  1.36s/it, loss=0.0158, lr=2.92e-05, step=5835]Training:   3%|‚ñé         | 5836/200000 [2:05:21<73:36:42,  1.36s/it, loss=0.0318, lr=2.92e-05, step=5836]Training:   3%|‚ñé         | 5837/200000 [2:05:22<74:24:46,  1.38s/it, loss=0.0318, lr=2.92e-05, step=5836]Training:   3%|‚ñé         | 5837/200000 [2:05:22<74:24:46,  1.38s/it, loss=0.0168, lr=2.92e-05, step=5837]Training:   3%|‚ñé         | 5838/200000 [2:05:23<69:32:01,  1.29s/it, loss=0.0168, lr=2.92e-05, step=5837]Training:   3%|‚ñé         | 5838/200000 [2:05:23<69:32:01,  1.29s/it, loss=0.0217, lr=2.92e-05, step=5838]Training:   3%|‚ñé         | 5839/200000 [2:05:25<66:03:10,  1.22s/it, loss=0.0217, lr=2.92e-05, step=5838]Training:   3%|‚ñé         | 5839/200000 [2:05:25<66:03:10,  1.22s/it, loss=0.0173, lr=2.92e-05, step=5839]Training:   3%|‚ñé         | 5840/200000 [2:05:26<67:49:10,  1.26s/it, loss=0.0173, lr=2.92e-05, step=5839]Training:   3%|‚ñé         | 5840/200000 [2:05:26<67:49:10,  1.26s/it, loss=0.0199, lr=2.92e-05, step=5840]Training:   3%|‚ñé         | 5841/200000 [2:05:27<69:00:32,  1.28s/it, loss=0.0199, lr=2.92e-05, step=5840]Training:   3%|‚ñé         | 5841/200000 [2:05:27<69:00:32,  1.28s/it, loss=0.0180, lr=2.92e-05, step=5841]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5842/200000 [2:05:28<65:44:21,  1.22s/it, loss=0.0180, lr=2.92e-05, step=5841]Training:   3%|‚ñé         | 5842/200000 [2:05:28<65:44:21,  1.22s/it, loss=0.0126, lr=2.92e-05, step=5842]Training:   3%|‚ñé         | 5843/200000 [2:05:30<67:37:44,  1.25s/it, loss=0.0126, lr=2.92e-05, step=5842]Training:   3%|‚ñé         | 5843/200000 [2:05:30<67:37:44,  1.25s/it, loss=0.0221, lr=2.92e-05, step=5843]Training:   3%|‚ñé         | 5844/200000 [2:05:31<69:16:41,  1.28s/it, loss=0.0221, lr=2.92e-05, step=5843]Training:   3%|‚ñé         | 5844/200000 [2:05:31<69:16:41,  1.28s/it, loss=0.0372, lr=2.92e-05, step=5844]Training:   3%|‚ñé         | 5845/200000 [2:05:32<70:01:22,  1.30s/it, loss=0.0372, lr=2.92e-05, step=5844]Training:   3%|‚ñé         | 5845/200000 [2:05:32<70:01:22,  1.30s/it, loss=0.0117, lr=2.92e-05, step=5845]Training:   3%|‚ñé         | 5846/200000 [2:05:33<66:25:15,  1.23s/it, loss=0.0117, lr=2.92e-05, step=5845]Training:   3%|‚ñé         | 5846/200000 [2:05:33<66:25:15,  1.23s/it, loss=0.0311, lr=2.92e-05, step=5846]Training:   3%|‚ñé         | 5847/200000 [2:05:35<70:07:02,  1.30s/it, loss=0.0311, lr=2.92e-05, step=5846]Training:   3%|‚ñé         | 5847/200000 [2:05:35<70:07:02,  1.30s/it, loss=0.0172, lr=2.92e-05, step=5847]Training:   3%|‚ñé         | 5848/200000 [2:05:36<72:08:13,  1.34s/it, loss=0.0172, lr=2.92e-05, step=5847]Training:   3%|‚ñé         | 5848/200000 [2:05:36<72:08:13,  1.34s/it, loss=0.0158, lr=2.92e-05, step=5848]Training:   3%|‚ñé         | 5849/200000 [2:05:37<67:52:44,  1.26s/it, loss=0.0158, lr=2.92e-05, step=5848]Training:   3%|‚ñé         | 5849/200000 [2:05:37<67:52:44,  1.26s/it, loss=0.0162, lr=2.92e-05, step=5849]Training:   3%|‚ñé         | 5850/200000 [2:05:38<64:54:24,  1.20s/it, loss=0.0162, lr=2.92e-05, step=5849]Training:   3%|‚ñé         | 5850/200000 [2:05:38<64:54:24,  1.20s/it, loss=0.0134, lr=2.92e-05, step=5850]Training:   3%|‚ñé         | 5851/200000 [2:05:40<68:04:49,  1.26s/it, loss=0.0134, lr=2.92e-05, step=5850]Training:   3%|‚ñé         | 5851/200000 [2:05:40<68:04:49,  1.26s/it, loss=0.0153, lr=2.93e-05, step=5851]Training:   3%|‚ñé         | 5852/200000 [2:05:41<70:31:47,  1.31s/it, loss=0.0153, lr=2.93e-05, step=5851]Training:   3%|‚ñé         | 5852/200000 [2:05:41<70:31:47,  1.31s/it, loss=0.0150, lr=2.93e-05, step=5852]Training:   3%|‚ñé         | 5853/200000 [2:05:42<66:45:24,  1.24s/it, loss=0.0150, lr=2.93e-05, step=5852]Training:   3%|‚ñé         | 5853/200000 [2:05:42<66:45:24,  1.24s/it, loss=0.0129, lr=2.93e-05, step=5853]Training:   3%|‚ñé         | 5854/200000 [2:05:44<69:22:45,  1.29s/it, loss=0.0129, lr=2.93e-05, step=5853]Training:   3%|‚ñé         | 5854/200000 [2:05:44<69:22:45,  1.29s/it, loss=0.0152, lr=2.93e-05, step=5854]Training:   3%|‚ñé         | 5855/200000 [2:05:45<70:21:47,  1.30s/it, loss=0.0152, lr=2.93e-05, step=5854]Training:   3%|‚ñé         | 5855/200000 [2:05:45<70:21:47,  1.30s/it, loss=0.0112, lr=2.93e-05, step=5855]Training:   3%|‚ñé         | 5856/200000 [2:05:46<71:52:03,  1.33s/it, loss=0.0112, lr=2.93e-05, step=5855]Training:   3%|‚ñé         | 5856/200000 [2:05:46<71:52:03,  1.33s/it, loss=0.0209, lr=2.93e-05, step=5856]Training:   3%|‚ñé         | 5857/200000 [2:05:48<67:41:19,  1.26s/it, loss=0.0209, lr=2.93e-05, step=5856]Training:   3%|‚ñé         | 5857/200000 [2:05:48<67:41:19,  1.26s/it, loss=0.0150, lr=2.93e-05, step=5857]Training:   3%|‚ñé         | 5858/200000 [2:05:49<71:37:08,  1.33s/it, loss=0.0150, lr=2.93e-05, step=5857]Training:   3%|‚ñé         | 5858/200000 [2:05:49<71:37:08,  1.33s/it, loss=0.0108, lr=2.93e-05, step=5858]Training:   3%|‚ñé         | 5859/200000 [2:05:51<74:46:15,  1.39s/it, loss=0.0108, lr=2.93e-05, step=5858]Training:   3%|‚ñé         | 5859/200000 [2:05:51<74:46:15,  1.39s/it, loss=0.0089, lr=2.93e-05, step=5859]Training:   3%|‚ñé         | 5860/200000 [2:05:52<69:44:14,  1.29s/it, loss=0.0089, lr=2.93e-05, step=5859]Training:   3%|‚ñé         | 5860/200000 [2:05:52<69:44:14,  1.29s/it, loss=0.0131, lr=2.93e-05, step=5860]Training:   3%|‚ñé         | 5861/200000 [2:05:53<66:11:50,  1.23s/it, loss=0.0131, lr=2.93e-05, step=5860]Training:   3%|‚ñé         | 5861/200000 [2:05:53<66:11:50,  1.23s/it, loss=0.0193, lr=2.93e-05, step=5861]Training:   3%|‚ñé         | 5862/200000 [2:05:54<70:19:57,  1.30s/it, loss=0.0193, lr=2.93e-05, step=5861]Training:   3%|‚ñé         | 5862/200000 [2:05:54<70:19:57,  1.30s/it, loss=0.0141, lr=2.93e-05, step=5862]Training:   3%|‚ñé         | 5863/200000 [2:05:55<66:37:05,  1.24s/it, loss=0.0141, lr=2.93e-05, step=5862]Training:   3%|‚ñé         | 5863/200000 [2:05:55<66:37:05,  1.24s/it, loss=0.0195, lr=2.93e-05, step=5863]Training:   3%|‚ñé         | 5864/200000 [2:05:57<67:42:02,  1.26s/it, loss=0.0195, lr=2.93e-05, step=5863]Training:   3%|‚ñé         | 5864/200000 [2:05:57<67:42:02,  1.26s/it, loss=0.0177, lr=2.93e-05, step=5864]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5865/200000 [2:05:58<64:48:10,  1.20s/it, loss=0.0177, lr=2.93e-05, step=5864]Training:   3%|‚ñé         | 5865/200000 [2:05:58<64:48:10,  1.20s/it, loss=0.0152, lr=2.93e-05, step=5865]Training:   3%|‚ñé         | 5866/200000 [2:05:59<68:56:22,  1.28s/it, loss=0.0152, lr=2.93e-05, step=5865]Training:   3%|‚ñé         | 5866/200000 [2:05:59<68:56:22,  1.28s/it, loss=0.0161, lr=2.93e-05, step=5866]Training:   3%|‚ñé         | 5867/200000 [2:06:01<71:23:02,  1.32s/it, loss=0.0161, lr=2.93e-05, step=5866]Training:   3%|‚ñé         | 5867/200000 [2:06:01<71:23:02,  1.32s/it, loss=0.0266, lr=2.93e-05, step=5867]Training:   3%|‚ñé         | 5868/200000 [2:06:02<73:49:45,  1.37s/it, loss=0.0266, lr=2.93e-05, step=5867]Training:   3%|‚ñé         | 5868/200000 [2:06:02<73:49:45,  1.37s/it, loss=0.0131, lr=2.93e-05, step=5868]Training:   3%|‚ñé         | 5869/200000 [2:06:03<74:57:13,  1.39s/it, loss=0.0131, lr=2.93e-05, step=5868]Training:   3%|‚ñé         | 5869/200000 [2:06:03<74:57:13,  1.39s/it, loss=0.0162, lr=2.93e-05, step=5869]Training:   3%|‚ñé         | 5870/200000 [2:06:05<69:53:21,  1.30s/it, loss=0.0162, lr=2.93e-05, step=5869]Training:   3%|‚ñé         | 5870/200000 [2:06:05<69:53:21,  1.30s/it, loss=0.0236, lr=2.93e-05, step=5870]Training:   3%|‚ñé         | 5871/200000 [2:06:06<66:19:34,  1.23s/it, loss=0.0236, lr=2.93e-05, step=5870]Training:   3%|‚ñé         | 5871/200000 [2:06:06<66:19:34,  1.23s/it, loss=0.0149, lr=2.94e-05, step=5871]Training:   3%|‚ñé         | 5872/200000 [2:06:07<68:15:13,  1.27s/it, loss=0.0149, lr=2.94e-05, step=5871]Training:   3%|‚ñé         | 5872/200000 [2:06:07<68:15:13,  1.27s/it, loss=0.0122, lr=2.94e-05, step=5872]Training:   3%|‚ñé         | 5873/200000 [2:06:08<70:43:53,  1.31s/it, loss=0.0122, lr=2.94e-05, step=5872]Training:   3%|‚ñé         | 5873/200000 [2:06:08<70:43:53,  1.31s/it, loss=0.0188, lr=2.94e-05, step=5873]Training:   3%|‚ñé         | 5874/200000 [2:06:09<66:57:28,  1.24s/it, loss=0.0188, lr=2.94e-05, step=5873]Training:   3%|‚ñé         | 5874/200000 [2:06:09<66:57:28,  1.24s/it, loss=0.0261, lr=2.94e-05, step=5874]Training:   3%|‚ñé         | 5875/200000 [2:06:11<69:38:28,  1.29s/it, loss=0.0261, lr=2.94e-05, step=5874]Training:   3%|‚ñé         | 5875/200000 [2:06:11<69:38:28,  1.29s/it, loss=0.0149, lr=2.94e-05, step=5875]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5876/200000 [2:06:12<70:49:49,  1.31s/it, loss=0.0149, lr=2.94e-05, step=5875]Training:   3%|‚ñé         | 5876/200000 [2:06:12<70:49:49,  1.31s/it, loss=0.0157, lr=2.94e-05, step=5876]Training:   3%|‚ñé         | 5877/200000 [2:06:14<71:09:53,  1.32s/it, loss=0.0157, lr=2.94e-05, step=5876]Training:   3%|‚ñé         | 5877/200000 [2:06:14<71:09:53,  1.32s/it, loss=0.0146, lr=2.94e-05, step=5877]Training:   3%|‚ñé         | 5878/200000 [2:06:15<67:13:49,  1.25s/it, loss=0.0146, lr=2.94e-05, step=5877]Training:   3%|‚ñé         | 5878/200000 [2:06:15<67:13:49,  1.25s/it, loss=0.0450, lr=2.94e-05, step=5878]Training:   3%|‚ñé         | 5879/200000 [2:06:16<71:55:43,  1.33s/it, loss=0.0450, lr=2.94e-05, step=5878]Training:   3%|‚ñé         | 5879/200000 [2:06:16<71:55:43,  1.33s/it, loss=0.0131, lr=2.94e-05, step=5879]Training:   3%|‚ñé         | 5880/200000 [2:06:18<74:47:40,  1.39s/it, loss=0.0131, lr=2.94e-05, step=5879]Training:   3%|‚ñé         | 5880/200000 [2:06:18<74:47:40,  1.39s/it, loss=0.0419, lr=2.94e-05, step=5880]Training:   3%|‚ñé         | 5881/200000 [2:06:19<69:45:12,  1.29s/it, loss=0.0419, lr=2.94e-05, step=5880]Training:   3%|‚ñé         | 5881/200000 [2:06:19<69:45:12,  1.29s/it, loss=0.0206, lr=2.94e-05, step=5881]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5882/200000 [2:06:20<66:14:09,  1.23s/it, loss=0.0206, lr=2.94e-05, step=5881]Training:   3%|‚ñé         | 5882/200000 [2:06:20<66:14:09,  1.23s/it, loss=0.0221, lr=2.94e-05, step=5882]Training:   3%|‚ñé         | 5883/200000 [2:06:21<69:21:13,  1.29s/it, loss=0.0221, lr=2.94e-05, step=5882]Training:   3%|‚ñé         | 5883/200000 [2:06:21<69:21:13,  1.29s/it, loss=0.0136, lr=2.94e-05, step=5883]Training:   3%|‚ñé         | 5884/200000 [2:06:22<65:57:59,  1.22s/it, loss=0.0136, lr=2.94e-05, step=5883]Training:   3%|‚ñé         | 5884/200000 [2:06:22<65:57:59,  1.22s/it, loss=0.0139, lr=2.94e-05, step=5884]Training:   3%|‚ñé         | 5885/200000 [2:06:24<67:42:48,  1.26s/it, loss=0.0139, lr=2.94e-05, step=5884]Training:   3%|‚ñé         | 5885/200000 [2:06:24<67:42:48,  1.26s/it, loss=0.0121, lr=2.94e-05, step=5885]Training:   3%|‚ñé         | 5886/200000 [2:06:25<64:48:30,  1.20s/it, loss=0.0121, lr=2.94e-05, step=5885]Training:   3%|‚ñé         | 5886/200000 [2:06:25<64:48:30,  1.20s/it, loss=0.0170, lr=2.94e-05, step=5886]Training:   3%|‚ñé         | 5887/200000 [2:06:26<68:49:30,  1.28s/it, loss=0.0170, lr=2.94e-05, step=5886]Training:   3%|‚ñé         | 5887/200000 [2:06:26<68:49:30,  1.28s/it, loss=0.0211, lr=2.94e-05, step=5887]Training:   3%|‚ñé         | 5888/200000 [2:06:28<71:04:53,  1.32s/it, loss=0.0211, lr=2.94e-05, step=5887]Training:   3%|‚ñé         | 5888/200000 [2:06:28<71:04:53,  1.32s/it, loss=0.0207, lr=2.94e-05, step=5888]Training:   3%|‚ñé         | 5889/200000 [2:06:29<73:02:34,  1.35s/it, loss=0.0207, lr=2.94e-05, step=5888]Training:   3%|‚ñé         | 5889/200000 [2:06:29<73:02:34,  1.35s/it, loss=0.0195, lr=2.94e-05, step=5889]Training:   3%|‚ñé         | 5890/200000 [2:06:30<74:15:40,  1.38s/it, loss=0.0195, lr=2.94e-05, step=5889]Training:   3%|‚ñé         | 5890/200000 [2:06:30<74:15:40,  1.38s/it, loss=0.0230, lr=2.94e-05, step=5890]Training:   3%|‚ñé         | 5891/200000 [2:06:32<69:21:58,  1.29s/it, loss=0.0230, lr=2.94e-05, step=5890]Training:   3%|‚ñé         | 5891/200000 [2:06:32<69:21:58,  1.29s/it, loss=0.0274, lr=2.95e-05, step=5891]Training:   3%|‚ñé         | 5892/200000 [2:06:33<65:56:06,  1.22s/it, loss=0.0274, lr=2.95e-05, step=5891]Training:   3%|‚ñé         | 5892/200000 [2:06:33<65:56:06,  1.22s/it, loss=0.0121, lr=2.95e-05, step=5892]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5893/200000 [2:06:34<67:39:26,  1.25s/it, loss=0.0121, lr=2.95e-05, step=5892]Training:   3%|‚ñé         | 5893/200000 [2:06:34<67:39:26,  1.25s/it, loss=0.0144, lr=2.95e-05, step=5893]Training:   3%|‚ñé         | 5894/200000 [2:06:35<68:29:19,  1.27s/it, loss=0.0144, lr=2.95e-05, step=5893]Training:   3%|‚ñé         | 5894/200000 [2:06:35<68:29:19,  1.27s/it, loss=0.0170, lr=2.95e-05, step=5894]Training:   3%|‚ñé         | 5895/200000 [2:06:36<65:19:29,  1.21s/it, loss=0.0170, lr=2.95e-05, step=5894]Training:   3%|‚ñé         | 5895/200000 [2:06:36<65:19:29,  1.21s/it, loss=0.0179, lr=2.95e-05, step=5895]Training:   3%|‚ñé         | 5896/200000 [2:06:38<67:17:44,  1.25s/it, loss=0.0179, lr=2.95e-05, step=5895]Training:   3%|‚ñé         | 5896/200000 [2:06:38<67:17:44,  1.25s/it, loss=0.0191, lr=2.95e-05, step=5896]Training:   3%|‚ñé         | 5897/200000 [2:06:39<69:01:26,  1.28s/it, loss=0.0191, lr=2.95e-05, step=5896]Training:   3%|‚ñé         | 5897/200000 [2:06:39<69:01:26,  1.28s/it, loss=0.0204, lr=2.95e-05, step=5897]Training:   3%|‚ñé         | 5898/200000 [2:06:40<70:00:27,  1.30s/it, loss=0.0204, lr=2.95e-05, step=5897]Training:   3%|‚ñé         | 5898/200000 [2:06:40<70:00:27,  1.30s/it, loss=0.0219, lr=2.95e-05, step=5898]Training:   3%|‚ñé         | 5899/200000 [2:06:41<66:23:19,  1.23s/it, loss=0.0219, lr=2.95e-05, step=5898]Training:   3%|‚ñé         | 5899/200000 [2:06:41<66:23:19,  1.23s/it, loss=0.0208, lr=2.95e-05, step=5899]Training:   3%|‚ñé         | 5900/200000 [2:06:43<69:22:58,  1.29s/it, loss=0.0208, lr=2.95e-05, step=5899]Training:   3%|‚ñé         | 5900/200000 [2:06:43<69:22:58,  1.29s/it, loss=0.0145, lr=2.95e-05, step=5900]00:59:58.104 [I] step=5900 loss=0.0189 lr=2.93e-05 grad_norm=0.32 time=128.5s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 5901/200000 [2:06:44<71:58:50,  1.34s/it, loss=0.0145, lr=2.95e-05, step=5900]Training:   3%|‚ñé         | 5901/200000 [2:06:44<71:58:50,  1.34s/it, loss=0.0133, lr=2.95e-05, step=5901]Training:   3%|‚ñé         | 5902/200000 [2:06:45<67:55:17,  1.26s/it, loss=0.0133, lr=2.95e-05, step=5901]Training:   3%|‚ñé         | 5902/200000 [2:06:45<67:55:17,  1.26s/it, loss=0.0167, lr=2.95e-05, step=5902]Training:   3%|‚ñé         | 5903/200000 [2:06:46<64:57:45,  1.20s/it, loss=0.0167, lr=2.95e-05, step=5902]Training:   3%|‚ñé         | 5903/200000 [2:06:46<64:57:45,  1.20s/it, loss=0.0222, lr=2.95e-05, step=5903]Training:   3%|‚ñé         | 5904/200000 [2:06:48<67:25:31,  1.25s/it, loss=0.0222, lr=2.95e-05, step=5903]Training:   3%|‚ñé         | 5904/200000 [2:06:48<67:25:31,  1.25s/it, loss=0.0776, lr=2.95e-05, step=5904]Training:   3%|‚ñé         | 5905/200000 [2:06:49<70:10:56,  1.30s/it, loss=0.0776, lr=2.95e-05, step=5904]Training:   3%|‚ñé         | 5905/200000 [2:06:49<70:10:56,  1.30s/it, loss=0.0173, lr=2.95e-05, step=5905]Training:   3%|‚ñé         | 5906/200000 [2:06:50<66:31:36,  1.23s/it, loss=0.0173, lr=2.95e-05, step=5905]Training:   3%|‚ñé         | 5906/200000 [2:06:50<66:31:36,  1.23s/it, loss=0.0444, lr=2.95e-05, step=5906]Training:   3%|‚ñé         | 5907/200000 [2:06:52<69:48:06,  1.29s/it, loss=0.0444, lr=2.95e-05, step=5906]Training:   3%|‚ñé         | 5907/200000 [2:06:52<69:48:06,  1.29s/it, loss=0.0206, lr=2.95e-05, step=5907]Training:   3%|‚ñé         | 5908/200000 [2:06:53<70:42:11,  1.31s/it, loss=0.0206, lr=2.95e-05, step=5907]Training:   3%|‚ñé         | 5908/200000 [2:06:53<70:42:11,  1.31s/it, loss=0.0199, lr=2.95e-05, step=5908]Training:   3%|‚ñé         | 5909/200000 [2:06:54<72:10:11,  1.34s/it, loss=0.0199, lr=2.95e-05, step=5908]Training:   3%|‚ñé         | 5909/200000 [2:06:54<72:10:11,  1.34s/it, loss=0.0147, lr=2.95e-05, step=5909]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5910/200000 [2:06:56<67:56:16,  1.26s/it, loss=0.0147, lr=2.95e-05, step=5909]Training:   3%|‚ñé         | 5910/200000 [2:06:56<67:56:16,  1.26s/it, loss=0.0306, lr=2.95e-05, step=5910]Training:   3%|‚ñé         | 5911/200000 [2:06:57<71:47:54,  1.33s/it, loss=0.0306, lr=2.95e-05, step=5910]Training:   3%|‚ñé         | 5911/200000 [2:06:57<71:47:54,  1.33s/it, loss=0.0213, lr=2.96e-05, step=5911]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5912/200000 [2:06:59<74:57:28,  1.39s/it, loss=0.0213, lr=2.96e-05, step=5911]Training:   3%|‚ñé         | 5912/200000 [2:06:59<74:57:28,  1.39s/it, loss=0.0199, lr=2.96e-05, step=5912]Training:   3%|‚ñé         | 5913/200000 [2:07:00<69:51:51,  1.30s/it, loss=0.0199, lr=2.96e-05, step=5912]Training:   3%|‚ñé         | 5913/200000 [2:07:00<69:51:51,  1.30s/it, loss=0.0137, lr=2.96e-05, step=5913]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5914/200000 [2:07:01<66:17:56,  1.23s/it, loss=0.0137, lr=2.96e-05, step=5913]Training:   3%|‚ñé         | 5914/200000 [2:07:01<66:17:56,  1.23s/it, loss=0.0131, lr=2.96e-05, step=5914]Training:   3%|‚ñé         | 5915/200000 [2:07:02<70:20:21,  1.30s/it, loss=0.0131, lr=2.96e-05, step=5914]Training:   3%|‚ñé         | 5915/200000 [2:07:02<70:20:21,  1.30s/it, loss=0.0263, lr=2.96e-05, step=5915]Training:   3%|‚ñé         | 5916/200000 [2:07:03<66:37:07,  1.24s/it, loss=0.0263, lr=2.96e-05, step=5915]Training:   3%|‚ñé         | 5916/200000 [2:07:03<66:37:07,  1.24s/it, loss=0.0117, lr=2.96e-05, step=5916]Training:   3%|‚ñé         | 5917/200000 [2:07:05<68:24:10,  1.27s/it, loss=0.0117, lr=2.96e-05, step=5916]Training:   3%|‚ñé         | 5917/200000 [2:07:05<68:24:10,  1.27s/it, loss=0.0178, lr=2.96e-05, step=5917]Training:   3%|‚ñé         | 5918/200000 [2:07:06<65:17:12,  1.21s/it, loss=0.0178, lr=2.96e-05, step=5917]Training:   3%|‚ñé         | 5918/200000 [2:07:06<65:17:12,  1.21s/it, loss=0.0126, lr=2.96e-05, step=5918]Training:   3%|‚ñé         | 5919/200000 [2:07:07<69:19:24,  1.29s/it, loss=0.0126, lr=2.96e-05, step=5918]Training:   3%|‚ñé         | 5919/200000 [2:07:07<69:19:24,  1.29s/it, loss=0.0136, lr=2.96e-05, step=5919]Training:   3%|‚ñé         | 5920/200000 [2:07:09<71:42:59,  1.33s/it, loss=0.0136, lr=2.96e-05, step=5919]Training:   3%|‚ñé         | 5920/200000 [2:07:09<71:42:59,  1.33s/it, loss=0.0168, lr=2.96e-05, step=5920]Training:   3%|‚ñé         | 5921/200000 [2:07:10<73:56:06,  1.37s/it, loss=0.0168, lr=2.96e-05, step=5920]Training:   3%|‚ñé         | 5921/200000 [2:07:10<73:56:06,  1.37s/it, loss=0.0195, lr=2.96e-05, step=5921]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5922/200000 [2:07:12<75:02:46,  1.39s/it, loss=0.0195, lr=2.96e-05, step=5921]Training:   3%|‚ñé         | 5922/200000 [2:07:12<75:02:46,  1.39s/it, loss=0.0175, lr=2.96e-05, step=5922]Training:   3%|‚ñé         | 5923/200000 [2:07:13<69:52:47,  1.30s/it, loss=0.0175, lr=2.96e-05, step=5922]Training:   3%|‚ñé         | 5923/200000 [2:07:13<69:52:47,  1.30s/it, loss=0.0147, lr=2.96e-05, step=5923]Training:   3%|‚ñé         | 5924/200000 [2:07:14<66:19:59,  1.23s/it, loss=0.0147, lr=2.96e-05, step=5923]Training:   3%|‚ñé         | 5924/200000 [2:07:14<66:19:59,  1.23s/it, loss=0.0197, lr=2.96e-05, step=5924]Training:   3%|‚ñé         | 5925/200000 [2:07:15<67:48:28,  1.26s/it, loss=0.0197, lr=2.96e-05, step=5924]Training:   3%|‚ñé         | 5925/200000 [2:07:15<67:48:28,  1.26s/it, loss=0.0173, lr=2.96e-05, step=5925]Training:   3%|‚ñé         | 5926/200000 [2:07:16<70:04:39,  1.30s/it, loss=0.0173, lr=2.96e-05, step=5925]Training:   3%|‚ñé         | 5926/200000 [2:07:16<70:04:39,  1.30s/it, loss=0.0190, lr=2.96e-05, step=5926]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5927/200000 [2:07:17<66:27:11,  1.23s/it, loss=0.0190, lr=2.96e-05, step=5926]Training:   3%|‚ñé         | 5927/200000 [2:07:17<66:27:11,  1.23s/it, loss=0.0257, lr=2.96e-05, step=5927]Training:   3%|‚ñé         | 5928/200000 [2:07:19<68:47:35,  1.28s/it, loss=0.0257, lr=2.96e-05, step=5927]Training:   3%|‚ñé         | 5928/200000 [2:07:19<68:47:35,  1.28s/it, loss=0.0283, lr=2.96e-05, step=5928]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5929/200000 [2:07:20<70:23:20,  1.31s/it, loss=0.0283, lr=2.96e-05, step=5928]Training:   3%|‚ñé         | 5929/200000 [2:07:20<70:23:20,  1.31s/it, loss=0.0108, lr=2.96e-05, step=5929]Training:   3%|‚ñé         | 5930/200000 [2:07:22<70:41:49,  1.31s/it, loss=0.0108, lr=2.96e-05, step=5929]Training:   3%|‚ñé         | 5930/200000 [2:07:22<70:41:49,  1.31s/it, loss=0.0151, lr=2.96e-05, step=5930]Training:   3%|‚ñé         | 5931/200000 [2:07:23<66:50:48,  1.24s/it, loss=0.0151, lr=2.96e-05, step=5930]Training:   3%|‚ñé         | 5931/200000 [2:07:23<66:50:48,  1.24s/it, loss=0.0186, lr=2.97e-05, step=5931]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5932/200000 [2:07:24<70:45:26,  1.31s/it, loss=0.0186, lr=2.97e-05, step=5931]Training:   3%|‚ñé         | 5932/200000 [2:07:24<70:45:26,  1.31s/it, loss=0.0192, lr=2.97e-05, step=5932]Training:   3%|‚ñé         | 5933/200000 [2:07:26<73:49:54,  1.37s/it, loss=0.0192, lr=2.97e-05, step=5932]Training:   3%|‚ñé         | 5933/200000 [2:07:26<73:49:54,  1.37s/it, loss=0.0112, lr=2.97e-05, step=5933]Training:   3%|‚ñé         | 5934/200000 [2:07:27<69:06:26,  1.28s/it, loss=0.0112, lr=2.97e-05, step=5933]Training:   3%|‚ñé         | 5934/200000 [2:07:27<69:06:26,  1.28s/it, loss=0.0201, lr=2.97e-05, step=5934]Training:   3%|‚ñé         | 5935/200000 [2:07:28<65:45:13,  1.22s/it, loss=0.0201, lr=2.97e-05, step=5934]Training:   3%|‚ñé         | 5935/200000 [2:07:28<65:45:13,  1.22s/it, loss=0.0155, lr=2.97e-05, step=5935]Training:   3%|‚ñé         | 5936/200000 [2:07:29<69:08:10,  1.28s/it, loss=0.0155, lr=2.97e-05, step=5935]Training:   3%|‚ñé         | 5936/200000 [2:07:29<69:08:10,  1.28s/it, loss=0.0262, lr=2.97e-05, step=5936]Training:   3%|‚ñé         | 5937/200000 [2:07:30<65:47:21,  1.22s/it, loss=0.0262, lr=2.97e-05, step=5936]Training:   3%|‚ñé         | 5937/200000 [2:07:30<65:47:21,  1.22s/it, loss=0.0207, lr=2.97e-05, step=5937]Training:   3%|‚ñé         | 5938/200000 [2:07:32<66:53:06,  1.24s/it, loss=0.0207, lr=2.97e-05, step=5937]Training:   3%|‚ñé         | 5938/200000 [2:07:32<66:53:06,  1.24s/it, loss=0.0128, lr=2.97e-05, step=5938]Training:   3%|‚ñé         | 5939/200000 [2:07:33<64:09:11,  1.19s/it, loss=0.0128, lr=2.97e-05, step=5938]Training:   3%|‚ñé         | 5939/200000 [2:07:33<64:09:11,  1.19s/it, loss=0.0225, lr=2.97e-05, step=5939]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5940/200000 [2:07:34<68:24:14,  1.27s/it, loss=0.0225, lr=2.97e-05, step=5939]Training:   3%|‚ñé         | 5940/200000 [2:07:34<68:24:14,  1.27s/it, loss=0.0181, lr=2.97e-05, step=5940]Training:   3%|‚ñé         | 5941/200000 [2:07:36<71:26:31,  1.33s/it, loss=0.0181, lr=2.97e-05, step=5940]Training:   3%|‚ñé         | 5941/200000 [2:07:36<71:26:31,  1.33s/it, loss=0.0165, lr=2.97e-05, step=5941]Training:   3%|‚ñé         | 5942/200000 [2:07:37<73:33:15,  1.36s/it, loss=0.0165, lr=2.97e-05, step=5941]Training:   3%|‚ñé         | 5942/200000 [2:07:37<73:33:15,  1.36s/it, loss=0.0160, lr=2.97e-05, step=5942]Training:   3%|‚ñé         | 5943/200000 [2:07:38<74:36:48,  1.38s/it, loss=0.0160, lr=2.97e-05, step=5942]Training:   3%|‚ñé         | 5943/200000 [2:07:38<74:36:48,  1.38s/it, loss=0.0211, lr=2.97e-05, step=5943]Training:   3%|‚ñé         | 5944/200000 [2:07:40<69:37:48,  1.29s/it, loss=0.0211, lr=2.97e-05, step=5943]Training:   3%|‚ñé         | 5944/200000 [2:07:40<69:37:48,  1.29s/it, loss=0.0128, lr=2.97e-05, step=5944]Training:   3%|‚ñé         | 5945/200000 [2:07:41<66:06:17,  1.23s/it, loss=0.0128, lr=2.97e-05, step=5944]Training:   3%|‚ñé         | 5945/200000 [2:07:41<66:06:17,  1.23s/it, loss=0.0126, lr=2.97e-05, step=5945]Training:   3%|‚ñé         | 5946/200000 [2:07:42<68:05:19,  1.26s/it, loss=0.0126, lr=2.97e-05, step=5945]Training:   3%|‚ñé         | 5946/200000 [2:07:42<68:05:19,  1.26s/it, loss=0.0180, lr=2.97e-05, step=5946]Training:   3%|‚ñé         | 5947/200000 [2:07:43<69:12:29,  1.28s/it, loss=0.0180, lr=2.97e-05, step=5946]Training:   3%|‚ñé         | 5947/200000 [2:07:43<69:12:29,  1.28s/it, loss=0.0245, lr=2.97e-05, step=5947]Training:   3%|‚ñé         | 5948/200000 [2:07:44<65:48:55,  1.22s/it, loss=0.0245, lr=2.97e-05, step=5947]Training:   3%|‚ñé         | 5948/200000 [2:07:44<65:48:55,  1.22s/it, loss=0.0191, lr=2.97e-05, step=5948]Training:   3%|‚ñé         | 5949/200000 [2:07:46<67:50:30,  1.26s/it, loss=0.0191, lr=2.97e-05, step=5948]Training:   3%|‚ñé         | 5949/200000 [2:07:46<67:50:30,  1.26s/it, loss=0.0138, lr=2.97e-05, step=5949]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5950/200000 [2:07:47<69:25:08,  1.29s/it, loss=0.0138, lr=2.97e-05, step=5949]Training:   3%|‚ñé         | 5950/200000 [2:07:47<69:25:08,  1.29s/it, loss=0.0167, lr=2.97e-05, step=5950]Training:   3%|‚ñé         | 5951/200000 [2:07:48<69:57:38,  1.30s/it, loss=0.0167, lr=2.97e-05, step=5950]Training:   3%|‚ñé         | 5951/200000 [2:07:48<69:57:38,  1.30s/it, loss=0.0223, lr=2.98e-05, step=5951]Training:   3%|‚ñé         | 5952/200000 [2:07:49<66:22:12,  1.23s/it, loss=0.0223, lr=2.98e-05, step=5951]Training:   3%|‚ñé         | 5952/200000 [2:07:49<66:22:12,  1.23s/it, loss=0.0131, lr=2.98e-05, step=5952]Training:   3%|‚ñé         | 5953/200000 [2:07:51<69:13:24,  1.28s/it, loss=0.0131, lr=2.98e-05, step=5952]Training:   3%|‚ñé         | 5953/200000 [2:07:51<69:13:24,  1.28s/it, loss=0.0153, lr=2.98e-05, step=5953]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5954/200000 [2:07:52<71:49:54,  1.33s/it, loss=0.0153, lr=2.98e-05, step=5953]Training:   3%|‚ñé         | 5954/200000 [2:07:52<71:49:54,  1.33s/it, loss=0.0149, lr=2.98e-05, step=5954]Training:   3%|‚ñé         | 5955/200000 [2:07:53<67:38:55,  1.26s/it, loss=0.0149, lr=2.98e-05, step=5954]Training:   3%|‚ñé         | 5955/200000 [2:07:53<67:38:55,  1.26s/it, loss=0.0126, lr=2.98e-05, step=5955]Training:   3%|‚ñé         | 5956/200000 [2:07:54<64:44:30,  1.20s/it, loss=0.0126, lr=2.98e-05, step=5955]Training:   3%|‚ñé         | 5956/200000 [2:07:54<64:44:30,  1.20s/it, loss=0.0139, lr=2.98e-05, step=5956]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5957/200000 [2:07:56<67:56:34,  1.26s/it, loss=0.0139, lr=2.98e-05, step=5956]Training:   3%|‚ñé         | 5957/200000 [2:07:56<67:56:34,  1.26s/it, loss=0.0142, lr=2.98e-05, step=5957]Training:   3%|‚ñé         | 5958/200000 [2:07:57<70:19:25,  1.30s/it, loss=0.0142, lr=2.98e-05, step=5957]Training:   3%|‚ñé         | 5958/200000 [2:07:57<70:19:25,  1.30s/it, loss=0.0411, lr=2.98e-05, step=5958]Training:   3%|‚ñé         | 5959/200000 [2:07:58<66:35:59,  1.24s/it, loss=0.0411, lr=2.98e-05, step=5958]Training:   3%|‚ñé         | 5959/200000 [2:07:58<66:35:59,  1.24s/it, loss=0.0333, lr=2.98e-05, step=5959]Training:   3%|‚ñé         | 5960/200000 [2:08:00<69:26:28,  1.29s/it, loss=0.0333, lr=2.98e-05, step=5959]Training:   3%|‚ñé         | 5960/200000 [2:08:00<69:26:28,  1.29s/it, loss=0.0321, lr=2.98e-05, step=5960]Training:   3%|‚ñé         | 5961/200000 [2:08:01<70:22:58,  1.31s/it, loss=0.0321, lr=2.98e-05, step=5960]Training:   3%|‚ñé         | 5961/200000 [2:08:01<70:22:58,  1.31s/it, loss=0.0136, lr=2.98e-05, step=5961]Training:   3%|‚ñé         | 5962/200000 [2:08:02<71:47:49,  1.33s/it, loss=0.0136, lr=2.98e-05, step=5961]Training:   3%|‚ñé         | 5962/200000 [2:08:02<71:47:49,  1.33s/it, loss=0.0230, lr=2.98e-05, step=5962]Training:   3%|‚ñé         | 5963/200000 [2:08:04<67:39:10,  1.26s/it, loss=0.0230, lr=2.98e-05, step=5962]Training:   3%|‚ñé         | 5963/200000 [2:08:04<67:39:10,  1.26s/it, loss=0.0139, lr=2.98e-05, step=5963]Training:   3%|‚ñé         | 5964/200000 [2:08:05<71:36:22,  1.33s/it, loss=0.0139, lr=2.98e-05, step=5963]Training:   3%|‚ñé         | 5964/200000 [2:08:05<71:36:22,  1.33s/it, loss=0.0139, lr=2.98e-05, step=5964]Training:   3%|‚ñé         | 5965/200000 [2:08:07<74:44:34,  1.39s/it, loss=0.0139, lr=2.98e-05, step=5964]Training:   3%|‚ñé         | 5965/200000 [2:08:07<74:44:34,  1.39s/it, loss=0.0208, lr=2.98e-05, step=5965]Training:   3%|‚ñé         | 5966/200000 [2:08:08<69:44:00,  1.29s/it, loss=0.0208, lr=2.98e-05, step=5965]Training:   3%|‚ñé         | 5966/200000 [2:08:08<69:44:00,  1.29s/it, loss=0.0144, lr=2.98e-05, step=5966]Training:   3%|‚ñé         | 5967/200000 [2:08:09<66:11:39,  1.23s/it, loss=0.0144, lr=2.98e-05, step=5966]Training:   3%|‚ñé         | 5967/200000 [2:08:09<66:11:39,  1.23s/it, loss=0.0122, lr=2.98e-05, step=5967]Training:   3%|‚ñé         | 5968/200000 [2:08:10<70:18:43,  1.30s/it, loss=0.0122, lr=2.98e-05, step=5967]Training:   3%|‚ñé         | 5968/200000 [2:08:10<70:18:43,  1.30s/it, loss=0.0126, lr=2.98e-05, step=5968]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5969/200000 [2:08:11<66:36:56,  1.24s/it, loss=0.0126, lr=2.98e-05, step=5968]Training:   3%|‚ñé         | 5969/200000 [2:08:11<66:36:56,  1.24s/it, loss=0.0143, lr=2.98e-05, step=5969]Training:   3%|‚ñé         | 5970/200000 [2:08:13<67:43:24,  1.26s/it, loss=0.0143, lr=2.98e-05, step=5969]Training:   3%|‚ñé         | 5970/200000 [2:08:13<67:43:24,  1.26s/it, loss=0.0337, lr=2.98e-05, step=5970]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5971/200000 [2:08:14<64:46:53,  1.20s/it, loss=0.0337, lr=2.98e-05, step=5970]Training:   3%|‚ñé         | 5971/200000 [2:08:14<64:46:53,  1.20s/it, loss=0.0186, lr=2.99e-05, step=5971]Training:   3%|‚ñé         | 5972/200000 [2:08:15<68:57:35,  1.28s/it, loss=0.0186, lr=2.99e-05, step=5971]Training:   3%|‚ñé         | 5972/200000 [2:08:15<68:57:35,  1.28s/it, loss=0.0167, lr=2.99e-05, step=5972]Training:   3%|‚ñé         | 5973/200000 [2:08:17<71:25:44,  1.33s/it, loss=0.0167, lr=2.99e-05, step=5972]Training:   3%|‚ñé         | 5973/200000 [2:08:17<71:25:44,  1.33s/it, loss=0.0180, lr=2.99e-05, step=5973]Training:   3%|‚ñé         | 5974/200000 [2:08:18<73:02:49,  1.36s/it, loss=0.0180, lr=2.99e-05, step=5973]Training:   3%|‚ñé         | 5974/200000 [2:08:18<73:02:49,  1.36s/it, loss=0.0107, lr=2.99e-05, step=5974]Training:   3%|‚ñé         | 5975/200000 [2:08:19<74:09:14,  1.38s/it, loss=0.0107, lr=2.99e-05, step=5974]Training:   3%|‚ñé         | 5975/200000 [2:08:19<74:09:14,  1.38s/it, loss=0.0181, lr=2.99e-05, step=5975]Training:   3%|‚ñé         | 5976/200000 [2:08:20<69:18:53,  1.29s/it, loss=0.0181, lr=2.99e-05, step=5975]Training:   3%|‚ñé         | 5976/200000 [2:08:20<69:18:53,  1.29s/it, loss=0.0202, lr=2.99e-05, step=5976]Training:   3%|‚ñé         | 5977/200000 [2:08:22<65:55:26,  1.22s/it, loss=0.0202, lr=2.99e-05, step=5976]Training:   3%|‚ñé         | 5977/200000 [2:08:22<65:55:26,  1.22s/it, loss=0.0132, lr=2.99e-05, step=5977]Training:   3%|‚ñé         | 5978/200000 [2:08:23<67:56:06,  1.26s/it, loss=0.0132, lr=2.99e-05, step=5977]Training:   3%|‚ñé         | 5978/200000 [2:08:23<67:56:06,  1.26s/it, loss=0.0136, lr=2.99e-05, step=5978]Training:   3%|‚ñé         | 5979/200000 [2:08:24<70:18:31,  1.30s/it, loss=0.0136, lr=2.99e-05, step=5978]Training:   3%|‚ñé         | 5979/200000 [2:08:24<70:18:31,  1.30s/it, loss=0.0201, lr=2.99e-05, step=5979]Training:   3%|‚ñé         | 5980/200000 [2:08:25<66:37:40,  1.24s/it, loss=0.0201, lr=2.99e-05, step=5979]Training:   3%|‚ñé         | 5980/200000 [2:08:25<66:37:40,  1.24s/it, loss=0.0163, lr=2.99e-05, step=5980]Training:   3%|‚ñé         | 5981/200000 [2:08:27<69:15:26,  1.29s/it, loss=0.0163, lr=2.99e-05, step=5980]Training:   3%|‚ñé         | 5981/200000 [2:08:27<69:15:26,  1.29s/it, loss=0.0203, lr=2.99e-05, step=5981]Training:   3%|‚ñé         | 5982/200000 [2:08:28<70:33:19,  1.31s/it, loss=0.0203, lr=2.99e-05, step=5981]Training:   3%|‚ñé         | 5982/200000 [2:08:28<70:33:19,  1.31s/it, loss=0.0087, lr=2.99e-05, step=5982]Training:   3%|‚ñé         | 5983/200000 [2:08:29<70:55:02,  1.32s/it, loss=0.0087, lr=2.99e-05, step=5982]Training:   3%|‚ñé         | 5983/200000 [2:08:29<70:55:02,  1.32s/it, loss=0.0282, lr=2.99e-05, step=5983]Training:   3%|‚ñé         | 5984/200000 [2:08:31<67:01:50,  1.24s/it, loss=0.0282, lr=2.99e-05, step=5983]Training:   3%|‚ñé         | 5984/200000 [2:08:31<67:01:50,  1.24s/it, loss=0.0114, lr=2.99e-05, step=5984]Training:   3%|‚ñé         | 5985/200000 [2:08:32<70:38:34,  1.31s/it, loss=0.0114, lr=2.99e-05, step=5984]Training:   3%|‚ñé         | 5985/200000 [2:08:32<70:38:34,  1.31s/it, loss=0.0166, lr=2.99e-05, step=5985]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5986/200000 [2:08:34<73:49:11,  1.37s/it, loss=0.0166, lr=2.99e-05, step=5985]Training:   3%|‚ñé         | 5986/200000 [2:08:34<73:49:11,  1.37s/it, loss=0.0111, lr=2.99e-05, step=5986]Training:   3%|‚ñé         | 5987/200000 [2:08:35<69:05:46,  1.28s/it, loss=0.0111, lr=2.99e-05, step=5986]Training:   3%|‚ñé         | 5987/200000 [2:08:35<69:05:46,  1.28s/it, loss=0.0157, lr=2.99e-05, step=5987]Training:   3%|‚ñé         | 5988/200000 [2:08:36<65:48:04,  1.22s/it, loss=0.0157, lr=2.99e-05, step=5987]Training:   3%|‚ñé         | 5988/200000 [2:08:36<65:48:04,  1.22s/it, loss=0.0155, lr=2.99e-05, step=5988]Training:   3%|‚ñé         | 5989/200000 [2:08:37<69:17:41,  1.29s/it, loss=0.0155, lr=2.99e-05, step=5988]Training:   3%|‚ñé         | 5989/200000 [2:08:37<69:17:41,  1.29s/it, loss=0.0227, lr=2.99e-05, step=5989]Training:   3%|‚ñé         | 5990/200000 [2:08:38<65:55:29,  1.22s/it, loss=0.0227, lr=2.99e-05, step=5989]Training:   3%|‚ñé         | 5990/200000 [2:08:38<65:55:29,  1.22s/it, loss=0.0225, lr=2.99e-05, step=5990]Training:   3%|‚ñé         | 5991/200000 [2:08:40<67:48:50,  1.26s/it, loss=0.0225, lr=2.99e-05, step=5990]Training:   3%|‚ñé         | 5991/200000 [2:08:40<67:48:50,  1.26s/it, loss=0.0142, lr=3.00e-05, step=5991]Training:   3%|‚ñé         | 5992/200000 [2:08:41<64:50:22,  1.20s/it, loss=0.0142, lr=3.00e-05, step=5991]Training:   3%|‚ñé         | 5992/200000 [2:08:41<64:50:22,  1.20s/it, loss=0.0221, lr=3.00e-05, step=5992]Training:   3%|‚ñé         | 5993/200000 [2:08:42<68:49:34,  1.28s/it, loss=0.0221, lr=3.00e-05, step=5992]Training:   3%|‚ñé         | 5993/200000 [2:08:42<68:49:34,  1.28s/it, loss=0.0213, lr=3.00e-05, step=5993]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 5994/200000 [2:08:43<71:02:55,  1.32s/it, loss=0.0213, lr=3.00e-05, step=5993]Training:   3%|‚ñé         | 5994/200000 [2:08:43<71:02:55,  1.32s/it, loss=0.0139, lr=3.00e-05, step=5994]Training:   3%|‚ñé         | 5995/200000 [2:08:45<73:23:43,  1.36s/it, loss=0.0139, lr=3.00e-05, step=5994]Training:   3%|‚ñé         | 5995/200000 [2:08:45<73:23:43,  1.36s/it, loss=0.0100, lr=3.00e-05, step=5995]Training:   3%|‚ñé         | 5996/200000 [2:08:46<74:21:01,  1.38s/it, loss=0.0100, lr=3.00e-05, step=5995]Training:   3%|‚ñé         | 5996/200000 [2:08:46<74:21:01,  1.38s/it, loss=0.0134, lr=3.00e-05, step=5996]Training:   3%|‚ñé         | 5997/200000 [2:08:47<69:30:10,  1.29s/it, loss=0.0134, lr=3.00e-05, step=5996]Training:   3%|‚ñé         | 5997/200000 [2:08:47<69:30:10,  1.29s/it, loss=0.0223, lr=3.00e-05, step=5997]Training:   3%|‚ñé         | 5998/200000 [2:08:49<66:04:02,  1.23s/it, loss=0.0223, lr=3.00e-05, step=5997]Training:   3%|‚ñé         | 5998/200000 [2:08:49<66:04:02,  1.23s/it, loss=0.0207, lr=3.00e-05, step=5998]Training:   3%|‚ñé         | 5999/200000 [2:08:50<68:04:41,  1.26s/it, loss=0.0207, lr=3.00e-05, step=5998]Training:   3%|‚ñé         | 5999/200000 [2:08:50<68:04:41,  1.26s/it, loss=0.0199, lr=3.00e-05, step=5999]Training:   3%|‚ñé         | 6000/200000 [2:08:51<69:04:35,  1.28s/it, loss=0.0199, lr=3.00e-05, step=5999]Training:   3%|‚ñé         | 6000/200000 [2:08:51<69:04:35,  1.28s/it, loss=0.0301, lr=3.00e-05, step=6000]01:02:06.076 [I] step=6000 loss=0.0190 lr=2.98e-05 grad_norm=0.38 time=128.0s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 6001/200000 [2:08:52<65:45:06,  1.22s/it, loss=0.0301, lr=3.00e-05, step=6000]Training:   3%|‚ñé         | 6001/200000 [2:08:52<65:45:06,  1.22s/it, loss=0.0147, lr=3.00e-05, step=6001]Training:   3%|‚ñé         | 6002/200000 [2:08:54<67:23:38,  1.25s/it, loss=0.0147, lr=3.00e-05, step=6001]Training:   3%|‚ñé         | 6002/200000 [2:08:54<67:23:38,  1.25s/it, loss=0.0087, lr=3.00e-05, step=6002]Training:   3%|‚ñé         | 6003/200000 [2:08:55<69:03:16,  1.28s/it, loss=0.0087, lr=3.00e-05, step=6002]Training:   3%|‚ñé         | 6003/200000 [2:08:55<69:03:16,  1.28s/it, loss=0.0271, lr=3.00e-05, step=6003]Training:   3%|‚ñé         | 6004/200000 [2:08:56<69:42:21,  1.29s/it, loss=0.0271, lr=3.00e-05, step=6003]Training:   3%|‚ñé         | 6004/200000 [2:08:56<69:42:21,  1.29s/it, loss=0.0181, lr=3.00e-05, step=6004]Training:   3%|‚ñé         | 6005/200000 [2:08:57<66:08:13,  1.23s/it, loss=0.0181, lr=3.00e-05, step=6004]Training:   3%|‚ñé         | 6005/200000 [2:08:57<66:08:13,  1.23s/it, loss=0.0128, lr=3.00e-05, step=6005]Training:   3%|‚ñé         | 6006/200000 [2:08:59<70:03:32,  1.30s/it, loss=0.0128, lr=3.00e-05, step=6005]Training:   3%|‚ñé         | 6006/200000 [2:08:59<70:03:32,  1.30s/it, loss=0.0140, lr=3.00e-05, step=6006]Training:   3%|‚ñé         | 6007/200000 [2:09:00<72:15:28,  1.34s/it, loss=0.0140, lr=3.00e-05, step=6006]Training:   3%|‚ñé         | 6007/200000 [2:09:00<72:15:28,  1.34s/it, loss=0.0384, lr=3.00e-05, step=6007]Training:   3%|‚ñé         | 6008/200000 [2:09:01<68:00:24,  1.26s/it, loss=0.0384, lr=3.00e-05, step=6007]Training:   3%|‚ñé         | 6008/200000 [2:09:01<68:00:24,  1.26s/it, loss=0.0159, lr=3.00e-05, step=6008]Training:   3%|‚ñé         | 6009/200000 [2:09:02<65:00:08,  1.21s/it, loss=0.0159, lr=3.00e-05, step=6008]Training:   3%|‚ñé         | 6009/200000 [2:09:02<65:00:08,  1.21s/it, loss=0.0260, lr=3.00e-05, step=6009]Training:   3%|‚ñé         | 6010/200000 [2:09:04<68:12:23,  1.27s/it, loss=0.0260, lr=3.00e-05, step=6009]Training:   3%|‚ñé         | 6010/200000 [2:09:04<68:12:23,  1.27s/it, loss=0.0142, lr=3.00e-05, step=6010]Training:   3%|‚ñé         | 6011/200000 [2:09:05<70:51:02,  1.31s/it, loss=0.0142, lr=3.00e-05, step=6010]Training:   3%|‚ñé         | 6011/200000 [2:09:05<70:51:02,  1.31s/it, loss=0.0180, lr=3.01e-05, step=6011]Training:   3%|‚ñé         | 6012/200000 [2:09:06<66:59:06,  1.24s/it, loss=0.0180, lr=3.01e-05, step=6011]Training:   3%|‚ñé         | 6012/200000 [2:09:06<66:59:06,  1.24s/it, loss=0.0125, lr=3.01e-05, step=6012]Training:   3%|‚ñé         | 6013/200000 [2:09:08<69:53:52,  1.30s/it, loss=0.0125, lr=3.01e-05, step=6012]Training:   3%|‚ñé         | 6013/200000 [2:09:08<69:53:52,  1.30s/it, loss=0.0165, lr=3.01e-05, step=6013]Training:   3%|‚ñé         | 6014/200000 [2:09:09<70:42:12,  1.31s/it, loss=0.0165, lr=3.01e-05, step=6013]Training:   3%|‚ñé         | 6014/200000 [2:09:09<70:42:12,  1.31s/it, loss=0.0175, lr=3.01e-05, step=6014]Training:   3%|‚ñé         | 6015/200000 [2:09:10<72:02:15,  1.34s/it, loss=0.0175, lr=3.01e-05, step=6014]Training:   3%|‚ñé         | 6015/200000 [2:09:10<72:02:15,  1.34s/it, loss=0.0319, lr=3.01e-05, step=6015]Training:   3%|‚ñé         | 6016/200000 [2:09:12<67:49:14,  1.26s/it, loss=0.0319, lr=3.01e-05, step=6015]Training:   3%|‚ñé         | 6016/200000 [2:09:12<67:49:14,  1.26s/it, loss=0.0255, lr=3.01e-05, step=6016]Training:   3%|‚ñé         | 6017/200000 [2:09:13<71:38:15,  1.33s/it, loss=0.0255, lr=3.01e-05, step=6016]Training:   3%|‚ñé         | 6017/200000 [2:09:13<71:38:15,  1.33s/it, loss=0.0233, lr=3.01e-05, step=6017]Training:   3%|‚ñé         | 6018/200000 [2:09:15<74:29:11,  1.38s/it, loss=0.0233, lr=3.01e-05, step=6017]Training:   3%|‚ñé         | 6018/200000 [2:09:15<74:29:11,  1.38s/it, loss=0.0181, lr=3.01e-05, step=6018]Training:   3%|‚ñé         | 6019/200000 [2:09:16<69:30:52,  1.29s/it, loss=0.0181, lr=3.01e-05, step=6018]Training:   3%|‚ñé         | 6019/200000 [2:09:16<69:30:52,  1.29s/it, loss=0.0435, lr=3.01e-05, step=6019]Training:   3%|‚ñé         | 6020/200000 [2:09:17<66:04:40,  1.23s/it, loss=0.0435, lr=3.01e-05, step=6019]Training:   3%|‚ñé         | 6020/200000 [2:09:17<66:04:40,  1.23s/it, loss=0.0179, lr=3.01e-05, step=6020]Training:   3%|‚ñé         | 6021/200000 [2:09:18<70:06:52,  1.30s/it, loss=0.0179, lr=3.01e-05, step=6020]Training:   3%|‚ñé         | 6021/200000 [2:09:18<70:06:52,  1.30s/it, loss=0.0137, lr=3.01e-05, step=6021]Training:   3%|‚ñé         | 6022/200000 [2:09:19<66:29:06,  1.23s/it, loss=0.0137, lr=3.01e-05, step=6021]Training:   3%|‚ñé         | 6022/200000 [2:09:19<66:29:06,  1.23s/it, loss=0.0188, lr=3.01e-05, step=6022]WARNING:root:Token length (50) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6023/200000 [2:09:21<68:42:26,  1.28s/it, loss=0.0188, lr=3.01e-05, step=6022]Training:   3%|‚ñé         | 6023/200000 [2:09:21<68:42:26,  1.28s/it, loss=0.0213, lr=3.01e-05, step=6023]Training:   3%|‚ñé         | 6024/200000 [2:09:22<65:30:03,  1.22s/it, loss=0.0213, lr=3.01e-05, step=6023]Training:   3%|‚ñé         | 6024/200000 [2:09:22<65:30:03,  1.22s/it, loss=0.0165, lr=3.01e-05, step=6024]Training:   3%|‚ñé         | 6025/200000 [2:09:23<69:23:51,  1.29s/it, loss=0.0165, lr=3.01e-05, step=6024]Training:   3%|‚ñé         | 6025/200000 [2:09:23<69:23:51,  1.29s/it, loss=0.0153, lr=3.01e-05, step=6025]Training:   3%|‚ñé         | 6026/200000 [2:09:25<72:32:03,  1.35s/it, loss=0.0153, lr=3.01e-05, step=6025]Training:   3%|‚ñé         | 6026/200000 [2:09:25<72:32:03,  1.35s/it, loss=0.0135, lr=3.01e-05, step=6026]Training:   3%|‚ñé         | 6027/200000 [2:09:26<73:51:05,  1.37s/it, loss=0.0135, lr=3.01e-05, step=6026]Training:   3%|‚ñé         | 6027/200000 [2:09:26<73:51:05,  1.37s/it, loss=0.0190, lr=3.01e-05, step=6027]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6028/200000 [2:09:27<74:50:10,  1.39s/it, loss=0.0190, lr=3.01e-05, step=6027]Training:   3%|‚ñé         | 6028/200000 [2:09:28<74:50:10,  1.39s/it, loss=0.0134, lr=3.01e-05, step=6028]Training:   3%|‚ñé         | 6029/200000 [2:09:29<69:42:35,  1.29s/it, loss=0.0134, lr=3.01e-05, step=6028]Training:   3%|‚ñé         | 6029/200000 [2:09:29<69:42:35,  1.29s/it, loss=0.0286, lr=3.01e-05, step=6029]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6030/200000 [2:09:30<66:11:13,  1.23s/it, loss=0.0286, lr=3.01e-05, step=6029]Training:   3%|‚ñé         | 6030/200000 [2:09:30<66:11:13,  1.23s/it, loss=0.0421, lr=3.01e-05, step=6030]Training:   3%|‚ñé         | 6031/200000 [2:09:31<68:40:54,  1.27s/it, loss=0.0421, lr=3.01e-05, step=6030]Training:   3%|‚ñé         | 6031/200000 [2:09:31<68:40:54,  1.27s/it, loss=0.0204, lr=3.02e-05, step=6031]Training:   3%|‚ñé         | 6032/200000 [2:09:32<70:53:23,  1.32s/it, loss=0.0204, lr=3.02e-05, step=6031]Training:   3%|‚ñé         | 6032/200000 [2:09:32<70:53:23,  1.32s/it, loss=0.0252, lr=3.02e-05, step=6032]Training:   3%|‚ñé         | 6033/200000 [2:09:34<66:59:29,  1.24s/it, loss=0.0252, lr=3.02e-05, step=6032]Training:   3%|‚ñé         | 6033/200000 [2:09:34<66:59:29,  1.24s/it, loss=0.0099, lr=3.02e-05, step=6033]Training:   3%|‚ñé         | 6034/200000 [2:09:35<68:59:51,  1.28s/it, loss=0.0099, lr=3.02e-05, step=6033]Training:   3%|‚ñé         | 6034/200000 [2:09:35<68:59:51,  1.28s/it, loss=0.0162, lr=3.02e-05, step=6034]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6035/200000 [2:09:36<70:26:08,  1.31s/it, loss=0.0162, lr=3.02e-05, step=6034]Training:   3%|‚ñé         | 6035/200000 [2:09:36<70:26:08,  1.31s/it, loss=0.0375, lr=3.02e-05, step=6035]Training:   3%|‚ñé         | 6036/200000 [2:09:38<71:09:39,  1.32s/it, loss=0.0375, lr=3.02e-05, step=6035]Training:   3%|‚ñé         | 6036/200000 [2:09:38<71:09:39,  1.32s/it, loss=0.0135, lr=3.02e-05, step=6036]WARNING:root:Token length (53) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6037/200000 [2:09:39<67:10:58,  1.25s/it, loss=0.0135, lr=3.02e-05, step=6036]Training:   3%|‚ñé         | 6037/200000 [2:09:39<67:10:58,  1.25s/it, loss=0.0143, lr=3.02e-05, step=6037]Training:   3%|‚ñé         | 6038/200000 [2:09:40<70:56:36,  1.32s/it, loss=0.0143, lr=3.02e-05, step=6037]Training:   3%|‚ñé         | 6038/200000 [2:09:40<70:56:36,  1.32s/it, loss=0.1067, lr=3.02e-05, step=6038]Training:   3%|‚ñé         | 6039/200000 [2:09:42<74:00:59,  1.37s/it, loss=0.1067, lr=3.02e-05, step=6038]Training:   3%|‚ñé         | 6039/200000 [2:09:42<74:00:59,  1.37s/it, loss=0.0156, lr=3.02e-05, step=6039]Training:   3%|‚ñé         | 6040/200000 [2:09:43<69:12:48,  1.28s/it, loss=0.0156, lr=3.02e-05, step=6039]Training:   3%|‚ñé         | 6040/200000 [2:09:43<69:12:48,  1.28s/it, loss=0.0212, lr=3.02e-05, step=6040]Training:   3%|‚ñé         | 6041/200000 [2:09:44<65:53:32,  1.22s/it, loss=0.0212, lr=3.02e-05, step=6040]Training:   3%|‚ñé         | 6041/200000 [2:09:44<65:53:32,  1.22s/it, loss=0.0197, lr=3.02e-05, step=6041]Training:   3%|‚ñé         | 6042/200000 [2:09:45<69:05:57,  1.28s/it, loss=0.0197, lr=3.02e-05, step=6041]Training:   3%|‚ñé         | 6042/200000 [2:09:45<69:05:57,  1.28s/it, loss=0.0258, lr=3.02e-05, step=6042]Training:   3%|‚ñé         | 6043/200000 [2:09:46<65:48:29,  1.22s/it, loss=0.0258, lr=3.02e-05, step=6042]Training:   3%|‚ñé         | 6043/200000 [2:09:46<65:48:29,  1.22s/it, loss=0.0173, lr=3.02e-05, step=6043]Training:   3%|‚ñé         | 6044/200000 [2:09:48<67:37:06,  1.26s/it, loss=0.0173, lr=3.02e-05, step=6043]Training:   3%|‚ñé         | 6044/200000 [2:09:48<67:37:06,  1.26s/it, loss=0.0177, lr=3.02e-05, step=6044]Training:   3%|‚ñé         | 6045/200000 [2:09:49<64:42:50,  1.20s/it, loss=0.0177, lr=3.02e-05, step=6044]Training:   3%|‚ñé         | 6045/200000 [2:09:49<64:42:50,  1.20s/it, loss=0.0798, lr=3.02e-05, step=6045]Training:   3%|‚ñé         | 6046/200000 [2:09:50<68:49:33,  1.28s/it, loss=0.0798, lr=3.02e-05, step=6045]Training:   3%|‚ñé         | 6046/200000 [2:09:50<68:49:33,  1.28s/it, loss=0.0262, lr=3.02e-05, step=6046]Training:   3%|‚ñé         | 6047/200000 [2:09:52<70:53:34,  1.32s/it, loss=0.0262, lr=3.02e-05, step=6046]Training:   3%|‚ñé         | 6047/200000 [2:09:52<70:53:34,  1.32s/it, loss=0.1623, lr=3.02e-05, step=6047]Training:   3%|‚ñé         | 6048/200000 [2:09:53<73:00:19,  1.36s/it, loss=0.1623, lr=3.02e-05, step=6047]Training:   3%|‚ñé         | 6048/200000 [2:09:53<73:00:19,  1.36s/it, loss=0.0162, lr=3.02e-05, step=6048]Training:   3%|‚ñé         | 6049/200000 [2:09:54<73:30:25,  1.36s/it, loss=0.0162, lr=3.02e-05, step=6048]Training:   3%|‚ñé         | 6049/200000 [2:09:54<73:30:25,  1.36s/it, loss=0.0165, lr=3.02e-05, step=6049]Training:   3%|‚ñé         | 6050/200000 [2:09:56<68:50:01,  1.28s/it, loss=0.0165, lr=3.02e-05, step=6049]Training:   3%|‚ñé         | 6050/200000 [2:09:56<68:50:01,  1.28s/it, loss=0.0159, lr=3.02e-05, step=6050]Training:   3%|‚ñé         | 6051/200000 [2:09:57<65:31:23,  1.22s/it, loss=0.0159, lr=3.02e-05, step=6050]Training:   3%|‚ñé         | 6051/200000 [2:09:57<65:31:23,  1.22s/it, loss=0.0176, lr=3.03e-05, step=6051]Training:   3%|‚ñé         | 6052/200000 [2:09:58<68:25:47,  1.27s/it, loss=0.0176, lr=3.03e-05, step=6051]Training:   3%|‚ñé         | 6052/200000 [2:09:58<68:25:47,  1.27s/it, loss=0.0150, lr=3.03e-05, step=6052]Training:   3%|‚ñé         | 6053/200000 [2:09:59<69:42:25,  1.29s/it, loss=0.0150, lr=3.03e-05, step=6052]Training:   3%|‚ñé         | 6053/200000 [2:09:59<69:42:25,  1.29s/it, loss=0.0192, lr=3.03e-05, step=6053]Training:   3%|‚ñé         | 6054/200000 [2:10:00<66:11:49,  1.23s/it, loss=0.0192, lr=3.03e-05, step=6053]Training:   3%|‚ñé         | 6054/200000 [2:10:00<66:11:49,  1.23s/it, loss=0.0160, lr=3.03e-05, step=6054]Training:   3%|‚ñé         | 6055/200000 [2:10:02<67:49:15,  1.26s/it, loss=0.0160, lr=3.03e-05, step=6054]Training:   3%|‚ñé         | 6055/200000 [2:10:02<67:49:15,  1.26s/it, loss=0.0332, lr=3.03e-05, step=6055]Training:   3%|‚ñé         | 6056/200000 [2:10:03<69:22:39,  1.29s/it, loss=0.0332, lr=3.03e-05, step=6055]Training:   3%|‚ñé         | 6056/200000 [2:10:03<69:22:39,  1.29s/it, loss=0.0194, lr=3.03e-05, step=6056]Training:   3%|‚ñé         | 6057/200000 [2:10:04<70:12:28,  1.30s/it, loss=0.0194, lr=3.03e-05, step=6056]Training:   3%|‚ñé         | 6057/200000 [2:10:04<70:12:28,  1.30s/it, loss=0.0155, lr=3.03e-05, step=6057]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6058/200000 [2:10:05<66:33:39,  1.24s/it, loss=0.0155, lr=3.03e-05, step=6057]Training:   3%|‚ñé         | 6058/200000 [2:10:05<66:33:39,  1.24s/it, loss=0.0108, lr=3.03e-05, step=6058]Training:   3%|‚ñé         | 6059/200000 [2:10:07<69:22:11,  1.29s/it, loss=0.0108, lr=3.03e-05, step=6058]Training:   3%|‚ñé         | 6059/200000 [2:10:07<69:22:11,  1.29s/it, loss=0.0604, lr=3.03e-05, step=6059]Training:   3%|‚ñé         | 6060/200000 [2:10:08<71:52:58,  1.33s/it, loss=0.0604, lr=3.03e-05, step=6059]Training:   3%|‚ñé         | 6060/200000 [2:10:08<71:52:58,  1.33s/it, loss=0.0131, lr=3.03e-05, step=6060]Training:   3%|‚ñé         | 6061/200000 [2:10:09<67:42:11,  1.26s/it, loss=0.0131, lr=3.03e-05, step=6060]Training:   3%|‚ñé         | 6061/200000 [2:10:09<67:42:11,  1.26s/it, loss=0.0230, lr=3.03e-05, step=6061]Training:   3%|‚ñé         | 6062/200000 [2:10:11<64:47:39,  1.20s/it, loss=0.0230, lr=3.03e-05, step=6061]Training:   3%|‚ñé         | 6062/200000 [2:10:11<64:47:39,  1.20s/it, loss=0.0215, lr=3.03e-05, step=6062]Training:   3%|‚ñé         | 6063/200000 [2:10:12<68:04:17,  1.26s/it, loss=0.0215, lr=3.03e-05, step=6062]Training:   3%|‚ñé         | 6063/200000 [2:10:12<68:04:17,  1.26s/it, loss=0.0276, lr=3.03e-05, step=6063]Training:   3%|‚ñé         | 6064/200000 [2:10:13<70:52:41,  1.32s/it, loss=0.0276, lr=3.03e-05, step=6063]Training:   3%|‚ñé         | 6064/200000 [2:10:13<70:52:41,  1.32s/it, loss=0.0133, lr=3.03e-05, step=6064]Training:   3%|‚ñé         | 6065/200000 [2:10:14<67:00:19,  1.24s/it, loss=0.0133, lr=3.03e-05, step=6064]Training:   3%|‚ñé         | 6065/200000 [2:10:14<67:00:19,  1.24s/it, loss=0.0146, lr=3.03e-05, step=6065]Training:   3%|‚ñé         | 6066/200000 [2:10:16<69:43:53,  1.29s/it, loss=0.0146, lr=3.03e-05, step=6065]Training:   3%|‚ñé         | 6066/200000 [2:10:16<69:43:53,  1.29s/it, loss=0.0133, lr=3.03e-05, step=6066]Training:   3%|‚ñé         | 6067/200000 [2:10:17<71:15:04,  1.32s/it, loss=0.0133, lr=3.03e-05, step=6066]Training:   3%|‚ñé         | 6067/200000 [2:10:17<71:15:04,  1.32s/it, loss=0.0201, lr=3.03e-05, step=6067]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6068/200000 [2:10:19<71:46:30,  1.33s/it, loss=0.0201, lr=3.03e-05, step=6067]Training:   3%|‚ñé         | 6068/200000 [2:10:19<71:46:30,  1.33s/it, loss=0.0131, lr=3.03e-05, step=6068]Training:   3%|‚ñé         | 6069/200000 [2:10:20<67:39:09,  1.26s/it, loss=0.0131, lr=3.03e-05, step=6068]Training:   3%|‚ñé         | 6069/200000 [2:10:20<67:39:09,  1.26s/it, loss=0.0127, lr=3.03e-05, step=6069]Training:   3%|‚ñé         | 6070/200000 [2:10:21<72:27:20,  1.35s/it, loss=0.0127, lr=3.03e-05, step=6069]Training:   3%|‚ñé         | 6070/200000 [2:10:21<72:27:20,  1.35s/it, loss=0.0130, lr=3.03e-05, step=6070]Training:   3%|‚ñé         | 6071/200000 [2:10:23<75:18:37,  1.40s/it, loss=0.0130, lr=3.03e-05, step=6070]Training:   3%|‚ñé         | 6071/200000 [2:10:23<75:18:37,  1.40s/it, loss=0.0232, lr=3.04e-05, step=6071]Training:   3%|‚ñé         | 6072/200000 [2:10:24<70:05:02,  1.30s/it, loss=0.0232, lr=3.04e-05, step=6071]Training:   3%|‚ñé         | 6072/200000 [2:10:24<70:05:02,  1.30s/it, loss=0.0121, lr=3.04e-05, step=6072]Training:   3%|‚ñé         | 6073/200000 [2:10:25<66:25:22,  1.23s/it, loss=0.0121, lr=3.04e-05, step=6072]Training:   3%|‚ñé         | 6073/200000 [2:10:25<66:25:22,  1.23s/it, loss=0.0151, lr=3.04e-05, step=6073]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6074/200000 [2:10:26<70:23:24,  1.31s/it, loss=0.0151, lr=3.04e-05, step=6073]Training:   3%|‚ñé         | 6074/200000 [2:10:26<70:23:24,  1.31s/it, loss=0.0132, lr=3.04e-05, step=6074]Training:   3%|‚ñé         | 6075/200000 [2:10:27<66:39:43,  1.24s/it, loss=0.0132, lr=3.04e-05, step=6074]Training:   3%|‚ñé         | 6075/200000 [2:10:27<66:39:43,  1.24s/it, loss=0.0175, lr=3.04e-05, step=6075]Training:   3%|‚ñé         | 6076/200000 [2:10:29<67:46:14,  1.26s/it, loss=0.0175, lr=3.04e-05, step=6075]Training:   3%|‚ñé         | 6076/200000 [2:10:29<67:46:14,  1.26s/it, loss=0.0211, lr=3.04e-05, step=6076]Training:   3%|‚ñé         | 6077/200000 [2:10:30<64:49:14,  1.20s/it, loss=0.0211, lr=3.04e-05, step=6076]Training:   3%|‚ñé         | 6077/200000 [2:10:30<64:49:14,  1.20s/it, loss=0.0158, lr=3.04e-05, step=6077]Training:   3%|‚ñé         | 6078/200000 [2:10:31<68:58:47,  1.28s/it, loss=0.0158, lr=3.04e-05, step=6077]Training:   3%|‚ñé         | 6078/200000 [2:10:31<68:58:47,  1.28s/it, loss=0.0095, lr=3.04e-05, step=6078]Training:   3%|‚ñé         | 6079/200000 [2:10:33<71:25:45,  1.33s/it, loss=0.0095, lr=3.04e-05, step=6078]Training:   3%|‚ñé         | 6079/200000 [2:10:33<71:25:45,  1.33s/it, loss=0.0146, lr=3.04e-05, step=6079]Training:   3%|‚ñé         | 6080/200000 [2:10:34<73:40:37,  1.37s/it, loss=0.0146, lr=3.04e-05, step=6079]Training:   3%|‚ñé         | 6080/200000 [2:10:34<73:40:37,  1.37s/it, loss=0.0142, lr=3.04e-05, step=6080]Training:   3%|‚ñé         | 6081/200000 [2:10:36<74:33:38,  1.38s/it, loss=0.0142, lr=3.04e-05, step=6080]Training:   3%|‚ñé         | 6081/200000 [2:10:36<74:33:38,  1.38s/it, loss=0.0196, lr=3.04e-05, step=6081]Training:   3%|‚ñé         | 6082/200000 [2:10:37<69:33:25,  1.29s/it, loss=0.0196, lr=3.04e-05, step=6081]Training:   3%|‚ñé         | 6082/200000 [2:10:37<69:33:25,  1.29s/it, loss=0.0137, lr=3.04e-05, step=6082]Training:   3%|‚ñé         | 6083/200000 [2:10:38<66:03:28,  1.23s/it, loss=0.0137, lr=3.04e-05, step=6082]Training:   3%|‚ñé         | 6083/200000 [2:10:38<66:03:28,  1.23s/it, loss=0.0100, lr=3.04e-05, step=6083]Training:   3%|‚ñé         | 6084/200000 [2:10:39<68:45:39,  1.28s/it, loss=0.0100, lr=3.04e-05, step=6083]Training:   3%|‚ñé         | 6084/200000 [2:10:39<68:45:39,  1.28s/it, loss=0.0227, lr=3.04e-05, step=6084]Training:   3%|‚ñé         | 6085/200000 [2:10:41<70:46:19,  1.31s/it, loss=0.0227, lr=3.04e-05, step=6084]Training:   3%|‚ñé         | 6085/200000 [2:10:41<70:46:19,  1.31s/it, loss=0.0126, lr=3.04e-05, step=6085]Training:   3%|‚ñé         | 6086/200000 [2:10:42<66:53:37,  1.24s/it, loss=0.0126, lr=3.04e-05, step=6085]Training:   3%|‚ñé         | 6086/200000 [2:10:42<66:53:37,  1.24s/it, loss=0.0180, lr=3.04e-05, step=6086]Training:   3%|‚ñé         | 6087/200000 [2:10:43<69:26:57,  1.29s/it, loss=0.0180, lr=3.04e-05, step=6086]Training:   3%|‚ñé         | 6087/200000 [2:10:43<69:26:57,  1.29s/it, loss=0.0277, lr=3.04e-05, step=6087]Training:   3%|‚ñé         | 6088/200000 [2:10:44<70:47:11,  1.31s/it, loss=0.0277, lr=3.04e-05, step=6087]Training:   3%|‚ñé         | 6088/200000 [2:10:44<70:47:11,  1.31s/it, loss=0.0207, lr=3.04e-05, step=6088]Training:   3%|‚ñé         | 6089/200000 [2:10:46<71:29:25,  1.33s/it, loss=0.0207, lr=3.04e-05, step=6088]Training:   3%|‚ñé         | 6089/200000 [2:10:46<71:29:25,  1.33s/it, loss=0.0151, lr=3.04e-05, step=6089]Training:   3%|‚ñé         | 6090/200000 [2:10:47<67:27:25,  1.25s/it, loss=0.0151, lr=3.04e-05, step=6089]Training:   3%|‚ñé         | 6090/200000 [2:10:47<67:27:25,  1.25s/it, loss=0.0185, lr=3.04e-05, step=6090]Training:   3%|‚ñé         | 6091/200000 [2:10:48<71:20:19,  1.32s/it, loss=0.0185, lr=3.04e-05, step=6090]Training:   3%|‚ñé         | 6091/200000 [2:10:48<71:20:19,  1.32s/it, loss=0.0241, lr=3.05e-05, step=6091]Training:   3%|‚ñé         | 6092/200000 [2:10:50<74:20:29,  1.38s/it, loss=0.0241, lr=3.05e-05, step=6091]Training:   3%|‚ñé         | 6092/200000 [2:10:50<74:20:29,  1.38s/it, loss=0.0391, lr=3.05e-05, step=6092]Training:   3%|‚ñé         | 6093/200000 [2:10:51<69:25:11,  1.29s/it, loss=0.0391, lr=3.05e-05, step=6092]Training:   3%|‚ñé         | 6093/200000 [2:10:51<69:25:11,  1.29s/it, loss=0.0371, lr=3.05e-05, step=6093]Training:   3%|‚ñé         | 6094/200000 [2:10:52<65:59:15,  1.23s/it, loss=0.0371, lr=3.05e-05, step=6093]Training:   3%|‚ñé         | 6094/200000 [2:10:52<65:59:15,  1.23s/it, loss=0.0242, lr=3.05e-05, step=6094]Training:   3%|‚ñé         | 6095/200000 [2:10:53<69:00:06,  1.28s/it, loss=0.0242, lr=3.05e-05, step=6094]Training:   3%|‚ñé         | 6095/200000 [2:10:53<69:00:06,  1.28s/it, loss=0.0274, lr=3.05e-05, step=6095]Training:   3%|‚ñé         | 6096/200000 [2:10:54<65:40:50,  1.22s/it, loss=0.0274, lr=3.05e-05, step=6095]Training:   3%|‚ñé         | 6096/200000 [2:10:54<65:40:50,  1.22s/it, loss=0.0254, lr=3.05e-05, step=6096]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6097/200000 [2:10:56<67:24:30,  1.25s/it, loss=0.0254, lr=3.05e-05, step=6096]Training:   3%|‚ñé         | 6097/200000 [2:10:56<67:24:30,  1.25s/it, loss=0.0224, lr=3.05e-05, step=6097]Training:   3%|‚ñé         | 6098/200000 [2:10:57<64:34:53,  1.20s/it, loss=0.0224, lr=3.05e-05, step=6097]Training:   3%|‚ñé         | 6098/200000 [2:10:57<64:34:53,  1.20s/it, loss=0.0134, lr=3.05e-05, step=6098]Training:   3%|‚ñé         | 6099/200000 [2:10:58<68:36:07,  1.27s/it, loss=0.0134, lr=3.05e-05, step=6098]Training:   3%|‚ñé         | 6099/200000 [2:10:58<68:36:07,  1.27s/it, loss=0.0092, lr=3.05e-05, step=6099]Training:   3%|‚ñé         | 6100/200000 [2:11:00<70:54:37,  1.32s/it, loss=0.0092, lr=3.05e-05, step=6099]Training:   3%|‚ñé         | 6100/200000 [2:11:00<70:54:37,  1.32s/it, loss=0.0171, lr=3.05e-05, step=6100]01:04:14.980 [I] step=6100 loss=0.0227 lr=3.03e-05 grad_norm=0.43 time=128.9s                     (701675:train_pytorch.py:582)
Training:   3%|‚ñé         | 6101/200000 [2:11:01<72:55:08,  1.35s/it, loss=0.0171, lr=3.05e-05, step=6100]Training:   3%|‚ñé         | 6101/200000 [2:11:01<72:55:08,  1.35s/it, loss=0.0220, lr=3.05e-05, step=6101]Training:   3%|‚ñé         | 6102/200000 [2:11:03<74:05:24,  1.38s/it, loss=0.0220, lr=3.05e-05, step=6101]Training:   3%|‚ñé         | 6102/200000 [2:11:03<74:05:24,  1.38s/it, loss=0.0364, lr=3.05e-05, step=6102]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6103/200000 [2:11:04<69:16:38,  1.29s/it, loss=0.0364, lr=3.05e-05, step=6102]Training:   3%|‚ñé         | 6103/200000 [2:11:04<69:16:38,  1.29s/it, loss=0.0132, lr=3.05e-05, step=6103]Training:   3%|‚ñé         | 6104/200000 [2:11:05<65:52:53,  1.22s/it, loss=0.0132, lr=3.05e-05, step=6103]Training:   3%|‚ñé         | 6104/200000 [2:11:05<65:52:53,  1.22s/it, loss=0.0233, lr=3.05e-05, step=6104]Training:   3%|‚ñé         | 6105/200000 [2:11:06<68:25:31,  1.27s/it, loss=0.0233, lr=3.05e-05, step=6104]Training:   3%|‚ñé         | 6105/200000 [2:11:06<68:25:31,  1.27s/it, loss=0.0327, lr=3.05e-05, step=6105]Training:   3%|‚ñé         | 6106/200000 [2:11:07<69:01:53,  1.28s/it, loss=0.0327, lr=3.05e-05, step=6105]Training:   3%|‚ñé         | 6106/200000 [2:11:07<69:01:53,  1.28s/it, loss=0.0114, lr=3.05e-05, step=6106]Training:   3%|‚ñé         | 6107/200000 [2:11:09<65:39:42,  1.22s/it, loss=0.0114, lr=3.05e-05, step=6106]Training:   3%|‚ñé         | 6107/200000 [2:11:09<65:39:42,  1.22s/it, loss=0.0183, lr=3.05e-05, step=6107]Training:   3%|‚ñé         | 6108/200000 [2:11:10<67:26:37,  1.25s/it, loss=0.0183, lr=3.05e-05, step=6107]Training:   3%|‚ñé         | 6108/200000 [2:11:10<67:26:37,  1.25s/it, loss=0.0149, lr=3.05e-05, step=6108]Training:   3%|‚ñé         | 6109/200000 [2:11:11<69:06:50,  1.28s/it, loss=0.0149, lr=3.05e-05, step=6108]Training:   3%|‚ñé         | 6109/200000 [2:11:11<69:06:50,  1.28s/it, loss=0.0166, lr=3.05e-05, step=6109]Training:   3%|‚ñé         | 6110/200000 [2:11:13<70:05:27,  1.30s/it, loss=0.0166, lr=3.05e-05, step=6109]Training:   3%|‚ñé         | 6110/200000 [2:11:13<70:05:27,  1.30s/it, loss=0.0236, lr=3.05e-05, step=6110]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6111/200000 [2:11:14<66:26:48,  1.23s/it, loss=0.0236, lr=3.05e-05, step=6110]Training:   3%|‚ñé         | 6111/200000 [2:11:14<66:26:48,  1.23s/it, loss=0.0220, lr=3.06e-05, step=6111]Training:   3%|‚ñé         | 6112/200000 [2:11:15<70:16:23,  1.30s/it, loss=0.0220, lr=3.06e-05, step=6111]Training:   3%|‚ñé         | 6112/200000 [2:11:15<70:16:23,  1.30s/it, loss=0.0225, lr=3.06e-05, step=6112]Training:   3%|‚ñé         | 6113/200000 [2:11:17<72:22:57,  1.34s/it, loss=0.0225, lr=3.06e-05, step=6112]Training:   3%|‚ñé         | 6113/200000 [2:11:17<72:22:57,  1.34s/it, loss=0.0106, lr=3.06e-05, step=6113]Training:   3%|‚ñé         | 6114/200000 [2:11:18<68:04:53,  1.26s/it, loss=0.0106, lr=3.06e-05, step=6113]Training:   3%|‚ñé         | 6114/200000 [2:11:18<68:04:53,  1.26s/it, loss=0.0134, lr=3.06e-05, step=6114]Training:   3%|‚ñé         | 6115/200000 [2:11:19<65:01:45,  1.21s/it, loss=0.0134, lr=3.06e-05, step=6114]Training:   3%|‚ñé         | 6115/200000 [2:11:19<65:01:45,  1.21s/it, loss=0.0254, lr=3.06e-05, step=6115]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6116/200000 [2:11:20<68:09:55,  1.27s/it, loss=0.0254, lr=3.06e-05, step=6115]Training:   3%|‚ñé         | 6116/200000 [2:11:20<68:09:55,  1.27s/it, loss=0.0254, lr=3.06e-05, step=6116]Training:   3%|‚ñé         | 6117/200000 [2:11:22<71:27:29,  1.33s/it, loss=0.0254, lr=3.06e-05, step=6116]Training:   3%|‚ñé         | 6117/200000 [2:11:22<71:27:29,  1.33s/it, loss=0.0229, lr=3.06e-05, step=6117]Training:   3%|‚ñé         | 6118/200000 [2:11:23<67:23:59,  1.25s/it, loss=0.0229, lr=3.06e-05, step=6117]Training:   3%|‚ñé         | 6118/200000 [2:11:23<67:23:59,  1.25s/it, loss=0.0098, lr=3.06e-05, step=6118]Training:   3%|‚ñé         | 6119/200000 [2:11:24<70:08:28,  1.30s/it, loss=0.0098, lr=3.06e-05, step=6118]Training:   3%|‚ñé         | 6119/200000 [2:11:24<70:08:28,  1.30s/it, loss=0.0171, lr=3.06e-05, step=6119]Training:   3%|‚ñé         | 6120/200000 [2:11:25<71:36:36,  1.33s/it, loss=0.0171, lr=3.06e-05, step=6119]Training:   3%|‚ñé         | 6120/200000 [2:11:25<71:36:36,  1.33s/it, loss=0.0305, lr=3.06e-05, step=6120]Training:   3%|‚ñé         | 6121/200000 [2:11:27<71:49:45,  1.33s/it, loss=0.0305, lr=3.06e-05, step=6120]Training:   3%|‚ñé         | 6121/200000 [2:11:27<71:49:45,  1.33s/it, loss=0.0237, lr=3.06e-05, step=6121]Training:   3%|‚ñé         | 6122/200000 [2:11:28<67:39:24,  1.26s/it, loss=0.0237, lr=3.06e-05, step=6121]Training:   3%|‚ñé         | 6122/200000 [2:11:28<67:39:24,  1.26s/it, loss=0.0121, lr=3.06e-05, step=6122]Training:   3%|‚ñé         | 6123/200000 [2:11:29<71:32:39,  1.33s/it, loss=0.0121, lr=3.06e-05, step=6122]Training:   3%|‚ñé         | 6123/200000 [2:11:29<71:32:39,  1.33s/it, loss=0.0167, lr=3.06e-05, step=6123]Training:   3%|‚ñé         | 6124/200000 [2:11:31<74:43:42,  1.39s/it, loss=0.0167, lr=3.06e-05, step=6123]Training:   3%|‚ñé         | 6124/200000 [2:11:31<74:43:42,  1.39s/it, loss=0.0140, lr=3.06e-05, step=6124]Training:   3%|‚ñé         | 6125/200000 [2:11:32<69:41:30,  1.29s/it, loss=0.0140, lr=3.06e-05, step=6124]Training:   3%|‚ñé         | 6125/200000 [2:11:32<69:41:30,  1.29s/it, loss=0.0122, lr=3.06e-05, step=6125]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6126/200000 [2:11:33<66:11:18,  1.23s/it, loss=0.0122, lr=3.06e-05, step=6125]Training:   3%|‚ñé         | 6126/200000 [2:11:33<66:11:18,  1.23s/it, loss=0.0325, lr=3.06e-05, step=6126]Training:   3%|‚ñé         | 6127/200000 [2:11:35<70:11:03,  1.30s/it, loss=0.0325, lr=3.06e-05, step=6126]Training:   3%|‚ñé         | 6127/200000 [2:11:35<70:11:03,  1.30s/it, loss=0.0154, lr=3.06e-05, step=6127]Training:   3%|‚ñé         | 6128/200000 [2:11:36<66:32:01,  1.24s/it, loss=0.0154, lr=3.06e-05, step=6127]Training:   3%|‚ñé         | 6128/200000 [2:11:36<66:32:01,  1.24s/it, loss=0.0130, lr=3.06e-05, step=6128]Training:   3%|‚ñé         | 6129/200000 [2:11:37<67:51:12,  1.26s/it, loss=0.0130, lr=3.06e-05, step=6128]Training:   3%|‚ñé         | 6129/200000 [2:11:37<67:51:12,  1.26s/it, loss=0.0118, lr=3.06e-05, step=6129]Training:   3%|‚ñé         | 6130/200000 [2:11:38<64:54:14,  1.21s/it, loss=0.0118, lr=3.06e-05, step=6129]Training:   3%|‚ñé         | 6130/200000 [2:11:38<64:54:14,  1.21s/it, loss=0.0091, lr=3.06e-05, step=6130]Training:   3%|‚ñé         | 6131/200000 [2:11:39<69:04:07,  1.28s/it, loss=0.0091, lr=3.06e-05, step=6130]Training:   3%|‚ñé         | 6131/200000 [2:11:39<69:04:07,  1.28s/it, loss=0.0163, lr=3.07e-05, step=6131]Training:   3%|‚ñé         | 6132/200000 [2:11:41<71:30:50,  1.33s/it, loss=0.0163, lr=3.07e-05, step=6131]Training:   3%|‚ñé         | 6132/200000 [2:11:41<71:30:50,  1.33s/it, loss=0.0184, lr=3.07e-05, step=6132]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6133/200000 [2:11:42<72:50:36,  1.35s/it, loss=0.0184, lr=3.07e-05, step=6132]Training:   3%|‚ñé         | 6133/200000 [2:11:42<72:50:36,  1.35s/it, loss=0.0121, lr=3.07e-05, step=6133]Training:   3%|‚ñé         | 6134/200000 [2:11:44<74:12:20,  1.38s/it, loss=0.0121, lr=3.07e-05, step=6133]Training:   3%|‚ñé         | 6134/200000 [2:11:44<74:12:20,  1.38s/it, loss=0.0345, lr=3.07e-05, step=6134]Training:   3%|‚ñé         | 6135/200000 [2:11:45<69:19:39,  1.29s/it, loss=0.0345, lr=3.07e-05, step=6134]Training:   3%|‚ñé         | 6135/200000 [2:11:45<69:19:39,  1.29s/it, loss=0.0412, lr=3.07e-05, step=6135]Training:   3%|‚ñé         | 6136/200000 [2:11:46<65:57:41,  1.22s/it, loss=0.0412, lr=3.07e-05, step=6135]Training:   3%|‚ñé         | 6136/200000 [2:11:46<65:57:41,  1.22s/it, loss=0.0259, lr=3.07e-05, step=6136]Training:   3%|‚ñé         | 6137/200000 [2:11:47<67:14:41,  1.25s/it, loss=0.0259, lr=3.07e-05, step=6136]Training:   3%|‚ñé         | 6137/200000 [2:11:47<67:14:41,  1.25s/it, loss=0.0192, lr=3.07e-05, step=6137]Training:   3%|‚ñé         | 6138/200000 [2:11:49<69:36:19,  1.29s/it, loss=0.0192, lr=3.07e-05, step=6137]Training:   3%|‚ñé         | 6138/200000 [2:11:49<69:36:19,  1.29s/it, loss=0.0154, lr=3.07e-05, step=6138]Training:   3%|‚ñé         | 6139/200000 [2:11:50<66:06:24,  1.23s/it, loss=0.0154, lr=3.07e-05, step=6138]Training:   3%|‚ñé         | 6139/200000 [2:11:50<66:06:24,  1.23s/it, loss=0.0139, lr=3.07e-05, step=6139]Training:   3%|‚ñé         | 6140/200000 [2:11:51<68:28:54,  1.27s/it, loss=0.0139, lr=3.07e-05, step=6139]Training:   3%|‚ñé         | 6140/200000 [2:11:51<68:28:54,  1.27s/it, loss=0.0150, lr=3.07e-05, step=6140]slurmstepd: error: *** JOB 342871 ON dgx-40 CANCELLED AT 2026-01-31T01:05:05 DUE TO PREEMPTION ***
Training:   3%|‚ñé         | 6141/200000 [2:11:52<70:01:23,  1.30s/it, loss=0.0150, lr=3.07e-05, step=6140]Training:   3%|‚ñé         | 6141/200000 [2:11:52<70:01:23,  1.30s/it, loss=0.0129, lr=3.07e-05, step=6141]Training:   3%|‚ñé         | 6142/200000 [2:11:54<70:58:03,  1.32s/it, loss=0.0129, lr=3.07e-05, step=6141]Training:   3%|‚ñé         | 6142/200000 [2:11:54<70:58:03,  1.32s/it, loss=0.0256, lr=3.07e-05, step=6142]Training:   3%|‚ñé         | 6143/200000 [2:11:55<67:02:50,  1.25s/it, loss=0.0256, lr=3.07e-05, step=6142]Training:   3%|‚ñé         | 6143/200000 [2:11:55<67:02:50,  1.25s/it, loss=0.0601, lr=3.07e-05, step=6143]Training:   3%|‚ñé         | 6144/200000 [2:11:56<71:32:33,  1.33s/it, loss=0.0601, lr=3.07e-05, step=6143]Training:   3%|‚ñé         | 6144/200000 [2:11:56<71:32:33,  1.33s/it, loss=0.0118, lr=3.07e-05, step=6144]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6145/200000 [2:11:58<74:19:45,  1.38s/it, loss=0.0118, lr=3.07e-05, step=6144]Training:   3%|‚ñé         | 6145/200000 [2:11:58<74:19:45,  1.38s/it, loss=0.0146, lr=3.07e-05, step=6145]Training:   3%|‚ñé         | 6146/200000 [2:11:59<69:24:17,  1.29s/it, loss=0.0146, lr=3.07e-05, step=6145]Training:   3%|‚ñé         | 6146/200000 [2:11:59<69:24:17,  1.29s/it, loss=0.0175, lr=3.07e-05, step=6146]Training:   3%|‚ñé         | 6147/200000 [2:12:00<65:58:36,  1.23s/it, loss=0.0175, lr=3.07e-05, step=6146]Training:   3%|‚ñé         | 6147/200000 [2:12:00<65:58:36,  1.23s/it, loss=0.0101, lr=3.07e-05, step=6147]Training:   3%|‚ñé         | 6148/200000 [2:12:01<69:49:24,  1.30s/it, loss=0.0101, lr=3.07e-05, step=6147]Training:   3%|‚ñé         | 6148/200000 [2:12:01<69:49:24,  1.30s/it, loss=0.0185, lr=3.07e-05, step=6148]Training:   3%|‚ñé         | 6149/200000 [2:12:03<66:15:28,  1.23s/it, loss=0.0185, lr=3.07e-05, step=6148]Training:   3%|‚ñé         | 6149/200000 [2:12:03<66:15:28,  1.23s/it, loss=0.0119, lr=3.07e-05, step=6149]Training:   3%|‚ñé         | 6150/200000 [2:12:04<67:43:15,  1.26s/it, loss=0.0119, lr=3.07e-05, step=6149]Training:   3%|‚ñé         | 6150/200000 [2:12:04<67:43:15,  1.26s/it, loss=0.0168, lr=3.07e-05, step=6150]Training:   3%|‚ñé         | 6151/200000 [2:12:05<64:47:34,  1.20s/it, loss=0.0168, lr=3.07e-05, step=6150]Training:   3%|‚ñé         | 6151/200000 [2:12:05<64:47:34,  1.20s/it, loss=0.0162, lr=3.08e-05, step=6151]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6152/200000 [2:12:06<68:50:02,  1.28s/it, loss=0.0162, lr=3.08e-05, step=6151]Training:   3%|‚ñé         | 6152/200000 [2:12:06<68:50:02,  1.28s/it, loss=0.0176, lr=3.08e-05, step=6152]WARNING:root:Token length (52) exceeds max length (48), truncating. Consider increasing the `max_token_len` in your model config if this happens frequently.
Training:   3%|‚ñé         | 6153/200000 [2:12:08<70:57:38,  1.32s/it, loss=0.0176, lr=3.08e-05, step=6152]Training:   3%|‚ñé         | 6153/200000 [2:12:08<70:57:38,  1.32s/it, loss=0.0122, lr=3.08e-05, step=6153]Training:   3%|‚ñé         | 6154/200000 [2:12:09<72:36:29,  1.35s/it, loss=0.0122, lr=3.08e-05, step=6153]Training:   3%|‚ñé         | 6154/200000 [2:12:09<72:36:29,  1.35s/it, loss=0.0132, lr=3.08e-05, step=6154]Training:   3%|‚ñé         | 6155/200000 [2:12:11<74:01:30,  1.37s/it, loss=0.0132, lr=3.08e-05, step=6154]Training:   3%|‚ñé         | 6155/200000 [2:12:11<74:01:30,  1.37s/it, loss=0.0361, lr=3.08e-05, step=6155]Training:   3%|‚ñé         | 6156/200000 [2:12:12<69:12:23,  1.29s/it, loss=0.0361, lr=3.08e-05, step=6155]Training:   3%|‚ñé         | 6156/200000 [2:12:12<69:12:23,  1.29s/it, loss=0.0248, lr=3.08e-05, step=6156]Training:   3%|‚ñé         | 6157/200000 [2:12:13<65:48:41,  1.22s/it, loss=0.0248, lr=3.08e-05, step=6156]Training:   3%|‚ñé         | 6157/200000 [2:12:13<65:48:41,  1.22s/it, loss=0.0171, lr=3.08e-05, step=6157]Training:   3%|‚ñé         | 6158/200000 [2:12:14<67:38:57,  1.26s/it, loss=0.0171, lr=3.08e-05, step=6157]Training:   3%|‚ñé         | 6158/200000 [2:12:14<67:38:57,  1.26s/it, loss=0.0329, lr=3.08e-05, step=6158]Training:   3%|‚ñé         | 6159/200000 [2:12:15<69:07:11,  1.28s/it, loss=0.0329, lr=3.08e-05, step=6158]Training:   3%|‚ñé         | 6159/200000 [2:12:15<69:07:11,  1.28s/it, loss=0.0241, lr=3.08e-05, step=6159]Training:   3%|‚ñé         | 6160/200000 [2:12:17<65:43:58,  1.22s/it, loss=0.0241, lr=3.08e-05, step=6159]Training:   3%|‚ñé         | 6160/200000 [2:12:17<65:43:58,  1.22s/it, loss=0.0171, lr=3.08e-05, step=6160]Training:   3%|‚ñé         | 6161/200000 [2:12:18<67:37:15,  1.26s/it, loss=0.0171, lr=3.08e-05, step=6160]Training:   3%|‚ñé         | 6161/200000 [2:12:18<67:37:15,  1.26s/it, loss=0.0149, lr=3.08e-05, step=6161]Training:   3%|‚ñé         | 6162/200000 [2:12:19<69:13:43,  1.29s/it, loss=0.0149, lr=3.08e-05, step=6161]Training:   3%|‚ñé         | 6162/200000 [2:12:19<69:13:43,  1.29s/it, loss=0.0187, lr=3.08e-05, step=6162]Training:   3%|‚ñé         | 6163/200000 [2:12:21<69:01:41,  1.28s/it, loss=0.0187, lr=3.08e-05, step=6162]Training:   3%|‚ñé         | 6163/200000 [2:12:21<69:01:41,  1.28s/it, loss=0.0184, lr=3.08e-05, step=6163]Training:   3%|‚ñé         | 6164/200000 [2:12:22<65:41:48,  1.22s/it, loss=0.0184, lr=3.08e-05, step=6163]Training:   3%|‚ñé         | 6164/200000 [2:12:22<65:41:48,  1.22s/it, loss=0.0139, lr=3.08e-05, step=6164]Sat Jan 31 16:20:18 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   22C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   21C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   22C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   22C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   22C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   21C    P0             64W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   24C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Sun Feb  1 22:40:17 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   22C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   35C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   22C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   23C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   23C    P0             76W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Mon Feb  2 19:30:00 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   24C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   29C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   47C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   25C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   54C    P0             73W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   30C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   30C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Mon Feb  2 22:07:20 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   23C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   26C    P0             71W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   21C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   25C    P0             73W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   25C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Tue Feb  3 16:59:29 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   22C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   22C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   22C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   36C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   21C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   22C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   24C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Tue Feb  3 19:11:52 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   22C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   24C    P0            104W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   24C    P0            102W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   24C    P0            117W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   48C    P0            126W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   23C    P0            116W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   24C    P0             96W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   25C    P0             99W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Wed Feb  4 10:58:17 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H800                    On  | 00000000:1B:00.0 Off |                    0 |
| N/A   19C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H800                    On  | 00000000:43:00.0 Off |                    0 |
| N/A   19C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H800                    On  | 00000000:52:00.0 Off |                    0 |
| N/A   21C    P0              68W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H800                    On  | 00000000:61:00.0 Off |                    0 |
| N/A   19C    P0              67W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H800                    On  | 00000000:9D:00.0 Off |                    0 |
| N/A   19C    P0              66W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H800                    On  | 00000000:C3:00.0 Off |                    0 |
| N/A   19C    P0              65W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H800                    On  | 00000000:D1:00.0 Off |                    0 |
| N/A   19C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H800                    On  | 00000000:DF:00.0 Off |                    0 |
| N/A   21C    P0              67W / 700W |      4MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
